{"papers":[{"url":"https://www.semanticscholar.org/paper/7cf64070fd3d7e53d80f260c10e6bd7018d580e1","title":"IdealGPT: Iteratively Decomposing Vision and Language Reasoning via Large Language Models","venue":"Conference on Empirical Methods in Natural Language Processing","year":2023,"referenceCount":51,"citationCount":15,"influentialCitationCount":3,"publicationDate":"24/05/2023","authors":"Haoxuan You,Rui Sun,Zhecan Wang,Long Chen,Gengyu Wang,Hammad A. Ayyubi,Kai-Wei Chang,Shih-Fu Chang","id":"7cf64070fd3d7e53d80f260c10e6bd7018d580e1","summary":"The IdealGPT framework is presented, a framework that iteratively decomposes VL reasoning using large language models (LLMs) and outperforms the best existing GPT-4-like models by an absolute 10% on VCR and 15% on SNLI-VE.","score":5},{"url":"https://www.semanticscholar.org/paper/fd755dc7b5b206c17fd953db04e1c888d45b6e4e","title":"LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark","venue":"Neural Information Processing Systems","year":2023,"referenceCount":99,"citationCount":42,"influentialCitationCount":8,"publicationDate":"11/06/2023","authors":"Zhen-fei Yin,Jiong Wang,Jianjian Cao,Zhelun Shi,Dingning Liu,Mukai Li,Lu Sheng,Lei Bai,Xiaoshui Huang,Zhiyong Wang,Wanli Ouyang,Jing Shao","id":"fd755dc7b5b206c17fd953db04e1c888d45b6e4e","summary":"This work extends the research of MLLMs to point clouds and presents the LAMM-Dataset and LAMm-Benchmark for 2D image and 3D point cloud understanding and establishes an extensible framework to facilitate the extension of M LLMs to additional modalities.","score":5},{"url":"https://www.semanticscholar.org/paper/966852963a88a28786b798c91b6662d6e501e590","title":"AssistGPT: A General Multi-modal Assistant that can Plan, Execute, Inspect, and Learn","venue":"arXiv.org","year":2023,"referenceCount":73,"citationCount":25,"influentialCitationCount":3,"publicationDate":"14/06/2023","authors":"Difei Gao,Lei Ji,Luowei Zhou,Kevin Lin,Joya Chen,Zihan Fan,Mike Zheng Shou","id":"966852963a88a28786b798c91b6662d6e501e590","summary":"A multi-modal AI assistant with an interleaved code and language reasoning approach called Plan, Execute, Inspect, and Learn (PEIL) to integrate LLMs with various tools, and a Learner is designed to enable the model to autonomously explore and discover the optimal solution.","score":5},{"url":"https://www.semanticscholar.org/paper/ca31b8584b6c022ef15ddfe994fe361e002b7729","title":"A Comprehensive Overview of Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":447,"citationCount":55,"influentialCitationCount":2,"publicationDate":"12/07/2023","authors":"Humza Naveed,Asad Ullah Khan,Shi Qiu,Muhammad Saqib,Saeed Anwar,Muhammad Usman,Nick Barnes,A. Mian","id":"ca31b8584b6c022ef15ddfe994fe361e002b7729","summary":"A self-contained comprehensive overview of the existing literature on a broad range of LLM-related concepts discusses relevant background concepts along with covering the advanced topics at the frontier of research in LLMs.","score":5},{"url":"https://www.semanticscholar.org/paper/f34b6b4f75fb4e6f2c5654ee13fb2479c6170b3a","title":"Kosmos-2.5: A Multimodal Literate Model","venue":"arXiv.org","year":2023,"referenceCount":84,"citationCount":12,"influentialCitationCount":0,"publicationDate":"20/09/2023","authors":"Tengchao Lv,Yupan Huang,Jingye Chen,Lei Cui,Shuming Ma,Ya-Chi Chang,Shaohan Huang,Wenhui Wang,Li Dong,Weiyao Luo,Shaoxiang Wu,Guoxin Wang,Cha Zhang,Furu Wei","id":"f34b6b4f75fb4e6f2c5654ee13fb2479c6170b3a","summary":"Kosmos-2.5 can be readily adapted for any text-intensive image understanding task with different prompts through supervised fine-tuning, making it a general-purpose tool for real-world applications involving text-rich images and paves the way for the future scaling of multimodal large language models.","score":5},{"url":"https://www.semanticscholar.org/paper/092245d86b77181c36f972b1b7a17a59cd989c4a","title":"Guiding Instruction-based Image Editing via Multimodal Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":67,"citationCount":9,"influentialCitationCount":0,"publicationDate":"29/09/2023","authors":"Tsu-Jui Fu,Wenze Hu,Xianzhi Du,William Yang Wang,Yinfei Yang,Zhe Gan","id":"092245d86b77181c36f972b1b7a17a59cd989c4a","summary":"This work investigates how MLLMs facilitate edit instructions and presents MLLM-Guided Image Editing (MGIE), which learns to derive expressive instructions and provides explicit guidance and can lead to a notable improvement in automatic metrics and human evaluation while maintaining competitive inference efficiency.","score":5},{"url":"https://www.semanticscholar.org/paper/107fb6eec2febbae12db29bf3e311aaf5680027c","title":"Video-LLaVA: Learning United Visual Representation by Alignment Before Projection","venue":"arXiv.org","year":2023,"referenceCount":54,"citationCount":15,"influentialCitationCount":1,"publicationDate":"16/11/2023","authors":"Bin Lin,Bin Zhu,Yang Ye,Munan Ning,Peng Jin,Li Yuan","id":"107fb6eec2febbae12db29bf3e311aaf5680027c","summary":"This work unify visual representation into the language feature space to advance the foundational LLM towards a unified LVLM, and establishes a simple but robust LVLM baseline, Video-LLaVA, which learns from a mixed dataset of images and videos, mutually enhancing each other.","score":5},{"url":"https://www.semanticscholar.org/paper/6d2ab31aa75468f5458b9d96192c3f4a28f55d73","title":"DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving","venue":"arXiv.org","year":2023,"referenceCount":80,"citationCount":6,"influentialCitationCount":2,"publicationDate":"14/12/2023","authors":"Wenhai Wang,Jiangwei Xie,ChuanYang Hu,Haoming Zou,Jianan Fan,Wenwen Tong,Yang Wen,Silei Wu,Hanming Deng,Zhiqi Li,Hao Tian,Lewei Lu,Xizhou Zhu,Xiaogang Wang,Yu Qiao,Jifeng Dai","id":"6d2ab31aa75468f5458b9d96192c3f4a28f55d73","summary":"DriveMLM is introduced, an LLM-based AD framework that can perform close-loop autonomous driving in realistic simulators and can plug-and-play in existing AD systems such as Apollo for close-loop driving.","score":5},{"url":"https://www.semanticscholar.org/paper/6a33e58ef961a3a0a5657518b2be86395eb7c8d0","title":"InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks","venue":"arXiv.org","year":2023,"referenceCount":185,"citationCount":7,"influentialCitationCount":3,"publicationDate":"21/12/2023","authors":"Zhe Chen,Jiannan Wu,Wenhai Wang,Weijie Su,Guo Chen,Sen Xing,Zhong Muyan,Qinglong Zhang,Xizhou Zhu,Lewei Lu,Bin Li,Ping Luo,Tong Lu,Yu Qiao,Jifeng Dai","id":"6a33e58ef961a3a0a5657518b2be86395eb7c8d0","summary":"A large-scale vision-language foundation model (InternVL), which scales up the vision foundation model to 6 billion parameters and progressively aligns it with the LLM, using web-scale image-text data from various sources is designed.","score":5},{"url":"https://www.semanticscholar.org/paper/6bdfffbf92d01c8b543088d40d46233610e469a8","title":"CLIP in Medical Imaging: A Comprehensive Survey","venue":"arXiv.org","year":2023,"referenceCount":217,"citationCount":3,"influentialCitationCount":0,"publicationDate":"12/12/2023","authors":"Zihao Zhao,Yuxiao Liu,Han Wu,Yonghao Li,Sheng Wang,L. Teng,Disheng Liu,Xiang Li,Zhiming Cui,Qian Wang,Dinggang Shen","id":"6bdfffbf92d01c8b543088d40d46233610e469a8","summary":"This survey offers an in-depth exploration of the CLIP paradigm within the domain of medical imaging, regarding both refined CLIP pre-training and CLIP-driven applications, and investigates the adaptation of CLIP pre-training in the medical domain.","score":5},{"url":"https://www.semanticscholar.org/paper/06091944b864d6dc473cab63321a95fb9c4067cc","title":"ChatCAD+: Towards a Universal and Reliable Interactive CAD using LLMs","venue":"arXiv.org","year":2023,"referenceCount":50,"citationCount":10,"influentialCitationCount":1,"publicationDate":"25/05/2023","authors":"Zihao Zhao,Sheng Wang,Jinchen Gu,Yitao Zhu,Lanzhuju Mei,Zixu Zhuang,Zhiming Cui,Qian Wang,Dinggang Shen","id":"06091944b864d6dc473cab63321a95fb9c4067cc","summary":"ChatCAD+, which is designed to be universal and reliable, is introduced, capable of handling medical images from diverse domains and leveraging up-to-date information from reputable medical websites to provide reliable medical advice.","score":5},{"url":"https://www.semanticscholar.org/paper/f4793adffd6f67ffcb93ccfc5672ab301b8a2b96","title":"PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering","venue":"arXiv.org","year":2023,"referenceCount":40,"citationCount":35,"influentialCitationCount":3,"publicationDate":"17/05/2023","authors":"Xiaoman Zhang,Chaoyi Wu,Ziheng Zhao,Weixiong Lin,Ya Zhang,Yanfeng Wang,Weidi Xie","id":"f4793adffd6f67ffcb93ccfc5672ab301b8a2b96","summary":"This paper proposes a generative-based model for medical visual understanding by aligning visual information from a pre-trained vision encoder with a large language model, and establishes a scalable pipeline to construct a large-scale medical visual question-answering dataset.","score":5},{"url":"https://www.semanticscholar.org/paper/baa1dc079d98ca76b0173c8d653fed759fd0a371","title":"A scoping review on multimodal deep learning in biomedical images and texts","venue":"Journal of Biomedical Informatics","year":2023,"referenceCount":147,"citationCount":5,"influentialCitationCount":0,"publicationDate":"14/07/2023","authors":"Zhaoyi Sun,Mingquan Lin,Qingqing Zhu,Qianqian Xie,Fei Wang,Zhiyong Lu,Yifan Peng","id":"baa1dc079d98ca76b0173c8d653fed759fd0a371","summary":"This study reviewed the current uses of multimodal deep learning on five tasks: report generation, Visual question answering, Cross-modal retrieval, computer-aided diagnosis, and Semantic segmentation, and highlighted the diverse applications and potential of MDL.","score":5},{"url":"https://www.semanticscholar.org/paper/d48fa3ed73817563130ef217d85011ce1fbe7470","title":"BESTMVQA: A Benchmark Evaluation System for Medical Visual Question Answering","venue":"arXiv.org","year":2023,"referenceCount":23,"citationCount":0,"influentialCitationCount":0,"publicationDate":"13/12/2023","authors":"Xiaojie Hong,Zixin Song,Liangzhi Li,Xiaoli Wang,Feiyan Liu","id":"d48fa3ed73817563130ef217d85011ce1fbe7470","summary":"A Benchmark Evaluation SysTem for Medical Visual Question Answering, denoted by BESTMVQA, is developed, which provides a useful tool for users to automatically build Med-V QA datasets, which helps overcoming the data insufficient problem.","score":5},{"url":"https://www.semanticscholar.org/paper/570079bbdd8758dfe865097e05719313c9c1301a","title":"LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model","venue":"arXiv.org","year":2023,"referenceCount":79,"citationCount":211,"influentialCitationCount":29,"publicationDate":"28/04/2023","authors":"Peng Gao,Jiaming Han,Renrui Zhang,Ziyi Lin,Shijie Geng,Aojun Zhou,W. Zhang,Pan Lu,Conghui He,Xiangyu Yue,Hongsheng Li,Y. Qiao","id":"570079bbdd8758dfe865097e05719313c9c1301a","summary":"This work augments LLaMA-Adapter by unlocking more learnable parameters and proposes an early fusion strategy to feed visual tokens only into the early LLM layers, contributing to better visual knowledge incorporation and achieves strong multi-modal reasoning with only a small-scale image-text and instruction dataset.","score":4},{"url":"https://www.semanticscholar.org/paper/d6d3604f369bb0415cbe814e43ca3131323b03e2","title":"Otter: A Multi-Modal Model with In-Context Instruction Tuning","venue":"arXiv.org","year":2023,"referenceCount":37,"citationCount":223,"influentialCitationCount":34,"publicationDate":"05/05/2023","authors":"Bo Li,Yuanhan Zhang,Liangyu Chen,Jinghao Wang,Jingkang Yang,Ziwei Liu","id":"d6d3604f369bb0415cbe814e43ca3131323b03e2","summary":"Otter, a multi-modal model based on OpenFlamingo (open-sourced version of DeepMind's Flamingo), trained on MIMIC-IT and showcasing improved instruction-following ability and in-context learning is introduced.","score":4},{"url":"https://www.semanticscholar.org/paper/d98536f24272e258b1d399074b64284d64786099","title":"AVIS: Autonomous Visual Information Seeking with Large Language Models","venue":"Neural Information Processing Systems","year":2023,"referenceCount":133,"citationCount":9,"influentialCitationCount":1,"publicationDate":"13/06/2023","authors":"Ziniu Hu,Ahmet Iscen,Chen Sun,Kai-Wei Chang,Yizhou Sun,David A. Ross,C. Schmid,A. Fathi","id":"d98536f24272e258b1d399074b64284d64786099","summary":"An autonomous information seeking visual question answering framework that leverages a Large Language Model to dynamically strategize the utilization of external tools and to investigate their outputs, thereby acquiring the indispensable knowledge needed to provide answers to the posed questions is proposed.","score":4},{"url":"https://www.semanticscholar.org/paper/ebddfdc5d845a788e8062eddbbf7a335737cb99b","title":"What Matters in Training a GPT4-Style Language Model with Multimodal Inputs?","venue":"arXiv.org","year":2023,"referenceCount":108,"citationCount":33,"influentialCitationCount":3,"publicationDate":"05/07/2023","authors":"Yan Zeng,Hanbo Zhang,Jiani Zheng,Jiangnan Xia,Guoqiang Wei,Yang Wei,Yuchen Zhang,Tao Kong","id":"ebddfdc5d845a788e8062eddbbf7a335737cb99b","summary":"Lynx is presented, which performs the most accurate multi-modal understanding while keeping the best multi- modal generation ability compared to existing open-sourced GPT4-style models.","score":4},{"url":"https://www.semanticscholar.org/paper/6bcc6ab9c28805d4067e99b2cdc7524550fe80e1","title":"PointLLM: Empowering Large Language Models to Understand Point Clouds","venue":"arXiv.org","year":2023,"referenceCount":72,"citationCount":31,"influentialCitationCount":4,"publicationDate":"31/08/2023","authors":"Runsen Xu,Xiaolong Wang,Tai Wang,Yilun Chen,Jiangmiao Pang,Dahua Lin","id":"6bcc6ab9c28805d4067e99b2cdc7524550fe80e1","summary":"Experimental results reveal PointLLM's superior performance over existing 2D and 3D baselines, with a notable achievement in human-evaluated object captioning tasks where it surpasses human annotators in over 50% of the samples.","score":4},{"url":"https://www.semanticscholar.org/paper/bee68767debbdc96d6f75947e544a8be98b869e3","title":"Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model: Explorations with GPT4-Vision and Beyond","venue":"arXiv.org","year":2023,"referenceCount":60,"citationCount":12,"influentialCitationCount":2,"publicationDate":"03/10/2023","authors":"Liang Chen,Yichi Zhang,Shuhuai Ren,Haozhe Zhao,Zefan Cai,Yuchi Wang,Tianyu Liu,Baobao Chang","id":"bee68767debbdc96d6f75947e544a8be98b869e3","summary":"This study introduces a new benchmark called PCA-EVAL, which evaluates embodied decision-making from the perspectives of Perception, Cognition, and Action and proposes HOLMES, a multi-agent cooperation framework that allows LLMs to leverage MLLMs and APIs to gather multimodal information for informed decision- making.","score":4},{"url":"https://www.semanticscholar.org/paper/807f336176070bd3f95b82a16f125ee99b7d2c80","title":"Woodpecker: Hallucination Correction for Multimodal Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":49,"citationCount":15,"influentialCitationCount":2,"publicationDate":"24/10/2023","authors":"Shukang Yin,Chaoyou Fu,Sirui Zhao,Tong Xu,Hao Wang,Dianbo Sui,Yunhang Shen,Ke Li,Xingguo Sun,Enhong Chen","id":"807f336176070bd3f95b82a16f125ee99b7d2c80","summary":"Implemented in a post-remedy manner, Woodpecker can easily serve different MLLMs, while being interpretable by accessing intermediate outputs of the five stages, and shows the huge potential of this new paradigm.","score":4},{"url":"https://www.semanticscholar.org/paper/5eea245cc12c55905d4df827d0c9776c5ddfa743","title":"Compositional Chain-of-Thought Prompting for Large Multimodal Models","venue":"arXiv.org","year":2023,"referenceCount":91,"citationCount":4,"influentialCitationCount":2,"publicationDate":"27/11/2023","authors":"Chancharik Mitra,Brandon Huang,Trevor Darrell,Roei Herzig","id":"5eea245cc12c55905d4df827d0c9776c5ddfa743","summary":"The proposed Compositional Chain-of-Thought (CCoT) approach not only improves LMM performance on several vision and language VL compositional benchmarks but also improves the performance of several popular LMMs on general multimodal benchmarks, without the need for fine-tuning or annotated ground-truth SGs.","score":4},{"url":"https://www.semanticscholar.org/paper/4f2a56102bcbf0fe79379c4c27daecbccfb35a26","title":"MLLM-Tool: A Multimodal Large Language Model For Tool Agent Learning","venue":"arXiv.org","year":2024,"referenceCount":44,"citationCount":3,"influentialCitationCount":0,"publicationDate":"19/01/2024","authors":"Chenyu Wang,Weixin Luo,Qianyu Chen,Haonan Mai,Jindi Guo,Sixun Dong,Xiaohua Xuan,Zhengxin Li,Lin Ma,Shenghua Gao","id":"4f2a56102bcbf0fe79379c4c27daecbccfb35a26","summary":"This paper proposes MLLM-Tool, a system incorporating open-source LLMs and multi-modal encoders so that the learnt LLMs can be conscious of multi-modal input instruction and then select the function-matched tool correctly.","score":4},{"url":"https://www.semanticscholar.org/paper/5e7274bcda47b704b6797bb14be8b7a61c047a61","title":"Uncertainty-Aware Evaluation for Vision-Language Models","venue":"","year":2024,"referenceCount":64,"citationCount":0,"influentialCitationCount":0,"publicationDate":"22/02/2024","authors":"Vasily Kostumov,Bulat Nutfullin,Oleg Pilipenko,Eugene Ilyushin","id":"5e7274bcda47b704b6797bb14be8b7a61c047a61","summary":"It is shown that models with the highest accuracy may also have the highest uncertainty, which confirms the importance of measuring it for VLMs, and a correlation between model uncertainty and its language model part is revealed.","score":4},{"url":"https://www.semanticscholar.org/paper/efc694164312006c543ef745611348ef64e68dda","title":"Towards Language Models That Can See: Computer Vision Through the LENS of Natural Language","venue":"arXiv.org","year":2023,"referenceCount":64,"citationCount":24,"influentialCitationCount":2,"publicationDate":"28/06/2023","authors":"William Berrios,Gautam Mittal,Tristan Thrush,Douwe Kiela,Amanpreet Singh","id":"efc694164312006c543ef745611348ef64e68dda","summary":"This work proposes LENS, a modular approach for tackling computer vision problems by leveraging the power of large language models (LLMs) with a language model to reason over outputs from a set of independent and highly descriptive vision modules that provide exhaustive information about an image.","score":4},{"url":"https://www.semanticscholar.org/paper/93886752191db25efd096a65af7b09df5c0a64e0","title":"Data-Centric Foundation Models in Computational Healthcare: A Survey","venue":"arXiv.org","year":2024,"referenceCount":316,"citationCount":3,"influentialCitationCount":0,"publicationDate":"04/01/2024","authors":"Yunkun Zhang,Jin Gao,Zheling Tan,Lingfeng Zhou,Kexin Ding,Mu Zhou,Shaoting Zhang,Dequan Wang","id":"93886752191db25efd096a65af7b09df5c0a64e0","summary":"A wide range of data-centric approaches in the FM era (from model pre-training to inference) towards improving the healthcare workflow are investigated and a promising outlook of FM-based analytics to enhance the performance of patient outcome and clinical workflow in the evolving landscape of healthcare and medicine is offered.","score":4},{"url":"https://www.semanticscholar.org/paper/20fcc01d12a50f1da2af71d85f0a269b3ba48b77","title":"LMEye: An Interactive Perception Network for Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":57,"citationCount":11,"influentialCitationCount":1,"publicationDate":"05/05/2023","authors":"Yunxin Li,Baotian Hu,Xinyu Chen,Lin Ma,M. Zhang","id":"20fcc01d12a50f1da2af71d85f0a269b3ba48b77","summary":"LMEye, a human-like eye with a play-and-plug interactive perception network, designed to enable dynamic interaction between LLMs and external vision information, is introduced, demonstrating that it significantly improves the zero-shot performance on various multimodal tasks compared to previous methods, with less parameters.","score":4},{"url":"https://www.semanticscholar.org/paper/8badb0587fef2ffc078b0cec549eb8ec96ed3ad4","title":"Self-Chained Image-Language Model for Video Localization and Question Answering","venue":"Neural Information Processing Systems","year":2023,"referenceCount":97,"citationCount":27,"influentialCitationCount":6,"publicationDate":"11/05/2023","authors":"Shoubin Yu,Jaemin Cho,Prateek Yadav,Mohit Bansal","id":"8badb0587fef2ffc078b0cec549eb8ec96ed3ad4","summary":"Self-Chained Video Localization-Answering (SeViLA), a novel framework that leverages a single image-language model (BLIP-2) to tackle both temporal keyframe localization and QA on videos, and achieves the state-of-the-art in both fine-tuning and zero-shot settings.","score":4},{"url":"https://www.semanticscholar.org/paper/659a12d71d8709c132ccd9ccd235f0024cae0239","title":"The All-Seeing Project: Towards Panoptic Visual Recognition and Understanding of the Open World","venue":"arXiv.org","year":2023,"referenceCount":115,"citationCount":19,"influentialCitationCount":4,"publicationDate":"03/08/2023","authors":"Weiyun Wang,Min Shi,Qingyun Li,Wen Wang,Zhenhang Huang,Linjie Xing,Zhe Chen,Hao Li,Xizhou Zhu,Zhiguo Cao,Yushi Chen,Tong Lu,Jifeng Dai,Y. Qiao","id":"659a12d71d8709c132ccd9ccd235f0024cae0239","summary":"The All-Seeing model (ASM), a unified framework for panoptic visual recognition and understanding, is developed with open-ended language prompts and locations, which allows it to generalize to various vision and language tasks with remarkable zero-shot performance.","score":4},{"url":"https://www.semanticscholar.org/paper/b1721374889899950994f67029fe899de257c140","title":"A Foundational Multimodal Vision Language AI Assistant for Human Pathology","venue":"arXiv.org","year":2023,"referenceCount":126,"citationCount":1,"influentialCitationCount":0,"publicationDate":"13/12/2023","authors":"Ming Y. Lu,Bowen Chen,Drew F. K. Williamson,Richard J. Chen,Kenji Ikamura,Georg Gerber,Ivy Liang,L. Le,Tong Ding,Anil V. Parwani,Faisal Mahmood","id":"b1721374889899950994f67029fe899de257c140","summary":"PathChat is presented, a vision-language generalist AI assistant for human pathology using an in-house developed foundational vision encoder pretrained on 100 million histology images from over 100,000 patient cases and 1.18 million pathology image-caption pairs.","score":4},{"url":"https://www.semanticscholar.org/paper/a050c9b0c321839e4427ab9defa3463be7825ac4","title":"MM-LLMs: Recent Advances in MultiModal Large Language Models","venue":"arXiv.org","year":2024,"referenceCount":254,"citationCount":3,"influentialCitationCount":0,"publicationDate":"24/01/2024","authors":"Duzhen Zhang,Yahan Yu,Chenxing Li,Jiahua Dong,Dan Su,Chenhui Chu,Dong Yu","id":"a050c9b0c321839e4427ab9defa3463be7825ac4","summary":"A taxonomy encompassing $122$ MM-LLMs, each characterized by its specific formulations is introduced and a review of selected MM-LLMs on mainstream benchmarks and key training recipes to enhance the potency of MM-LLMs are summarized.","score":4},{"url":"https://www.semanticscholar.org/paper/31a7d8c4a5ab6bab522494b57270249105c8748e","title":"BiomedGPT: A Unified and Generalist Biomedical Generative Pre-trained Transformer for Vision, Language, and Multimodal Tasks","venue":"arXiv.org","year":2023,"referenceCount":147,"citationCount":38,"influentialCitationCount":2,"publicationDate":2023,"authors":"Kai Zhang,Jun Yu,Zhilin Yan,Yixin Liu,Eashan Adhikarla,S. Fu,Xun Chen,Chen Chen,Yuyin Zhou,Xiang Li,Lifang He,B. Davison,Quanzheng Li,Yong Chen,Hongfang Liu,Lichao Sun","id":"31a7d8c4a5ab6bab522494b57270249105c8748e","summary":"A unified and generalist BiomedGPT model, which leverages self-supervision on large and diverse datasets to accept multi-modal inputs and perform a range of downstream tasks, which presents a significant step forward in developing unified and generalist models for biomedicine.","score":4},{"url":"https://www.semanticscholar.org/paper/f22d71c7ce9720ba1f717a4f1181488200e78198","title":"LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day","venue":"Neural Information Processing Systems","year":2023,"referenceCount":46,"citationCount":98,"influentialCitationCount":15,"publicationDate":"01/06/2023","authors":"Chunyuan Li,Cliff Wong,Sheng Zhang,Naoto Usuyama,Haotian Liu,Jianwei Yang,Tristan Naumann,Hoifung Poon,Jianfeng Gao","id":"f22d71c7ce9720ba1f717a4f1181488200e78198","summary":"This paper proposes a cost-efficient approach for training a vision-language conversational assistant that can answer open-ended research questions of biomedical images, and releases instruction-following data and the LLaVA-Med model, which exhibits excellent multimodal conversational capability.","score":4},{"url":"https://www.semanticscholar.org/paper/df0ddb588a200d095743e9d26fc4a9318619766e","title":"Towards Generalist Foundation Model for Radiology","venue":"arXiv.org","year":2023,"referenceCount":59,"citationCount":25,"influentialCitationCount":2,"publicationDate":"04/08/2023","authors":"Chaoyi Wu,Xiaoman Zhang,Ya Zhang,Yanfeng Wang,Weidi Xie","id":"df0ddb588a200d095743e9d26fc4a9318619766e","summary":"This study constructs a large-scale Medical Multi-modal Dataset, MedMD, which consists of 16M 2D and 3D medical scans with high-quality text descriptions or reports across various data formats, modalities, and tasks, covering over 5000 distinct diseases, and proposes a new evaluation benchmark, RadBench, aiming to comprehensively assess the capability of foundation models in handling practical clinical problems.","score":4},{"url":"https://www.semanticscholar.org/paper/61cadcfa555cbef120df7c017ef02e87f19900b7","title":"Free Form Medical Visual Question Answering in Radiology","venue":"arXiv.org","year":2024,"referenceCount":48,"citationCount":0,"influentialCitationCount":0,"publicationDate":"23/01/2024","authors":"Abhishek Narayanan,Rushabh Musthyala,Rahul Sankar,Anirudh Prasad Nistala,P. Singh,Jacopo Cirrone","id":"61cadcfa555cbef120df7c017ef02e87f19900b7","summary":"This research delves into the effective representation of radiology images and the joint learning of multimodal representations, surpassing existing methods and innovatively augment the SLAKE dataset, enabling the model to respond to a more diverse array of questions.","score":4},{"url":"https://www.semanticscholar.org/paper/44ccf252018f71898d52d89539f17d77a4f8d548","title":"Chart Understanding with Large Language Model","venue":"","year":null,"referenceCount":39,"citationCount":0,"influentialCitationCount":0,"publicationDate":null,"authors":"Yaser James,Will Li,John Feng","id":"44ccf252018f71898d52d89539f17d77a4f8d548","summary":"A baseline multimodal model is introduced that integrates text and charts to enhance the chart comprehension capabilities of existing models, offering more pertinent insights and information related to the depicted charts.","score":3},{"url":"https://www.semanticscholar.org/paper/54a8b153ed04a872da878d695239bdc413dc782c","title":"InternGPT: Solving Vision-Centric Tasks by Interacting with Chatbots Beyond Language","venue":"arXiv.org","year":2023,"referenceCount":83,"citationCount":39,"influentialCitationCount":0,"publicationDate":"09/05/2023","authors":"Zhaoyang Liu,Yinan He,Wenhai Wang,Weiyun Wang,Yi Wang,Shoufa Chen,Qing-Long Zhang,Yang Yang,Qingyun Li,Jiashuo Yu,Kunchang Li,Zhe Chen,Xuecheng Yang,Xizhou Zhu,Yali Wang,Limin Wang,Ping Luo,Jifeng Dai,Yu Qiao","id":"54a8b153ed04a872da878d695239bdc413dc782c","summary":"By incorporating pointing instructions, the proposed iGPT significantly improves the efficiency of communication between users and chatbots, as well as the accuracy of chatbots in vision-centric tasks, especially in complicated visual scenarios where the number of objects is greater than 2.","score":3},{"url":"https://www.semanticscholar.org/paper/66d755730f5d08a6f4fcc5e81f24982ba389dca9","title":"LayoutGPT: Compositional Visual Planning and Generation with Large Language Models","venue":"Neural Information Processing Systems","year":2023,"referenceCount":65,"citationCount":27,"influentialCitationCount":4,"publicationDate":"24/05/2023","authors":"Weixi Feng,Wanrong Zhu,Tsu-Jui Fu,Varun Jampani,Arjun Reddy Akula,Xuehai He,Sugato Basu,X. Wang,William Yang Wang","id":"66d755730f5d08a6f4fcc5e81f24982ba389dca9","summary":"This work proposes LayoutGPT, a method to compose in-context visual demonstrations in style sheet language to enhance the visual planning skills of LLMs, and shows superior performance in converting challenging language concepts like numerical and spatial relations to layout arrangements for faithful text-to-image generation.","score":3},{"url":"https://www.semanticscholar.org/paper/af705d648b5b16daa3dcc593bc593f2574d76c07","title":"Grammar Prompting for Domain-Specific Language Generation with Large Language Models","venue":"Neural Information Processing Systems","year":2023,"referenceCount":102,"citationCount":6,"influentialCitationCount":0,"publicationDate":"30/05/2023","authors":"Bailin Wang,Zi Wang,Xuezhi Wang,Yuan Cao,R. Saurous,Yoon Kim","id":"af705d648b5b16daa3dcc593bc593f2574d76c07","summary":"Experiments demonstrate that grammar prompting can enable LLMs to perform competitively on a diverse set of DSL generation tasks, including semantic parsing, Overnight, GeoQuery, PDDL planning, and even molecule generation (SMILES).","score":3},{"url":"https://www.semanticscholar.org/paper/dd0612ce863f64b0f69d0d9f708c52e829f6f859","title":"TPTU: Large Language Model-based AI Agents for Task Planning and Tool Usage","venue":"","year":2023,"referenceCount":95,"citationCount":7,"influentialCitationCount":0,"publicationDate":"07/08/2023","authors":"Jingqing Ruan,Yihong Chen,Bin Zhang,Zhiwei Xu,Tianpeng Bao,Guoqing Du,Shiwei Shi,Hangyu Mao,Xingyu Zeng,Rui Zhao","id":"dd0612ce863f64b0f69d0d9f708c52e829f6f859","summary":"A structured framework tailored for LLM-based AI Agents is proposed and two distinct types of agents are designed to execute the inference process, which emphasizes the substantial potential of these models, while also identifying areas that need more investigation and improvement.","score":3},{"url":"https://www.semanticscholar.org/paper/c5db6c2726911b72d534f97bd4d1ed63f6431340","title":"Mobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual Perception","venue":"arXiv.org","year":2024,"referenceCount":25,"citationCount":3,"influentialCitationCount":0,"publicationDate":"29/01/2024","authors":"Junyang Wang,Haiyang Xu,Jiabo Ye,Mingshi Yan,Weizhou Shen,Ji Zhang,Fei Huang,Jitao Sang","id":"c5db6c2726911b72d534f97bd4d1ed63f6431340","summary":"Different from previous solutions that rely on XML files of Apps or mobile system metadata, Mobile-Agent allows for greater adaptability across diverse mobile operating environments in a vision-centric way, thereby eliminating the necessity for system-specific customizations.","score":3},{"url":"https://www.semanticscholar.org/paper/ca055cfb9d4d47124cc035c346f38577825fcacf","title":"Enhance Reasoning Ability of Visual-Language Models via Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":40,"citationCount":0,"influentialCitationCount":0,"publicationDate":"22/05/2023","authors":"Yueting Yang,Xintong Zhang,Wenjuan Han","id":"ca055cfb9d4d47124cc035c346f38577825fcacf","summary":"A method called TReE is proposed, which transfers the reasoning ability of a large language model to a visual language model in zero-shot scenarios, and contains three stages: observation, thinking, and re-thinking.","score":3},{"url":"https://www.semanticscholar.org/paper/50c1414fe41d0cb9db6f0933c9319aa124beac5d","title":"Contextual Object Detection with Multimodal Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":87,"citationCount":21,"influentialCitationCount":0,"publicationDate":"29/05/2023","authors":"Yuhang Zang,Wei Li,Jun Han,Kaiyang Zhou,Chen Change Loy","id":"50c1414fe41d0cb9db6f0933c9319aa124beac5d","summary":"The ContextDET is presented, a unified multimodal model that is capable of end-to-end differentiable modeling of visual-language contexts, so as to locate, identify, and associate visual objects with language inputs for human-AI interaction.","score":3},{"url":"https://www.semanticscholar.org/paper/79150cb420d15830c8d36f0e91eea1b02e177f0f","title":"Sticker820K: Empowering Interactive Retrieval with Stickers","venue":"arXiv.org","year":2023,"referenceCount":36,"citationCount":0,"influentialCitationCount":0,"publicationDate":"12/06/2023","authors":"Sijie Zhao,Yixiao Ge,Zhongang Qi,Lin Song,Xiaohan Ding,Zehua Xie,Ying Shan","id":"79150cb420d15830c8d36f0e91eea1b02e177f0f","summary":"The StickerCLIP is proposed as a benchmark model on the Sticker820K dataset, demonstrating strong superiority over the CLIP for the text-to-image retrieval task, and the recently popularized LLM is extended by means of prompt tuning, integrating its ability for sticker retrieval and allowing users to retrieve stickers through instructions.","score":3},{"url":"https://www.semanticscholar.org/paper/697e0add95e880bd42e00bef838181e105f91981","title":"MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":60,"citationCount":148,"influentialCitationCount":32,"publicationDate":"23/06/2023","authors":"Chaoyou Fu,Peixian Chen,Yunhang Shen,Yulei Qin,Mengdan Zhang,Xu Lin,Zhenyu Qiu,Wei Lin,Jinrui Yang,Xiawu Zheng,Ke Li,Xing Sun,Rongrong Ji","id":"697e0add95e880bd42e00bef838181e105f91981","summary":"This paper presents the first comprehensive MLLM Evaluation benchmark MME, which measures both perception and cognition abilities on a total of 14 subtasks and suggests that existing MLLMs still have a large room for improvement, but also reveals the potential directions for the subsequent model optimization.","score":3},{"url":"https://www.semanticscholar.org/paper/1fd31b74f5e1eeb67341982fd35a613c6fad10e0","title":"Link-Context Learning for Multimodal LLMs","venue":"arXiv.org","year":2023,"referenceCount":33,"citationCount":3,"influentialCitationCount":1,"publicationDate":"15/08/2023","authors":"Yan Tai,Weichen Fan,Zhao Zhang,Feng Zhu,Rui Zhao,Ziwei Liu","id":"1fd31b74f5e1eeb67341982fd35a613c6fad10e0","summary":"This work proposes link-context learning (LCL), which emphasizes \"reasoning from cause and effect\" to augment the learning capabilities of MLLMs and introduces the ISEKAI dataset, comprising exclusively of unseen generated image-label pairs designed for link- context learning.","score":3},{"url":"https://www.semanticscholar.org/paper/d39182113cd4176ead48027b4fc05fe06ec6aaca","title":"Language Models as Black-Box Optimizers for Vision-Language Models","venue":"arXiv.org","year":2023,"referenceCount":107,"citationCount":3,"influentialCitationCount":0,"publicationDate":"12/09/2023","authors":"Samuel Yu,Shihong Liu,Zhiqiu Lin,Deepak Pathak,Deva Ramanan","id":"d39182113cd4176ead48027b4fc05fe06ec6aaca","summary":"This work proposes employing chat-based LLMs to search for the best text prompt for VLMs and highlights the advantage of conversational feedback that incorporates both positive and negative prompts, suggesting that LLMs can utilize the implicit gradient direction in textual feedback for a more efficient search.","score":3},{"url":"https://www.semanticscholar.org/paper/7a7128e9696dfedc24bf432c4b4c3aafa5e95a1a","title":"An Empirical Study of Scaling Instruct-Tuned Large Multimodal Models","venue":"arXiv.org","year":2023,"referenceCount":22,"citationCount":7,"influentialCitationCount":0,"publicationDate":"18/09/2023","authors":"Yadong Lu,Chunyuan Li,Haotian Liu,Jianwei Yang,Jianfeng Gao,Yelong Shen","id":"7a7128e9696dfedc24bf432c4b4c3aafa5e95a1a","summary":"An empirical study of scaling LLaVA up to 33B and 65B/70B and performance of LoRA/QLoRA tuning of LMM are comparable to the performance of full-model fine-tuning, finding that scaling LMM consistently enhances model performance and improves language capabilities.","score":3},{"url":"https://www.semanticscholar.org/paper/6ae4705139494fcb6b790b6dd6c4225b40ee40f8","title":"GLaMM: Pixel Grounding Large Multimodal Model","venue":"arXiv.org","year":2023,"referenceCount":63,"citationCount":15,"influentialCitationCount":2,"publicationDate":"06/11/2023","authors":"H. Rasheed,Muhammad Maaz,Sahal Shaji Mullappilly,Abdelrahman M. Shaker,Salman H. Khan,Hisham Cholakkal,R. Anwer,Erix Xing,Ming-Hsuan Yang,F. Khan","id":"6ae4705139494fcb6b790b6dd6c4225b40ee40f8","summary":"This work presents Grounding LMM (GLaMM), the first model that can generate natural language responses seamlessly intertwined with corresponding object segmentation masks and is flexible enough to accept both textual and optional visual prompts (region of interest) as input.","score":3},{"url":"https://www.semanticscholar.org/paper/7b0a186b0140ee91fb13991c9c7187f3dc3b0670","title":"Visual Programming for Zero-shot Open-Vocabulary 3D Visual Grounding","venue":"arXiv.org","year":2023,"referenceCount":67,"citationCount":0,"influentialCitationCount":0,"publicationDate":"26/11/2023","authors":"Zhihao Yuan,Jinke Ren,Chun-Mei Feng,Hengshuang Zhao,Shuguang Cui,Zhen Li","id":"7b0a186b0140ee91fb13991c9c7187f3dc3b0670","summary":"This work proposes a novel visual programming approach for zero-shot open-vocabulary 3DVG, leveraging the capabilities of large language models (LLMs) and develops an innovative language-object correlation module to extend the scope of existing 3D object detectors into open- Vocabulary scenarios.","score":3},{"url":"https://www.semanticscholar.org/paper/769a924d0af014acec326f50c15c5d70d258a969","title":"LLMGA: Multimodal Large Language Model based Generation Assistant","venue":"arXiv.org","year":2023,"referenceCount":65,"citationCount":2,"influentialCitationCount":0,"publicationDate":"27/11/2023","authors":"Bin Xia,Shiyin Wang,Yingfan Tao,Yitong Wang,Jiaya Jia","id":"769a924d0af014acec326f50c15c5d70d258a969","summary":"This paper introduces a Multimodal Large Language Model-based Generation Assistant (LLMGA), leveraging the vast reservoir of knowledge and proficiency in reasoning, comprehension, and response inherent in Large Language Models to assist users in image generation and editing.","score":3},{"url":"https://www.semanticscholar.org/paper/cc9627c4475b9ad1c7e4105f3bceb1e7b59b7cab","title":"Zero-Shot Video Question Answering with Procedural Programs","venue":"arXiv.org","year":2023,"referenceCount":65,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/12/2023","authors":"Rohan Choudhury,Koichiro Niinuma,Kris M. Kitani,László A. Jeni","id":"cc9627c4475b9ad1c7e4105f3bceb1e7b59b7cab","summary":"This work proposes to answer zero-shot questions about videos by generating short procedural programs that derive a final answer from solving a sequence of visual subtasks, using a large language model to generate such programs from an input question and an API of visual modules in the prompt, then executes them to obtain the output.","score":3},{"url":"https://www.semanticscholar.org/paper/c672ec79f55cef8f7a32cd8dddfa981b893f1567","title":"V*: Guided Visual Search as a Core Mechanism in Multimodal LLMs","venue":"arXiv.org","year":2023,"referenceCount":60,"citationCount":8,"influentialCitationCount":1,"publicationDate":"21/12/2023","authors":"Penghao Wu,Saining Xie","id":"c672ec79f55cef8f7a32cd8dddfa981b893f1567","summary":"This work introduces V*, an LLM-guided visual search mechanism that employs the world knowledge in LLMs for efficient visual querying and results in a new MLLM meta-architecture, named Show, sEArch, and TelL (SEAL).","score":3},{"url":"https://www.semanticscholar.org/paper/5f58863dd6474d6f127be995b5871e7c60f2792f","title":"Video Understanding with Large Language Models: A Survey","venue":"arXiv.org","year":2023,"referenceCount":218,"citationCount":2,"influentialCitationCount":1,"publicationDate":"29/12/2023","authors":"Yunlong Tang,Jing Bi,Siting Xu,Luchuan Song,Susan Liang,Teng Wang,Daoan Zhang,Jie An,Jingyang Lin,Rongyi Zhu,A. Vosoughi,Chao Huang,Zeliang Zhang,Feng Zheng,Jianguo Zhang,Ping Luo,Jiebo Luo,Chenliang Xu","id":"5f58863dd6474d6f127be995b5871e7c60f2792f","summary":"The emergent capabilities of Vid-LLMs are surprisingly advanced, particularly their ability for open-ended spatial-temporal reasoning combined with commonsense knowledge, suggesting a promising path for future video understanding.","score":3},{"url":"https://www.semanticscholar.org/paper/002d2c4569d070a55fe69c25ebccad8e9ddae572","title":"Incorporating Visual Experts to Resolve the Information Loss in Multimodal Large Language Models","venue":"arXiv.org","year":2024,"referenceCount":58,"citationCount":0,"influentialCitationCount":0,"publicationDate":"06/01/2024","authors":"Xin He,Longhui Wei,Lingxi Xie,Qi Tian","id":"002d2c4569d070a55fe69c25ebccad8e9ddae572","summary":"A novel method is introduced that incorporates multi-task encoders and visual tools into the existing MLLMs training and inference pipeline, aiming to provide a more comprehensive and accurate summarization of visual inputs.","score":3},{"url":"https://www.semanticscholar.org/paper/0a8a776054a087118f4f9523994ef084b2b2469a","title":"Advancing Large Multi-modal Models with Explicit Chain-of-Reasoning and Visual Question Generation","venue":"arXiv.org","year":2024,"referenceCount":87,"citationCount":0,"influentialCitationCount":0,"publicationDate":"18/01/2024","authors":"Kohei Uehara,Nabarun Goswami,Hanqin Wang,Toshiaki Baba,Kohtaro Tanaka,Tomohiro Hashimoto,Kai Wang,Rei Ito,Takagi Naoya,Ryo Umagami,Yingyi Wen,Tanachai Anakewat,Tatsuya Harada","id":"0a8a776054a087118f4f9523994ef084b2b2469a","summary":"This paper designs an LMM, which has high capabilities on region awareness to address the intricate requirements of image-text alignment, and introduces a system that can ask a question to acquire necessary knowledge, thereby enhancing the robustness and explicability of the reasoning process.","score":3},{"url":"https://www.semanticscholar.org/paper/83ee82e62f2eae18cc3472120eb9004109895a31","title":"Delving into Multi-modal Multi-task Foundation Models for Road Scene Understanding: From Learning Paradigm Perspectives","venue":"arXiv.org","year":2024,"referenceCount":213,"citationCount":0,"influentialCitationCount":0,"publicationDate":"05/02/2024","authors":"Sheng Luo,Wei Chen,Wanxin Tian,Rui Liu,Luanxuan Hou,Xiubao Zhang,Haifeng Shen,Ruiqi Wu,Shuyi Geng,Yi Zhou,Ling Shao,Yi Yang,Bojun Gao,Qun Li,Guobin Wu","id":"83ee82e62f2eae18cc3472120eb9004109895a31","summary":"A systematic analysis of MM-VUFMs specifically designed for road scenes, referring to task-specific models, unified multi-modal models, unified multi-task models, and foundation model prompting techniques to highlight their advanced capabilities in diverse learning paradigms.","score":3},{"url":"https://www.semanticscholar.org/paper/28fbbf98bac1bb941162df553ca034d600cb59a6","title":"Rephrase, Augment, Reason: Visual Grounding of Questions for Vision-Language Models","venue":"arXiv.org","year":2023,"referenceCount":108,"citationCount":2,"influentialCitationCount":0,"publicationDate":"09/10/2023","authors":"Archiki Prasad,Elias Stengel-Eskin,Mohit Bansal","id":"28fbbf98bac1bb941162df553ca034d600cb59a6","summary":"Rephrase, Augment and Reason (RepARe), a gradient-free framework that extracts salient details about the image using the underlying LVLM as a captioner and reasoner, in order to propose modifications to the original question, is presented.","score":3},{"url":"https://www.semanticscholar.org/paper/cb5f53cd7d43ac6f48abf4dde6fc58e53a083b0e","title":"Accelerating the integration of ChatGPT and other large‐scale AI models into biomedical research and healthcare","venue":"MedComm – Future Medicine","year":2023,"referenceCount":99,"citationCount":21,"influentialCitationCount":0,"publicationDate":"17/05/2023","authors":"Ding‐Qiao Wang,Long‐Yu Feng,Jin‐Guo Ye,Jin‐Gen Zou,Yingfeng Zheng","id":"cb5f53cd7d43ac6f48abf4dde6fc58e53a083b0e","summary":"","score":3},{"url":"https://www.semanticscholar.org/paper/2010e5fb3a804ac376412b4fa65ee83f34d5e1d9","title":"A Systematic Evaluation of GPT-4V's Multimodal Capability for Medical Image Analysis","venue":"","year":2023,"referenceCount":65,"citationCount":0,"influentialCitationCount":0,"publicationDate":"31/10/2023","authors":"Yingshu Li,Yunyi Liu,Zhanyu Wang,Xinyu Liang,Lei Wang,Lingqiao Liu,Leyang Cui,Zhaopeng Tu,Longyue Wang,Luping Zhou","id":"2010e5fb3a804ac376412b4fa65ee83f34d5e1d9","summary":"An evaluation of GPT-4V's multimodal capability for medical image analysis shows that it excels in understanding medical images and is able to generate high-quality radiology reports and effectively answer questions about medical images, but it is found that its performance for medical visual grounding needs to be substantially improved.","score":3},{"url":"https://www.semanticscholar.org/paper/3130643a5d02f0e849d83bb1f85577a924081f36","title":"Paxion: Patching Action Knowledge in Video-Language Foundation Models","venue":"Neural Information Processing Systems","year":2023,"referenceCount":63,"citationCount":8,"influentialCitationCount":1,"publicationDate":"18/05/2023","authors":"Zhenhailong Wang,Ansel Blume,Sha Li,Genglin Liu,Jaemin Cho,Zineng Tang,Mohit Bansal,Heng Ji","id":"3130643a5d02f0e849d83bb1f85577a924081f36","summary":"This work proposes a novel framework, Paxion, along with a new Discriminative Video Dynamics Modeling (DVDM) objective, and introduces the DVDM objective to train the Knowledge Patcher, which forces the model to encode the correlation between the action text and the correct ordering of video frames.","score":3},{"url":"https://www.semanticscholar.org/paper/fc6a2f7478f68adefd69e2071f27e38aa1647f2f","title":"Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond","venue":"","year":2023,"referenceCount":85,"citationCount":60,"influentialCitationCount":15,"publicationDate":"24/08/2023","authors":"Jinze Bai,Shuai Bai,Shusheng Yang,Shijie Wang,Sinan Tan,Peng Wang,Junyang Lin,Chang Zhou,Jingren Zhou","id":"fc6a2f7478f68adefd69e2071f27e38aa1647f2f","summary":"The Qwen-VL series, a set of large-scale vision-language models (LVLMs) designed to perceive and understand both texts and images, set new records for generalist models under similar model scales on a broad range of visual-centric benchmarks.","score":3},{"url":"https://www.semanticscholar.org/paper/96c43227831c4c3b12b7c64809e78674cea3a8a1","title":"DeepSpeed-VisualChat: Multi-Round Multi-Image Interleave Chat via Multi-Modal Causal Attention","venue":"arXiv.org","year":2023,"referenceCount":47,"citationCount":3,"influentialCitationCount":1,"publicationDate":"25/09/2023","authors":"Z. Yao,Xiaoxia Wu,Conglong Li,Minjia Zhang,Heyang Qi,Olatunji Ruwase,A. A. Awan,Samyam Rajbhandari,Yuxiong He","id":"96c43227831c4c3b12b7c64809e78674cea3a8a1","summary":"The DeepSpeed-VisualChat framework is presented, designed to optimize Large Language Models by incorporating multi-modal capabilities, with a focus on enhancing the proficiency of Large Vision and Language Models in handling interleaved inputs.","score":3}]}
{"papers":[{"url":"https://www.semanticscholar.org/paper/c7492913370b5726eaa6ced163a60de6c9d4bb7f","title":"A Survey of Large Language Models for Healthcare: from Data, Technology, and Applications to Accountability and Ethics","venue":"arXiv.org","year":2023,"referenceCount":377,"citationCount":27,"influentialCitationCount":2,"publicationDate":"09/10/2023","authors":"Kai He,Rui Mao,Qika Lin,Yucheng Ruan,Xiang Lan,Mengling Feng,Erik Cambria","id":"c7492913370b5726eaa6ced163a60de6c9d4bb7f","summary":"It is contended that a significant paradigm shift is underway, transitioning from PLMs to LLMs, which encompasses a move from discriminative AI approaches to generativeAI approaches, as well as a shift from model-centered methodologies to datacentered methodologies.","score":5},{"url":"https://www.semanticscholar.org/paper/4d4a96708fc67403176bb2b891b564af7a20c148","title":"RAMM: Retrieval-augmented Biomedical Visual Question Answering with Multi-modal Pre-training","venue":"ACM Multimedia","year":2023,"referenceCount":49,"citationCount":4,"influentialCitationCount":0,"publicationDate":"01/03/2023","authors":"Zheng Yuan,Qiao Jin,Chuanqi Tan,Zhengyun Zhao,Hongyi Yuan,Fei Huang,Songfang Huang","id":"4d4a96708fc67403176bb2b891b564af7a20c148","summary":"This paper collects a new biomedical dataset named PMCPM which offers patient-based image-text pairs containing diverse patient situations from PubMed and proposes a retrieval-augmented pretrain-and-finetune paradigm named RAMM for biomedical VQA to overcome the data limitation issue.","score":4},{"url":"https://www.semanticscholar.org/paper/ebedc4d7a2356090904baba4104ef0832bc236df","title":"A Survey on Multimodal Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":108,"citationCount":110,"influentialCitationCount":2,"publicationDate":"23/06/2023","authors":"Shukang Yin,Chaoyou Fu,Sirui Zhao,Ke Li,Xing Sun,Tong Xu,Enhong Chen","id":"ebedc4d7a2356090904baba4104ef0832bc236df","summary":"This paper presents the formulation of MLLM and delineate its related concepts, and discusses the key techniques and applications, including Multimodal Instruction Tuning (M-IT), Multimmodal In-Context Learning (M -ICL), MultIModal Chain of Thought (m-CoT), and LLM-Aided Visual Reasoning (LAVR).","score":4},{"url":"https://www.semanticscholar.org/paper/2c7e346aa311fec4dda04bdf3a214ce2026d8807","title":"Medical Vision Language Pretraining: A survey","venue":"arXiv.org","year":2023,"referenceCount":161,"citationCount":2,"influentialCitationCount":0,"publicationDate":"11/12/2023","authors":"Prashant Shrestha,Sanskar Amgain,Bidur Khanal,C. Linte,Binod Bhattarai","id":"2c7e346aa311fec4dda04bdf3a214ce2026d8807","summary":"This paper reviews existing works through the lens of different pretraining objectives, architectures, downstream evaluation tasks, and datasets utilized for pretraining and downstream tasks, then dives into current challenges in medical VLP, discussing existing and potential solutions, and concludes by highlighting future directions.","score":4},{"url":"https://www.semanticscholar.org/paper/5ce94181ea702f69c3651dce721d6bd8026b8106","title":"TPTU: Task Planning and Tool Usage of Large Language Model-based AI Agents","venue":"arXiv.org","year":2023,"referenceCount":91,"citationCount":34,"influentialCitationCount":5,"publicationDate":2023,"authors":"Jingqing Ruan,Yihong Chen,Bin Zhang,Zhiwei Xu,Tianpeng Bao,Guoqing Du,Shiwei Shi,Hangyu Mao,Xingyu Zeng,Rui Zhao","id":"5ce94181ea702f69c3651dce721d6bd8026b8106","summary":"A structured framework tailored for LLM-based AI Agents is proposed and two distinct types of agents are designed to execute the inference process, which emphasizes the substantial potential of these models, while also identifying areas that need more investigation and improvement.","score":3},{"url":"https://www.semanticscholar.org/paper/6845bea94b2fb17d4377b3bb2bd10f73a959f9cc","title":"Reasoning with Language Model Prompting: A Survey","venue":"Annual Meeting of the Association for Computational Linguistics","year":2022,"referenceCount":219,"citationCount":126,"influentialCitationCount":2,"publicationDate":"19/12/2022","authors":"Shuofei Qiao,Yixin Ou,Ningyu Zhang,Xiang Chen,Yunzhi Yao,Shumin Deng,Chuanqi Tan,Fei Huang,Huajun Chen","id":"6845bea94b2fb17d4377b3bb2bd10f73a959f9cc","summary":"This paper provides a comprehensive survey of cutting-edge research on reasoning with language model prompting with comparisons and summaries and provides systematic resources to help beginners.","score":3},{"url":"https://www.semanticscholar.org/paper/0ebc861f5478561f12941e6b48aad30574e996d8","title":"Video ChatCaptioner: Towards Enriched Spatiotemporal Descriptions","venue":"arXiv.org","year":2023,"referenceCount":44,"citationCount":11,"influentialCitationCount":0,"publicationDate":"09/04/2023","authors":"Jun Chen,Deyao Zhu,Kilichbek Haydarov,Xiang Li,Mohamed Elhoseiny","id":"0ebc861f5478561f12941e6b48aad30574e996d8","summary":"This work introduces Video ChatCaptioner, an innovative approach for creating more comprehensive spatiotemporal video descriptions, specifically designed to select frames for posing video content-driven questions and shows promise as a method for enhancing video content.","score":3},{"url":"https://www.semanticscholar.org/paper/a5036f31f0e629dc661f120b8c3b1f374d479ab8","title":"Visual Instruction Tuning","venue":"Neural Information Processing Systems","year":2023,"referenceCount":63,"citationCount":791,"influentialCitationCount":255,"publicationDate":"17/04/2023","authors":"Haotian Liu,Chunyuan Li,Qingyang Wu,Yong Jae Lee","id":"a5036f31f0e629dc661f120b8c3b1f374d479ab8","summary":"This paper presents LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding and introduces GPT-4 generated visual instruction tuning data, the model and code base publicly available.","score":3},{"url":"https://www.semanticscholar.org/paper/170c97c7215f42edfb20c2248f954879e91ef86e","title":"Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models","venue":"Neural Information Processing Systems","year":2023,"referenceCount":72,"citationCount":133,"influentialCitationCount":17,"publicationDate":"19/04/2023","authors":"Pan Lu,Baolin Peng,Hao Cheng,Michel Galley,Kai-Wei Chang,Y. Wu,Song-Chun Zhu,Jianfeng Gao","id":"170c97c7215f42edfb20c2248f954879e91ef86e","summary":"This paper demonstrates the effectiveness of Chameleon on two multi-modal knowledge-intensive reasoning tasks: ScienceQA and TabMWP and shows that the GPT-4-powered planner exhibits more consistent and rational tool selection via inferring potential constraints from instructions, compared to a ChatGPT- powered planner.","score":3},{"url":"https://www.semanticscholar.org/paper/ca6a2bc279be5a3349a22bfd6866ed633d18734b","title":"MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":49,"citationCount":622,"influentialCitationCount":134,"publicationDate":"20/04/2023","authors":"Deyao Zhu,Jun Chen,Xiaoqian Shen,Xiang Li,Mohamed Elhoseiny","id":"ca6a2bc279be5a3349a22bfd6866ed633d18734b","summary":"MiniGPT-4 is presented, which aligns a frozen visual encoder with a frozen advanced LLM, Vicuna, using one projection layer to uncovers that properly aligning the visual features with an advanced large language model can possess numerous advanced multi-modal abilities demonstrated by G PT-4.","score":3},{"url":"https://www.semanticscholar.org/paper/570079bbdd8758dfe865097e05719313c9c1301a","title":"LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model","venue":"arXiv.org","year":2023,"referenceCount":79,"citationCount":219,"influentialCitationCount":30,"publicationDate":"28/04/2023","authors":"Peng Gao,Jiaming Han,Renrui Zhang,Ziyi Lin,Shijie Geng,Aojun Zhou,W. Zhang,Pan Lu,Conghui He,Xiangyu Yue,Hongsheng Li,Y. Qiao","id":"570079bbdd8758dfe865097e05719313c9c1301a","summary":"This work augments LLaMA-Adapter by unlocking more learnable parameters and proposes an early fusion strategy to feed visual tokens only into the early LLM layers, contributing to better visual knowledge incorporation and achieves strong multi-modal reasoning with only a small-scale image-text and instruction dataset.","score":3},{"url":"https://www.semanticscholar.org/paper/d6d3604f369bb0415cbe814e43ca3131323b03e2","title":"Otter: A Multi-Modal Model with In-Context Instruction Tuning","venue":"arXiv.org","year":2023,"referenceCount":39,"citationCount":228,"influentialCitationCount":34,"publicationDate":"05/05/2023","authors":"Bo Li,Yuanhan Zhang,Liangyu Chen,Jinghao Wang,Jingkang Yang,Ziwei Liu","id":"d6d3604f369bb0415cbe814e43ca3131323b03e2","summary":"Otter, a multi-modal model based on OpenFlamingo (open-sourced version of DeepMind's Flamingo), trained on MIMIC-IT and showcasing improved instruction-following ability and in-context learning is introduced.","score":3},{"url":"https://www.semanticscholar.org/paper/66d755730f5d08a6f4fcc5e81f24982ba389dca9","title":"LayoutGPT: Compositional Visual Planning and Generation with Large Language Models","venue":"Neural Information Processing Systems","year":2023,"referenceCount":65,"citationCount":30,"influentialCitationCount":4,"publicationDate":"24/05/2023","authors":"Weixi Feng,Wanrong Zhu,Tsu-Jui Fu,Varun Jampani,Arjun Reddy Akula,Xuehai He,Sugato Basu,X. Wang,William Yang Wang","id":"66d755730f5d08a6f4fcc5e81f24982ba389dca9","summary":"This work proposes LayoutGPT, a method to compose in-context visual demonstrations in style sheet language to enhance the visual planning skills of LLMs, and shows superior performance in converting challenging language concepts like numerical and spatial relations to layout arrangements for faithful text-to-image generation.","score":3},{"url":"https://www.semanticscholar.org/paper/9837349417e36ef5be06da0fd6c74042148bdaa2","title":"Visual Programming for Text-to-Image Generation and Evaluation","venue":"arXiv.org","year":2023,"referenceCount":68,"citationCount":23,"influentialCitationCount":4,"publicationDate":"24/05/2023","authors":"Jaemin Cho,Abhaysinh Zala,Mohit Bansal","id":"9837349417e36ef5be06da0fd6c74042148bdaa2","summary":"This work proposes two novel interpretable/explainable visual programming frameworks for text-to-image (T2I) generation and evaluation and introduces VPEval, an interpretable and explainable evaluation framework for T2I generation based on visual programming.","score":3},{"url":"https://www.semanticscholar.org/paper/7cf64070fd3d7e53d80f260c10e6bd7018d580e1","title":"IdealGPT: Iteratively Decomposing Vision and Language Reasoning via Large Language Models","venue":"Conference on Empirical Methods in Natural Language Processing","year":2023,"referenceCount":51,"citationCount":16,"influentialCitationCount":3,"publicationDate":"24/05/2023","authors":"Haoxuan You,Rui Sun,Zhecan Wang,Long Chen,Gengyu Wang,Hammad A. Ayyubi,Kai-Wei Chang,Shih-Fu Chang","id":"7cf64070fd3d7e53d80f260c10e6bd7018d580e1","summary":"The IdealGPT framework is presented, a framework that iteratively decomposes VL reasoning using large language models (LLMs) and outperforms the best existing GPT-4-like models by an absolute 10% on VCR and 15% on SNLI-VE.","score":3},{"url":"https://www.semanticscholar.org/paper/5ff2f5212713ec424662ac3c9e4aa5a8790d40cf","title":"ANPL: Towards Natural Programming with Interactive Decomposition","venue":"Neural Information Processing Systems","year":2023,"referenceCount":71,"citationCount":1,"influentialCitationCount":0,"publicationDate":"29/05/2023","authors":"Di Huang,Ziyuan Nan,Xingui Hu,Pengwei Jin,Shaohui Peng,Yuanbo Wen,Rui Zhang,Zidong Du,Qi Guo,Yewen Pu,Yunji Chen","id":"5ff2f5212713ec424662ac3c9e4aa5a8790d40cf","summary":"This paper introduces ANPL, an interactive programming system that ensures users can always refine the generated code towards their specific programmatic intents via structured decompositions, and deploys ANPL on the Abstraction and Reasoning Corpus, a set of unique tasks that are challenging for state-of-the-art AI systems.","score":3},{"url":"https://www.semanticscholar.org/paper/af705d648b5b16daa3dcc593bc593f2574d76c07","title":"Grammar Prompting for Domain-Specific Language Generation with Large Language Models","venue":"Neural Information Processing Systems","year":2023,"referenceCount":102,"citationCount":6,"influentialCitationCount":0,"publicationDate":"30/05/2023","authors":"Bailin Wang,Zi Wang,Xuezhi Wang,Yuan Cao,R. Saurous,Yoon Kim","id":"af705d648b5b16daa3dcc593bc593f2574d76c07","summary":"Experiments demonstrate that grammar prompting can enable LLMs to perform competitively on a diverse set of DSL generation tasks, including semantic parsing, Overnight, GeoQuery, PDDL planning, and even molecule generation (SMILES).","score":3},{"url":"https://www.semanticscholar.org/paper/fd755dc7b5b206c17fd953db04e1c888d45b6e4e","title":"LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark","venue":"Neural Information Processing Systems","year":2023,"referenceCount":99,"citationCount":42,"influentialCitationCount":8,"publicationDate":"11/06/2023","authors":"Zhen-fei Yin,Jiong Wang,Jianjian Cao,Zhelun Shi,Dingning Liu,Mukai Li,Lu Sheng,Lei Bai,Xiaoshui Huang,Zhiyong Wang,Wanli Ouyang,Jing Shao","id":"fd755dc7b5b206c17fd953db04e1c888d45b6e4e","summary":"This work extends the research of MLLMs to point clouds and presents the LAMM-Dataset and LAMm-Benchmark for 2D image and 3D point cloud understanding and establishes an extensible framework to facilitate the extension of M LLMs to additional modalities.","score":3},{"url":"https://www.semanticscholar.org/paper/d98536f24272e258b1d399074b64284d64786099","title":"AVIS: Autonomous Visual Information Seeking with Large Language Models","venue":"Neural Information Processing Systems","year":2023,"referenceCount":133,"citationCount":10,"influentialCitationCount":1,"publicationDate":"13/06/2023","authors":"Ziniu Hu,Ahmet Iscen,Chen Sun,Kai-Wei Chang,Yizhou Sun,David A. Ross,C. Schmid,A. Fathi","id":"d98536f24272e258b1d399074b64284d64786099","summary":"An autonomous information seeking visual question answering framework that leverages a Large Language Model to dynamically strategize the utilization of external tools and to investigate their outputs, thereby acquiring the indispensable knowledge needed to provide answers to the posed questions is proposed.","score":3},{"url":"https://www.semanticscholar.org/paper/051549d8ef56937b2f4d113afdcf8c7586d3770b","title":"Towards AGI in Computer Vision: Lessons Learned from GPT and Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":169,"citationCount":2,"influentialCitationCount":0,"publicationDate":"14/06/2023","authors":"Lingxi Xie,Longhui Wei,Xiaopeng Zhang,Kaifeng Bi,Xiaotao Gu,Jianlong Chang,Qi Tian","id":"051549d8ef56937b2f4d113afdcf8c7586d3770b","summary":"It is pointed out that the essential weakness of CV lies in lacking a paradigm to learn from environments, yet NLP has accomplished the task in the text world and is still far from a system like GPT that naturally integrates all tasks.","score":3},{"url":"https://www.semanticscholar.org/paper/966852963a88a28786b798c91b6662d6e501e590","title":"AssistGPT: A General Multi-modal Assistant that can Plan, Execute, Inspect, and Learn","venue":"arXiv.org","year":2023,"referenceCount":73,"citationCount":26,"influentialCitationCount":3,"publicationDate":"14/06/2023","authors":"Difei Gao,Lei Ji,Luowei Zhou,Kevin Lin,Joya Chen,Zihan Fan,Mike Zheng Shou","id":"966852963a88a28786b798c91b6662d6e501e590","summary":"A multi-modal AI assistant with an interleaved code and language reasoning approach called Plan, Execute, Inspect, and Learn (PEIL) to integrate LLMs with various tools, and a Learner is designed to enable the model to autonomously explore and discover the optimal solution.","score":3},{"url":"https://www.semanticscholar.org/paper/094883e42bb9a41f602c0715c1059bc431e33fb2","title":"GPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest","venue":"arXiv.org","year":2023,"referenceCount":96,"citationCount":74,"influentialCitationCount":6,"publicationDate":"07/07/2023","authors":"Shilong Zhang,Pei Sun,Shoufa Chen,Min Xiao,Wenqi Shao,Wenwei Zhang,Kai Chen,Ping Luo","id":"094883e42bb9a41f602c0715c1059bc431e33fb2","summary":"Spatial instruction tuning is proposed, which introduces the reference to the region-of-interest (RoI) in the instruction, which achieves a remarkable accuracy of 81.6%, surpassing all existing models by a significant margin and almost reaching human-level performance of 85.0%.","score":3},{"url":"https://www.semanticscholar.org/paper/ca31b8584b6c022ef15ddfe994fe361e002b7729","title":"A Comprehensive Overview of Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":447,"citationCount":58,"influentialCitationCount":2,"publicationDate":"12/07/2023","authors":"Humza Naveed,Asad Ullah Khan,Shi Qiu,Muhammad Saqib,Saeed Anwar,Muhammad Usman,Nick Barnes,A. Mian","id":"ca31b8584b6c022ef15ddfe994fe361e002b7729","summary":"A self-contained comprehensive overview of the existing literature on a broad range of LLM-related concepts discusses relevant background concepts along with covering the advanced topics at the frontier of research in LLMs.","score":3},{"url":"https://www.semanticscholar.org/paper/584ca135b61482fd89247113da87d784f738dbfa","title":"Foundational Models Defining a New Era in Vision: A Survey and Outlook","venue":"arXiv.org","year":2023,"referenceCount":365,"citationCount":22,"influentialCitationCount":4,"publicationDate":"25/07/2023","authors":"Muhammad Awais,Muzammal Naseer,Salman Siddique Khan,R. Anwer,Hisham Cholakkal,M. Shah,Ming Yang,F. Khan","id":"584ca135b61482fd89247113da87d784f738dbfa","summary":"A comprehensive review of emerging foundational models in computer vision, including typical architecture designs to combine different modalities, training objectives, pre-training datasets, fine-tuning mechanisms, and the common prompting patterns; textual, visual, and heterogeneous.","score":3},{"url":"https://www.semanticscholar.org/paper/446fb5dead075a1a08862662738f462e9a0e91c8","title":"Tool Documentation Enables Zero-Shot Tool-Usage with Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":85,"citationCount":16,"influentialCitationCount":0,"publicationDate":"01/08/2023","authors":"Cheng-Yu Hsieh,Sibei Chen,Chun-Liang Li,Yasuhisa Fujii,Alexander J. Ratner,Chen-Yu Lee,Ranjay Krishna,Tomas Pfister","id":"446fb5dead075a1a08862662738f462e9a0e91c8","summary":"This work advocates the use of tool documentation, descriptions for the individual tool usage, over demonstrations, and shows that tool documentation is significantly more valuable than demonstrations, with zero-shot documentation significantly outperforming few-shot without documentation.","score":3},{"url":"https://www.semanticscholar.org/paper/dd0612ce863f64b0f69d0d9f708c52e829f6f859","title":"TPTU: Large Language Model-based AI Agents for Task Planning and Tool Usage","venue":"","year":2023,"referenceCount":95,"citationCount":7,"influentialCitationCount":0,"publicationDate":"07/08/2023","authors":"Jingqing Ruan,Yihong Chen,Bin Zhang,Zhiwei Xu,Tianpeng Bao,Guoqing Du,Shiwei Shi,Hangyu Mao,Xingyu Zeng,Rui Zhao","id":"dd0612ce863f64b0f69d0d9f708c52e829f6f859","summary":"A structured framework tailored for LLM-based AI Agents is proposed and two distinct types of agents are designed to execute the inference process, which emphasizes the substantial potential of these models, while also identifying areas that need more investigation and improvement.","score":3},{"url":"https://www.semanticscholar.org/paper/d6c2523ab97416c2692cbbeab082ed1790e8e55e","title":"VisIT-Bench: A Benchmark for Vision-Language Instruction Following Inspired by Real-World Use","venue":"arXiv.org","year":2023,"referenceCount":99,"citationCount":23,"influentialCitationCount":1,"publicationDate":"12/08/2023","authors":"Yonatan Bitton,Hritik Bansal,Jack Hessel,Rulin Shao,Wanrong Zhu,Anas Awadalla,Josh Gardner,Rohan Taori,L. Schimdt","id":"d6c2523ab97416c2692cbbeab082ed1790e8e55e","summary":"This work introduces VisIT-Bench (Visual InsTruction Benchmark), a benchmark for evaluation of instruction-following vision-language models for real-world use, and curates 70 'instruction families' that it envision instruction tuned vision- language models should be able to address.","score":3},{"url":"https://www.semanticscholar.org/paper/894ed1aba8e42a4ec27ba53ecde383b14c5128ca","title":"Examining User-Friendly and Open-Sourced Large GPT Models: A Survey on Language, Multimodal, and Scientific GPT Models","venue":"arXiv.org","year":2023,"referenceCount":178,"citationCount":2,"influentialCitationCount":0,"publicationDate":"27/08/2023","authors":"Kaiyuan Gao,Su He,Zhenyu He,Jiacheng Lin,Qizhi Pei,Jie Shao,Wei Zhang","id":"894ed1aba8e42a4ec27ba53ecde383b14c5128ca","summary":"This survey paper provides an examination of alternative open-sourced models of large GPTs, focusing on user-friendly and relatively small models that facilitate easier deployment and accessibility, and aims to equip researchers, practitioners, and enthusiasts with a thorough understanding of these models.","score":3},{"url":"https://www.semanticscholar.org/paper/4eb87eaa193929dbef93fa2db9419245a8e8916f","title":"TextBind: Multi-turn Interleaved Multimodal Instruction-following in the Wild","venue":"arXiv.org","year":2023,"referenceCount":67,"citationCount":4,"influentialCitationCount":1,"publicationDate":"14/09/2023","authors":"Huayang Li,Siheng Li,Deng Cai,Longyue Wang,Lemao Liu,Taro Watanabe,Yujiu Yang,Shuming Shi","id":"4eb87eaa193929dbef93fa2db9419245a8e8916f","summary":"This work introduces TextBind, an almost annotation-free framework for empowering larger language models with the multi-turn interleaved multimodal instruction-following capabilities, and devise MIM, a language model-centric architecture that seamlessly integrates image encoder and decoder models.","score":3},{"url":"https://www.semanticscholar.org/paper/3ec464696db25acc2c39a6d967ec3df09e06f633","title":"Multimodal Multi-Hop Question Answering Through a Conversation Between Tools and Efficiently Finetuned Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":33,"citationCount":1,"influentialCitationCount":0,"publicationDate":"16/09/2023","authors":"Hossein Rajabzadeh,Suyuchen Wang,Hyock Ju Kwon,Bang Liu","id":"3ec464696db25acc2c39a6d967ec3df09e06f633","summary":"A tool-interacting divide-and-conquer strategy enabling large language models (LLMs) to answer complex multimodal multi-hop questions, demonstrating the efficacy and generality of this approach.","score":3},{"url":"https://www.semanticscholar.org/paper/f34b6b4f75fb4e6f2c5654ee13fb2479c6170b3a","title":"Kosmos-2.5: A Multimodal Literate Model","venue":"arXiv.org","year":2023,"referenceCount":84,"citationCount":13,"influentialCitationCount":0,"publicationDate":"20/09/2023","authors":"Tengchao Lv,Yupan Huang,Jingye Chen,Lei Cui,Shuming Ma,Ya-Chi Chang,Shaohan Huang,Wenhui Wang,Li Dong,Weiyao Luo,Shaoxiang Wu,Guoxin Wang,Cha Zhang,Furu Wei","id":"f34b6b4f75fb4e6f2c5654ee13fb2479c6170b3a","summary":"Kosmos-2.5 can be readily adapted for any text-intensive image understanding task with different prompts through supervised fine-tuning, making it a general-purpose tool for real-world applications involving text-rich images and paves the way for the future scaling of multimodal large language models.","score":3},{"url":"https://www.semanticscholar.org/paper/7b689adb8c156d6158660f90d1c86888ee281f63","title":"DreamLLM: Synergistic Multimodal Comprehension and Creation","venue":"arXiv.org","year":2023,"referenceCount":169,"citationCount":38,"influentialCitationCount":4,"publicationDate":"20/09/2023","authors":"Runpei Dong,Chunrui Han,Yuang Peng,Zekun Qi,Zheng Ge,Jinrong Yang,Liang Zhao,Jian‚ÄêYuan Sun,Hongyu Zhou,Hao-Ran Wei,Xiangwen Kong,Xiangyu Zhang,Kaisheng Ma,Li Yi","id":"7b689adb8c156d6158660f90d1c86888ee281f63","summary":"A learning framework that first achieves versatile Multimodal Large Language Models (MLLMs) empowered with frequently overlooked synergy between multimodal comprehension and creation, reaping from the enhanced learning synergy.","score":3},{"url":"https://www.semanticscholar.org/paper/092245d86b77181c36f972b1b7a17a59cd989c4a","title":"Guiding Instruction-based Image Editing via Multimodal Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":67,"citationCount":10,"influentialCitationCount":1,"publicationDate":"29/09/2023","authors":"Tsu-Jui Fu,Wenze Hu,Xianzhi Du,William Yang Wang,Yinfei Yang,Zhe Gan","id":"092245d86b77181c36f972b1b7a17a59cd989c4a","summary":"This work investigates how MLLMs facilitate edit instructions and presents MLLM-Guided Image Editing (MGIE), which learns to derive expressive instructions and provides explicit guidance and can lead to a notable improvement in automatic metrics and human evaluation while maintaining competitive inference efficiency.","score":3},{"url":"https://www.semanticscholar.org/paper/7f1ba5630c3baa09b11cc665b3f71cdb117e5ffb","title":"OpenLEAF: Open-Domain Interleaved Image-Text Generation and Evaluation","venue":"arXiv.org","year":2023,"referenceCount":40,"citationCount":3,"influentialCitationCount":0,"publicationDate":"11/10/2023","authors":"Jie An,Zhengyuan Yang,Linjie Li,Jianfeng Wang,K. Lin,Zicheng Liu,Lijuan Wang,Jiebo Luo","id":"7f1ba5630c3baa09b11cc665b3f71cdb117e5ffb","summary":"A new interleaved generation framework based on prompting large-language models (LLMs) and pre-trained text-to-image (T2I) models, namely OpenLEAF is proposed, which can generate high-quality image-text content for various domains and applications.","score":3},{"url":"https://www.semanticscholar.org/paper/a710efa9247207a72f06e0c9db302fd3ecab5fbb","title":"Towards Robust Multi-Modal Reasoning via Model Selection","venue":"arXiv.org","year":2023,"referenceCount":56,"citationCount":1,"influentialCitationCount":0,"publicationDate":"12/10/2023","authors":"Xiangyan Liu,Rongxue Li,Wei Ji,Tao Lin","id":"a710efa9247207a72f06e0c9db302fd3ecab5fbb","summary":"This framework improves model selection and bolsters the robustness of multi-modal agents in multi-step reasoning, and enables dynamic model selection, considering both user inputs and subtask dependencies, thereby robustifying the overall reasoning process.","score":3},{"url":"https://www.semanticscholar.org/paper/1d14a708622917da4b9820ada6d32af24fc1651a","title":"Idea2Img: Iterative Self-Refinement with GPT-4V(ision) for Automatic Image Design and Generation","venue":"arXiv.org","year":2023,"referenceCount":56,"citationCount":6,"influentialCitationCount":0,"publicationDate":"12/10/2023","authors":"Zhengyuan Yang,Jianfeng Wang,Linjie Li,Kevin Lin,Chung-Ching Lin,Zicheng Liu,Lijuan Wang","id":"1d14a708622917da4b9820ada6d32af24fc1651a","summary":null,"score":3},{"url":"https://www.semanticscholar.org/paper/c020f15be1dee20f9e2e0c5a6f05f272b5508325","title":"LLaVA-Interactive: An All-in-One Demo for Image Chat, Segmentation, Generation and Editing","venue":"arXiv.org","year":2023,"referenceCount":36,"citationCount":6,"influentialCitationCount":1,"publicationDate":"01/11/2023","authors":"Wei-Ge Chen,Irina Spiridonova,Jianwei Yang,Jianfeng Gao,Chun-yue Li","id":"c020f15be1dee20f9e2e0c5a6f05f272b5508325","summary":"The development of LLaVA-Interactive is extremely cost-efficient as the system combines three multimodal skills of pre-built AI models without additional model training: visual chat of L LaVA, image segmentation from SEEM, as well as image generation and editing from GLIGEN.","score":3},{"url":"https://www.semanticscholar.org/paper/ecfff30e570498b6c87c5d1319a0cbcdf7a8b86d","title":"LLaVA-Plus: Learning to Use Tools for Creating Multimodal Agents","venue":"arXiv.org","year":2023,"referenceCount":52,"citationCount":17,"influentialCitationCount":4,"publicationDate":"09/11/2023","authors":"Shilong Liu,Hao Cheng,Haotian Liu,Hao Zhang,Feng Li,Tianhe Ren,Xueyan Zou,Jianwei Yang,Hang Su,Jun-Juan Zhu,Lei Zhang,Jianfeng Gao,Chun-yue Li","id":"ecfff30e570498b6c87c5d1319a0cbcdf7a8b86d","summary":"LLaVA-Plus is distinct in that the image query is directly grounded and actively engaged throughout the entire human-AI interaction sessions, significantly improving tool use performance and enabling new scenarios.","score":3},{"url":"https://www.semanticscholar.org/paper/2fb605f67fee79cad94952ddfe0f686e926f49f5","title":"GPT-4V in Wonderland: Large Multimodal Models for Zero-Shot Smartphone GUI Navigation","venue":"arXiv.org","year":2023,"referenceCount":58,"citationCount":17,"influentialCitationCount":0,"publicationDate":"13/11/2023","authors":"An Yan,Zhengyuan Yang,Wanrong Zhu,K. Lin,Linjie Li,Jianfeng Wang,Jianwei Yang,Yiwu Zhong,Julian McAuley,Jianfeng Gao,Zicheng Liu,Lijuan Wang","id":"2fb605f67fee79cad94952ddfe0f686e926f49f5","summary":"The findings demonstrate that large multimodal models, specifically GPT-4V, excel in zero-shot GUI navigation through its advanced screen interpretation, action reasoning, and precise action localization capabilities.","score":3},{"url":"https://www.semanticscholar.org/paper/aad3d2e690f6c73f04a14622ceff51464bbc560e","title":"Chat-UniVi: Unified Visual Representation Empowers Large Language Models with Image and Video Understanding","venue":"arXiv.org","year":2023,"referenceCount":74,"citationCount":11,"influentialCitationCount":2,"publicationDate":"14/11/2023","authors":"Peng Jin,Ryuichi Takanobu,Caiwan Zhang,Xiaochun Cao,Li Yuan","id":"aad3d2e690f6c73f04a14622ceff51464bbc560e","summary":"This work introduces Chat-UniVi, a unified vision-language model capable of comprehending and engaging in conversations involving images and videos through a unified visual representation that consistently outperforms even existing methods exclusively designed for either images or videos.","score":3},{"url":"https://www.semanticscholar.org/paper/107fb6eec2febbae12db29bf3e311aaf5680027c","title":"Video-LLaVA: Learning United Visual Representation by Alignment Before Projection","venue":"arXiv.org","year":2023,"referenceCount":54,"citationCount":17,"influentialCitationCount":1,"publicationDate":"16/11/2023","authors":"Bin Lin,Bin Zhu,Yang Ye,Munan Ning,Peng Jin,Li Yuan","id":"107fb6eec2febbae12db29bf3e311aaf5680027c","summary":"This work unify visual representation into the language feature space to advance the foundational LLM towards a unified LVLM, and establishes a simple but robust LVLM baseline, Video-LLaVA, which learns from a mixed dataset of images and videos, mutually enhancing each other.","score":3},{"url":"https://www.semanticscholar.org/paper/246017780386eba39d6cda760a1c2c70356baa50","title":"VIoTGPT: Learning to Schedule Vision Tools towards Intelligent Video Internet of Things","venue":"arXiv.org","year":2023,"referenceCount":78,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/12/2023","authors":"Yaoyao Zhong,Mengshi Qi,Rui Wang,Yuhan Qiu,Yang Zhang,Huadong Ma","id":"246017780386eba39d6cda760a1c2c70356baa50","summary":"VIoTGPT is built, the framework based on LLMs to correctly interact with humans, query knowledge videos, and invoke vision models to accomplish complicated tasks to address the challenges posed by the fine-grained and interrelated vision tool usage of VIoT.","score":3},{"url":"https://www.semanticscholar.org/paper/6d2ab31aa75468f5458b9d96192c3f4a28f55d73","title":"DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving","venue":"arXiv.org","year":2023,"referenceCount":80,"citationCount":6,"influentialCitationCount":2,"publicationDate":"14/12/2023","authors":"Wenhai Wang,Jiangwei Xie,ChuanYang Hu,Haoming Zou,Jianan Fan,Wenwen Tong,Yang Wen,Silei Wu,Hanming Deng,Zhiqi Li,Hao Tian,Lewei Lu,Xizhou Zhu,Xiaogang Wang,Yu Qiao,Jifeng Dai","id":"6d2ab31aa75468f5458b9d96192c3f4a28f55d73","summary":"DriveMLM is introduced, an LLM-based AD framework that can perform close-loop autonomous driving in realistic simulators and can plug-and-play in existing AD systems such as Apollo for close-loop driving.","score":3},{"url":"https://www.semanticscholar.org/paper/35a17f896847614a71df772bbe2b66ae231cabc7","title":"CLOVA: A Closed-Loop Visual Assistant with Tool Usage and Update","venue":"arXiv.org","year":2023,"referenceCount":83,"citationCount":3,"influentialCitationCount":0,"publicationDate":"18/12/2023","authors":"Zhi Gao,Yuntao Du,Xintong Zhang,Xiaojian Ma,Wenjuan Han,Song-Chun Zhu,Qing Li","id":"35a17f896847614a71df772bbe2b66ae231cabc7","summary":"Experiments show that CLOVA outperforms tool-usage methods by 5% in visual question answering and multiple-image reasoning tasks, by 10% in knowledge tagging tasks, and by 20% in image editing tasks, highlighting the significance of the learning capability for general visual assistants.","score":3},{"url":"https://www.semanticscholar.org/paper/24fc9ad715372358bd0108eeb7c944b915963293","title":"ASSISTGUI: Task-Oriented Desktop Graphical User Interface Automation","venue":"arXiv.org","year":2023,"referenceCount":53,"citationCount":4,"influentialCitationCount":0,"publicationDate":"20/12/2023","authors":"Difei Gao,Lei Ji,Zechen Bai,Mingyu Ouyang,Peiran Li,Dongxing Mao,Qinchen Wu,Weichen Zhang,Peiyi Wang,Xiangwu Guo,Hengxu Wang,Luowei Zhou,Mike Zheng Shou","id":"24fc9ad715372358bd0108eeb7c944b915963293","summary":"An advanced Actor-Critic Embodied Agent framework is proposed, which incorporates a sophisticated GUI parser driven by an LLM-agent and an enhanced reasoning mechanism adept at handling lengthy procedural tasks that outshine existing methods in performance.","score":3},{"url":"https://www.semanticscholar.org/paper/6a33e58ef961a3a0a5657518b2be86395eb7c8d0","title":"InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks","venue":"arXiv.org","year":2023,"referenceCount":185,"citationCount":8,"influentialCitationCount":3,"publicationDate":"21/12/2023","authors":"Zhe Chen,Jiannan Wu,Wenhai Wang,Weijie Su,Guo Chen,Sen Xing,Zhong Muyan,Qinglong Zhang,Xizhou Zhu,Lewei Lu,Bin Li,Ping Luo,Tong Lu,Yu Qiao,Jifeng Dai","id":"6a33e58ef961a3a0a5657518b2be86395eb7c8d0","summary":"A large-scale vision-language foundation model (InternVL), which scales up the vision foundation model to 6 billion parameters and progressively aligns it with the LLM, using web-scale image-text data from various sources is designed.","score":3},{"url":"https://www.semanticscholar.org/paper/a06d3e9e90008c64c45a0029d580541d5f646771","title":"If LLM Is the Wizard, Then Code Is the Wand: A Survey on How Code Empowers Large Language Models to Serve as Intelligent Agents","venue":"arXiv.org","year":2024,"referenceCount":168,"citationCount":8,"influentialCitationCount":0,"publicationDate":"01/01/2024","authors":"Ke Yang,Jiateng Liu,John Wu,Chaoqi Yang,Y. Fung,Sha Li,Zixuan Huang,Xu Cao,Xingyao Wang,Yiquan Wang,Heng Ji,Chengxiang Zhai","id":"a06d3e9e90008c64c45a0029d580541d5f646771","summary":"An overview of the various benefits of integrating code into LLMs' training data and how these profound capabilities of LLMs, brought by code, have led to their emergence as intelligent agents in situations where the ability to understand instructions, decompose goals, plan and execute actions, and refine from feedback are crucial to their success on downstream tasks are traced.","score":3},{"url":"https://www.semanticscholar.org/paper/4a48d628e53f554eb6ef09a457ca855188b96171","title":"DoraemonGPT: Toward Understanding Dynamic Scenes with Large Language Models","venue":"arXiv.org","year":2024,"referenceCount":123,"citationCount":3,"influentialCitationCount":1,"publicationDate":"16/01/2024","authors":"Zongxin Yang,Guikun Chen,Xiaodi Li,Wenguan Wang,Yi Yang","id":"4a48d628e53f554eb6ef09a457ca855188b96171","summary":"DoraemonGPT, a comprehensive and conceptually elegant system driven by LLMs to handle dynamic video tasks, and a novel LLM-driven planner based on Monte Carlo Tree Search is introduced to explore the large planning space for scheduling various tools.","score":3},{"url":"https://www.semanticscholar.org/paper/23957040943f883542f47850c709b9e7f9d6fa55","title":"Prompting Large Vision-Language Models for Compositional Reasoning","venue":"arXiv.org","year":2024,"referenceCount":34,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/01/2024","authors":"Timothy Ossowski,Ming Jiang,Junjie Hu","id":"23957040943f883542f47850c709b9e7f9d6fa55","summary":"This paper makes an exploratory step using a novel generative method that prompts large vision-language models (e.g., GPT-4) to depict images and perform compositional reasoning, and outperforms other embedding-based methods on the Winoground dataset, and obtains further improvement of up to 10% accuracy when enhanced with the optimal description.","score":3},{"url":"https://www.semanticscholar.org/paper/140cfda71bfff852c3e205b7ad61854b78c76982","title":"Mastering Text-to-Image Diffusion: Recaptioning, Planning, and Generating with Multimodal LLMs","venue":"arXiv.org","year":2024,"referenceCount":77,"citationCount":7,"influentialCitationCount":0,"publicationDate":"22/01/2024","authors":"Ling Yang,Zhaochen Yu,Chenlin Meng,Minkai Xu,Stefano Ermon,Bin Cui","id":"140cfda71bfff852c3e205b7ad61854b78c76982","summary":"This paper proposes a brand new training-free text-to-image generation/editing framework, namely Recaption, Plan and Generate (RPG), harnessing the powerful chain-of-thought reasoning ability of multimodal LLMs to enhance the compositionality of text-to-image diffusion models.","score":3},{"url":"https://www.semanticscholar.org/paper/a050c9b0c321839e4427ab9defa3463be7825ac4","title":"MM-LLMs: Recent Advances in MultiModal Large Language Models","venue":"arXiv.org","year":2024,"referenceCount":254,"citationCount":3,"influentialCitationCount":0,"publicationDate":"24/01/2024","authors":"Duzhen Zhang,Yahan Yu,Chenxing Li,Jiahua Dong,Dan Su,Chenhui Chu,Dong Yu","id":"a050c9b0c321839e4427ab9defa3463be7825ac4","summary":"A taxonomy encompassing $122$ MM-LLMs, each characterized by its specific formulations is introduced and a review of selected MM-LLMs on mainstream benchmarks and key training recipes to enhance the potency of MM-LLMs are summarized.","score":3},{"url":"https://www.semanticscholar.org/paper/2cea424c7dce71042c24d43317521abdc4c0ffb4","title":"Large Multimodal Agents: A Survey","venue":"","year":2024,"referenceCount":80,"citationCount":0,"influentialCitationCount":0,"publicationDate":"23/02/2024","authors":"Junlin Xie,Zhihong Chen,Ruifei Zhang,Xiang Wan,Guanbin Li","id":"2cea424c7dce71042c24d43317521abdc4c0ffb4","summary":"This paper conducts a systematic review of LLM-driven multimodal agents, and introduces the essential components involved in developing LMAs and categorizes the current body of research into four distinct types.","score":3},{"url":"https://www.semanticscholar.org/paper/74c68aed85f2fe8019113bbdb533fcba7e3ce0bd","title":"ShapeLLM: Universal 3D Object Understanding for Embodied Interaction","venue":"","year":2024,"referenceCount":196,"citationCount":0,"influentialCitationCount":0,"publicationDate":"27/02/2024","authors":"Zekun Qi,Runpei Dong,Shaochen Zhang,Haoran Geng,Chunrui Han,Zheng Ge,Li Yi,Kaisheng Ma","id":"74c68aed85f2fe8019113bbdb533fcba7e3ce0bd","summary":"ShapeLLM is the first 3D Multimodal Large Language Model designed for embodied interaction, exploring a universal 3D object understanding with 3D point clouds and languages, built upon an improved 3D encoder by extending ReCon to ReCon++ that benefits from multi-view image distillation for enhanced geometry understanding.","score":3},{"url":"https://www.semanticscholar.org/paper/6bdfffbf92d01c8b543088d40d46233610e469a8","title":"CLIP in Medical Imaging: A Comprehensive Survey","venue":"arXiv.org","year":2023,"referenceCount":218,"citationCount":3,"influentialCitationCount":0,"publicationDate":"12/12/2023","authors":"Zihao Zhao,Yuxiao Liu,Han Wu,Yonghao Li,Sheng Wang,L. Teng,Disheng Liu,Xiang Li,Zhiming Cui,Qian Wang,Dinggang Shen","id":"6bdfffbf92d01c8b543088d40d46233610e469a8","summary":"This survey offers an in-depth exploration of the CLIP paradigm within the domain of medical imaging, regarding both refined CLIP pre-training and CLIP-driven applications, and investigates the adaptation of CLIP pre-training in the medical domain.","score":3},{"url":"https://www.semanticscholar.org/paper/8ecdbfe011b7189fa0ee49ffc4e42a93d728a371","title":"On Evaluating Adversarial Robustness of Large Vision-Language Models","venue":"Neural Information Processing Systems","year":2023,"referenceCount":108,"citationCount":36,"influentialCitationCount":3,"publicationDate":"26/05/2023","authors":"Yunqing Zhao,Tianyu Pang,Chao Du,Xiao Yang,Chongxuan Li,Ngai-Man Cheung,Min Lin","id":"8ecdbfe011b7189fa0ee49ffc4e42a93d728a371","summary":"Evaluating the robustness of open-source large VLMs in the most realistic and high-risk setting, where adversaries have only black-box system access and seek to deceive the model into returning the targeted responses is proposed.","score":3},{"url":"https://www.semanticscholar.org/paper/52941cadbd340344f3e0a6f50719fe55b3de5088","title":"Multimodal Large Language Models: A Survey","venue":"BigData Congress [Services Society]","year":2023,"referenceCount":75,"citationCount":13,"influentialCitationCount":0,"publicationDate":"22/11/2023","authors":"Jiayang Wu,Wensheng Gan,Zefeng Chen,Shicheng Wan,Philip S. Yu","id":"52941cadbd340344f3e0a6f50719fe55b3de5088","summary":"A range of multimodal products are introduced, focusing on the efforts of major technology companies, and a compilation of the latest algorithms and commonly used datasets are presented, providing researchers with valuable resources for experimentation and evaluation.","score":3},{"url":"https://www.semanticscholar.org/paper/88bddfb7d1e0462be8fe99fdbd71c658140cb17b","title":"From Image to Language: A Critical Analysis of Visual Question Answering (VQA) Approaches, Challenges, and Opportunities","venue":"arXiv.org","year":2023,"referenceCount":304,"citationCount":1,"influentialCitationCount":0,"publicationDate":"01/11/2023","authors":"Md Farhan Ishmam,Md Sakib Hossain Shovon,M. F. Mridha,Nilanjan Dey","id":"88bddfb7d1e0462be8fe99fdbd71c658140cb17b","summary":"This work presents a survey in the domain of VQA that delves into the intricacies of V QA datasets and methods over the field's history, introduces a detailed taxonomy to categorize the facets of VZA, and highlights the recent trends, challenges, and scopes for improvement.","score":3},{"url":"https://www.semanticscholar.org/paper/da9579539385daedd33a0de0f814e2977ad0d1f5","title":"Towards Unifying Medical Vision-and-Language Pre-training via Soft Prompts","venue":"IEEE International Conference on Computer Vision","year":2023,"referenceCount":72,"citationCount":7,"influentialCitationCount":1,"publicationDate":"17/02/2023","authors":"Zhihong Chen,Shizhe Diao,Benyou Wang,Guanbin Li,Xiang Wan","id":"da9579539385daedd33a0de0f814e2977ad0d1f5","summary":"This work unify the input format by introducing visual and textual prompts, which serve as DETR-like queries that assist in extracting features when one of the modalities is missing, and proposes an effective yet straightforward scheme named PTUnifier to unify the two types.","score":3},{"url":"https://www.semanticscholar.org/paper/8f3138f7ee5127faab265793be8ae278bc49d9b1","title":"PMC-CLIP: Contrastive Language-Image Pre-training using Biomedical Documents","venue":"International Conference on Medical Image Computing and Computer-Assisted Intervention","year":2023,"referenceCount":33,"citationCount":25,"influentialCitationCount":2,"publicationDate":"13/03/2023","authors":"Weixiong Lin,Ziheng Zhao,Xiaoman Zhang,Chaoyi Wu,Ya Zhang,Yanfeng Wang,Weidi Xie","id":"8f3138f7ee5127faab265793be8ae278bc49d9b1","summary":"PMC-OA, a biomedical dataset with 1.6M image-caption pairs collected from PubMedCentral's OpenAccess subset, is built and released, which is 8 times larger than before and achieves state-of-the-art results on various downstream tasks.","score":3},{"url":"https://www.semanticscholar.org/paper/ac4d13b6a4f9fb67337099f4602135a0351f5c99","title":"Towards Medical Artificial General Intelligence via Knowledge-Enhanced Multimodal Pretraining","venue":"arXiv.org","year":2023,"referenceCount":59,"citationCount":3,"influentialCitationCount":0,"publicationDate":"26/04/2023","authors":"Bingqian Lin,Zicong Chen,Mingjie Li,Haokun Lin,Hang Xu,Yi Zhu,Jian-zhuo Liu,Wenjia Cai,Lei Yang,Shen Zhao,Chenfei Wu,Ling Chen,Xiaojun Chang,Yi Yang,L. Xing,Xiaodan Liang","id":"ac4d13b6a4f9fb67337099f4602135a0351f5c99","summary":"The proposed MOTOR successfully mimics the human practice of fulfilling a\"medical student\" to accelerate the process of becoming a\"specialist\" and believes that this work makes a significant stride in realizing MAGI.","score":3},{"url":"https://www.semanticscholar.org/paper/f4793adffd6f67ffcb93ccfc5672ab301b8a2b96","title":"PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering","venue":"arXiv.org","year":2023,"referenceCount":40,"citationCount":36,"influentialCitationCount":3,"publicationDate":"17/05/2023","authors":"Xiaoman Zhang,Chaoyi Wu,Ziheng Zhao,Weixiong Lin,Ya Zhang,Yanfeng Wang,Weidi Xie","id":"f4793adffd6f67ffcb93ccfc5672ab301b8a2b96","summary":"This paper proposes a generative-based model for medical visual understanding by aligning visual information from a pre-trained vision encoder with a large language model, and establishes a scalable pipeline to construct a large-scale medical visual question-answering dataset.","score":3},{"url":"https://www.semanticscholar.org/paper/07d45ce7de598ef03b400f8ddba7d2e055e77a08","title":"BiomedGPT: A Unified and Generalist Biomedical Generative Pre-trained Transformer for Vision, Language, and Multimodal Tasks","venue":"","year":2023,"referenceCount":131,"citationCount":0,"influentialCitationCount":0,"publicationDate":"26/05/2023","authors":"Kai Zhang,Jun Yu,Eashan Adhikarla,Rong Zhou,Zhiling Yan,Yixin Liu,Zheng Liu,Lifang He,Brian D. Davison,Xiang Li,Hui Ren,S. Fu,James Zou,Wei Liu,Jing Huang,Chen Chen,Yuyin Zhou,Tianming Liu,Xun Chen,Yong Chen,Quanzheng Li,Hongfang Liu,Lichao Sun","id":"07d45ce7de598ef03b400f8ddba7d2e055e77a08","summary":"BiomedGPT is proposed, the first open-source and generalist visual language AI for diverse biomedical tasks and facilitates zero-shot transfer learning, greatly enhancing its utility as a biomedical assistant, similar to ChatGPT.","score":3},{"url":"https://www.semanticscholar.org/paper/f22d71c7ce9720ba1f717a4f1181488200e78198","title":"LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day","venue":"Neural Information Processing Systems","year":2023,"referenceCount":46,"citationCount":107,"influentialCitationCount":15,"publicationDate":"01/06/2023","authors":"Chunyuan Li,Cliff Wong,Sheng Zhang,Naoto Usuyama,Haotian Liu,Jianwei Yang,Tristan Naumann,Hoifung Poon,Jianfeng Gao","id":"f22d71c7ce9720ba1f717a4f1181488200e78198","summary":"This paper proposes a cost-efficient approach for training a vision-language conversational assistant that can answer open-ended research questions of biomedical images, and releases instruction-following data and the LLaVA-Med model, which exhibits excellent multimodal conversational capability.","score":3},{"url":"https://www.semanticscholar.org/paper/64fa56962dd0f4bbe206be6142fbe0315c4e7c2f","title":"Multi-modal Pre-training for Medical Vision-language Understanding and Generation: An Empirical Study with A New Benchmark","venue":"arXiv.org","year":2023,"referenceCount":71,"citationCount":2,"influentialCitationCount":1,"publicationDate":"10/06/2023","authors":"Li Xu,Bo Liu,Ameer Hamza Khan,Lu Fan,Xiao-Ming Wu","id":"64fa56962dd0f4bbe206be6142fbe0315c4e7c2f","summary":"RadioGraphy Captions (RGC), a high-quality, multi-modality radiographic dataset containing 18,434 image-caption pairs collected from an open-access online database MedPix, is proposed, which can be used as a pre-training dataset or a new benchmark for medical report generation and medical image-text retrieval.","score":3},{"url":"https://www.semanticscholar.org/paper/e0e9ba0c01d441e1fdcb8628d3f743d387b0b017","title":"UIT-Saviors at MEDVQA-GI 2023: Improving Multimodal Learning with Image Enhancement for Gastrointestinal Visual Question Answering","venue":"Conference and Labs of the Evaluation Forum","year":2023,"referenceCount":44,"citationCount":2,"influentialCitationCount":0,"publicationDate":"06/07/2023","authors":"T. M. Thai,A. T. Vo,Hao K. Tieu,Linh Bui,T. Nguyen","id":"e0e9ba0c01d441e1fdcb8628d3f743d387b0b017","summary":"This study highlights the dominance of Transformer-based vision models over the CNNs and demonstrates the effectiveness of the image enhancement process, with six out of the eight vision models achieving better F1-Score.","score":3},{"url":"https://www.semanticscholar.org/paper/baa1dc079d98ca76b0173c8d653fed759fd0a371","title":"A scoping review on multimodal deep learning in biomedical images and texts","venue":"Journal of Biomedical Informatics","year":2023,"referenceCount":148,"citationCount":5,"influentialCitationCount":0,"publicationDate":"14/07/2023","authors":"Zhaoyi Sun,Mingquan Lin,Qingqing Zhu,Qianqian Xie,Fei Wang,Zhiyong Lu,Yifan Peng","id":"baa1dc079d98ca76b0173c8d653fed759fd0a371","summary":"This study reviewed the current uses of multimodal deep learning on five tasks: report generation, Visual question answering, Cross-modal retrieval, computer-aided diagnosis, and Semantic segmentation, and highlighted the diverse applications and potential of MDL.","score":3},{"url":"https://www.semanticscholar.org/paper/df0ddb588a200d095743e9d26fc4a9318619766e","title":"Towards Generalist Foundation Model for Radiology","venue":"arXiv.org","year":2023,"referenceCount":59,"citationCount":26,"influentialCitationCount":2,"publicationDate":"04/08/2023","authors":"Chaoyi Wu,Xiaoman Zhang,Ya Zhang,Yanfeng Wang,Weidi Xie","id":"df0ddb588a200d095743e9d26fc4a9318619766e","summary":"This study constructs a large-scale Medical Multi-modal Dataset, MedMD, which consists of 16M 2D and 3D medical scans with high-quality text descriptions or reports across various data formats, modalities, and tasks, covering over 5000 distinct diseases, and proposes a new evaluation benchmark, RadBench, aiming to comprehensively assess the capability of foundation models in handling practical clinical problems.","score":3},{"url":"https://www.semanticscholar.org/paper/f0950a3f27c0fefffba60ae1c9a8ee360d5eb55f","title":"Instruction Tuning for Large Language Models: A Survey","venue":"arXiv.org","year":2023,"referenceCount":150,"citationCount":101,"influentialCitationCount":3,"publicationDate":"21/08/2023","authors":"Shengyu Zhang,Linfeng Dong,Xiaoya Li,Sen Zhang,Xiaofei Sun,Shuhe Wang,Jiwei Li,Runyi Hu,Tianwei Zhang,Fei Wu,Guoyin Wang","id":"f0950a3f27c0fefffba60ae1c9a8ee360d5eb55f","summary":"A systematic review of the literature, including the general methodology of IT, the construction of IT datasets, the training of IT models, and applications to different modalities, domains and applications, along with an analysis on aspects that influence the outcome of IT.","score":3},{"url":"https://www.semanticscholar.org/paper/a0476578761e983d5ab2083abab07b81236c1d58","title":"Asymmetric cross-modal attention network with multimodal augmented mixup for medical visual question answering","venue":"Artif. Intell. Medicine","year":2023,"referenceCount":59,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/10/2023","authors":"Yong Li,Qihao Yang,Fu Lee Wang,Lap-Kei Lee,Yingying Qu,Tianyong Hao","id":"a0476578761e983d5ab2083abab07b81236c1d58","summary":"A new Asymmetric Cross Modal Attention network called ACMA is proposed, which constructs an image- guided attention and a question-guided attention to improve multimodal interactions from insufficient data.","score":3},{"url":"https://www.semanticscholar.org/paper/da9134f694959b68027c33c8e998ffb3d41305da","title":"Exploring Question Decomposition for Zero-Shot VQA","venue":"Neural Information Processing Systems","year":2023,"referenceCount":60,"citationCount":0,"influentialCitationCount":0,"publicationDate":"25/10/2023","authors":"Zaid Khan,B. Vijaykumar,S. Schulter,Manmohan Chandraker,Yun Fu","id":"da9134f694959b68027c33c8e998ffb3d41305da","summary":"A model-driven selective decomposition approach for second-guessing predictions and correcting errors is introduced, and its effectiveness on eight VQA tasks across three domains is validated, showing consistent improvements in accuracy.","score":3},{"url":"https://www.semanticscholar.org/paper/749104d1a207f5bc192c7d95a12856b5e7f84d1f","title":"Mapping medical image-text to a joint space via masked modeling","venue":"Medical Image Anal.","year":2023,"referenceCount":43,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/11/2023","authors":"Zhihong Chen,Yuhao Du,Jinpeng Hu,Yang Liu,Guanbin Li,Xiang Wan,Tsung-Hui Chang","id":"749104d1a207f5bc192c7d95a12856b5e7f84d1f","summary":"A self-supervised learning paradigm, multi-modal masked autoencoders (M3AE) is introduced, which learns to map medical images and texts to a joint space by reconstructing pixels and tokens from randomly masked images andtext.","score":3},{"url":"https://www.semanticscholar.org/paper/8d2709ed1788a67e64425fb410bb49f3ee49e088","title":"Large Language Models Illuminate a Progressive Pathway to Artificial Healthcare Assistant: A Review","venue":"arXiv.org","year":2023,"referenceCount":186,"citationCount":1,"influentialCitationCount":0,"publicationDate":"03/11/2023","authors":"Mingze Yuan,Peng Bao,J. Yuan,Yunhao Shen,Zi Chen,Yi Xie,Jie Zhao,Yang Chen,Li Zhang,Lin Shen,Bin Dong","id":"8d2709ed1788a67e64425fb410bb49f3ee49e088","summary":"This review offers an extensive analysis on the transformative potential of LLMs in modern medicine and highlights the pivotal need for continuous optimizations and ethical oversight before these models can be effectively integrated into clinical practice.","score":3},{"url":"https://www.semanticscholar.org/paper/352252231462c24440bc0016638ea5fe8d4c6f7e","title":"UniDCP: Unifying Multiple Medical Vision-language Tasks via Dynamic Cross-modal Learnable Prompts","venue":"arXiv.org","year":2023,"referenceCount":67,"citationCount":0,"influentialCitationCount":0,"publicationDate":"18/12/2023","authors":"Chenlu Zhan,Yufei Zhang,Yu Lin,Gaoang Wang,Hongwei Wang","id":"352252231462c24440bc0016638ea5fe8d4c6f7e","summary":"UniDCP is the first Med-VLP model capable of performing all 8 medical uni-modal and cross-modal tasks over 14 corresponding datasets, consistently yielding superior results over diverse state-of-the-art methods.","score":3},{"url":"https://www.semanticscholar.org/paper/63de69245502d9a22de04581a4b5c0168d596aa3","title":"Generalizing Visual Question Answering from Synthetic to Human-Written Questions via a Chain of QA with a Large Language Model","venue":"arXiv.org","year":2024,"referenceCount":26,"citationCount":0,"influentialCitationCount":0,"publicationDate":"12/01/2024","authors":"Taehee Kim,Yeongjae Cho,Heejun Shin,Yohan Jo,Dongmyung Shin","id":"63de69245502d9a22de04581a4b5c0168d596aa3","summary":"The effectiveness of CoQAH is tested on two types of human-written VQA datasets for 3D-rendered and chest X-ray images and found that it achieved state-of-the-art accuracy in both types of data.","score":3},{"url":"https://www.semanticscholar.org/paper/61cadcfa555cbef120df7c017ef02e87f19900b7","title":"Free Form Medical Visual Question Answering in Radiology","venue":"arXiv.org","year":2024,"referenceCount":48,"citationCount":0,"influentialCitationCount":0,"publicationDate":"23/01/2024","authors":"Abhishek Narayanan,Rushabh Musthyala,Rahul Sankar,Anirudh Prasad Nistala,P. Singh,Jacopo Cirrone","id":"61cadcfa555cbef120df7c017ef02e87f19900b7","summary":"This research delves into the effective representation of radiology images and the joint learning of multimodal representations, surpassing existing methods and innovatively augment the SLAKE dataset, enabling the model to respond to a more diverse array of questions.","score":3},{"url":"https://www.semanticscholar.org/paper/7580327ffc9bd5daef83fe8285c0476ca074051d","title":"OmniMedVQA: A New Large-Scale Comprehensive Evaluation Benchmark for Medical LVLM","venue":"arXiv.org","year":2024,"referenceCount":119,"citationCount":0,"influentialCitationCount":0,"publicationDate":"14/02/2024","authors":"Yutao Hu,Tian-Xin Li,Quanfeng Lu,Wenqi Shao,Junjun He,Yu Qiao,Ping Luo","id":"7580327ffc9bd5daef83fe8285c0476ca074051d","summary":"OmniMedVQA is introduced, a novel comprehensive medical Visual Question Answering (VQA) benchmark collected from 75 different medical datasets, including 12 different modalities and covering more than 20 distinct anatomical regions, calling for a more versatile and robust LVLM in the biomedical field.","score":3},{"url":"https://www.semanticscholar.org/paper/a3d418b4e35a02e4306505ab660a6bcd44c3c752","title":"Asclepius: A Spectrum Evaluation Benchmark for Medical Multi-Modal Large Language Models","venue":"arXiv.org","year":2024,"referenceCount":42,"citationCount":0,"influentialCitationCount":0,"publicationDate":"17/02/2024","authors":"Wenxuan Wang,Yihang Su,Jingyuan Huan,Jie Liu,Wenting Chen,Yudi Zhang,Cheng-Yi Li,Kao-Jung Chang,Xiaohan Xin,LinLin Shen,Michael R. Lyu","id":"a3d418b4e35a02e4306505ab660a6bcd44c3c752","summary":"This work introduces Asclepius, a novel Med-MLLM benchmark that rigorously and comprehensively assesses model capability in terms of distinct medical specialties and different diagnostic capacities, and sets a precedent for future evaluations and the safe deployment of these models in clinical environments.","score":3},{"url":"https://www.semanticscholar.org/paper/bca0bbd01ea917b7a9fe369288ea3ba03d3b1ff3","title":"A Survey of Large Language Models in Medicine: Progress, Application, and Challenge","venue":"arXiv.org","year":2023,"referenceCount":183,"citationCount":5,"influentialCitationCount":0,"publicationDate":"09/11/2023","authors":"Hongjian Zhou,Boyang Gu,Xinyu Zou,Yiru Li,Sam S. Chen,Peilin Zhou,Junling Liu,Y. Hua,Chengfeng Mao,Xian Wu,Zheng Li,Fenglin Liu","id":"bca0bbd01ea917b7a9fe369288ea3ba03d3b1ff3","summary":"A detailed overview of the development and deployment of LLMs in medicine, including the challenges and opportunities they face, is provided and a detailed introduction to the principles of existing medical LLMs are provided, including their basic model structures, number of parameters, and sources and scales of data used for model development.","score":2},{"url":"https://www.semanticscholar.org/paper/3c50ef336232da0885ef61da386c98eac964b7cd","title":"PathAsst: A Generative Foundation AI Assistant Towards Artificial General Intelligence of Pathology","venue":"","year":2023,"referenceCount":41,"citationCount":10,"influentialCitationCount":2,"publicationDate":"24/05/2023","authors":"Yuxuan Sun,Chenglu Zhu,S. Zheng,Kai Zhang,Zhongyi Shui,Xiaoxuan Yu,Yi-Lei Zhao,Honglin Li,Yunlong Zhang,Ruojia Zhao,Xinheng Lyu,Lin Yang","id":"3c50ef336232da0885ef61da386c98eac964b7cd","summary":"The experimental results of PathAsst show the potential of harnessing AI-powered generative foundation model to improve pathology diagnosis and treatment processes.","score":2},{"url":"https://www.semanticscholar.org/paper/a3711dbf296b5ddd97ba93826660cd3995611625","title":"Towards A Foundation Model for Generalist Robots: Diverse Skill Learning at Scale via Automated Task and Scene Generation","venue":"arXiv.org","year":2023,"referenceCount":115,"citationCount":4,"influentialCitationCount":0,"publicationDate":2023,"authors":"Zhou Xian,Th√©ophile Gervet,Zhenjia Xu,Yi-Ling Qiao,Tsun-Hsuan Wang","id":"a3711dbf296b5ddd97ba93826660cd3995611625","summary":null,"score":2},{"url":"https://www.semanticscholar.org/paper/692bc40edf4785d88c39e0c0fe9f270541fecf8a","title":"Towards Generalist Robots: A Promising Paradigm via Generative Simulation","venue":"","year":2023,"referenceCount":124,"citationCount":3,"influentialCitationCount":0,"publicationDate":"17/05/2023","authors":"Zhou Xian,Th√©ophile Gervet,Zhenjia Xu,Yi-Ling Qiao,Tsun-Hsuan Wang,Yian Wang","id":"692bc40edf4785d88c39e0c0fe9f270541fecf8a","summary":"This document presents a specific idea for mining knowledge in the latest large-scale foundation models for robotics research, which uses a fully automated generative pipeline which uses these models to generate diversified tasks, scenes and training supervisions at scale, thereby scaling up low-level skill learning and ultimately leading to a foundation model for robotics that empowers generalist robots.","score":2},{"url":"https://www.semanticscholar.org/paper/e45036dddb5f27d3e87d2f14a2d9e6a402e7b5b7","title":"SheetCopilot: Bringing Software Productivity to the Next Level through Large Language Models","venue":"Neural Information Processing Systems","year":2023,"referenceCount":93,"citationCount":11,"influentialCitationCount":2,"publicationDate":"30/05/2023","authors":"Hongxin Li,Jingran Su,Yuntao Chen,Qing Li,Zhaoxiang Zhang","id":"e45036dddb5f27d3e87d2f14a2d9e6a402e7b5b7","summary":"This work proposes a SheetCopilot agent that takes natural language task and control spreadsheet to fulfill the requirements, and proposes a set of atomic actions as an abstraction of spreadsheet software functionalities.","score":2},{"url":"https://www.semanticscholar.org/paper/28c6ac721f54544162865f41c5692e70d61bccab","title":"A Survey on Large Language Model based Autonomous Agents","venue":"arXiv.org","year":2023,"referenceCount":184,"citationCount":205,"influentialCitationCount":18,"publicationDate":"22/08/2023","authors":"Lei Wang,Chengbang Ma,Xueyang Feng,Zeyu Zhang,Hao-ran Yang,Jingsen Zhang,Zhi-Yang Chen,Jiakai Tang,Xu Chen,Yankai Lin,Wayne Xin Zhao,Zhewei Wei,Ji-rong Wen","id":"28c6ac721f54544162865f41c5692e70d61bccab","summary":"A systematic review of the field of LLM-based autonomous agents from a holistic perspective, and proposes a unified framework that encompasses a majority of the previous work.","score":2},{"url":"https://www.semanticscholar.org/paper/c62711f6b5d8620ba36bc2c378ec6ab53f6e197c","title":"RoboGen: Towards Unleashing Infinite Data for Automated Robot Learning via Generative Simulation","venue":"arXiv.org","year":2023,"referenceCount":127,"citationCount":10,"influentialCitationCount":0,"publicationDate":"02/11/2023","authors":"Yufei Wang,Zhou Xian,Feng Chen,Tsun-Hsuan Wang,Yian Wang,Zackory M. Erickson,David Held,Chuang Gan","id":"c62711f6b5d8620ba36bc2c378ec6ab53f6e197c","summary":"RoboGen is presented, a generative robotic agent that automatically learns diverse robotic skills at scale via generative simulation and attempts to extract the extensive and versatile knowledge embedded in large-scale models and transfer them to the field of robotics.","score":2},{"url":"https://www.semanticscholar.org/paper/8ec7d50250203543a0098d99f04957b22bbe2c77","title":"How to Bridge the Gap between Modalities: A Comprehensive Survey on Multimodal Large Language Model","venue":"arXiv.org","year":2023,"referenceCount":117,"citationCount":1,"influentialCitationCount":0,"publicationDate":"10/11/2023","authors":"Shezheng Song,Xiaopeng Li,Shasha Li","id":"8ec7d50250203543a0098d99f04957b22bbe2c77","summary":"This paper aims to explore modality alignment methods for LLMs and their existing capabilities, and surveys existing modal alignment methods in MLLMs into four groups: Multimodal Converters that change data into something LLMs can understand, and Data-Driven methods that teach LLMs to understand specific types of data in a dataset.","score":2},{"url":"https://www.semanticscholar.org/paper/cf7d69709bdeddd561c183178bbc1f0c2e156a08","title":"Analyzing Modular Approaches for Visual Question Decomposition","venue":"Conference on Empirical Methods in Natural Language Processing","year":2023,"referenceCount":41,"citationCount":0,"influentialCitationCount":0,"publicationDate":"10/11/2023","authors":"Apoorv Khandelwal,Ellie Pavlick,Chen Sun","id":"cf7d69709bdeddd561c183178bbc1f0c2e156a08","summary":"ViperGPT's reported gains over BLIP-2 can be attributed to its selection of task-specific modules, and when it is run using a more task-agnostic selection of modules, these gains go away, but on some benchmarks, modular approaches significantly benefit by representing subtasks with natural language, instead of code.","score":2},{"url":"https://www.semanticscholar.org/paper/ef321c6f174ac59916ac54ec40ad18bca5b58e5c","title":"PerceptionGPT: Effectively Fusing Visual Perception into LLM","venue":"arXiv.org","year":2023,"referenceCount":50,"citationCount":3,"influentialCitationCount":0,"publicationDate":"11/11/2023","authors":"Renjie Pi,Lewei Yao,Jiahui Gao,Jipeng Zhang,Tong Zhang","id":"ef321c6f174ac59916ac54ec40ad18bca5b58e5c","summary":"A novel end-to-end framework named PerceptionGPT, which efficiently and effectively equips the VLLMs with visual perception abilities by leveraging the representation power of LLMs' token embedding and demonstrates significant improvements over previous methods with much fewer trainable parameters and GPU hours.","score":2},{"url":"https://www.semanticscholar.org/paper/f32ea390686b1eee3ba5b53c7a85e9e9385d4b94","title":"Visual Program Distillation: Distilling Tools and Programmatic Reasoning into Vision-Language Models","venue":"arXiv.org","year":2023,"referenceCount":59,"citationCount":0,"influentialCitationCount":0,"publicationDate":"05/12/2023","authors":"Yushi Hu,Otilia Stretcu,Chun-Ta Lu,Krishnamurthy Viswanathan,K. Hata,Enming Luo,Ranjay Krishna,Ariel Fuxman","id":"f32ea390686b1eee3ba5b53c7a85e9e9385d4b94","summary":"Visual Program Distillation (VPD) is proposed, an instruction tuning framework that produces a vision-language model (VLM) capable of solving complex visual tasks with a single forward pass and improves the VLM's ability to count, understand spatial relations, and reason compositionally.","score":2},{"url":"https://www.semanticscholar.org/paper/5502d769595981009e43344f8914e287acca2359","title":"ModaVerse: Efficiently Transforming Modalities with LLMs","venue":"arXiv.org","year":2024,"referenceCount":56,"citationCount":0,"influentialCitationCount":0,"publicationDate":"12/01/2024","authors":"Xinyu Wang,Bohan Zhuang,Qi Wu","id":"5502d769595981009e43344f8914e287acca2359","summary":"ModaVerse is introduced, a Multi-modal Large Language Model (MLLM) capable of comprehending and transforming content across various modalities including images, videos, and audio, and a novel Input/Output (I/O) alignment mechanism that operates directly at the level of natural language.","score":2},{"url":"https://www.semanticscholar.org/paper/7e6c1bb54bb2e36cc1092b080e9928942f7f8a68","title":"TroVE: Inducing Verifiable and Efficient Toolboxes for Solving Programmatic Tasks","venue":"arXiv.org","year":2024,"referenceCount":33,"citationCount":0,"influentialCitationCount":0,"publicationDate":"23/01/2024","authors":"Zhiruo Wang,Daniel Fried,Graham Neubig","id":"7e6c1bb54bb2e36cc1092b080e9928942f7f8a68","summary":"TROVE is presented, a training-free method of inducing a verifiable and efficient toolbox of functions, by generating via using, growing, and periodically trimming the toolbox.","score":2},{"url":"https://www.semanticscholar.org/paper/5e7274bcda47b704b6797bb14be8b7a61c047a61","title":"Uncertainty-Aware Evaluation for Vision-Language Models","venue":"","year":2024,"referenceCount":64,"citationCount":0,"influentialCitationCount":0,"publicationDate":"22/02/2024","authors":"Vasily Kostumov,Bulat Nutfullin,Oleg Pilipenko,Eugene Ilyushin","id":"5e7274bcda47b704b6797bb14be8b7a61c047a61","summary":"It is shown that models with the highest accuracy may also have the highest uncertainty, which confirms the importance of measuring it for VLMs, and a correlation between model uncertainty and its language model part is revealed.","score":2},{"url":"https://www.semanticscholar.org/paper/1b5e69a5b0f179e90f356a9c8cc1a39f77471dab","title":"Selective\"Selective Prediction\": Reducing Unnecessary Abstention in Vision-Language Reasoning","venue":"","year":2024,"referenceCount":38,"citationCount":0,"influentialCitationCount":0,"publicationDate":"23/02/2024","authors":"Tejas Srinivasan,Jack Hessel,Tanmay Gupta,Bill Yuchen Lin,Yejin Choi,Jesse Thomason,Khyathi Raghavi Chandu","id":"1b5e69a5b0f179e90f356a9c8cc1a39f77471dab","summary":"ReCoVERR, an inference-time algorithm to reduce the over-abstention of a selective vision-language system without decreasing prediction accuracy, enables two VLMs, BLIP2 and InstructBLIP, to answer up to 20% more questions on the A-OKVQA task than vanilla selective prediction without decreasing system accuracy, thus improving overall system reliability.","score":2},{"url":"https://www.semanticscholar.org/paper/44ccf252018f71898d52d89539f17d77a4f8d548","title":"Chart Understanding with Large Language Model","venue":"","year":null,"referenceCount":39,"citationCount":0,"influentialCitationCount":0,"publicationDate":null,"authors":"Yaser James,Will Li,John Feng","id":"44ccf252018f71898d52d89539f17d77a4f8d548","summary":"A baseline multimodal model is introduced that integrates text and charts to enhance the chart comprehension capabilities of existing models, offering more pertinent insights and information related to the depicted charts.","score":2},{"url":"https://www.semanticscholar.org/paper/96a6df2b4aa50cfbd8984933e9c66b0763fc08a6","title":"Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V","venue":"","year":2023,"referenceCount":88,"citationCount":26,"influentialCitationCount":5,"publicationDate":2023,"authors":"Jianwei Yang,Hao Zhang,Feng Li,Xueyan Zou,Chun-yue Li,Jianfeng Gao","id":"96a6df2b4aa50cfbd8984933e9c66b0763fc08a6","summary":null,"score":2},{"url":"https://www.semanticscholar.org/paper/ed9943d73eb42116fe33564b5065c78b5ca0b16e","title":"RestGPT: Connecting Large Language Models with Real-World Applications via RESTful APIs","venue":"arXiv.org","year":2023,"referenceCount":20,"citationCount":23,"influentialCitationCount":1,"publicationDate":2023,"authors":"Yifan Song,Weimin Xiong,Dawei Zhu,Cheng Li,Ke Wang,Ye Tian,Sujian Li","id":"ed9943d73eb42116fe33564b5065c78b5ca0b16e","summary":"This paper introduces RestGPT, which leverages LLMs to solve user requests by connecting with RESTful APIs and proposes a coarse-to-fine online planning mechanism to enhance the ability of planning and API selection.","score":2},{"url":"https://www.semanticscholar.org/paper/d1120d67b700e4dfe8b39eb1e48fbdea4e1a0c43","title":"HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face","venue":"Neural Information Processing Systems","year":2023,"referenceCount":43,"citationCount":409,"influentialCitationCount":48,"publicationDate":2023,"authors":"Yongliang Shen,Kaitao Song,Xu Tan,D. Li,Weiming Lu,Y. Zhuang","id":"d1120d67b700e4dfe8b39eb1e48fbdea4e1a0c43","summary":"HuggingGPT is an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities to solve AI tasks and can tackle a wide range of sophisticated AI tasks spanning different modalities and domains.","score":2},{"url":"https://www.semanticscholar.org/paper/13b5b69355555e0c8b702261c5de3b4172ba653c","title":"The Art of SOCRATIC QUESTIONING: Zero-shot Multimodal Reasoning with Recursive Thinking and Self-Questioning","venue":"arXiv.org","year":2023,"referenceCount":27,"citationCount":8,"influentialCitationCount":0,"publicationDate":2023,"authors":"Jingyuan Qi,Zhiyang Xu,Ying Shen,Minqian Liu,dingnan jin,Qifan Wang,Lifu Huang","id":"13b5b69355555e0c8b702261c5de3b4172ba653c","summary":"Qualitative analysis clearly demonstrates the intermediate thinking steps elicited by S OCRATIC Q UESTIONING are similar to the human‚Äôs recursively thinking process of a complex reasoning problem.","score":2},{"url":"https://www.semanticscholar.org/paper/53df959bcf6499c45e316086a96a624389a39a52","title":"Accountable Textual-Visual Chat Learns to Reject Human Instructions in Image Re-creation","venue":"","year":2023,"referenceCount":130,"citationCount":0,"influentialCitationCount":0,"publicationDate":"10/03/2023","authors":"Zhiwei Zhang,Yuliang Liu","id":"53df959bcf6499c45e316086a96a624389a39a52","summary":"This paper introduces two novel multimodal datasets: the synthetic CLEVR-ATVC dataset (620K) and the manually pictured Fruit-ATVC dataset (50K) and introduces specific rules as supervisory signals within the datasets to facilitate the accountability of multimodal systems in rejecting human requests.","score":2},{"url":"https://www.semanticscholar.org/paper/ac7771c332da42b29a913b116bd6ef622cbf89cf","title":"TaskMatrix.AI: Completing Tasks by Connecting Foundation Models with Millions of APIs","venue":"Intelligent Computing","year":2023,"referenceCount":30,"citationCount":107,"influentialCitationCount":4,"publicationDate":"29/03/2023","authors":"Yaobo Liang,Chenfei Wu,Ting Song,Wenshan Wu,Yan Xia,Yu Liu,Yangyiwen Ou,Shuai Lu,Lei Ji,Shaoguang Mao,Yuntao Wang,Linjun Shou,Ming Gong,Nan Duan","id":"ac7771c332da42b29a913b116bd6ef622cbf89cf","summary":"The vision of how to build such an ecosystem is presented, each key component is explained, and study cases are used to illustrate both the feasibility of this vision and the main challenges the authors need to address next.","score":2},{"url":"https://www.semanticscholar.org/paper/352420ee61a8da783ca7750170793613b18b8d9c","title":"Tool Learning with Foundation Models","venue":"arXiv.org","year":2023,"referenceCount":238,"citationCount":107,"influentialCitationCount":6,"publicationDate":"17/04/2023","authors":"Yujia Qin,Shengding Hu,Yankai Lin,Weize Chen,Ning Ding,Ganqu Cui,Zheni Zeng,Yufei Huang,Chaojun Xiao,Chi Han,Y. Fung,Yusheng Su,Huadong Wang,Cheng Qian,Runchu Tian,Kunlun Zhu,Shi Liang,Xingyu Shen,Bokai Xu,Zhen Zhang,Yining Ye,Bo Li,Ziwei Tang,Jing Yi,Yu Zhu,Zhenning Dai,Lan Yan,Xin Cong,Ya-Ting Lu,Weilin Zhao,Yuxiang Huang,Jun-Han Yan,Xu Han,Xian Sun,Dahai Li,Jason Phang,Cheng Yang,Tongshuang Wu,Heng Ji,Zhiyuan Liu,Maosong Sun","id":"352420ee61a8da783ca7750170793613b18b8d9c","summary":"A systematic investigation of tool learning is presented, including its cognitive origins, the paradigm shift of foundation models, and the complementary roles of tools and models to inspire future research in integrating tools with foundation models.","score":2},{"url":"https://www.semanticscholar.org/paper/7e32aac43e9f1df49e116add03327ee6f365dbf3","title":"mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality","venue":"arXiv.org","year":2023,"referenceCount":36,"citationCount":325,"influentialCitationCount":40,"publicationDate":"27/04/2023","authors":"Qinghao Ye,Haiyang Xu,Guohai Xu,Jiabo Ye,Ming Yan,Yi Zhou,Junyan Wang,Anwen Hu,Pengcheng Shi,Yaya Shi,Chenliang Li,Yuanhong Xu,Hehong Chen,Junfeng Tian,Qiang Qi,Ji Zhang,Feiyan Huang","id":"7e32aac43e9f1df49e116add03327ee6f365dbf3","summary":null,"score":2},{"url":"https://www.semanticscholar.org/paper/54a8b153ed04a872da878d695239bdc413dc782c","title":"InternGPT: Solving Vision-Centric Tasks by Interacting with Chatbots Beyond Language","venue":"arXiv.org","year":2023,"referenceCount":83,"citationCount":39,"influentialCitationCount":0,"publicationDate":"09/05/2023","authors":"Zhaoyang Liu,Yinan He,Wenhai Wang,Weiyun Wang,Yi Wang,Shoufa Chen,Qing-Long Zhang,Yang Yang,Qingyun Li,Jiashuo Yu,Kunchang Li,Zhe Chen,Xuecheng Yang,Xizhou Zhu,Yali Wang,Limin Wang,Ping Luo,Jifeng Dai,Yu Qiao","id":"54a8b153ed04a872da878d695239bdc413dc782c","summary":"By incorporating pointing instructions, the proposed iGPT significantly improves the efficiency of communication between users and chatbots, as well as the accuracy of chatbots in vision-centric tasks, especially in complicated visual scenarios where the number of objects is greater than 2.","score":2},{"url":"https://www.semanticscholar.org/paper/d48cb91b9e555194f7494c4d4bb9815021d3ee45","title":"VideoChat: Chat-Centric Video Understanding","venue":"arXiv.org","year":2023,"referenceCount":65,"citationCount":130,"influentialCitationCount":21,"publicationDate":"10/05/2023","authors":"Kunchang Li,Yinan He,Yi Wang,Yizhuo Li,Wen Wang,Ping Luo,Yali Wang,Limin Wang,Yu Qiao","id":"d48cb91b9e555194f7494c4d4bb9815021d3ee45","summary":"An end-to-end chat-centric video understanding system that integrates video foundation models and large language models via a learnable neural interface, excelling in spatiotemporal reasoning, event localization, and causal relationship inference is initiated.","score":2},{"url":"https://www.semanticscholar.org/paper/42a30dc5470f54ec249f25d3c31e05d7c376c8e3","title":"VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks","venue":"Neural Information Processing Systems","year":2023,"referenceCount":81,"citationCount":127,"influentialCitationCount":7,"publicationDate":"18/05/2023","authors":"Wen Wang,Zhe Chen,Xiaokang Chen,Jiannan Wu,Xizhou Zhu,Gang Zeng,Ping Luo,Tong Lu,Jie Zhou,Y. Qiao,Jifeng Dai","id":"42a30dc5470f54ec249f25d3c31e05d7c376c8e3","summary":"This work presents an LLM-based framework for vision-centric tasks, termed VisionLLM, which provides a unified perspective for vision and language tasks by treating images as a foreign language and aligning vision-focused tasks with language tasks that can be flexibly defined and managed using language instructions.","score":2},{"url":"https://www.semanticscholar.org/paper/2195676f111ad492c50f4d4c96abb2bd3d72f7fc","title":"Instruct2Act: Mapping Multi-modality Instructions to Robotic Actions with Large Language Model","venue":"arXiv.org","year":2023,"referenceCount":55,"citationCount":33,"influentialCitationCount":3,"publicationDate":"18/05/2023","authors":"Siyuan Huang,Zhengkai Jiang,Hao Dong,Y. Qiao,Peng Gao,Hongsheng Li","id":"2195676f111ad492c50f4d4c96abb2bd3d72f7fc","summary":"This paper presents Instruct2Act, a framework that utilizes Large Language Models to map multi-modal instructions to sequential actions for robotic manipulation tasks, employing the LLM model to generate Python programs that constitute a comprehensive perception, planning, and action loop for robotic tasks.","score":2},{"url":"https://www.semanticscholar.org/paper/ca055cfb9d4d47124cc035c346f38577825fcacf","title":"Enhance Reasoning Ability of Visual-Language Models via Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":40,"citationCount":0,"influentialCitationCount":0,"publicationDate":"22/05/2023","authors":"Yueting Yang,Xintong Zhang,Wenjuan Han","id":"ca055cfb9d4d47124cc035c346f38577825fcacf","summary":"A method called TReE is proposed, which transfers the reasoning ability of a large language model to a visual language model in zero-shot scenarios, and contains three stages: observation, thinking, and re-thinking.","score":2},{"url":"https://www.semanticscholar.org/paper/6fe2a4f7594abadafc9feb05e96ab7e9e634ce8f","title":"Album Storytelling with Iterative Story-aware Captioning and Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":58,"citationCount":3,"influentialCitationCount":1,"publicationDate":"22/05/2023","authors":"Munan Ning,Yujia Xie,Dongdong Chen,Zeyin Song,Lu Yuan,Yonghong Tian,Qixiang Ye,Liuliang Yuan","id":"6fe2a4f7594abadafc9feb05e96ab7e9e634ce8f","summary":"This work proposes a new iterative album storytelling pipeline, which starts with an initial story and builds a story-aware caption model to refine the captions using the whole story as guidance, then feeds into the LLMs to generate a new refined story.","score":2},{"url":"https://www.semanticscholar.org/paper/8da9b1436212b233fc49c7daf1ba15c22874ff5a","title":"CREATOR: Tool Creation for Disentangling Abstract and Concrete Reasoning of Large Language Models","venue":"Conference on Empirical Methods in Natural Language Processing","year":2023,"referenceCount":36,"citationCount":5,"influentialCitationCount":1,"publicationDate":"23/05/2023","authors":"Cheng Qian,Chi Han,Y. Fung,Yujia Qin,Zhiyuan Liu,Heng Ji","id":"8da9b1436212b233fc49c7daf1ba15c22874ff5a","summary":"The proposed CREATOR, a novel framework that enables LLMs to create their own tools using documentation and code realization, disentangles abstract tool creation and concrete decision execution, resulting in improved performance.","score":2},{"url":"https://www.semanticscholar.org/paper/90027ca7802645671a69b00b65e1fa94e6b63544","title":"ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models","venue":"arXiv.org","year":2023,"referenceCount":41,"citationCount":34,"influentialCitationCount":2,"publicationDate":"23/05/2023","authors":"Binfeng Xu,Zhiyuan Peng,Bowen Lei,Subhabrata Mukherjee,Yuchen Liu,Dongkuan Xu","id":"90027ca7802645671a69b00b65e1fa94e6b63544","summary":"This study proposes a modular paradigm ReWOO (Reasoning WithOut Observation) that detaches the reasoning process from external observations, thus significantly reducing token consumption and demonstrating robustness under tool-failure scenarios.","score":2},{"url":"https://www.semanticscholar.org/paper/69335077fcacbff7a7cf25697da1949e6bdfa968","title":"The Art of SOCRATIC QUESTIONING: Recursive Thinking with Large Language Models","venue":"Conference on Empirical Methods in Natural Language Processing","year":2023,"referenceCount":44,"citationCount":3,"influentialCitationCount":1,"publicationDate":"24/05/2023","authors":"Jingyuan Qi,Zhiyang Xu,Ying Shen,Minqian Liu,dingnan jin,Qifan Wang,Lifu Huang","id":"69335077fcacbff7a7cf25697da1949e6bdfa968","summary":"The qualitative analysis clearly shows that the intermediate reasoning steps elicited by SOCRATIC QUESTIONING are similar to humans' recursively thinking process of complex reasoning problems.","score":2},{"url":"https://www.semanticscholar.org/paper/00cb69a9f280317d1c59ac5827551ee9b10642b8","title":"EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought","venue":"Neural Information Processing Systems","year":2023,"referenceCount":73,"citationCount":64,"influentialCitationCount":3,"publicationDate":"24/05/2023","authors":"Yao Mu,Qinglong Zhang,Mengkang Hu,Wen Wang,Mingyu Ding,Jun Jin,Bin Wang,Jifeng Dai,Y. Qiao,Ping Luo","id":"00cb69a9f280317d1c59ac5827551ee9b10642b8","summary":"This work introduces EmbodiedGPT, an end-to-end multi-modal foundation model for embodied AI, empowering embodied agents with multi- modal understanding and execution capabilities, and significantly enhances the success rate of the embodied control task by extracting more effective features.","score":2},{"url":"https://www.semanticscholar.org/paper/9c3a9b4821daa03cb5369041d59d2714329a3811","title":"Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models","venue":"Neural Information Processing Systems","year":2023,"referenceCount":53,"citationCount":36,"influentialCitationCount":4,"publicationDate":"24/05/2023","authors":"Gen Luo,Yiyi Zhou,Tianhe Ren,Shen Chen,Xiaoshuai Sun,Rongrong Ji","id":"9c3a9b4821daa03cb5369041d59d2714329a3811","summary":"A novel and affordable solution for the effective VL adaption of LLMs, called Mixture-of-Modality Adaptation (MMA), which adopts lightweight modules, i.e., adapters, to bridge the gap between LLMs and VL tasks, which also enables the joint optimization of the image and language models","score":2},{"url":"https://www.semanticscholar.org/paper/c6ac708b65b24c20f80831d518c1795ce8133ad5","title":"ChatBridge: Bridging Modalities with Large Language Model as a Language Catalyst","venue":"arXiv.org","year":2023,"referenceCount":72,"citationCount":23,"influentialCitationCount":4,"publicationDate":"25/05/2023","authors":"Zijia Zhao,Longteng Guo,Tongtian Yue,Si-Qing Chen,Shuai Shao,Xinxin Zhu,Zehuan Yuan,Jing Liu","id":"c6ac708b65b24c20f80831d518c1795ce8133ad5","summary":"It is shown that only language-paired two-modality data is sufficient to connect all modalities and ChatBridge, a novel multimodal language model that leverages the expressive capabilities of language as the catalyst to bridge the gap between various modalities, is presented.","score":2},{"url":"https://www.semanticscholar.org/paper/50c1414fe41d0cb9db6f0933c9319aa124beac5d","title":"Contextual Object Detection with Multimodal Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":87,"citationCount":22,"influentialCitationCount":0,"publicationDate":"29/05/2023","authors":"Yuhang Zang,Wei Li,Jun Han,Kaiyang Zhou,Chen Change Loy","id":"50c1414fe41d0cb9db6f0933c9319aa124beac5d","summary":"The ContextDET is presented, a unified multimodal model that is capable of end-to-end differentiable modeling of visual-language contexts, so as to locate, identify, and associate visual objects with language inputs for human-AI interaction.","score":2},{"url":"https://www.semanticscholar.org/paper/b458fc5261595f44b36325e5eaea1f874d65138f","title":"GPT4Tools: Teaching Large Language Model to Use Tools via Self-instruction","venue":"Neural Information Processing Systems","year":2023,"referenceCount":66,"citationCount":69,"influentialCitationCount":9,"publicationDate":"30/05/2023","authors":"Rui Yang,Lin Song,Yanwei Li,Sijie Zhao,Yixiao Ge,Xiu Li,Ying Shan","id":"b458fc5261595f44b36325e5eaea1f874d65138f","summary":"The GPT4Tools based on self-instruct to enable open-source LLMs, such as LLaMA and OPT, to use tools, generates an instruction-following dataset by prompting an advanced teacher with various multi-modal contexts using the Low-Rank Adaptation (LoRA) optimization.","score":2},{"url":"https://www.semanticscholar.org/paper/615962d8969c8e0ffe43319689dce6c50cbf1f29","title":"Responsible Task Automation: Empowering Large Language Models as Responsible Task Automators","venue":"arXiv.org","year":2023,"referenceCount":46,"citationCount":4,"influentialCitationCount":1,"publicationDate":"02/06/2023","authors":"Zhizheng Zhang,Xiaoyi Zhang,Wenxuan Xie,Yan Lu","id":"615962d8969c8e0ffe43319689dce6c50cbf1f29","summary":"This paper presents Responsible Task Automation (ResponsibleTA) as a fundamental framework to facilitate responsible collaboration between LLM-based coordinators and executors for task automation with three empowered capabilities: predicting the feasibility of the commands for executors, verifying the completeness of executors and enhancing the security.","score":2},{"url":"https://www.semanticscholar.org/paper/d47524cd5c3c4b57af2e5a29f6f91c420310f236","title":"MIMIC-IT: Multi-Modal In-Context Instruction Tuning","venue":"arXiv.org","year":2023,"referenceCount":55,"citationCount":75,"influentialCitationCount":10,"publicationDate":"08/06/2023","authors":"Bo Li,Yuanhan Zhang,Liangyu Chen,Jinghao Wang,Fanyi Pu,Jingkang Yang,C. Li,Ziwei Liu","id":"d47524cd5c3c4b57af2e5a29f6f91c420310f236","summary":"MultI-Modal In-Context Instruction Tuning (MIMIC-IT), a dataset comprising 2.8 million multimodal instruction-response pairs, with 2.2 million unique instructions derived from images and videos, is presented and a large VLM named Otter is trained.","score":2},{"url":"https://www.semanticscholar.org/paper/ed30969f0e4811473144ffe83c1baa6d54f02202","title":"RestGPT: Connecting Large Language Models with Real-World RESTful APIs","venue":"","year":2023,"referenceCount":25,"citationCount":12,"influentialCitationCount":1,"publicationDate":"11/06/2023","authors":"Yifan Song,Weimin Xiong,Dawei Zhu,Wenhao Wu,Han Qian,Mingbo Song,Hailiang Huang,Cheng Li,Ke Wang,Rong Yao,Ye Tian,Sujian Li","id":"ed30969f0e4811473144ffe83c1baa6d54f02202","summary":"This paper proposes RestGPT, which exploits the power of LLMs and conducts a coarse-to-fine online planning mechanism to enhance the abilities of task decomposition and API selection and paves a new way towards AGI.","score":2},{"url":"https://www.semanticscholar.org/paper/4c4d176c6e28f48041f215d563f6ee8633534cff","title":"Valley: Video Assistant with Large Language model Enhanced abilitY","venue":"arXiv.org","year":2023,"referenceCount":39,"citationCount":40,"influentialCitationCount":9,"publicationDate":"12/06/2023","authors":"Ruipu Luo,Ziwang Zhao,Min Yang,Junwei Dong,Ming-Hui Qiu,Pengcheng Lu,Tao Wang,Zhongyu Wei","id":"4c4d176c6e28f48041f215d563f6ee8633534cff","summary":"A novel multi-modal foundation model capable of comprehending video, image, and language within a general framework is developed, and Qualitative experiments demonstrate that Valley has the potential to function as a highly effective video assistant that can make complex video understanding scenarios easy.","score":2},{"url":"https://www.semanticscholar.org/paper/473eb062612a17c965eaa62136322f0dec6b1f8e","title":"Data-Copilot: Bridging Billions of Data and Humans with Autonomous Workflow","venue":"arXiv.org","year":2023,"referenceCount":37,"citationCount":21,"influentialCitationCount":1,"publicationDate":"12/06/2023","authors":"Wenqi Zhang,Yongliang Shen,Weiming Lu,Y. Zhuang","id":"473eb062612a17c965eaa62136322f0dec6b1f8e","summary":"This work proposes Data-Copilot, an LLM-based system that connects numerous data sources on one end and caters to diverse human demands on the other end, and autonomously transforms raw data into visualization results that best match the user's intent.","score":2},{"url":"https://www.semanticscholar.org/paper/ea566f87f6253bb2d32cf7b61cd3e2535a0c3f42","title":"mPLUG-DocOwl: Modularized Multimodal Large Language Model for Document Understanding","venue":"arXiv.org","year":2023,"referenceCount":37,"citationCount":29,"influentialCitationCount":5,"publicationDate":"04/07/2023","authors":"Jiabo Ye,Anwen Hu,Haiyang Xu,Qinghao Ye,Mingshi Yan,Yuhao Dan,Chenlin Zhao,Guohai Xu,Chenliang Li,Junfeng Tian,Qiang Qi,Ji Zhang,Feiyan Huang","id":"ea566f87f6253bb2d32cf7b61cd3e2535a0c3f42","summary":"Experimental results show that the proposed mPLUG-DocOwl model outperforms existing multi-modal models, demonstrating its strong ability of document understanding, and also generalizes well on various downstream tasks.","score":2},{"url":"https://www.semanticscholar.org/paper/ebddfdc5d845a788e8062eddbbf7a335737cb99b","title":"What Matters in Training a GPT4-Style Language Model with Multimodal Inputs?","venue":"arXiv.org","year":2023,"referenceCount":108,"citationCount":34,"influentialCitationCount":3,"publicationDate":"05/07/2023","authors":"Yan Zeng,Hanbo Zhang,Jiani Zheng,Jiangnan Xia,Guoqiang Wei,Yang Wei,Yuchen Zhang,Tao Kong","id":"ebddfdc5d845a788e8062eddbbf7a335737cb99b","summary":"Lynx is presented, which performs the most accurate multi-modal understanding while keeping the best multi- modal generation ability compared to existing open-sourced GPT4-style models.","score":2},{"url":"https://www.semanticscholar.org/paper/2e52178416b93a0a7dc1cb7d21d04c7e72c2ce0f","title":"ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring Instruction Tuning","venue":"arXiv.org","year":2023,"referenceCount":39,"citationCount":18,"influentialCitationCount":3,"publicationDate":"18/07/2023","authors":"Liang Zhao,En Yu,Zheng Ge,Jinrong Yang,Hao-Ran Wei,Hongyu Zhou,Jian‚ÄêYuan Sun,Yuang Peng,Runpei Dong,Chunrui Han,Xiangyu Zhang","id":"2e52178416b93a0a7dc1cb7d21d04c7e72c2ce0f","summary":"This study proposes ChatSpot, a unified end-to-end multimodal large language model that supports diverse forms of interactivity including mouse clicks, drag-and-drop, and drawing boxes, which provides a more flexible and seamless interactive experience.","score":2},{"url":"https://www.semanticscholar.org/paper/bbcd5cc4bf6c77282e88cae07f7f2adb1da818ca","title":"Testing the Depth of ChatGPT's Comprehension via Cross-Modal Tasks Based on ASCII-Art: GPT3.5's Abilities in Regard to Recognizing and Generating ASCII-Art Are Not Totally Lacking","venue":"arXiv.org","year":2023,"referenceCount":86,"citationCount":0,"influentialCitationCount":0,"publicationDate":"28/07/2023","authors":"David Bayani","id":"bbcd5cc4bf6c77282e88cae07f7f2adb1da818ca","summary":"This work examines GPT3.5's aptitude for visual tasks, where the inputs feature content provided as ASCII-art without overt distillation into a lingual summary, and conducts experiments analyzing the model's performance on image recognition tasks after various transforms typical in visual settings.","score":2},{"url":"https://www.semanticscholar.org/paper/ad113d8b6f8f6dace7abd12dc88af520aaaf3fc7","title":"LISA: Reasoning Segmentation via Large Language Model","venue":"arXiv.org","year":2023,"referenceCount":64,"citationCount":71,"influentialCitationCount":16,"publicationDate":"01/08/2023","authors":"Xin Lai,Zhuotao Tian,Yukang Chen,Yanwei Li,Yuhui Yuan,Shu Liu,Jiaya Jia","id":"ad113d8b6f8f6dace7abd12dc88af520aaaf3fc7","summary":"This work proposes a new segmentation task -- reasoning segmentation, designed to output a segmentation mask given a complex and implicit query text, and presents LISA, which inherits the language generation capabilities of the multi-modal Large Language Model (LLM) while also possessing the ability to produce segmentation masks.","score":2},{"url":"https://www.semanticscholar.org/paper/4f2be887e991efa85f7b874e7ab871080a745c39","title":"CAESURA: Language Models as Multi-Modal Query Planners","venue":"arXiv.org","year":2023,"referenceCount":30,"citationCount":0,"influentialCitationCount":0,"publicationDate":"07/08/2023","authors":"Matthias Urban,Carsten Binnig","id":"4f2be887e991efa85f7b874e7ab871080a745c39","summary":"This paper proposes Language-Model-Driven Query Planning, a new paradigm of query planning that uses Language Models to translate natural language queries into executable query plans that can contain complex operators that are able to process arbitrary modalities.","score":2},{"url":"https://www.semanticscholar.org/paper/d53945d4afb4528590d79e20de52883d29037e86","title":"FashionLOGO: Prompting Multimodal Large Language Models for Fashion Logo Embeddings","venue":"arXiv.org","year":2023,"referenceCount":44,"citationCount":0,"influentialCitationCount":0,"publicationDate":"17/08/2023","authors":"Yulin Su,Min Yang,Minghui Qiu,Jing Wang,Tao Wang","id":"d53945d4afb4528590d79e20de52883d29037e86","summary":"This work explores how MLLMs can improve logo embedding by prompting them to generate explicit textual knowledge through three types of prompts, including image OCR, brief captions, and detailed descriptions prompts, in a zero-shot setting and adopt a cross-attention transformer to enable image embedding queries to learn supplementary knowledge from textual embeddings automatically.","score":2},{"url":"https://www.semanticscholar.org/paper/eb5cf10406a8ad31e0ebe56b36571d5db4758a62","title":"PUMGPT: A Large Vision-Language Model for Product Understanding","venue":"arXiv.org","year":2023,"referenceCount":38,"citationCount":1,"influentialCitationCount":0,"publicationDate":"18/08/2023","authors":"Shuhui Wu,Zengming Tang,Zongyi Guo,Weiwei Zhang,Baoliang Cui,Haihong Tang,Weiming Lu","id":"eb5cf10406a8ad31e0ebe56b36571d5db4758a62","summary":"This paper presents PUMGPT, a large vision-language model that aims at unifying all product understanding tasks under a singular model structure, and proposes Layer-wise Adapters (LA), an approach that provides enhanced alignment with fewer visual tokens and enables parameter-efficient fine-tuning.","score":2},{"url":"https://www.semanticscholar.org/paper/ef1ebdb24ce45fb57e271783a0c9a877fe0e79c3","title":"Position-Enhanced Visual Instruction Tuning for Multimodal Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":34,"citationCount":21,"influentialCitationCount":2,"publicationDate":"25/08/2023","authors":"Chi Chen,Ruoyu Qin,Fuwen Luo,Xiaoyue Mi,Peng Li,Maosong Sun,Yang Liu","id":"ef1ebdb24ce45fb57e271783a0c9a877fe0e79c3","summary":"This paper proposes Position-enhanced Visual Instruction Tuning (PVIT), which extends the functionality of MLLMs by integrating an additional region-level vision encoder, which promotes a more detailed comprehension of images for the MLLM.","score":2},{"url":"https://www.semanticscholar.org/paper/3b36d16985286b03e06e8404a7be49a9713d37b9","title":"Confucius: Iterative Tool Learning from Introspection Feedback by Easy-to-Difficult Curriculum","venue":"arXiv.org","year":2023,"referenceCount":44,"citationCount":6,"influentialCitationCount":0,"publicationDate":"27/08/2023","authors":"Shen Gao,Zhengliang Shi,Minghang Zhu,Bowen Fang,Xin Xin,Pengjie Ren,Zhumin Chen,Jun Ma","id":"3b36d16985286b03e06e8404a7be49a9713d37b9","summary":"The Confucius is proposed, a novel tool learning framework to train large language models to use complicated tools in real-world scenarios, which contains two main phases: a multi-stage learning method to teach the LLM to use various tools from an easy-to-difficult curriculum and the Iterative Self-instruct from Introspective Feedback to dynamically construct the dataset to improve the ability to use the complicated tool.","score":2},{"url":"https://www.semanticscholar.org/paper/6bcc6ab9c28805d4067e99b2cdc7524550fe80e1","title":"PointLLM: Empowering Large Language Models to Understand Point Clouds","venue":"arXiv.org","year":2023,"referenceCount":72,"citationCount":31,"influentialCitationCount":4,"publicationDate":"31/08/2023","authors":"Runsen Xu,Xiaolong Wang,Tai Wang,Yilun Chen,Jiangmiao Pang,Dahua Lin","id":"6bcc6ab9c28805d4067e99b2cdc7524550fe80e1","summary":"Experimental results reveal PointLLM's superior performance over existing 2D and 3D baselines, with a notable achievement in human-evaluated object captioning tasks where it surpasses human annotators in over 50% of the samples.","score":2},{"url":"https://www.semanticscholar.org/paper/c237a22698223e4060d83027f399f4fb2aa24291","title":"Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations","venue":"arXiv.org","year":2023,"referenceCount":58,"citationCount":17,"influentialCitationCount":1,"publicationDate":"31/08/2023","authors":"Xu Huang,Jianxun Lian,Yuxuan Lei,Jing Yao,Defu Lian,Xing Xie","id":"c237a22698223e4060d83027f399f4fb2aa24291","summary":"InteRecAgent enables traditional recommender systems, such as those ID-based matrix factorization models, to become interactive systems with a natural language interface through the integration of LLMs, and introduces an efficient framework, which employs LLMs as the brain and recommender models as tools.","score":2},{"url":"https://www.semanticscholar.org/paper/d39182113cd4176ead48027b4fc05fe06ec6aaca","title":"Language Models as Black-Box Optimizers for Vision-Language Models","venue":"arXiv.org","year":2023,"referenceCount":107,"citationCount":4,"influentialCitationCount":0,"publicationDate":"12/09/2023","authors":"Samuel Yu,Shihong Liu,Zhiqiu Lin,Deepak Pathak,Deva Ramanan","id":"d39182113cd4176ead48027b4fc05fe06ec6aaca","summary":"This work proposes employing chat-based LLMs to search for the best text prompt for VLMs and highlights the advantage of conversational feedback that incorporates both positive and negative prompts, suggesting that LLMs can utilize the implicit gradient direction in textual feedback for a more efficient search.","score":2},{"url":"https://www.semanticscholar.org/paper/a1426b13b74dbad17b34606d25aabe1d61f6e11a","title":"CRAFT: Customizing LLMs by Creating and Retrieving from Specialized Toolsets","venue":"arXiv.org","year":2023,"referenceCount":61,"citationCount":17,"influentialCitationCount":2,"publicationDate":"29/09/2023","authors":"Lifan Yuan,Yangyi Chen,Xingyao Wang,Y. Fung,Hao Peng,Heng Ji","id":"a1426b13b74dbad17b34606d25aabe1d61f6e11a","summary":"CRAFT is designed to be flexible and offers a plug-and-play approach to adapt off-the-shelf LLMs to unseen domains and modalities, without any finetuning, and achieves substantial improvements compared to strong baselines.","score":2},{"url":"https://www.semanticscholar.org/paper/bee68767debbdc96d6f75947e544a8be98b869e3","title":"Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model: Explorations with GPT4-Vision and Beyond","venue":"arXiv.org","year":2023,"referenceCount":60,"citationCount":13,"influentialCitationCount":2,"publicationDate":"03/10/2023","authors":"Liang Chen,Yichi Zhang,Shuhuai Ren,Haozhe Zhao,Zefan Cai,Yuchi Wang,Tianyu Liu,Baobao Chang","id":"bee68767debbdc96d6f75947e544a8be98b869e3","summary":"This study introduces a new benchmark called PCA-EVAL, which evaluates embodied decision-making from the perspectives of Perception, Cognition, and Action and proposes HOLMES, a multi-agent cooperation framework that allows LLMs to leverage MLLMs and APIs to gather multimodal information for informed decision- making.","score":2},{"url":"https://www.semanticscholar.org/paper/8918e3cc21ecaf81532e452d3b9518360d14860e","title":"Reinforced UI Instruction Grounding: Towards a Generic UI Task Automation API","venue":"arXiv.org","year":2023,"referenceCount":48,"citationCount":2,"influentialCitationCount":0,"publicationDate":"07/10/2023","authors":"Zhizheng Zhang,Wenxuan Xie,Xiaoyi Zhang,Yan Lu","id":"8918e3cc21ecaf81532e452d3b9518360d14860e","summary":"This work builds a multimodal model to ground natural language instructions in given UI screenshots as a generic UI task automation executor and proposes an innovative Reinforcement Learning (RL) based algorithm to supervise the tokens in such sequence jointly with visually semantic metrics, which effectively strengthens the spatial decoding capability of the pixel-to-sequence paradigm.","score":2},{"url":"https://www.semanticscholar.org/paper/84f9bc5f89dac53662fb467b6af8ff26415ca3e7","title":"InstructDET: Diversifying Referring Object Detection with Generalized Instructions","venue":"arXiv.org","year":2023,"referenceCount":62,"citationCount":2,"influentialCitationCount":0,"publicationDate":"08/10/2023","authors":"Ronghao Dang,Jiangyan Feng,Haodong Zhang,Chongjian Ge,Lin Song,Lijun Gong,Chengju Liu,Qi Chen,Feng Zhu,Rui Zhao,Yibing Song","id":"84f9bc5f89dac53662fb467b6af8ff26415ca3e7","summary":"A data-centric method for referring object detection (ROD) that localizes target objects based on user instructions and shows that a conventional ROD model surpasses existing methods on standard REC datasets and the authors' InDET test set.","score":2},{"url":"https://www.semanticscholar.org/paper/33095b1334bed852e3652bd9d7da3f4df0cdf485","title":"ViCor: Bridging Visual Understanding and Commonsense Reasoning with Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":34,"citationCount":1,"influentialCitationCount":0,"publicationDate":"09/10/2023","authors":"KAI-QING Zhou,Kwonjoon Lee,Teruhisa Misu,Xin Eric Wang","id":"33095b1334bed852e3652bd9d7da3f4df0cdf485","summary":"This work explores the synergistic capabilities of pre-trained vision-and-language models (VLMs) and large language models (LLMs) for visual commonsense reasoning (VCR) and suggests a collaborative approach where LLMs, when uncertain about their reasoning, actively direct VLMs to concentrate on and gather relevant visual elements to support potential commonsense inferences.","score":2},{"url":"https://www.semanticscholar.org/paper/03bf1da1caa5f63203d43ed78c12c35a78fc6ed9","title":"EasyGen: Easing Multimodal Generation with a Bidirectional Conditional Diffusion Model and LLMs","venue":"","year":2023,"referenceCount":59,"citationCount":3,"influentialCitationCount":0,"publicationDate":"13/10/2023","authors":"Xiangyu Zhao,Bo Liu,Qijiong Liu,Guangyuan Shi,Xiao-Ming Wu","id":"03bf1da1caa5f63203d43ed78c12c35a78fc6ed9","summary":"Comprehensive quantitative and qualitative experiments show that EasyGen excels in data-efficient training, high-quality image generation, and extendibility, effectively addressing the challenges in multimodal generation.","score":2},{"url":"https://www.semanticscholar.org/paper/36b923d97d7cfaf73d11c55c15ea46605ba974a5","title":"BiLL-VTG: Bridging Large Language Models and Lightweight Visual Tools for Video-based Texts Generation","venue":"arXiv.org","year":2023,"referenceCount":68,"citationCount":0,"influentialCitationCount":0,"publicationDate":"16/10/2023","authors":"Ji Qi,Kaixuan Ji,Jifan Yu,Duokang Wang,Bin Xu,Lei Hou,Juanzi Li","id":"36b923d97d7cfaf73d11c55c15ea46605ba974a5","summary":"BiLL-VTG is introduced, a fast adaptive framework that leverages large language models (LLMs) to reasoning on videos based on essential lightweight visual tools and an Instruction-oriented Video Events Recognition (InsOVER) algorithm based on the efficient Hungarian matching to localize corresponding video events using linguistic instructions, enabling LLMs to interact with long videos.","score":2},{"url":"https://www.semanticscholar.org/paper/7451d756118628474dc022813eb952a21d34c5f6","title":"Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V","venue":"arXiv.org","year":2023,"referenceCount":48,"citationCount":8,"influentialCitationCount":3,"publicationDate":"17/10/2023","authors":"Jianwei Yang,Hao Zhang,Feng Li,Xueyan Zou,Chun-yue Li,Jianfeng Gao","id":"7451d756118628474dc022813eb952a21d34c5f6","summary":"The experiments show that GPT-4V with SoM in zero-shot setting outperforms the state-of-the-art fully-finetuned referring expression comprehension and segmentation model on RefCOCOg, and the effectiveness of SoM on a wide range of fine-grained vision and multimodal tasks is validated.","score":2},{"url":"https://www.semanticscholar.org/paper/79e7ead8f59b17431de2b86af10dc0c30a1f5a2b","title":"ToolChain*: Efficient Action Space Navigation in Large Language Models with A* Search","venue":"arXiv.org","year":2023,"referenceCount":60,"citationCount":10,"influentialCitationCount":0,"publicationDate":"20/10/2023","authors":"Yuchen Zhuang,Xiang Chen,Tong Yu,Saayan Mitra,Victor S. Bursztyn,Ryan A. Rossi,Somdeb Sarkhel,Chao Zhang","id":"79e7ead8f59b17431de2b86af10dc0c30a1f5a2b","summary":"This work proposes ToolChain*, an efficient tree search-based planning algorithm for LLM-based agents that formulates the entire action space as a decision tree, where each node represents a possible API function call involved in a solution plan.","score":2},{"url":"https://www.semanticscholar.org/paper/f90c522b284a6c065fa5126216a26a7415a2b9fa","title":"MoqaGPT : Zero-Shot Multi-modal Open-domain Question Answering with Large Language Model","venue":"Conference on Empirical Methods in Natural Language Processing","year":2023,"referenceCount":30,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/10/2023","authors":"Le Zhang,Yihong Wu,Fengran Mo,Jian-Yun Nie,Aishwarya Agrawal","id":"f90c522b284a6c065fa5126216a26a7415a2b9fa","summary":"MoqaGPT, a straightforward and flexible framework built upon LLMs, retrieves and extracts answers from each modality separately, then fuses this multi-modal information using LLMs to produce a final answer.","score":2},{"url":"https://www.semanticscholar.org/paper/8e3e7deb95d2a984cba615ec847e64f354626cdf","title":"WebWISE: Web Interface Control and Sequential Exploration with Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":35,"citationCount":1,"influentialCitationCount":0,"publicationDate":"24/10/2023","authors":"Heyi Tao,TV Sethuraman,Michal Shlapentokh-Rothman,Derek Hoiem","id":"8e3e7deb95d2a984cba615ec847e64f354626cdf","summary":"This paper investigates using a Large Language Model (LLM) to automatically perform web software tasks using click, scroll, and text input operations using filtered Document Object Model elements as observations and performs tasks step-by-step, sequentially generating small programs based on the current observations.","score":2},{"url":"https://www.semanticscholar.org/paper/807f336176070bd3f95b82a16f125ee99b7d2c80","title":"Woodpecker: Hallucination Correction for Multimodal Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":49,"citationCount":16,"influentialCitationCount":2,"publicationDate":"24/10/2023","authors":"Shukang Yin,Chaoyou Fu,Sirui Zhao,Tong Xu,Hao Wang,Dianbo Sui,Yunhang Shen,Ke Li,Xingguo Sun,Enhong Chen","id":"807f336176070bd3f95b82a16f125ee99b7d2c80","summary":"Implemented in a post-remedy manner, Woodpecker can easily serve different MLLMs, while being interpretable by accessing intermediate outputs of the five stages, and shows the huge potential of this new paradigm.","score":2},{"url":"https://www.semanticscholar.org/paper/0212dca18cd0765deed0b6ba80a796f0ad46e066","title":"mPLUG-Octopus: The Versatile Assistant Empowered by A Modularized End-to-End Multimodal LLM","venue":"ACM Multimedia","year":2023,"referenceCount":10,"citationCount":0,"influentialCitationCount":0,"publicationDate":"26/10/2023","authors":"Qinghao Ye,Haiyang Xu,Mingshi Yan,Chenlin Zhao,Junyang Wang,Xiaoshan Yang,Ji Zhang,Fei Huang,J. Sang,Changsheng Xu","id":"0212dca18cd0765deed0b6ba80a796f0ad46e066","summary":null,"score":2},{"url":"https://www.semanticscholar.org/paper/76a3f4a79ae9a00db2f2b5f6877021d8deb96ada","title":"SPHINX: The Joint Mixing of Weights, Tasks, and Visual Embeddings for Multi-modal Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":90,"citationCount":23,"influentialCitationCount":2,"publicationDate":"13/11/2023","authors":"Ziyi Lin,Chris Liu,Renrui Zhang,Peng Gao,Longtian Qiu,Han Xiao,Han Qiu,Chen Lin,Wenqi Shao,Keqin Chen,Jiaming Han,Siyuan Huang,Yichi Zhang,Xuming He,Hongsheng Li,Y. Qiao","id":"76a3f4a79ae9a00db2f2b5f6877021d8deb96ada","summary":"SPHINX, a versatile multi-modal large language model (MLLM) with a joint mixing of model weights, tuning tasks, and visual embeddings is presented, and an efficient strategy aiming to better capture fine-grained appearances of high-resolution images is proposed.","score":2},{"url":"https://www.semanticscholar.org/paper/0f993809c1fe00403ecea66d8f572832f075cfe4","title":"MMC: Advancing Multimodal Chart Understanding with Large-scale Instruction Tuning","venue":"arXiv.org","year":2023,"referenceCount":38,"citationCount":8,"influentialCitationCount":0,"publicationDate":"15/11/2023","authors":"Fuxiao Liu,Xiaoyang Wang,Wenlin Yao,Jianshu Chen,Kaiqiang Song,Sangwoo Cho,Yaser Yacoob,Dong Yu","id":"0f993809c1fe00403ecea66d8f572832f075cfe4","summary":"This work provides an instruction-tuning methodology and benchmark to advance multimodal understanding of charts and develops MultiModal Chart Assistant (MMCA), an LMM that achieves state-of-the-art performance on existing chart QA benchmarks.","score":2},{"url":"https://www.semanticscholar.org/paper/ab8169d6e4dfabfe7c30ebec1bb871bf3e1551cd","title":"GAIA: a benchmark for General AI Assistants","venue":"arXiv.org","year":2023,"referenceCount":40,"citationCount":25,"influentialCitationCount":4,"publicationDate":"21/11/2023","authors":"Gr√©goire Mialon,Cl√©mentine Fourrier,Craig Swift,Thomas Wolf,Y. LeCun,Thomas Scialom","id":"ab8169d6e4dfabfe7c30ebec1bb871bf3e1551cd","summary":"GAIA proposes real-world questions that require a set of fundamental abilities such as reasoning, multi-modality handling, web browsing, and generally tool-use proficiency, and shows that human respondents obtain 92% vs. 15% for GPT-4 equipped with plugins.","score":2},{"url":"https://www.semanticscholar.org/paper/ee2c769943f9e46c3bbee117d1ecf14566b7bf1f","title":"Boosting the Power of Small Multimodal Reasoning Models to Match Larger Models with Self-Consistency Training","venue":"arXiv.org","year":2023,"referenceCount":62,"citationCount":0,"influentialCitationCount":0,"publicationDate":"23/11/2023","authors":"Cheng Tan,Jingxuan Wei,Zhangyang Gao,Linzhuang Sun,Siyuan Li,Xihong Yang,Stan Z. Li","id":"ee2c769943f9e46c3bbee117d1ecf14566b7bf1f","summary":"This work proposes MC-CoT, a self-consistency training strategy that generates multiple rationales and answers, subsequently selecting the most accurate through a voting process, and demonstrates that this approach significantly improves model performance across various benchmarks.","score":2},{"url":"https://www.semanticscholar.org/paper/7b0a186b0140ee91fb13991c9c7187f3dc3b0670","title":"Visual Programming for Zero-shot Open-Vocabulary 3D Visual Grounding","venue":"arXiv.org","year":2023,"referenceCount":67,"citationCount":0,"influentialCitationCount":0,"publicationDate":"26/11/2023","authors":"Zhihao Yuan,Jinke Ren,Chun-Mei Feng,Hengshuang Zhao,Shuguang Cui,Zhen Li","id":"7b0a186b0140ee91fb13991c9c7187f3dc3b0670","summary":"This work proposes a novel visual programming approach for zero-shot open-vocabulary 3DVG, leveraging the capabilities of large language models (LLMs) and develops an innovative language-object correlation module to extend the scope of existing 3D object detectors into open- Vocabulary scenarios.","score":2},{"url":"https://www.semanticscholar.org/paper/5eea245cc12c55905d4df827d0c9776c5ddfa743","title":"Compositional Chain-of-Thought Prompting for Large Multimodal Models","venue":"arXiv.org","year":2023,"referenceCount":91,"citationCount":4,"influentialCitationCount":2,"publicationDate":"27/11/2023","authors":"Chancharik Mitra,Brandon Huang,Trevor Darrell,Roei Herzig","id":"5eea245cc12c55905d4df827d0c9776c5ddfa743","summary":"The proposed Compositional Chain-of-Thought (CCoT) approach not only improves LMM performance on several vision and language VL compositional benchmarks but also improves the performance of several popular LMMs on general multimodal benchmarks, without the need for fine-tuning or annotated ground-truth SGs.","score":2},{"url":"https://www.semanticscholar.org/paper/8441c30ad4abdca9ee380aa6f22ffd731b10231b","title":"COLE: A Hierarchical Generation Framework for Graphic Design","venue":"arXiv.org","year":2023,"referenceCount":45,"citationCount":0,"influentialCitationCount":0,"publicationDate":"28/11/2023","authors":"Peidong Jia,Chenxuan Li,Zeyu Liu,Yichao Shen,Xingru Chen,Yuhui Yuan,Yinglin Zheng,Dong Chen,Ji Li,Xiaodong Xie,Shanghang Zhang,Baining Guo","id":"8441c30ad4abdca9ee380aa6f22ffd731b10231b","summary":"The key insight is to dissect the complex task of text-to-design generation into a hierarchy of simpler sub-tasks, each addressed by specialized models working collaboratively, to streamline the complex process and significantly enhance generation reliability.","score":2},{"url":"https://www.semanticscholar.org/paper/06b2ac5153e3d8d05c13c82f93d7f4e13eee6d0f","title":"Mitigating Fine-Grained Hallucination by Fine-Tuning Large Vision-Language Models with Caption Rewrites","venue":"Conference on Multimedia Modeling","year":2023,"referenceCount":39,"citationCount":7,"influentialCitationCount":0,"publicationDate":"04/12/2023","authors":"Lei Wang,Jiabang He,Shenshen Li,Ning Liu,Ee-Peng Lim","id":"06b2ac5153e3d8d05c13c82f93d7f4e13eee6d0f","summary":"ReCaption is proposed, a framework that consists of two components: rewriting captions using ChatGPT and fine-tuning the instruction-tuned LVLMs on the rewritten captions, and a fine-grained probing-based evaluation method named \\textit{Fine-Grained Object Hallucination Evaluation} (FGHE).","score":2},{"url":"https://www.semanticscholar.org/paper/10578bc0bdb3ebf9232931dd4961f55ba470caad","title":"LaMPilot: An Open Benchmark Dataset for Autonomous Driving with Language Model Programs","venue":"arXiv.org","year":2023,"referenceCount":54,"citationCount":7,"influentialCitationCount":0,"publicationDate":"07/12/2023","authors":"Yunsheng Ma,Can Cui,Xu Cao,Wenqian Ye,Peiran Liu,Juanwu Lu,Amr Abdelraouf,Rohit Gupta,Kyungtae Han,Aniket Bera,J. Rehg,Ziran Wang","id":"10578bc0bdb3ebf9232931dd4961f55ba470caad","summary":"LaMPilot is presented, a novel framework for planning in the field of autonomous driving, rethinking the task as a code-generation process that leverages established behavioral primitives and shows that GPT-4, with human feedback, achieved an impressive task completion rate and a minimal collision rate.","score":2},{"url":"https://www.semanticscholar.org/paper/96a7b0fe722e6d2d5167ef25a6aff714a20233a0","title":"Exploring the Limits of ChatGPT in Software Security Applications","venue":"arXiv.org","year":2023,"referenceCount":153,"citationCount":0,"influentialCitationCount":0,"publicationDate":"08/12/2023","authors":"Fangzhou Wu,Qingzhao Zhang,Ati Priya Bajaj,Tiffany Bao,Ning Zhang,Ruoyu Wang,Chaowei Xiao","id":"96a7b0fe722e6d2d5167ef25a6aff714a20233a0","summary":"The exploration reveals that ChatGPT not only excels at generating code, which is the conventional application of language models, but also demonstrates strong capability in understanding user-provided commands in natural languages, reasoning about control and data flows within programs, generating complex data structures, and even decompiling assembly code.","score":2},{"url":"https://www.semanticscholar.org/paper/b240a1d8ec2860bdd7370daa3144268ce46ac018","title":"Vary: Scaling up the Vision Vocabulary for Large Vision-Language Models","venue":"arXiv.org","year":2023,"referenceCount":52,"citationCount":6,"influentialCitationCount":0,"publicationDate":"11/12/2023","authors":"Haoran Wei,Lingyu Kong,Jinyue Chen,Liang Zhao,Zheng Ge,Jinrong Yang,Jian‚ÄêYuan Sun,Chunrui Han,Xiangyu Zhang","id":"b240a1d8ec2860bdd7370daa3144268ce46ac018","summary":"Compared to the popular BLIP-2, MiniGPT4, and LLaVA, Vary can maintain its vanilla capabilities while enjoying more excellent fine-grained perception and understanding ability and is competent in new document parsing features (OCR or markdown conversion).","score":2},{"url":"https://www.semanticscholar.org/paper/33c3dbf86dd6e338e2a49bba9c2e2193d1eca08a","title":"Vista-LLaMA: Reliable Video Narrator via Equal Distance to Visual Tokens","venue":"arXiv.org","year":2023,"referenceCount":39,"citationCount":2,"influentialCitationCount":1,"publicationDate":"12/12/2023","authors":"Fan Ma,Xiaojie Jin,Heng Wang,Yuchen Xian,Jiashi Feng,Yi Yang","id":"33c3dbf86dd6e338e2a49bba9c2e2193d1eca08a","summary":"This work proposes Vista-LLaMA, a novel framework that maintains the consistent distance between all visual tokens and any language tokens, irrespective of the generated text length, and presents a sequential visual projector that projects the current video frame into tokens of language space with the assistance of the previous frame.","score":2},{"url":"https://www.semanticscholar.org/paper/55c6d16b550c606d62dd85084f0d373d8f087966","title":"VLAP: Efficient Video-Language Alignment via Frame Prompting and Distilling for Video Question Answering","venue":"arXiv.org","year":2023,"referenceCount":66,"citationCount":0,"influentialCitationCount":0,"publicationDate":"13/12/2023","authors":"Xijun Wang,Junbang Liang,Chun-Kai Wang,Kenan Deng,Yu Lou,Ming Lin,Shan Yang","id":"55c6d16b550c606d62dd85084f0d373d8f087966","summary":"This work proposes an efficient Video-Language Alignment via Frame-Prompting and Distilling (VLAP) network that addresses both efficient frame sampling and effective cross-modal alignment in a unified way and demonstrates the capability of selecting key frames with critical contents, thus improving the video-language alignment accuracy.","score":2},{"url":"https://www.semanticscholar.org/paper/17a32c825bd746a2625eddc2728092171a9ef72a","title":"Jack of All Tasks, Master of Many: Designing General-purpose Coarse-to-Fine Vision-Language Model","venue":"arXiv.org","year":2023,"referenceCount":126,"citationCount":0,"influentialCitationCount":0,"publicationDate":"19/12/2023","authors":"Shraman Pramanick,Guangxing Han,Rui Hou,Sayan Nag,Ser-nam Lim,Nicolas Ballas,Qifan Wang,Rama Chellappa,Amjad Almahairi","id":"17a32c825bd746a2625eddc2728092171a9ef72a","summary":"This work introduces VistaLLM, a powerful visual system that addresses coarse- and fine-grained VL tasks over single and multiple input images using a unified framework and addresses the lack of multi-image grounding datasets by introducing a novel task, AttCoSeg (Attribute-level Co-Segmentation), which boosts the model's reasoning and grounding capability over multiple input images.","score":2},{"url":"https://www.semanticscholar.org/paper/4599d5af850da482f591a02a3b17d56e0d358771","title":"Plan, Posture and Go: Towards Open-World Text-to-Motion Generation","venue":"arXiv.org","year":2023,"referenceCount":103,"citationCount":1,"influentialCitationCount":0,"publicationDate":"22/12/2023","authors":"Jinpeng Liu,Wen-Dao Dai,Chunyu Wang,Yiji Cheng,Yansong Tang,Xin Tong","id":"4599d5af850da482f591a02a3b17d56e0d358771","summary":"A divide-and-conquer framework named PRO-Motion, which consists of three modules as motion planner, posture-diffuser and go-diffuser, which demonstrates its capability of generating diverse and realistic motions from complex open-world prompts.","score":2},{"url":"https://www.semanticscholar.org/paper/9eab4104973f5de650544729a4a69d84c594da92","title":"A Vision Check-up for Language Models","venue":"arXiv.org","year":2024,"referenceCount":56,"citationCount":0,"influentialCitationCount":0,"publicationDate":"03/01/2024","authors":"Pratyusha Sharma,Tamar Rott Shaham,Manel Baradad,Stephanie Fu,Adrian Rodriguez-Munoz,Shivam Duggal,Phillip Isola,Antonio Torralba","id":"9eab4104973f5de650544729a4a69d84c594da92","summary":"Although LLM-generated images do not look like natural images, results on image generation and the ability of models to correct these generated images indicate that precise modeling of strings can teach language models about numerous aspects of the visual world.","score":2},{"url":"https://www.semanticscholar.org/paper/46f64681d7a0c7f80303bebb0d62a3b7acbcdcd9","title":"An Improved Baseline for Reasoning Segmentation with Large Language Model","venue":"arXiv.org","year":2023,"referenceCount":42,"citationCount":0,"influentialCitationCount":0,"publicationDate":"28/12/2023","authors":"Senqiao Yang,Tianyuan Qu,Xin Lai,Zhuotao Tian,Bohao Peng,Shu Liu,Jiaya Jia","id":"46f64681d7a0c7f80303bebb0d62a3b7acbcdcd9","summary":"LISA++ is introduced, an update to the existing LISA model, focusing on improving core functionalities while keeping the base architecture intact, and its adaptability and improved features highlight the versatility of the mask-as-embedding paradigm proposed by LISA and the potential as a foundational model for diverse applications.","score":2},{"url":"https://www.semanticscholar.org/paper/ff61aef2fef3a235bfaa123158a990c4f5f27d1a","title":"Small LLMs Are Weak Tool Learners: A Multi-LLM Agent","venue":"arXiv.org","year":2024,"referenceCount":44,"citationCount":4,"influentialCitationCount":0,"publicationDate":"14/01/2024","authors":"Weizhou Shen,Chenliang Li,Hongzhan Chen,Ming Yan,Xiaojun Quan,Hehong Chen,Ji Zhang,Fei Huang","id":"ff61aef2fef3a235bfaa123158a990c4f5f27d1a","summary":"Evaluation across various tool-use benchmarks illustrates that the proposed multi-LLM framework surpasses the traditional single-LLM approach, highlighting its efficacy and advantages in tool learning.","score":2},{"url":"https://www.semanticscholar.org/paper/4f2a56102bcbf0fe79379c4c27daecbccfb35a26","title":"MLLM-Tool: A Multimodal Large Language Model For Tool Agent Learning","venue":"arXiv.org","year":2024,"referenceCount":44,"citationCount":3,"influentialCitationCount":0,"publicationDate":"19/01/2024","authors":"Chenyu Wang,Weixin Luo,Qianyu Chen,Haonan Mai,Jindi Guo,Sixun Dong,Xiaohua Xuan,Zhengxin Li,Lin Ma,Shenghua Gao","id":"4f2a56102bcbf0fe79379c4c27daecbccfb35a26","summary":"This paper proposes MLLM-Tool, a system incorporating open-source LLMs and multi-modal encoders so that the learnt LLMs can be conscious of multi-modal input instruction and then select the function-matched tool correctly.","score":2},{"url":"https://www.semanticscholar.org/paper/c5db6c2726911b72d534f97bd4d1ed63f6431340","title":"Mobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual Perception","venue":"arXiv.org","year":2024,"referenceCount":25,"citationCount":5,"influentialCitationCount":0,"publicationDate":"29/01/2024","authors":"Junyang Wang,Haiyang Xu,Jiabo Ye,Mingshi Yan,Weizhou Shen,Ji Zhang,Fei Huang,Jitao Sang","id":"c5db6c2726911b72d534f97bd4d1ed63f6431340","summary":"Different from previous solutions that rely on XML files of Apps or mobile system metadata, Mobile-Agent allows for greater adaptability across diverse mobile operating environments in a vision-centric way, thereby eliminating the necessity for system-specific customizations.","score":2},{"url":"https://www.semanticscholar.org/paper/eaad6e351ab7ddb5a31bce3c5fe8bf38cd08c7f2","title":"Multimodal Query Suggestion with Multi-Agent Reinforcement Learning from Human Feedback","venue":"arXiv.org","year":2024,"referenceCount":52,"citationCount":0,"influentialCitationCount":0,"publicationDate":"07/02/2024","authors":"Zheng Wang,Bingzheng Gan,Wei Shi","id":"eaad6e351ab7ddb5a31bce3c5fe8bf38cd08c7f2","summary":"A novel Multimodal Query Suggestion (MMQS) task, which aims to generate query suggestions based on user query images to improve the intentionality and diversity of search results, is introduced and the RL4Sugg framework is presented, leveraging the power of Large Language Models with Multi-Agent Reinforcement Learning from Human Feedback to optimize the generation process.","score":2},{"url":"https://www.semanticscholar.org/paper/710b1e23b09e0b826f9d47e7cc23b5f4c0808c7e","title":"Multi-modal preference alignment remedies regression of visual instruction tuning on language model","venue":"arXiv.org","year":2024,"referenceCount":29,"citationCount":0,"influentialCitationCount":0,"publicationDate":"16/02/2024","authors":"Shengzhi Li,Rongyu Lin,Shichao Pei","id":"710b1e23b09e0b826f9d47e7cc23b5f4c0808c7e","summary":"A distillation-based multi-modal alignment model with fine-grained annotations on a small dataset that reconciles the textual and visual performance of MLLMs, restoring and boosting language capability after visual instruction tuning is proposed.","score":2},{"url":"https://www.semanticscholar.org/paper/83d201d503b863fec7d1225f00a141e722e03f17","title":"Using Left and Right Brains Together: Towards Vision and Language Planning","venue":"arXiv.org","year":2024,"referenceCount":49,"citationCount":0,"influentialCitationCount":0,"publicationDate":"16/02/2024","authors":"Jun Cen,Chenfei Wu,Xiao Liu,Sheng-Siang Yin,Yixuan Pei,Jinglong Yang,Qifeng Chen,Nan Duan,Jianguo Zhang","id":"83d201d503b863fec7d1225f00a141e722e03f17","summary":"This work introduces a novel vision-language planning framework to perform concurrent visual and language planning for tasks with inputs of any form, and demonstrates the superior performance of this approach.","score":2},{"url":"https://www.semanticscholar.org/paper/ded3266d047b36a963c1324aa9f98705d598bdcf","title":"From Summary to Action: Enhancing Large Language Models for Complex Tasks with Open World APIs","venue":"","year":2024,"referenceCount":36,"citationCount":0,"influentialCitationCount":0,"publicationDate":"28/02/2024","authors":"Yulong Liu,Yunlong Yuan,Chunwei Wang,Jianhua Han,Yongqiang Ma,Li Zhang,Nanning Zheng,Hang Xu","id":"ded3266d047b36a963c1324aa9f98705d598bdcf","summary":"A novel tool invocation pipeline designed to control massive real-world APIs, which mirrors the human task-solving process, addressing complicated real-life user queries and highlights Sum2Act's effectiveness in enhancing LLMs for complex real-world tasks.","score":2},{"url":"https://www.semanticscholar.org/paper/ac8089bb7944090cf1de5df25aadf5e6356f3040","title":"TempCompass: Do Video LLMs Really Understand Videos?","venue":"","year":2024,"referenceCount":22,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/03/2024","authors":"Yuanxin Liu,Shicheng Li,Yi Liu,Yuxiang Wang,Shuhuai Ren,Lei Li,Sishuo Chen,Xu Sun,Lu Hou","id":"ac8089bb7944090cf1de5df25aadf5e6356f3040","summary":"The TempCompass benchmark, which introduces a diversity of temporal aspects and task formats, and comprehensively evaluate 8 state-of-the-art (SOTA) Video LLMs and 3 Image LLMs, reveals the discerning fact that these models exhibit notably poor temporal perception ability.","score":2},{"url":"https://www.semanticscholar.org/paper/23684a07517870cffd1f97fafbaae16ba22bd2b7","title":"Large AI Models in Health Informatics: Applications, Challenges, and the Future","venue":"IEEE journal of biomedical and health informatics","year":2023,"referenceCount":347,"citationCount":34,"influentialCitationCount":1,"publicationDate":"21/03/2023","authors":"Jianing Qiu,Lin Li,Jiankai Sun,Jiachuan Peng,Peilun Shi,Rui Zhang,Yinzhao Dong,Kyle Lam,F. P. Lo,Bo Xiao,Wu Yuan,Dong Xu,Benny P. L. Lo","id":"23684a07517870cffd1f97fafbaae16ba22bd2b7","summary":"Seven key sectors in which large AI models are applicable and might have substantial influence, including: 1) bioinformatics; 2) medical diagnosis; 3) medical imaging; 4) medical informatics; 5) medical education; 6) public health; and 7) medical robotics are identified.","score":2},{"url":"https://www.semanticscholar.org/paper/cb5f53cd7d43ac6f48abf4dde6fc58e53a083b0e","title":"Accelerating the integration of ChatGPT and other large‚Äêscale AI models into biomedical research and healthcare","venue":"MedComm ‚Äì Future Medicine","year":2023,"referenceCount":99,"citationCount":22,"influentialCitationCount":0,"publicationDate":"17/05/2023","authors":"Ding‚ÄêQiao Wang,Long‚ÄêYu Feng,Jin‚ÄêGuo Ye,Jin‚ÄêGen Zou,Yingfeng Zheng","id":"cb5f53cd7d43ac6f48abf4dde6fc58e53a083b0e","summary":"","score":2},{"url":"https://www.semanticscholar.org/paper/51b169701290cd129e0781fc9f3a9918604c89b5","title":"Improving accuracy of GPT-3/4 results on biomedical data using a retrieval-augmented language model","venue":"arXiv.org","year":2023,"referenceCount":54,"citationCount":9,"influentialCitationCount":0,"publicationDate":"26/05/2023","authors":"D. Soong,S. Sridhar,Han Si,J. Wagner,Ana Caroline Costa S'a,Christina Y. Yu,Kubra Karagoz,Meijian Guan,Hisham K Hamadeh,Brandon Higgs","id":"51b169701290cd129e0781fc9f3a9918604c89b5","summary":"It is suggested that RetA models, supplemented with domain-specific corpora, may outperform general-purpose LLMs in accuracy and relevance within specific domains.","score":2},{"url":"https://www.semanticscholar.org/paper/06091944b864d6dc473cab63321a95fb9c4067cc","title":"ChatCAD+: Towards a Universal and Reliable Interactive CAD using LLMs","venue":"arXiv.org","year":2023,"referenceCount":50,"citationCount":10,"influentialCitationCount":1,"publicationDate":"25/05/2023","authors":"Zihao Zhao,Sheng Wang,Jinchen Gu,Yitao Zhu,Lanzhuju Mei,Zixu Zhuang,Zhiming Cui,Qian Wang,Dinggang Shen","id":"06091944b864d6dc473cab63321a95fb9c4067cc","summary":"ChatCAD+, which is designed to be universal and reliable, is introduced, capable of handling medical images from diverse domains and leveraging up-to-date information from reputable medical websites to provide reliable medical advice.","score":2},{"url":"https://www.semanticscholar.org/paper/6294f078e79828cac21e717813e8f3d02b18a97c","title":"The importance of resource awareness in artificial intelligence for healthcare","venue":"Nature Machine Intelligence","year":2023,"referenceCount":161,"citationCount":7,"influentialCitationCount":0,"publicationDate":"12/06/2023","authors":"Zhenge Jia,Jianxu Chen,Xiaowei Xu,J. Kheir,Jingtong Hu,Han Xiao,Sui Peng,X. Hu,Danny Chen,Yi Shi","id":"6294f078e79828cac21e717813e8f3d02b18a97c","summary":"It is highlighted that there are resource sustainability issues in AI/ML for healthcare and various algorithm/system innovations that will help address these issues are presented.","score":2},{"url":"https://www.semanticscholar.org/paper/c9dbdae8146b9f97e254f5d26fd6efde96eaa703","title":"Med-Flamingo: a Multimodal Medical Few-shot Learner","venue":"ML4H@NeurIPS","year":2023,"referenceCount":34,"citationCount":36,"influentialCitationCount":6,"publicationDate":"27/07/2023","authors":"Michael Moor,Qian Huang,Shirley Wu,Michihiro Yasunaga,C. Zakka,Yashodhara Dalmia,E. Reis,P. Rajpurkar,J. Leskovec","id":"c9dbdae8146b9f97e254f5d26fd6efde96eaa703","summary":"Med-Flamingo improves performance in generative medical VQA by up to 20\\% in clinician's rating and firstly enables multimodal medical few-shot adaptations, such as rationale generation, as well as releasing the model, code, and evaluation app.","score":2},{"url":"https://www.semanticscholar.org/paper/7e55d8701785818776323b4147cb13354c820469","title":"PaperQA: Retrieval-Augmented Generative Agent for Scientific Research","venue":"arXiv.org","year":2023,"referenceCount":77,"citationCount":6,"influentialCitationCount":1,"publicationDate":"08/12/2023","authors":"Jakub L'ala,Odhran O'Donoghue,Aleksandar Shtedritski,Sam Cox,Samuel G. Rodriques,Andrew D. White","id":"7e55d8701785818776323b4147cb13354c820469","summary":"PaperQA is an agent that performs information retrieval across full-text scientific articles, assesses the relevance of sources and passages, and uses RAG to provide answers, and exceeds performance of existing LLMs and LLM agents on current science QA benchmarks.","score":2},{"url":"https://www.semanticscholar.org/paper/93886752191db25efd096a65af7b09df5c0a64e0","title":"Data-Centric Foundation Models in Computational Healthcare: A Survey","venue":"arXiv.org","year":2024,"referenceCount":316,"citationCount":3,"influentialCitationCount":0,"publicationDate":"04/01/2024","authors":"Yunkun Zhang,Jin Gao,Zheling Tan,Lingfeng Zhou,Kexin Ding,Mu Zhou,Shaoting Zhang,Dequan Wang","id":"93886752191db25efd096a65af7b09df5c0a64e0","summary":"A wide range of data-centric approaches in the FM era (from model pre-training to inference) towards improving the healthcare workflow are investigated and a promising outlook of FM-based analytics to enhance the performance of patient outcome and clinical workflow in the evolving landscape of healthcare and medicine is offered.","score":2},{"url":"https://www.semanticscholar.org/paper/18d9b13e3383d98c181f4d7a2b3ca1503ed707a0","title":"No-boundary thinking for artificial intelligence in bioinformatics and education","venue":"Frontiers in Bioinformatics","year":2024,"referenceCount":26,"citationCount":0,"influentialCitationCount":0,"publicationDate":"08/01/2024","authors":"Prajay Patel,Nisha Pillai,Inimary T. Toby","id":"18d9b13e3383d98c181f4d7a2b3ca1503ed707a0","summary":"This session addressed various areas of AI in an open discussion and raised some perspectives on how popular tools like ChatGPT can be integrated into bioinformatics, communicating with scientists in different fields to properly utilize the potential of these algorithms, and how to continue educational outreach to further interest of data science and informatics to the next-generation of scientists.","score":2},{"url":"https://www.semanticscholar.org/paper/20fcc01d12a50f1da2af71d85f0a269b3ba48b77","title":"LMEye: An Interactive Perception Network for Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":57,"citationCount":12,"influentialCitationCount":1,"publicationDate":"05/05/2023","authors":"Yunxin Li,Baotian Hu,Xinyu Chen,Lin Ma,M. Zhang","id":"20fcc01d12a50f1da2af71d85f0a269b3ba48b77","summary":"LMEye, a human-like eye with a play-and-plug interactive perception network, designed to enable dynamic interaction between LLMs and external vision information, is introduced, demonstrating that it significantly improves the zero-shot performance on various multimodal tasks compared to previous methods, with less parameters.","score":2},{"url":"https://www.semanticscholar.org/paper/8efc20988021ce3b4b05dd44b13e27260ee9b99b","title":"Investigating Prompting Techniques for Zero- and Few-Shot Visual Question Answering","venue":"arXiv.org","year":2023,"referenceCount":59,"citationCount":2,"influentialCitationCount":0,"publicationDate":"16/06/2023","authors":"Rabiul Awal,Le Zhang,Aishwarya Agrawal","id":"8efc20988021ce3b4b05dd44b13e27260ee9b99b","summary":"Light is shed on the intricacies of prompting strategies in VLMs for VQA, emphasizing the synergistic use of captions, templates, and pre-processing to enhance model efficacy.","score":2},{"url":"https://www.semanticscholar.org/paper/efc694164312006c543ef745611348ef64e68dda","title":"Towards Language Models That Can See: Computer Vision Through the LENS of Natural Language","venue":"arXiv.org","year":2023,"referenceCount":64,"citationCount":24,"influentialCitationCount":2,"publicationDate":"28/06/2023","authors":"William Berrios,Gautam Mittal,Tristan Thrush,Douwe Kiela,Amanpreet Singh","id":"efc694164312006c543ef745611348ef64e68dda","summary":"This work proposes LENS, a modular approach for tackling computer vision problems by leveraging the power of large language models (LLMs) with a language model to reason over outputs from a set of independent and highly descriptive vision modules that provide exhaustive information about an image.","score":2},{"url":"https://www.semanticscholar.org/paper/ac2e5bf716aed246ca8914a6816ef73e00286099","title":"Beyond Segmentation: Road Network Generation with Multi-Modal LLMs","venue":"arXiv.org","year":2023,"referenceCount":34,"citationCount":2,"influentialCitationCount":0,"publicationDate":"15/10/2023","authors":"Sumedh Rasal,Sanjay K. Boddhu","id":"ac2e5bf716aed246ca8914a6816ef73e00286099","summary":"An innovative approach to road network generation through the utilization of a multi-modal Large Language Model (LLM), specifically designed to process aerial images of road layouts and produce detailed, navigable road networks within the input images.","score":2},{"url":"https://www.semanticscholar.org/paper/beb3e8acd816bac1a5b7fccfd073f79048877e33","title":"Frozen Transformers in Language Models Are Effective Visual Encoder Layers","venue":"arXiv.org","year":2023,"referenceCount":83,"citationCount":2,"influentialCitationCount":0,"publicationDate":"19/10/2023","authors":"Ziqi Pang,Ziyang Xie,Yunze Man,Yu-Xiong Wang","id":"beb3e8acd816bac1a5b7fccfd073f79048877e33","summary":"This paper reveals that large language models (LLMs), despite being trained solely on textual data, are surprisingly strong encoders for purely visual tasks in the absence of language and proposes the information filtering hypothesis to explain the effectiveness of pre-trained LLMs in visual encoding.","score":2},{"url":"https://www.semanticscholar.org/paper/96d104dfe727f78a35faaafe81481f3672b485ee","title":"Large Language Models are Visual Reasoning Coordinators","venue":"Neural Information Processing Systems","year":2023,"referenceCount":125,"citationCount":16,"influentialCitationCount":0,"publicationDate":"23/10/2023","authors":"Liangyu Chen,Boyi Li,Sheng Shen,Jingkang Yang,Chunyuan Li,Kurt Keutzer,Trevor Darrell,Ziwei Liu","id":"96d104dfe727f78a35faaafe81481f3672b485ee","summary":"This work proposes Cola, a novel paradigm that coordinates multiple VLMs for visual reasoning by facilitating natural language communication that leverages their distinct and complementary capabilities, and validate that a coordinator LLM indeed comprehends the instruction prompts as well as the separate functionalities of VL Ms and coordinates them to enable impressive visual reasoning capabilities.","score":2},{"url":"https://www.semanticscholar.org/paper/29b3ce4de9dd9d784ca1d876957950f4b2d3796a","title":"Towards Perceiving Small Visual Details in Zero-shot Visual Question Answering with Multimodal LLMs","venue":"","year":2023,"referenceCount":38,"citationCount":0,"influentialCitationCount":0,"publicationDate":"24/10/2023","authors":"Jiarui Zhang,Mahyar Khayatkhoei,P. Chhikara,Filip Ilievski","id":"29b3ce4de9dd9d784ca1d876957950f4b2d3796a","summary":"It is shown that MLLMs zero-shot accuracy in answering visual questions is very sensitive to the size of the visual subject of the question, declining up to 46% with size, and that visual cropping is a promising direction to improve their zero-shot performance.","score":2},{"url":"https://www.semanticscholar.org/paper/af5f256e9771bf9cd02451195e3a7ac693fde3ed","title":"Exploring the Reasoning Abilities of Multimodal Large Language Models (MLLMs): A Comprehensive Survey on Emerging Trends in Multimodal Reasoning","venue":"arXiv.org","year":2024,"referenceCount":178,"citationCount":2,"influentialCitationCount":0,"publicationDate":"10/01/2024","authors":"Yiqi Wang,Wentao Chen,Xiaotian Han,Xudong Lin,Haiteng Zhao,Yongfei Liu,Bohan Zhai,Jianbo Yuan,Quanzeng You,Hongxia Yang","id":"af5f256e9771bf9cd02451195e3a7ac693fde3ed","summary":"This survey comprehensively review the existing evaluation protocols of multimodal reasoning, categorize and illustrate the frontiers of MLLMs, introduce recent trends in applications of MLLMs on reasoning-intensive tasks, and finally discuss current practices and future directions.","score":2},{"url":"https://www.semanticscholar.org/paper/fed3376de52d70ba83050182e79466dddde45746","title":"On the Safety Concerns of Deploying LLMs/VLMs in Robotics: Highlighting the Risks and Vulnerabilities","venue":"arXiv.org","year":2024,"referenceCount":53,"citationCount":0,"influentialCitationCount":0,"publicationDate":"15/02/2024","authors":"Xiyang Wu,Ruiqi Xian,Tianrui Guan,Jing Liang,Souradip Chakraborty,Fuxiao Liu,Brian M. Sadler,Dinesh Manocha,A. S. Bedi","id":"fed3376de52d70ba83050182e79466dddde45746","summary":"It is shown that it is easy to manipulate or misguide the robot's actions, leading to safety hazards, and the critical need for robust countermeasures to ensure the safe and reliable deployment of the advanced LLM/VLM-based robotic systems.","score":2},{"url":"https://www.semanticscholar.org/paper/86188727c4d4f3eb064ae7ff0d9a3483b4ef47c1","title":"Typographic Attacks in Large Multimodal Models Can be Alleviated by More Informative Prompts","venue":"","year":2024,"referenceCount":46,"citationCount":0,"influentialCitationCount":0,"publicationDate":"29/02/2024","authors":"Hao Cheng,Erjia Xiao,Renjing Xu","id":"86188727c4d4f3eb064ae7ff0d9a3483b4ef47c1","summary":"It is proved that CLIP's performance of zero-shot classification on typo-ridden images can be significantly improved by providing more informative texts to match images, and it is proved that LMMs can utilize more informative prompts to leverage information in embeddings to differentiate between visual content and typos.","score":2},{"url":"https://www.semanticscholar.org/paper/17ca48ad1b944c897863f04ba9ffa72674dce1ce","title":"Parallel multi-head attention and term-weighted question embedding for medical visual question answering","venue":"Multimedia tools and applications","year":2023,"referenceCount":59,"citationCount":2,"influentialCitationCount":0,"publicationDate":"11/03/2023","authors":"Sruthy Manmadhan,Binsu C. Kovoor","id":"17ca48ad1b944c897863f04ba9ffa72674dce1ce","summary":"The proposed MaMVQA model achieved significantly increased accuracy in predicting answers to both close-ended and open-ended questions and outperforms previous state-of-the-art methods in terms of accuracy while requiring no external data to train the model.","score":2},{"url":"https://www.semanticscholar.org/paper/420087f314633a381e61e6c5cd73ccc2070a749e","title":"PeFoMed: Parameter Efficient Fine-tuning on Multimodal Large Language Models for Medical Visual Question Answering","venue":"arXiv.org","year":2024,"referenceCount":43,"citationCount":0,"influentialCitationCount":0,"publicationDate":"05/01/2024","authors":"Jinlong He,Pengfei Li,Gang Liu,Zixu Zhao,Shenjun Zhong","id":"420087f314633a381e61e6c5cd73ccc2070a749e","summary":"A parameter efficient framework for fine-tuning MLLM specifically tailored to Med-VQA applications is proposed and empirically validate it on a public benchmark dataset, revealing that it outperforms the GPT-4v model by a significant margin of 26% absolute accuracy on closed-ended questions.","score":2},{"url":"https://www.semanticscholar.org/paper/6ed96d6822a06ad9a735bc09e301bf41df61c534","title":"CheXagent: Towards a Foundation Model for Chest X-Ray Interpretation","venue":"arXiv.org","year":2024,"referenceCount":97,"citationCount":1,"influentialCitationCount":0,"publicationDate":"22/01/2024","authors":"Zhihong Chen,Maya Varma,Jean-Benoit Delbrouck,Magdalini Paschali,Louis Blankemeier,Dave Van Veen,Jeya Maria Jose Valanarasu,Alaa Youssef,Joseph Paul Cohen,Eduardo Pontes Reis,Emily B. Tsai,Andrew Johnston,Cameron Olsen,Tanishq Mathew Abraham,S. Gatidis,Akshay S Chaudhari,Curtis P. Langlotz","id":"6ed96d6822a06ad9a735bc09e301bf41df61c534","summary":"This work designs a clinical large language model (LLM) for parsing radiology reports, a vision encoder for representing CXR images, and a network to bridge the vision and language modalities and introduces a novel benchmark designed to systematically evaluate FMs across 8 clinically-relevant CXR interpretation tasks.","score":2},{"url":"https://www.semanticscholar.org/paper/357fc385caf2e5b9898c9140fa3ac9955e6bb3c6","title":"TeamS at VQA-Med 2021: BBN-Orchestra for Long-tailed Medical Visual Question Answering","venue":"Conference and Labs of the Evaluation Forum","year":2021,"referenceCount":16,"citationCount":7,"influentialCitationCount":1,"publicationDate":2021,"authors":"Sedigheh Eslami,Gerard de Melo,C. Meinel","id":"357fc385caf2e5b9898c9140fa3ac9955e6bb3c6","summary":"The proposed BBN-Orchestra is an ensemble of bilateral-branch networks (BBN) and successfully reduces overfitting to train and validation data in addition to effectively modeling the imbalanced long-tailed image distribution.","score":2},{"url":"https://www.semanticscholar.org/paper/31a7d8c4a5ab6bab522494b57270249105c8748e","title":"BiomedGPT: A Unified and Generalist Biomedical Generative Pre-trained Transformer for Vision, Language, and Multimodal Tasks","venue":"arXiv.org","year":2023,"referenceCount":147,"citationCount":39,"influentialCitationCount":2,"publicationDate":2023,"authors":"Kai Zhang,Jun Yu,Zhilin Yan,Yixin Liu,Eashan Adhikarla,S. Fu,Xun Chen,Chen Chen,Yuyin Zhou,Xiang Li,Lifang He,B. Davison,Quanzheng Li,Yong Chen,Hongfang Liu,Lichao Sun","id":"31a7d8c4a5ab6bab522494b57270249105c8748e","summary":"A unified and generalist BiomedGPT model, which leverages self-supervision on large and diverse datasets to accept multi-modal inputs and perform a range of downstream tasks, which presents a significant step forward in developing unified and generalist models for biomedicine.","score":2},{"url":"https://www.semanticscholar.org/paper/61c0b6a5e7aea48a1376b61a4a737137d602b242","title":"PubMedCLIP: How Much Does CLIP Benefit Visual Question Answering in the Medical Domain?","venue":"Findings","year":2023,"referenceCount":40,"citationCount":17,"influentialCitationCount":2,"publicationDate":2023,"authors":"Sedigheh Eslami,C. Meinel,Gerard de Melo","id":"61c0b6a5e7aea48a1376b61a4a737137d602b242","summary":"This work presents PubMedCLIP, a fine-tuned version of CLIP for the medical domain based on PubMed articles that achieves superior results improving the overall accuracy up to 3% in comparison to the state-of-the-art Model-Agnostic Meta-Learning (MAML) networks pre-trained only on visual data.","score":2},{"url":"https://www.semanticscholar.org/paper/b88f6aa65a4e1faf963494a76d28cc12112c9543","title":"A Critical Analysis of Benchmarks, Techniques, and Models in Medical Visual Question Answering","venue":"IEEE Access","year":2023,"referenceCount":232,"citationCount":0,"influentialCitationCount":0,"publicationDate":2023,"authors":"Suheer Al-Hadhrami,M. Menai,Saad Al-ahmadi,Ahmed Alnafessah","id":"b88f6aa65a4e1faf963494a76d28cc12112c9543","summary":"The statistical analysis of medical VQA from 2018 to 2023 and individual yearly analyses reveals consistent preferences for LSTM and VGGNet, except in 2018 when ResNet was more commonly used.","score":2},{"url":"https://www.semanticscholar.org/paper/3c83f80f06633ff4598d33c2959f8e4cdcad3e93","title":"Cross-Modal Self-Attention with Multi-Task Pre-Training for Medical Visual Question Answering","venue":"International Conference on Multimedia Retrieval","year":2021,"referenceCount":32,"citationCount":47,"influentialCitationCount":2,"publicationDate":"01/05/2021","authors":"Haifan Gong,Guanqi Chen,Sishuo Liu,Yizhou Yu,Guanbin Li","id":"3c83f80f06633ff4598d33c2959f8e4cdcad3e93","summary":"This work reformulates image feature pre-training as a multi-task learning paradigm and witness its extraordinary superiority, forcing it to take into account the applicability of features for the specific image comprehension task.","score":2},{"url":"https://www.semanticscholar.org/paper/e34b699cef0a711a8cb9c39ecea20ac2df1578f5","title":"Medical Visual Question Answering: A Survey","venue":"Artif. Intell. Medicine","year":2021,"referenceCount":114,"citationCount":34,"influentialCitationCount":3,"publicationDate":"19/11/2021","authors":"Zhihong Lin,Donghao Zhang,Qingyi Tao,Danli Shi,Gholamreza Haffari,Qi Wu,M. He,Z. Ge","id":"e34b699cef0a711a8cb9c39ecea20ac2df1578f5","summary":"This survey collects and discusses the publicly available medical VQA datasets up-to-date about the data source, data quantity, and task feature, and summarizes the approaches used in medical V QA tasks.","score":2},{"url":"https://www.semanticscholar.org/paper/4ef3d9e492479e28fa57d107e52acc6a0c803de2","title":"Does CLIP Benefit Visual Question Answering in the Medical Domain as Much as it Does in the General Domain?","venue":"arXiv.org","year":2021,"referenceCount":31,"citationCount":59,"influentialCitationCount":12,"publicationDate":"27/12/2021","authors":"Sedigheh Eslami,Gerard de Melo,C. Meinel","id":"4ef3d9e492479e28fa57d107e52acc6a0c803de2","summary":"A fine-tuned version of CLIP for the medical domain based on PubMed articles is presented, which leads to noticeable improvements for MedVQA and fundamental performance differences of VQA in general versus medical domains are witnessed.","score":2}]}
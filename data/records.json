{"papers":[{"url":"https://www.semanticscholar.org/paper/4d4a96708fc67403176bb2b891b564af7a20c148","title":"RAMM: Retrieval-augmented Biomedical Visual Question Answering with Multi-modal Pre-training","venue":"arXiv.org","year":2023,"referenceCount":49,"citationCount":3,"influentialCitationCount":0,"publicationDate":"01/03/2023","authors":"Zheng Yuan,Qiao Jin,Chuanqi Tan,Zhengyun Zhao,Hongyi Yuan,Fei Huang,Songfang Huang","citations":[{"paperId":"72b06aef94f798ad9035b5775c186fd2fd6a8f38","title":"Multi-domain improves out-of-distribution and data-limited scenarios for medical image analysis"},{"paperId":"effc7842011fac2b9eefceec58f2a730d4d54c02","title":"LADER: Log-Augmented DEnse Retrieval for Biomedical Literature Search"},{"paperId":"00a9a469bb019bf33eeee438c110f704b71cda73","title":"Retrieving Multimodal Information for Augmented Generation: A Survey"}],"references":[{"paperId":"81adb80e390f25d4a2d764b1063eeaa2c334d441","title":"Multi-modal Masked Autoencoders for Medical Vision-and-Language Pre-training"},{"paperId":"28ff0816f19a5e3e37eac5569de41872fd262f0a","title":"Align, Reason and Learn: Enhancing Medical Vision-and-Language Pre-training with Knowledge"},{"paperId":"02251886950770e82b3d68564d60cdfe15e73199","title":"Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks"},{"paperId":"4a21aa3c75c4f29f9b340a73ff24f20834a7b686","title":"Retrieval-Augmented Transformer for Image Captioning"},{"paperId":"f5c165b6317896a65151050201c737536fa17c31","title":"mPLUG: Effective and Efficient Vision-Language Learning by Cross-modal Skip-connections"},{"paperId":"0db5207510819b9956849eb84bfe8703f8f3688d","title":"BioBART: Pretraining and Evaluation of A Biomedical Generative Language Model"},{"paperId":"15115f67452f3305b69e6886cee98ac466d42cd5","title":"Retrieval Augmented Classification for Long-Tail Visual Recognition"},{"paperId":"4ef3d9e492479e28fa57d107e52acc6a0c803de2","title":"Does CLIP Benefit Visual Question Answering in the Medical Domain as Much as it Does in the General Domain?"},{"paperId":"94ff111c4d81bd03f159321728ceec8b4711c89d","title":"An Empirical Study of Training End-to-End Vision-and-Language Transformers"},{"paperId":"b82c5f9efdb2ae56baa084ca41aeddd8a665c1d1","title":"Align before Fuse: Vision and Language Representation Learning with Momentum Distillation"},{"paperId":"520bd2331cca8d5a9c032c186a2a0f7704ead6ff","title":"R-Drop: Regularized Dropout for Neural Networks"},{"paperId":"d9317660e2a538d9c018028956fd114d55330f82","title":"Multi-Modal Understanding and Generation for Medical Images and Text via Vision-Language Pre-Training"},{"paperId":"5bd42c29a5ba8a6c39547db89023d879e98a6b32","title":"Multiple Meta-model Quantifying for Medical Visual Question Answering"},{"paperId":"3c83f80f06633ff4598d33c2959f8e4cdcad3e93","title":"Cross-Modal Self-Attention with Multi-Task Pre-Training for Medical Visual Question Answering"},{"paperId":"58fe64beb45b18f63cbc001849a0dee3e4e60482","title":"Improving Biomedical Pretrained Language Models with Knowledge"},{"paperId":"17c2bb358169541f2d0a769f80779f46d1cd3d37","title":"MMBERT: Multimodal BERT Pretraining for Improved Medical VQA"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"93b6b79b4ef6c345f31722ce7c829385c6dce0d6","title":"Slake: A Semantically-Labeled Knowledge-Enhanced Dataset For Medical Visual Question Answering"},{"paperId":"a627232a97a7a63f8399d157f0b022eb1ccd547c","title":"Biomedical Question Answering: A Survey of Approaches and Challenges"},{"paperId":"b8d5b853f2212cbb48a43f1edec9b96d76d388ec","title":"Medical Visual Question Answering via Conditional Reasoning"},{"paperId":"a2f38d03fd363e920494ad65a5f0ad8bd18cd60b","title":"Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing"},{"paperId":"58ed1fbaabe027345f7bb3a6312d41c5aac63e22","title":"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"},{"paperId":"832fff14d2ed50eb7969c4c4b976c35776548f56","title":"REALM: Retrieval-Augmented Language Model Pre-Training"},{"paperId":"d1f407b16fb8d99487baee37ed0805676c58e7ac","title":"MIMIC-CXR, a de-identified publicly available database of chest radiographs with free-text reports"},{"paperId":"87f6a7c014ce206ac5b57299c07e10667d194b39","title":"Randaugment: Practical automated data augmentation with a reduced search space"},{"paperId":"33301b25a297b701bdc287e985c006375cb7bb21","title":"Overcoming Data Limitation in Medical Visual Question Answering"},{"paperId":"a81874b4a651a740fffbfc47ef96515e8c7f782f","title":"Latent Retrieval for Weakly Supervised Open Domain Question Answering"},{"paperId":"156d217b0a911af97fa1b5a71dc909ccef7a8028","title":"SciBERT: A Pretrained Language Model for Scientific Text"},{"paperId":"1e43c7084bdcb6b3102afaf301cce10faead2702","title":"BioBERT: a pre-trained biomedical language representation model for biomedical text mining"},{"paperId":"a564fabf130ff6e2742cfba90c7a4018937d764d","title":"Radiology Objects in COntext (ROCO): A Multimodal Image Dataset"},{"paperId":"a5d10341717c0519cf63151b496a6d2ed67aa05f","title":"Bilinear Attention Networks"},{"paperId":"d07284a6811f1b2745d91bdb06b040b57f226882","title":"Decoupled Weight Decay Regularization"},{"paperId":"8e9ad6f8b2bc97f0412fa0cc243ac6975864534a","title":"Multi-modal Factorized Bilinear Pooling with Co-attention Learning for Visual Question Answering"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"2cbb8de53759e75411bc528518947a3094fbce3a","title":"Billion-Scale Similarity Search with GPUs"},{"paperId":"2c1890864c1c2b750f48316dc8b650ba4772adc5","title":"Stacked Attention Networks for Image Question Answering"},{"paperId":"47ced790a563344efae66588b5fb7fe6cca29ed3","title":"The Probabilistic Relevance Framework: BM25 and Beyond"},{"paperId":"769eae977c287f7696ad8fd4cc568785fdbe1779","title":"PMC-Patients: A Large-scale Dataset of Patient Notes and Relations Extracted from Case Reports in PubMed Central"},{"paperId":"02c7c78fa8585b4f37420b9e0acbaf77d70108a8","title":"ViLMedic: a framework for research at the intersection of vision and language in medical AI"},{"paperId":"c8b25fab5608c3e033d34b4483ec47e68ba109b7","title":"Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"},{"paperId":"eae760ee19c7d0c4e2b39adca209abcbc59e0a37","title":"Overview of the ImageCLEF 2021: Multimedia Retrieval in Medical, Nature, Internet and Social Media Applications"},{"paperId":"519799a9e2a72b808d81c6bcba5de263503de053","title":"Contrastive Pre-training and Representation Distillation for Medical Visual Question Answering Based on Radiology Images"},{"paperId":"13e7212d5af59137ad770f42712d762247ebd3ed","title":"SYSU-HCP at VQA-Med 2021: A Data-centric Model with Efficient Training Methodology for Medical Visual Question Answering"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"9eeeb23546d3d2bbc73959bffc6819f2335f3c83","title":"VQA-Med: Overview of the Medical Visual Question Answering Task at ImageCLEF 2019"},{"paperId":null,"title":"Technologies, Volume 1 (Long and Short Papers) , pages 4171–4186, Minneapolis, Minnesota, June"},{"paperId":"18f9a6045ba01cb079c4fa49a630d71bbd27cd92","title":"Descriptor : A dataset of clinically generated visual questions and answers about radiology images"},{"paperId":"8fc8f0cca1ef3ffdc3a327e8834fd19c84598a4c","title":"The language model"},{"paperId":null,"title":"MM ’23, October 29-November 3, 2023, Ottawa, ON, Canada"}],"id":"4d4a96708fc67403176bb2b891b564af7a20c148","summary":"This paper collects a new biomedical dataset named PMCPM which offers patient-based image-text pairs containing diverse patient situations from PubMed and proposes a retrieval-augmented pretrain-and-finetune paradigm named RAMM for biomedical VQA to overcome the data limitation issue."},{"url":"https://www.semanticscholar.org/paper/93b6b79b4ef6c345f31722ce7c829385c6dce0d6","title":"Slake: A Semantically-Labeled Knowledge-Enhanced Dataset For Medical Visual Question Answering","venue":"IEEE International Symposium on Biomedical Imaging","year":2021,"referenceCount":15,"citationCount":73,"influentialCitationCount":8,"publicationDate":"18/02/2021","authors":"Bo Liu,Li-Ming Zhan,Li Xu,Lin Ma,Y. Yang,Xiao-Ming Wu","citations":[{"paperId":"a3d418b4e35a02e4306505ab660a6bcd44c3c752","title":"Asclepius: A Spectrum Evaluation Benchmark for Medical Multi-Modal Large Language Models"},{"paperId":"bc431ad78e91a73a69580b7d256d18777dcda313","title":"Prompt-based Personalized Federated Learning for Medical Visual Question Answering"},{"paperId":"0fd27cba73c1af107a32b3c6138b643f22cf8749","title":"MultiMedEval: A Benchmark and a Toolkit for Evaluating Medical Vision-Language Models"},{"paperId":"7580327ffc9bd5daef83fe8285c0476ca074051d","title":"OmniMedVQA: A New Large-Scale Comprehensive Evaluation Benchmark for Medical LVLM"},{"paperId":"61cadcfa555cbef120df7c017ef02e87f19900b7","title":"Free Form Medical Visual Question Answering in Radiology"},{"paperId":"63de69245502d9a22de04581a4b5c0168d596aa3","title":"Generalizing Visual Question Answering from Synthetic to Human-Written Questions via a Chain of QA with a Large Language Model"},{"paperId":"1e0d21dc2caf7b58342ddc8609fb30cdc1e27cd5","title":"MISS: A Generative Pretraining and Finetuning Approach for Med-VQA"},{"paperId":"1e1230ef1de1ba9c4f6cb4789184a295133afac0","title":"Visual Instruction Tuning towards General-Purpose Multimodal Model: A Survey"},{"paperId":"352252231462c24440bc0016638ea5fe8d4c6f7e","title":"UniDCP: Unifying Multiple Medical Vision-language Tasks via Dynamic Cross-modal Learnable Prompts"},{"paperId":"d48fa3ed73817563130ef217d85011ce1fbe7470","title":"BESTMVQA: A Benchmark Evaluation System for Medical Visual Question Answering"},{"paperId":"2c7e346aa311fec4dda04bdf3a214ce2026d8807","title":"Medical Vision Language Pretraining: A survey"},{"paperId":"d77bc1a237b67c57b0c1b99b4802e703747a9688","title":"BenchLMM: Benchmarking Cross-style Visual Capability of Large Multimodal Models"},{"paperId":"1f280fe1f800bae53ada94b30f244fab0cd9f795","title":"KI-MAG: A knowledge-infused abstractive question answering system in medical domain"},{"paperId":"8d2709ed1788a67e64425fb410bb49f3ee49e088","title":"Large Language Models Illuminate a Progressive Pathway to Artificial Healthcare Assistant: A Review"},{"paperId":"749104d1a207f5bc192c7d95a12856b5e7f84d1f","title":"Mapping medical image-text to a joint space via masked modeling"},{"paperId":"18c48b42941d16eeaf053ae18b1fe671934af134","title":"Scaling-up medical vision-and-language representation learning with federated learning"},{"paperId":"cb886bc9674d689d3a1f23713826374279894557","title":"EHRXQA: A Multi-Modal Question Answering Dataset for Electronic Health Records with Chest X-ray Images"},{"paperId":"da9134f694959b68027c33c8e998ffb3d41305da","title":"Exploring Question Decomposition for Zero-Shot VQA"},{"paperId":"c7492913370b5726eaa6ced163a60de6c9d4bb7f","title":"A Survey of Large Language Models for Healthcare: from Data, Technology, and Applications to Accountability and Ethics"},{"paperId":"910e82a2a825d3d1939908094c01134c2cf30dcc","title":"MITER: Medical Image-TExt joint adaptive pretRaining with multi-level contrastive learning"},{"paperId":"a0476578761e983d5ab2083abab07b81236c1d58","title":"Asymmetric cross-modal attention network with multimodal augmented mixup for medical visual question answering"},{"paperId":"f0950a3f27c0fefffba60ae1c9a8ee360d5eb55f","title":"Instruction Tuning for Large Language Models: A Survey"},{"paperId":"df0ddb588a200d095743e9d26fc4a9318619766e","title":"Towards Generalist Foundation Model for Radiology"},{"paperId":"304f8b4edea01fdb5a2f7f8b998c83188deeccff","title":"Towards Generalist Biomedical AI"},{"paperId":"1a78aee4ba07c3b2486083c3a500bc8b3bd4df7a","title":"MDKG: Graph-Based Medical Knowledge-Guided Dialogue Generation"},{"paperId":"a4e3f9a3f8c875679c7806d846853b4530843799","title":"A Medical Domain Visual Question Generation Model via Large Language Model"},{"paperId":"baa1dc079d98ca76b0173c8d653fed759fd0a371","title":"A scoping review on multimodal deep learning in biomedical images and texts"},{"paperId":"bf40c9e7832e1b2887cbf5798455f91705ea11ba","title":"Masked Vision and Language Pre-training with Unimodal and Multimodal Contrastive Losses for Medical Visual Question Answering"},{"paperId":"e0e9ba0c01d441e1fdcb8628d3f743d387b0b017","title":"UIT-Saviors at MEDVQA-GI 2023: Improving Multimodal Learning with Image Enhancement for Gastrointestinal Visual Question Answering"},{"paperId":"8ca86bc84bf332352e22837de2d001d4a93414a1","title":"Localized Questions in Medical Visual Question Answering"},{"paperId":"534675abb9d72fc0c08d080d4f73335ceb75902c","title":"Multimodal Prompt Retrieval for Generative Visual Question Answering"},{"paperId":"ebedc4d7a2356090904baba4104ef0832bc236df","title":"A Survey on Multimodal Large Language Models"},{"paperId":"f57afb6c8addfc7a32f9be5916a374a542d1a026","title":"ECG-QA: A Comprehensive Question Answering Dataset Combined With Electrocardiogram"},{"paperId":"64fa56962dd0f4bbe206be6142fbe0315c4e7c2f","title":"Multi-modal Pre-training for Medical Vision-language Understanding and Generation: An Empirical Study with A New Benchmark"},{"paperId":"f22d71c7ce9720ba1f717a4f1181488200e78198","title":"LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day"},{"paperId":"07d45ce7de598ef03b400f8ddba7d2e055e77a08","title":"BiomedGPT: A Unified and Generalist Biomedical Generative Pre-trained Transformer for Vision, Language, and Multimodal Tasks"},{"paperId":"f4793adffd6f67ffcb93ccfc5672ab301b8a2b96","title":"PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering"},{"paperId":"3b508a48a4b48d2a16dd790a2a04ffcf51c0b4a6","title":"SUR-adapter: Enhancing Text-to-Image Pre-trained Diffusion Models with Large Language Models"},{"paperId":"ac4d13b6a4f9fb67337099f4602135a0351f5c99","title":"Towards Medical Artificial General Intelligence via Knowledge-Enhanced Multimodal Pretraining"},{"paperId":"f7ea746cd2cc25628a7a553ac27d228198be42cb","title":"Pre-trained multilevel fuse network based on vision-conditioned reasoning and bilinear attentions for medical image visual question answering"},{"paperId":"8f3138f7ee5127faab265793be8ae278bc49d9b1","title":"PMC-CLIP: Contrastive Language-Image Pre-training using Biomedical Documents"},{"paperId":"785650a805851c7e945523e495c5a523c60f72a4","title":"Open-Ended Medical Visual Question Answering Through Prefix Tuning of Language Models"},{"paperId":"5814bd146b37e13115af4330caf3a751159a156f","title":"BiomedCLIP: a multimodal biomedical foundation model pretrained from fifteen million scientific image-text pairs"},{"paperId":"8200be2e8b9af243ee72a9d919a4f7fbe82a17d2","title":"Medical knowledge-based network for Patient-oriented Visual Question Answering"},{"paperId":"4d4a96708fc67403176bb2b891b564af7a20c148","title":"RAMM: Retrieval-augmented Biomedical Visual Question Answering with Multi-modal Pre-training"},{"paperId":"9259c41695c4451f1ca3e6bdc9829623b43f9a69","title":"Interpretable Medical Image Visual Question Answering via Multi-Modal Relationship Graph Learning"},{"paperId":"da9579539385daedd33a0de0f814e2977ad0d1f5","title":"Towards Unifying Medical Vision-and-Language Pre-training via Soft Prompts"},{"paperId":"940f303c2530a52c5fd3c52c9c64ceea4b53ab05","title":"Diversity Learning Based on Multi-Latent Space for Medical Image Visual Question Generation"},{"paperId":"2580d3fc39fed3989f10665559a955b847b7eb7f","title":"Medical Visual Question Answering via Conditional Reasoning and Contrastive Learning"},{"paperId":"90cd86b3c157e40cbaf1076f69cbd38d9c0781b9","title":"UnICLAM: Contrastive Representation Learning with Adversarial Masking for Unified and Interpretable Medical Vision Question Answering"},{"paperId":"5942335fdd35d1651aaabd7af4db129a29ed2a85","title":"How Well Apply Multimodal Mixup and Simple MLPs Backbone to Medical Visual Question Answering?"},{"paperId":"560e0114a023bdfd99eb60eb4d9d555a348600a0","title":"PiggyBack: Pretrained Visual Question Answering Environment for Backing up Non-deep Learning Professionals"},{"paperId":"170667a96f04adf3b3b83526f75fe8d1063e0f7a","title":"Self-Supervised Vision-Language Pretraining for Medial Visual Question Answering"},{"paperId":"6ae700c89a9a9a3da7e55dd51c4710b5ed8c8d4e","title":"Caption-Aware Medical VQA via Semantic Focusing and Progressive Cross-Modality Comprehension"},{"paperId":"28ff0816f19a5e3e37eac5569de41872fd262f0a","title":"Align, Reason and Learn: Enhancing Medical Vision-and-Language Pre-training with Knowledge"},{"paperId":"81adb80e390f25d4a2d764b1063eeaa2c334d441","title":"Multi-modal Masked Autoencoders for Medical Vision-and-Language Pre-training"},{"paperId":"0cbd644254462341a897d4bfa0134637662c3ab5","title":"A Transformer-based Medical Visual Question Answering Model"},{"paperId":"ef2edea434e487f288d4eed6f9b1dc480b917211","title":"Adversarial Learning to Improve Question Image Embedding in Medical Visual Question Answering"},{"paperId":"67f992f43cc777a3e1aedc14cf3a11582ccfa570","title":"OVQA: A Clinically Generated Visual Question Answering Dataset"},{"paperId":"e9480d62e216f77d5556b7eda769daa4c92d004d","title":"VQAMix: Conditional Triplet Mixup for Medical Visual Question Answering"},{"paperId":"22f2f53e7474af620268318ac3ff4bb5a4fe3ab4","title":"MED-GPVS: A Deep Learning-Based Joint Biomedical Image Classification and Visual Question Answering System for Precision e-Health"},{"paperId":"ab2ba04580edb4340a896b37543e77fdc2ec6bbf","title":"Hybrid deep learning model for answering visual medical questions"},{"paperId":"e678898301a66faab85dfa4c84e51118e434b8f2","title":"Vision-Language Transformer for Interpretable Pathology Visual Question Answering"},{"paperId":"4ef3d9e492479e28fa57d107e52acc6a0c803de2","title":"Does CLIP Benefit Visual Question Answering in the Medical Domain as Much as it Does in the General Domain?"},{"paperId":"e34b699cef0a711a8cb9c39ecea20ac2df1578f5","title":"Medical Visual Question Answering: A Survey"},{"paperId":"3c83f80f06633ff4598d33c2959f8e4cdcad3e93","title":"Cross-Modal Self-Attention with Multi-Task Pre-Training for Medical Visual Question Answering"},{"paperId":"b88f6aa65a4e1faf963494a76d28cc12112c9543","title":"A Critical Analysis of Benchmarks, Techniques, and Models in Medical Visual Question Answering"},{"paperId":"ba067bb339ce3d28c0f084a029485374b3773aba","title":"Reducing Knowledge Noise for Improved Semantic Analysis in Biomedical Natural Language Processing Applications"},{"paperId":"61c0b6a5e7aea48a1376b61a4a737137d602b242","title":"PubMedCLIP: How Much Does CLIP Benefit Visual Question Answering in the Medical Domain?"},{"paperId":"31a7d8c4a5ab6bab522494b57270249105c8748e","title":"BiomedGPT: A Unified and Generalist Biomedical Generative Pre-trained Transformer for Vision, Language, and Multimodal Tasks"},{"paperId":"801e653b5b8bfc43571861c633b2a46c455b7d1e","title":"Multistep Question-Driven Visual Question Answering for Remote Sensing"},{"paperId":"99267305914a44ad6d626b7a6406fd0079b5508d","title":"Incorporating Medical Knowledge to Transformer-based Language Models for Medical Dialogue Generation"},{"paperId":"357fc385caf2e5b9898c9140fa3ac9955e6bb3c6","title":"TeamS at VQA-Med 2021: BBN-Orchestra for Long-tailed Medical Visual Question Answering"}],"references":[{"paperId":"b8d5b853f2212cbb48a43f1edec9b96d76d388ec","title":"Medical Visual Question Answering via Conditional Reasoning"},{"paperId":"33301b25a297b701bdc287e985c006375cb7bb21","title":"Overcoming Data Limitation in Medical Visual Question Answering"},{"paperId":"4654aa505e5bcdb089d0df202cd7ceabc9d2d41f","title":"A large annotated medical image dataset for the development and evaluation of segmentation algorithms"},{"paperId":"03eb382e04cca8cca743f7799070869954f1402a","title":"CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning"},{"paperId":"b60630911d7746fba06de7c34abe98c9a61c6bcc","title":"FVQA: Fact-Based Visual Question Answering"},{"paperId":"8e759195eb4b4f0f480a8a2cf1c629bfd881d4e5","title":"Analyzing the Behavior of Visual Question Answering Models"},{"paperId":"5fa973b8d284145bf0ced9acf2913a74674260f6","title":"Yin and Yang: Balancing and Answering Binary Visual Questions"},{"paperId":"2c1890864c1c2b750f48316dc8b650ba4772adc5","title":"Stacked Attention Networks for Image Question Answering"},{"paperId":"97ad70a9fa3f99adf18030e5e38ebe3d90daa2db","title":"VQA: Visual Question Answering"},{"paperId":"eb42cf88027de515750f230b23b1a057dc782108","title":"Very Deep Convolutional Networks for Large-Scale Image Recognition"},{"paperId":"2582ab7c70c9e7fcb84545944eba8f3a7f253248","title":"Translating Embeddings for Modeling Multi-relational Data"},{"paperId":"038b582cccb00c54589c5563d9a00ee28dad83b0","title":"User-guided 3D active contour segmentation of anatomical structures: Significantly improved efficiency and reliability"},{"paperId":"05e882679d61f4c64a68ebe21826251a39f87e98","title":"ChestX-ray: Hospital-Scale Chest X-ray Database and Benchmarks on Weakly Supervised Classification and Localization of Common Thorax Diseases"},{"paperId":"18f9a6045ba01cb079c4fa49a630d71bbd27cd92","title":"Descriptor : A dataset of clinically generated visual questions and answers about radiology images"},{"paperId":null,"title":"CHAOS - Combined (CT- MR) Healthy Abdominal Organ Segmentation Challenge Data"}],"id":"93b6b79b4ef6c345f31722ce7c829385c6dce0d6","summary":"A large bilingual dataset, SLAKE, with comprehensive semantic labels annotated by experienced physicians and a new structural medical knowledge base for Med-VQA is presented, which includes richer modalities and covers more human body parts than the currently available dataset."},{"url":"https://www.semanticscholar.org/paper/02251886950770e82b3d68564d60cdfe15e73199","title":"Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks","venue":"arXiv.org","year":2022,"referenceCount":75,"citationCount":425,"influentialCitationCount":25,"publicationDate":"22/08/2022","authors":"Wenhui Wang,Hangbo Bao,Li Dong,Johan Bjorck,Zhiliang Peng,Qiang Liu,Kriti Aggarwal,O. Mohammed,Saksham Singhal,S. Som,Furu Wei","citations":[{"paperId":"527915316fb4ed374e68259d88c3d186c83089cb","title":"CLCE: An Approach to Refining Cross-Entropy and Contrastive Learning for Optimized Learning Fusion"},{"paperId":"7861d04b41c9848905cb73268040d10e23409c77","title":"Self-Guided Masked Autoencoders for Domain-Agnostic Self-Supervised Learning"},{"paperId":"66d5ff7efd85b4a4bbc572b005a6a9304a11d447","title":"Towards Seamless Adaptation of Pre-trained Models for Visual Place Recognition"},{"paperId":"ad7bf54ff801bcee3aa2745e36c66433fd5c8dcd","title":"Multimodal Learned Sparse Retrieval for Image Suggestion"},{"paperId":"a144458d489dcca348106b015b2d42338d319dcd","title":"CDYL for infrared and visible light image dense small object detection"},{"paperId":"a091bf215c716a146140f81c751712db628c8e20","title":"MobileVLM V2: Faster and Stronger Baseline for Vision Language Model"},{"paperId":"ae6e037df45bb7e9e4c4ce6758a1e05a68e96385","title":"A Deep-Learning-Based Low-Cost Micro-Leakage Measurement System for Industrial Applications"},{"paperId":"f5ed73107050a78595a4483eb39917bd6c278866","title":"M2-Encoder: Advancing Bilingual Image-Text Understanding by Large-scale Efficient Pretraining"},{"paperId":"cd1d7f5c4ce2d31ce9ee72db165a8272624da7d3","title":"MoE-LLaVA: Mixture of Experts for Large Vision-Language Models"},{"paperId":"66783bebc4ee086cfb8b6e25df80e08ecff10f86","title":"GeoDecoder: Empowering Multimodal Map Understanding"},{"paperId":"ca7ccea387f677ad449d1dda2b3ae4e775b9b62d","title":"Multimodal Pathway: Improve Transformers with Irrelevant Data from Other Modalities"},{"paperId":"a050c9b0c321839e4427ab9defa3463be7825ac4","title":"MM-LLMs: Recent Advances in MultiModal Large Language Models"},{"paperId":"cdce4525bc94b8b72d7330f4e26775142edd0018","title":"The Curious Case of Nonverbal Abstract Reasoning with Multi-Modal Large Language Models"},{"paperId":"1d24f2dfe942e2d67381d8fcfac3423145bd557f","title":"DGL: Dynamic Global-Local Prompt Tuning for Text-Video Retrieval"},{"paperId":"2858bee78af4ee085b1c21d2c67a1f669db30a05","title":"Seek for Incantations: Towards Accurate Text-to-Image Diffusion Synthesis through Prompt Engineering"},{"paperId":"0833d5b8c46fe7391e7decc3f11fea87e023d710","title":"SyCoCa: Symmetrizing Contrastive Captioners with Attentive Masking for Multimodal Alignment"},{"paperId":"f34302a575f8d225094ad451f96252c1639e34b9","title":"Few-shot Adaptation of Multi-modal Foundation Models: A Survey"},{"paperId":"5670e59032e9b70f5af366ef8641282c204b555e","title":"Masked Modeling for Self-supervised Representation Learning on Vision and Beyond"},{"paperId":"dc6a7257c19be0b48d200bcb66c493c4ac07a632","title":"Mask Grounding for Referring Image Segmentation"},{"paperId":"608a2b333fd8262e8c918f36c5700bafd3ea3cdd","title":"GeomVerse: A Systematic Evaluation of Large Models for Geometric Reasoning"},{"paperId":"617dc8b4126e3c250f6aa474eab65542385e1c16","title":"HEAR: Hearing Enhanced Audio Response for Video-grounded Dialogue"},{"paperId":"ea6982a936a2b263bbf46ff6eb27fc0b63fddaf7","title":"VL-GPT: A Generative Pre-trained Transformer for Vision and Language Understanding and Generation"},{"paperId":"bc3dc405652885b8610e7a7fd863e45edede2ea4","title":"General Object Foundation Model for Images and Videos at Scale"},{"paperId":"6768a6aeb61ad8522795d92bb0ca44f87a327a59","title":"Pixel Aligned Language Models"},{"paperId":"1c199e3d50349153c0b6200006b02fbe66c27acd","title":"X2-VLM: All-In-One Pre-trained Model For Vision-Language Tasks."},{"paperId":"55c6d16b550c606d62dd85084f0d373d8f087966","title":"VLAP: Efficient Video-Language Alignment via Frame Prompting and Distilling for Video Question Answering"},{"paperId":"b1721374889899950994f67029fe899de257c140","title":"A Foundational Multimodal Vision Language AI Assistant for Human Pathology"},{"paperId":"f1a81194349bfe012b1c01a1fd0b2bcd66889b62","title":"ProxyDet: Synthesizing Proxy Novel Classes via Classwise Mixup for Open-Vocabulary Object Detection"},{"paperId":"13925d7d5952b1ba5960dfe2c44d977be109b636","title":"4M: Massively Multimodal Masked Modeling"},{"paperId":"66b19b2fd01ebcce2118dc707e18202b4ba1f39a","title":"Transformer as Linear Expansion of Learngene"},{"paperId":"24df8cdc293ac8857b4a3afaeec0865b3d86a5fd","title":"User-Aware Prefix-Tuning is a Good Learner for Personalized Image Captioning"},{"paperId":"b06c32f80485fbe4e0ebbe2be8385ddd2e59c9b9","title":"Integrating Multiple Models For Effective Video Retrieval and Multi-stage Search"},{"paperId":"81ef54d3d3e00d866d03cd370a207114ac005fa2","title":"SYNC-CLIP: Synthetic Data Make CLIP Generalize Better in Data-Limited Scenarios"},{"paperId":"2feb5d47fedbc360649746123c3cbaafff1863ff","title":"From Pixels to Explanations: Uncovering the Reasoning Process in Visual Question Answering"},{"paperId":"769b794fe9f97268007676171f246d45e0631014","title":"Towards More Unified In-context Visual Understanding"},{"paperId":"d839519477bd2f2055ec189f796bce578c578102","title":"Foundation Models for Weather and Climate Data Understanding: A Comprehensive Survey"},{"paperId":"0efd32eb49f151af3df0f005f05cbf15982e78d3","title":"RHViT: A Robust Hierarchical Transformer for 3D Multimodal Brain Tumor Segmentation Using Biased Masked Image Modeling Pre-training"},{"paperId":"c3f75e1a55fed93fd3c5bc785d62a0081b1b0d20","title":"Rejuvenating image-GPT as Strong Visual Representation Learners"},{"paperId":"246017780386eba39d6cda760a1c2c70356baa50","title":"VIoTGPT: Learning to Schedule Vision Tools towards Intelligent Video Internet of Things"},{"paperId":"3a2bb5b4f1f724df9295de5e0e1235cc30769b82","title":"Omni-SMoLA: Boosting Generalist Multimodal Models with Soft Mixture of Low-rank Experts"},{"paperId":"e24241a8188d7c41d841cd6413da936adc247bb4","title":"VIDiff: Translating Videos via Multi-Modal Instructions with Diffusion Models"},{"paperId":"09157a8c0e7d7263ac035690118ddcbe295cee5c","title":"ShapeGPT: 3D Shape Generation with A Unified Multi-modal Language Model"},{"paperId":"bb5149c6705fb1a10c9c46196166c2a886b1b5af","title":"Elucidating and Overcoming the Challenges of Label Noise in Supervised Contrastive Learning"},{"paperId":"337f421a364a6fa8ca423afd627bee2426f69395","title":"Robot Learning in the Era of Foundation Models: A Survey"},{"paperId":"77e046e401b23d41b92142f3fc551272a91cd62e","title":"AI-Generated Images Introduce Invisible Relevance Bias to Text-Image Retrieval"},{"paperId":"7f807249c0ef0fe07d5e9c810684cd5daba0edc5","title":"De-fine: Decomposing and Refining Visual Programs with Auto-Feedback"},{"paperId":"56025f5034f7aebe1b7292284d33d3d0e3317614","title":"Deep Tensor Network"},{"paperId":"fddd97e8bc93e1c1737f7047a5ec77a0538ade8b","title":"Leveraging Foundation Models to Improve Lightweight Clients in Federated Learning"},{"paperId":"e5ce547819099f934b25a1b14001d0fc5a016af3","title":"Mirasol3B: A Multimodal Autoregressive model for time-aligned and contextual modalities"},{"paperId":"7eef96257d3eb163e6ad7701cc213d094faba9e4","title":"CREaTor: zero-shot cis-regulatory pattern modeling with attention mechanisms"},{"paperId":"312a08a5b0a9c904a937f32e75b3b8029bcae951","title":"VLIB: Unveiling insights through Visual and Linguistic Integration of Biorxiv data relevant to cancer via Multimodal Large Language Model"},{"paperId":"88bddfb7d1e0462be8fe99fdbd71c658140cb17b","title":"From Image to Language: A Critical Analysis of Visual Question Answering (VQA) Approaches, Challenges, and Opportunities"},{"paperId":"c3395bbd81e859d151bd6c632b8ce5fdd170dc1a","title":"Language Guided Visual Question Answering: Elevate Your Multimodal Language Model Using Knowledge-Enriched Prompts"},{"paperId":"227204838c27d26c524cb3e5fa7ee7101a32aab9","title":"Res-Tuning: A Flexible and Efficient Tuning Paradigm via Unbinding Tuner from Backbone"},{"paperId":"43abdd72a50706ca76e77a2d613a7435324f4997","title":"Intra-Modal Proxy Learning for Zero-Shot Visual Categorization with CLIP"},{"paperId":"264452694539d550e6f1a8ec87bffd9f622691ad","title":"Generating Context-Aware Natural Answers for Questions in 3D Scenes"},{"paperId":"5623e34a67bf22131219dafc2d3cb6b7d49d8ac2","title":"Text Augmented Spatial-aware Zero-shot Referring Image Segmentation"},{"paperId":"a3241277bde215e738b0edd179b705a751a0f947","title":"Entity Embeddings : Perspectives Towards an Omni-Modality Era for Large Language Models"},{"paperId":"9ee209ff36a7cb846af4f7a8791b772e54639f21","title":"Conversational Composed Retrieval with Iterative Sequence Refinement"},{"paperId":"158a533ab1372b327776fb274ce8465a2fb4e9f4","title":"LiFT: Transfer Learning in Vision-Language Models for Downstream Adaptation and Generalization"},{"paperId":"3398fe414d7d1f887bc909322c4c244ab2120442","title":"CARIS: Context-Aware Referring Image Segmentation"},{"paperId":"3f12da8f29e89579f5c133567955bbc9485bd9ec","title":"Modality-Agnostic Self-Supervised Learning with Meta-Learned Masked Auto-Encoder"},{"paperId":"29b3ce4de9dd9d784ca1d876957950f4b2d3796a","title":"Towards Perceiving Small Visual Details in Zero-shot Visual Question Answering with Multimodal LLMs"},{"paperId":"4f407e3baaa12da7470de49303e8b6bc129317af","title":"MARVEL: Unlocking the Multi-Modal Capability of Dense Retrieval via Visual Module Plugin"},{"paperId":"0d1b784a6f440d13d4f8d85f1a66ef3dde904067","title":"Image Clustering with External Guidance"},{"paperId":"b844ae48660313c5354b274073336faf9c2fcc34","title":"Non-Intrusive Adaptation: Input-Centric Parameter-efficient Fine-Tuning for Versatile Multimodal Modeling"},{"paperId":"36819c779070076e9364786223bd341dd9a89a77","title":"RoboLLM: Robotic Vision Tasks Grounded on Multimodal Large Language Models"},{"paperId":"bcb197654f39bb9312d8d0333646b71254d29239","title":"Mastering Robot Manipulation with Multimodal Prompts through Pretraining and Multi-task Fine-tuning"},{"paperId":"68e0e789b5147b1e7d028c7a825650075f4e26bf","title":"PaLI-3 Vision Language Models: Smaller, Faster, Stronger"},{"paperId":"03bf1da1caa5f63203d43ed78c12c35a78fc6ed9","title":"EasyGen: Easing Multimodal Generation with a Bidirectional Conditional Diffusion Model and LLMs"},{"paperId":"57a37c124ce272c20aacfcd595830d8fb8588fca","title":"IMITATE: Clinical Prior Guided Hierarchical Vision-Language Pre-training"},{"paperId":"b1e097be0e7fe37a90507b66abfbb6f22fef38b5","title":"Lightweight In-Context Tuning for Multimodal Unified Models"},{"paperId":"d282adc0a1c66e1b6b38a85883e1f60bf09e7f1d","title":"Leveraging Efficient Training and Feature Fusion in Transformers for Multimodal Classification"},{"paperId":"aec95e6330033e0ec39fb5a069d647288c03b945","title":"Assessing Large Language Models on Climate Information"},{"paperId":"52df6a9657ee57ba2518b4a620fd5637c5e1abd2","title":"Large Scale Masked Autoencoding for Reducing Label Requirements on SAR Data"},{"paperId":"1b65c321fb1bae97ad7c14112d11f838ccdc9f61","title":"Video Attribute Prototype Network: A New Perspective for Zero-Shot Video Classification"},{"paperId":"a99421f57eb4d64d8d7afe369d83d7bda14284fa","title":"Adapting Vision Foundation Models for Plant Phenotyping"},{"paperId":"a5d27bf7a2155d4ca016565a78b52ee90f81624c","title":"Beyond Task Performance: Evaluating and Reducing the Flaws of Large Multimodal Models with In-Context Learning"},{"paperId":"8254b2dce0a5c108777dd4d439f482d7b047c9f2","title":"Segment Every Reference Object in Spatial and Temporal Spaces"},{"paperId":"f4b5ed58491de52904c5eeb4b0c616dac507792f","title":"UniFormerV2: Unlocking the Potential of Image ViTs for Video Understanding"},{"paperId":"82731ea473d36f0ba9113dbcdba20943d9c9f302","title":"Self-Supervised Open-Ended Classification with Small Visual Language Models"},{"paperId":"819f477065088220a6f706cd9ef76dbcb4b4c134","title":"InstructCV: Instruction-Tuned Text-to-Image Diffusion Models as Vision Generalists"},{"paperId":"11a4284e335ba39330b59d9f42ca3272a6166991","title":"A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future"},{"paperId":"96c43227831c4c3b12b7c64809e78674cea3a8a1","title":"DeepSpeed-VisualChat: Multi-Round Multi-Image Interleave Chat via Multi-Modal Causal Attention"},{"paperId":"772724892819d7e6f15ce536753fdc32d022c0e0","title":"A Survey on Image-text Multimodal Models"},{"paperId":"4bfedad39c1cdd7cf2cc2beaa09d4fe8c5166f18","title":"TinyCLIP: CLIP Distillation via Affinity Mimicking and Weight Inheritance"},{"paperId":"4cfbc598cf52d38b201d41ee5b1f64b84341d5f1","title":"Sentence Attention Blocks for Answer Grounding"},{"paperId":"679dc88f230072f5b41ad0c85398203be1134a6c","title":"A Continual Learning Paradigm for Non-differentiable Visual Programming Frameworks on Visual Reasoning Tasks"},{"paperId":"0407c8b49c1c55914aca712e647743c96da674e0","title":"Empowering Visually Impaired Individuals: A Novel Use of Apple Live Photos and Android Motion Photos"},{"paperId":"5f4e88a7637f8edea3a207de2dc227a1012706b0","title":"Wall segmentation in 2D images using convolutional neural networks"},{"paperId":"75ca0ce6493c94bb48766b042243023a5439beeb","title":"InstructDiffusion: A Generalist Modeling Interface for Vision Tasks"},{"paperId":"d7e0ca05d9aade28559d25db59ea54afd1e1ac0e","title":"NICE: CVPR 2023 Challenge on Zero-shot Image Captioning"},{"paperId":"5ab8337d6c594c4cec1ae67428a21dbfca1aff15","title":"MultiWay-Adapater: Adapting large-scale multi-modal models for scalable image-text retrieval"},{"paperId":"390bfc1a1be1948d46a6a20b30953e96c11bfebe","title":"Unified Pre-training with Pseudo Texts for Text-To-Image Person Re-identification"},{"paperId":"a1cfe18cc2c063568d2e896fbf824e8a190d1cf5","title":"MAGMA: Music Aligned Generative Motion Autodecoder"},{"paperId":"964a3f6903602aa660ef3ae65bcf274df3d6c5c7","title":"BDC-Adapter: Brownian Distance Covariance for Better Vision-Language Reasoning"},{"paperId":"2d3ee3327e7c70196a0ea5e271e1e30da5551588","title":"RevColV2: Exploring Disentangled Representations in Masked Image Modeling"},{"paperId":"f12d3e0fefda050d0f80d800cb523e6563664f5a","title":"On the Study of Data Augmentation for Visual Place Recognition"},{"paperId":"8a809f59b354c4a524e0a58000af8ffd60ed67f2","title":"CLIP-Driven Prototype Network for Few-Shot Semantic Segmentation"},{"paperId":"17113e6e2703609cd63c744990f41206422b10c4","title":"ViLTA: Enhancing Vision-Language Pre-training through Textual Augmentation"},{"paperId":"e96ea949028f8e6966eb8ed8073da972d6b7befd","title":"InstaTune: Instantaneous Neural Architecture Search During Fine-Tuning"},{"paperId":"c4ead49c06b164adf9229f90ad098f1e44885c38","title":"When hard negative sampling meets supervised contrastive learning"},{"paperId":"38902cd7b19d10874d6ad3387d9a47ec8708a772","title":"Parameter-Efficient Transfer Learning for Audio-Visual-Language Tasks"},{"paperId":"a87947f88519dba980d0f16cdfb78ed09a8e02f0","title":"Computation-efficient Deep Learning for Computer Vision: A Survey"},{"paperId":"9315befa0f6e53dddb8ace2651c408bebe8b2cc8","title":"Video and Audio are Images: A Cross-Modal Mixer for Original Data on Video-Audio Retrieval"},{"paperId":"90a9a6f2b8d9f94ee5055d4a96654fa5de9eb53d","title":"Refine Neutrino Events Reconstruction with BEiT-3"},{"paperId":"58a282c89864f35bff1741f5ab439222da6bb3ec","title":"MLLM-DataEngine: An Iterative Refinement Approach for MLLM"},{"paperId":"e5daadcac08ebfc30759260cbd43812d4e3bc775","title":"DLIP: Distilling Language-Image Pre-training"},{"paperId":"fc6a2f7478f68adefd69e2071f27e38aa1647f2f","title":"Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond"},{"paperId":"6b42ea2e962c89dd28c0302cb58c178cd5ecf339","title":"CoCoOpter: Pre-train, prompt, and fine-tune the vision-language model for few-shot image classification"},{"paperId":"51ef336bb1bb9875d715abb78be93b58f952cb5c","title":"Dataset Quantization"},{"paperId":"362c78f5cc3fac37c3ca6d93f86019a45aad2324","title":"RLIPv2: Fast Scaling of Relational Language-Image Pre-training"},{"paperId":"80a791f644defb54f4eb24f99df31e6f995be3aa","title":"VL-PET: Vision-and-Language Parameter-Efficient Tuning via Granularity Control"},{"paperId":"2e3dcf5a5d58ac210d0d87e9f918540a8373211a","title":"GIT-Mol: A Multi-modal Large Language Model for Molecular Science with Graph, Image, and Text"},{"paperId":"f1b05df87ab27484efc23473c590bb167a6c8337","title":"Beyond Semantics: Learning a Behavior Augmented Relevance Model with Self-supervised Learning"},{"paperId":"fe779d52ef85c0387ed9a68cd90ca11033689bfa","title":"Temporally-Adaptive Models for Efficient Video Understanding"},{"paperId":"2c5f7663d551e948689109c848cb48e902fc8533","title":"MixReorg: Cross-Modal Mixed Patch Reorganization is a Good Mask Learner for Open-World Semantic Segmentation"},{"paperId":"64a80a33018a0fdc182b06111e32b2e08e186f6a","title":"3D-VisTA: Pre-trained Transformer for 3D Vision and Text Alignment"},{"paperId":"6a4e7c54648487cfce1d18344fdadf4a095c055b","title":"Distributionally Robust Classification on a Data Budget"},{"paperId":"d874af0891e0ed816d3db3379a700a2a42f1be08","title":"ConvFormer: Revisiting Transformer for Sequential User Modeling"},{"paperId":"659a12d71d8709c132ccd9ccd235f0024cae0239","title":"The All-Seeing Project: Towards Panoptic Visual Recognition and Understanding of the Open World"},{"paperId":"1cf01a830258133a18b3b0efb4ebf849491100a6","title":"Multimodal Adaptation of CLIP for Few-Shot Action Recognition"},{"paperId":"0fe88452660cb8a0e37f54bcd44f3cd6504354b5","title":"Unified Model for Image, Video, Audio and Language Tasks"},{"paperId":"8c403f0031ff0465042dc3789b1cc8e7d1363eb2","title":"Instance-Wise Adaptive Tuning and Caching for Vision-Language Models"},{"paperId":"5404f4e28645861ebed175e580e2b95a6fff9ca9","title":"UniBriVL: Robust Universal Representation and Generation of Audio Driven Diffusion Models"},{"paperId":"7534268b62f9e0034d91fa38f8ec4a66dd40a0a4","title":"BARTPhoBEiT: Pre-trained Sequence-to-Sequence and Image Transformers Models for Vietnamese Visual Question Answering"},{"paperId":"4a1b175ed90cd4cffd90ab5b594bd9a24e812f70","title":"Cross-Modal Concept Learning and Inference for Vision-Language Models"},{"paperId":"584ca135b61482fd89247113da87d784f738dbfa","title":"Foundational Models Defining a New Era in Vision: A Survey and Outlook"},{"paperId":"ed44aa02684dc0bc8396e12c16b776144d121db6","title":"MIMONet: Multi-Input Multi-Output On-Device Deep Learning"},{"paperId":"83c48aa341850af478247e3b34ba1ee1db9f1236","title":"Meta-Transformer: A Unified Framework for Multimodal Learning"},{"paperId":"a679c736d26dbcab41b483f2dbbc417da62c7a16","title":"UP-DP: Unsupervised Prompt Learning for Data Pre-Selection with Vision-Language Models"},{"paperId":"8ccfeee44d2d49857c9999c93cf20b93539d71c9","title":"DOMAS: Data Oriented Medical Visual Question Answering Using Swin Transformer"},{"paperId":"1b4012f38daa8f09299e16771973c91ce9464ee2","title":"DVPT: Dynamic Visual Prompt Tuning of Large Pre-trained Models for Medical Image Analysis"},{"paperId":"b1e40da545fed1b2849d708740525c490bba4f89","title":"Fuzzy Associational Rules and reasoning logic in computer vision models"},{"paperId":"fee492fe200fd8dbb0f9d762dfbbe188fd200599","title":"Mining of Single-Class by Active Learning for Semantic Segmentation"},{"paperId":"7e7d16f603ba848542abdbe4151fa3112f26bd72","title":"Deficiency-Aware Masked Transformer for Video Inpainting"},{"paperId":"177bf0086a714ff305e45b720cda82e992c9fc7c","title":"PiTL: Cross-modal Retrieval with Weakly-supervised Vision-language Pre-training via Prompting"},{"paperId":"06f7066e76e5acf7b03d4fda2c0d12d71a209185","title":"Fine-grained Text-Video Retrieval with Frozen Image Encoders"},{"paperId":"98f8793a18eaced0ce93f5202065496cc5a84943","title":"Bootstrapping Vision-Language Learning with Decoupled Language Pre-training"},{"paperId":"41c6028c620debae00ca5b30e2db5977225fec57","title":"mBLIP: Efficient Bootstrapping of Multilingual Vision-LLMs"},{"paperId":"3cc89f9e1a06960fd349cae901f4f5a1fb72a291","title":"Multimodal Molecular Pretraining via Modality Blending"},{"paperId":"73134ef3ac17961b4947c20aa5198c5b4affcc56","title":"EgoVLPv2: Egocentric Video-Language Pre-training with Fusion in the Backbone"},{"paperId":"9048483d7c9f44588818f4e0fc7876c65192cedc","title":"All in One: Exploring Unified Vision-Language Tracking with Multi-Modal Alignment"},{"paperId":"97ffad903208c7bea48e4c8be0a68e27fc33a478","title":"Distilling Large Vision-Language Model with Out-of-Distribution Generalizability"},{"paperId":"c5202ab27294d5c1eb4d2f0ca7e82afef91888f0","title":"VideoGLUE: Video General Understanding Evaluation of Foundation Models"},{"paperId":"9134351c33559499b44f191d2c8a43dc7b044f57","title":"Multiattribute multitask transformer framework for vision‐based structural health monitoring"},{"paperId":"bdfd7fc9478c391b7248b53205de670e611a4548","title":"sDREAMER: Self-distilled Mixture-of-Modality-Experts Transformer for Automatic Sleep Staging"},{"paperId":"662d707d3c8537f8f58dcbc57e696c26f0ec8c99","title":"Benchmarking Zero-Shot Recognition with Vision-Language Models: Challenges on Granularity and Specificity"},{"paperId":"242dbbef9e7f624525c57645f193e0b13a90ad44","title":"When Foundation Model Meets Federated Learning: Motivations, Challenges, and Future Directions"},{"paperId":"d212fa27f5868f0fd106e1a7bba908fd47da0816","title":"MotionGPT: Human Motion as a Foreign Language"},{"paperId":"dedbac319177d04ce63fe00d2fec24bdaab90d6d","title":"SugarCrepe: Fixing Hackable Benchmarks for Vision-Language Compositionality"},{"paperId":"8c88c693d5dc2802d3b76b85740e1f04fdaaf801","title":"Large sequence models for sequential decision-making: a survey"},{"paperId":"f2f5c0d00e6a4ccaf099c11a9790aa0afefe611f","title":"Generative Multimodal Entity Linking"},{"paperId":"948e8cfae92c2004f2dd5c9316f5972f8baaea21","title":"OBELISC: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents"},{"paperId":"b7c081edd67f011fc45879c01a5166f889d77491","title":"RS5M and GeoRSCLIP: A Large Scale Vision-Language Dataset and A Large Vision-Language Model for Remote Sensing"},{"paperId":"083772bdb6caa48fe6e8df82305d8826c1dbb406","title":"Improving Visual Question Answering by Multimodal Gate Fusion Network"},{"paperId":"8efc20988021ce3b4b05dd44b13e27260ee9b99b","title":"Investigating Prompting Techniques for Zero- and Few-Shot Visual Question Answering"},{"paperId":"c2b7d6a57229ac2bdef69b9e7bc3c280fb95eaf1","title":"Parameter-efficient is not sufficient: Exploring Parameter, Memory, and Time Efficient Adapter Tuning for Dense Predictions"},{"paperId":"a30b8f46950bdf40458f44b70cd7318c2ea86986","title":"LabelBench: A Comprehensive Framework for Benchmarking Adaptive Label-Efficient Learning"},{"paperId":"1474d4248e1cbcc91183456cdf1e7272e8a931de","title":"COSA: Concatenated Sample Pretrained Vision-Language Foundation Model"},{"paperId":"6d7766ad375128ffd43b8d718b9b4cb440a877df","title":"Transferring Knowledge for Food Image Segmentation using Transformers and Convolutions"},{"paperId":"3da05d18593683d50db57cebfc0c20069f8ea11e","title":"LOVM: Language-Only Vision Model Selection"},{"paperId":"0983883619a0ca597d055d0e58da2f514052913d","title":"Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration"},{"paperId":"b634f9ba35123d40f0af8d96a9c154025cf2cf2a","title":"Contrasting Intra-Modal and Ranking Cross-Modal Hard Negatives to Enhance Visio-Linguistic Compositional Understanding"},{"paperId":"dc8768de2287c90e99852635dc04b6e922459300","title":"ZeroForge: Feedforward Text-to-Shape Without 3D Supervision"},{"paperId":"c581d2ad3b092a2cc152d0c6f55fd6320f78eb3a","title":"A Survey of Vision-Language Pre-training from the Lens of Multimodal Machine Translation"},{"paperId":"e03295bd92bf0f85d95784a75851613ee2bd47c5","title":"Global and Local Semantic Completion Learning for Vision-Language Pre-training"},{"paperId":"ad4a998b7b2af6e607ea917d5e6974b0a097cc54","title":"Efficient Anomaly Detection with Budget Annotation Using Semi-Supervised Residual Transformer"},{"paperId":"5d321194696f1f75cf9da045e6022b2f20ba5b9c","title":"Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding"},{"paperId":"8213492345c67d2b0e692b6bb5c814d4f1aef8d2","title":"Benchmarking Robustness of Adaptation Methods on Pre-trained Vision-Language Models"},{"paperId":"f308832cba50169cb0e53da31ec2bd04ecaa5551","title":"UniDiff: Advancing Vision-Language Models with Generative and Discriminative Learning"},{"paperId":"2c600b2829a608ce366614dc4911ef672f2e4255","title":"On Masked Pre-training and the Marginal Likelihood"},{"paperId":"0dd88453703f0008019a9a55a364064f0e8aa5d0","title":"PV2TEA: Patching Visual Modality to Textual-Established Information Extraction"},{"paperId":"99368d6fc86e2eb181d9d36165cfed578bfe938d","title":"Q: How to Specialize Large Vision-Language Models to Data-Scarce VQA Tasks? A: Self-Train on Unlabeled Images!"},{"paperId":"a42fc49a300136d60aaebb668369010ee7746150","title":"Visual Language Pretrained Multiple Instance Zero-Shot Transfer for Histopathology Images"},{"paperId":"cfec3aed5b7a3c951f9fd6fe06fd4f69ca2c3519","title":"Fine-grained Image-text Matching by Cross-modal Hard Aligning Network"},{"paperId":"60bfce9b67d5e92e9d5d02028ca8e8adee14a42b","title":"Rip Current Segmentation: A Novel Benchmark and YOLOv8 Baseline Results"},{"paperId":"c0828c270bfa9d633f9727fcb0fdb1be703e362d","title":"Context-aware Alignment and Mutual Masking for 3D-Language Pre-training"},{"paperId":"0f4467a89f1ddf0a1bc956090f859ef9fcc81bc0","title":"TFRGAN: Leveraging Text Information for Blind Face Restoration with Extreme Degradation"},{"paperId":"8b304a31de4b511b888f9a70f143d546abc63091","title":"A Unified Multi-modal Structure for Retrieving Tracked Vehicles through Natural Language Descriptions"},{"paperId":"b42ceaedfd9830c5c3a67a0c312afc737443e561","title":"ManagerTower: Aggregating the Insights of Uni-Modal Experts for Vision-Language Representation Learning"},{"paperId":"fd928577d67dd01048d13f284a6256164bbcf2f0","title":"Learning without Forgetting for Vision-Language Models"},{"paperId":"4e33c5756aa18d248cf50fef9382acda1e0f65da","title":"VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset"},{"paperId":"3099d6f4965b4d73aa1e2b2880522ec89ed2dc0a","title":"PaLI-X: On Scaling up a Multilingual Vision and Language Model"},{"paperId":"024a25b2445ecb3a181c5e2f39fbf8b73a4c1a6f","title":"Emergent Modularity in Pre-trained Transformers"},{"paperId":"df7e899a2070d5823b30a58b34ddd9bee6bf0cbb","title":"Matrix Information Theory for Self-Supervised Learning"},{"paperId":"267f6d0ff0d6e278d8bf31dcc91430ba50c5ba78","title":"Benchmarking Diverse-Modal Entity Linking with Generative Models"},{"paperId":"cf5c2563f0ffa6c6057787cd6e710312d751fa85","title":"Toward Understanding Generative Data Augmentation"},{"paperId":"a99590d39c6ef362f6b788d2720c708e2ceab4bf","title":"LANISTR: Multimodal Learning from Structured and Unstructured Data"},{"paperId":"7073e3bc142b14579a999bb45a4b10d94394be25","title":"Detect Any Shadow: Segment Anything for Video Shadow Detection"},{"paperId":"c2c7ad3112c4b575e5d8163a0e574f9eb743cb52","title":"Zero-shot Visual Question Answering with Language Model Feedback"},{"paperId":"9fbcf610aa913c0394585097c2cdffc2e8b25bf3","title":"Email False Information Detection Based on Attention Mechanism and Contrastive Learning"},{"paperId":"06091944b864d6dc473cab63321a95fb9c4067cc","title":"ChatCAD+: Towards a Universal and Reliable Interactive CAD using LLMs"},{"paperId":"42e726e2ea5bbb946001947d1a5b31ccc6b7aef9","title":"VioLA: Unified Codec Language Models for Speech Recognition, Synthesis, and Translation"},{"paperId":"08b562aa8066c2342f0d03824221dea18f0a18d2","title":"Cream: Visually-Situated Natural Language Understanding with Contrastive Reading Model and Frozen Large Language Models"},{"paperId":"d57caa06a42baa90eb741a9afb10fe4fff8be82a","title":"SmartTrim: Adaptive Tokens and Parameters Pruning for Efficient Vision-Language Models"},{"paperId":"f99d3dd98c5ed66ecd167884b35940647a8c8991","title":"Multi-granularity Text Representation and Transformer-Based Fusion Method for Visual Question Answering"},{"paperId":"42585728685d7b2567e62b03463d1520f1bbe47e","title":"Perception Test: A Diagnostic Benchmark for Multimodal Video Models"},{"paperId":"590638c784c8a1e19f407809b8295f98cd4458e3","title":"Training Transitive and Commutative Multimodal Transformers with LoReTTa"},{"paperId":"6209b614db0065de89331196a1ae8aa59404f0db","title":"Know Your Self-supervised Learning: A Survey on Image-based Generative and Discriminative Training"},{"paperId":"c5b7f9e2af19db0c2bc9d57a113c17aa9d4d6fda","title":"S-CLIP: Semi-supervised Vision-Language Learning using Few Specialist Captions"},{"paperId":"6fe2a4f7594abadafc9feb05e96ab7e9e634ce8f","title":"Album Storytelling with Iterative Story-aware Captioning and Large Language Models"},{"paperId":"23c2ba16591edae0d5f461715c46f3a894946248","title":"VLAB: Enhancing Video Language Pre-training by Feature Adapting and Blending"},{"paperId":"bc2e8b613335259598ea5c49aea270469e9a35ed","title":"A PhD Student's Perspective on Research in NLP in the Era of Very Large Language Models"},{"paperId":"cb861bd77070e4441d66ffee0801f7048a9eacbe","title":"i-Code V2: An Autoregressive Generation Framework over Vision, Language, and Speech Data"},{"paperId":"1b0841c955ffc296348ab2bb1e98fdb0995e928b","title":"Enhancing Vision-Language Pre-Training with Jointly Learned Questioner and Dense Captioner"},{"paperId":"3130643a5d02f0e849d83bb1f85577a924081f36","title":"Paxion: Patching Action Knowledge in Video-Language Foundation Models"},{"paperId":"42a30dc5470f54ec249f25d3c31e05d7c376c8e3","title":"VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks"},{"paperId":"f1749d29b5e443c711f3feb2f4f9bf2ee7f163c6","title":"Sequence-to-Sequence Pre-training with Unified Modality Masking for Visual Document Understanding"},{"paperId":"019404442b3f5ea75e1693f1c3cd07b24665c5bf","title":"CLIP-VG: Self-paced Curriculum Adapting of CLIP for Visual Grounding"},{"paperId":"0c7ce5898dab92da540457b754254d72b8592fc2","title":"Parameter-efficient Tuning of Large-scale Multimodal Foundation Model"},{"paperId":"38be7643bcad936739550a1802220eb53ca9b1df","title":"Simple Token-Level Confidence Improves Caption Correctness"},{"paperId":"8badb0587fef2ffc078b0cec549eb8ec96ed3ad4","title":"Self-Chained Image-Language Model for Video Localization and Question Answering"},{"paperId":"8d3cc62fbe79b280a9084a43b295a4c77f7092ad","title":"Alternating Gradient Descent and Mixture-of-Experts for Integrated Multimodal Perception"},{"paperId":"0340c850e033abbf71c7214e403c8fe2be5ef91f","title":"Visual Tuning"},{"paperId":"511ad6b37cb028bdfbd6096e6d20aa4b8b34fafc","title":"Multi-Prompt with Depth Partitioned Cross-Modal Learning"},{"paperId":"0b5b753aa23be12f24b4592429514df6e53289bc","title":"Structure-CLIP: Towards Scene Graph Knowledge to Enhance Multi-modal Structured Representations"},{"paperId":"20fcc01d12a50f1da2af71d85f0a269b3ba48b77","title":"LMEye: An Interactive Perception Network for Large Language Models"},{"paperId":"0046306876ff2d5600699327e52bc29fa5e9ec91","title":"Transfer Visual Prompt Generator across LLMs"},{"paperId":"c3068e2a9f4cd374c7ff3be1b8f877b3d653e880","title":"Multimodal Neural Databases"},{"paperId":"5b678b4c4737c7d2e8ba98e211fed4834dd43732","title":"ArK: Augmented Reality with Knowledge Interactive Emergent Ability"},{"paperId":"131c6f328c11706de2c43cd16e0b7c5d5e610b6a","title":"Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond"},{"paperId":"268fa90820d0a1db5c35e056d291984ede3f0aed","title":"A Strong and Reproducible Object Detector with Only Public Datasets"},{"paperId":"6bfafb32b423c3f0456a10984814f89046def489","title":"A Cookbook of Self-Supervised Learning"},{"paperId":"2bc22b50f2a517fa57ae48135f42b18ba3da1cb4","title":"CoT-MoTE: Exploring ConTextual Masked Auto-Encoder Pre-training with Mixture-of-Textual-Experts for Passage Retrieval"},{"paperId":"06f7eefcfe454a37a92459a9646a1feb6e7c0382","title":"Enhancing Textbooks with Visuals from the Web for Improved Learning"},{"paperId":"b7d73f22d861f526541575a3b17449bd3c58ca74","title":"MVP-SEG: Multi-View Prompt Learning for Open-Vocabulary Semantic Segmentation"},{"paperId":"a43a3fadc9190e61b34f59a913f1716e443519e4","title":"On the Opportunities and Challenges of Foundation Models for Geospatial Artificial Intelligence"},{"paperId":"e8acb3e6ae754b18eb5e1d8466b11d6e1d81d1ae","title":"Modeling Dense Multimodal Interactions Between Biological Pathways and Histology for Survival Prediction"},{"paperId":"2a8d0f85252f486d6585621268ead2c941f0bd8a","title":"Improving Image Recognition by Retrieving from Web-Scale Image-Text Data"},{"paperId":"45c2f1672ef1de301868453acaf23f6df9a34b8a","title":"A Billion-scale Foundation Model for Remote Sensing Images"},{"paperId":"51615bee01c966ba055920e10778d5331d35bccd","title":"MoMo: A shared encoder Model for text, image and multi-Modal representations"},{"paperId":"0312e70901f352a6d95f23573788f9f7b737c983","title":"Training Large Language Models Efficiently with Sparsity and Dataflow"},{"paperId":"aae77fd74c99b2f6c10366267ce993b2d94141d5","title":"On Robustness in Multimodal Learning"},{"paperId":"87df8e436b91823b69ce660e1913bf723bc1cec5","title":"Token Boosting for Robust Self-Supervised Visual Transformer Pre-training"},{"paperId":"0ebc861f5478561f12941e6b48aad30574e996d8","title":"Video ChatCaptioner: Towards Enriched Spatiotemporal Descriptions"},{"paperId":"13581a46d32822e44cbeb1acdba4a59cef2b2ec1","title":"On Efficient Training of Large-Scale Deep Learning Models: A Literature Review"},{"paperId":"41721e0e0b7ce945d9d317bb83df2d9cb74b3114","title":"Attention: Marginal Probability is All You Need?"},{"paperId":"db1c83ef73d2f7731b0dd255835f2f26db749e17","title":"Not All Features Matter: Enhancing Few-shot CLIP with Adaptive Prior Refinement"},{"paperId":"599080fc0e2af249d2dc2e4ecb90a900c380926c","title":"A Novel Scenarios Engineering Methodology for Foundation Models in Metaverse"},{"paperId":"84b6fecf016d74512869c698c66c83729abdf359","title":"Self-Supervised Multimodal Learning: A Survey"},{"paperId":"c84e2801512069acbc63f1a7f73273281939428c","title":"A Study of Autoregressive Decoders for Multi-Tasking in Computer Vision"},{"paperId":"26eb6745006e94317cfb634123fe3015702fb224","title":"MaMMUT: A Simple Architecture for Joint Learning for MultiModal Tasks"},{"paperId":"a757999ed260d7bc45484dc6b4456bf33fe6f679","title":"LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention"},{"paperId":"d7adb2e6c8e381ca6c7a9a74ec5c54061573f9e1","title":"Revisiting Multimodal Representation in Contrastive Learning: From Patch and Token Embeddings to Finite Discrete Tokens"},{"paperId":"f9badd638eb683f2ba39fd089fbabb87c9a63787","title":"IFSeg: Image-free Semantic Segmentation via Vision-Language Model"},{"paperId":"fc8988585c6846fdeee33b34779a6a87b92c3e86","title":"Equivariant Similarity for Vision-Language Foundation Models"},{"paperId":"830d4beeaf56db871db842e3445c16b571f5a904","title":"Accelerating Vision-Language Pretraining with Free Language Modeling"},{"paperId":"8a4dd69533378b4e1e1d6429de4f2c6eab18e101","title":"CoBIT: A Contrastive Bi-directional Image-Text Generation Model"},{"paperId":"67317a73151316933c3943ff68b5f7cfcbc7e4c7","title":"MAGVLT: Masked Generative Vision-and-Language Transformer"},{"paperId":"3049c992adbd56e29c4d957ee0c4e9d05fe3c6d1","title":"EVA-02: A Visual Representation for Neon Genesis"},{"paperId":"f760912a793b53b0164670956e27bad9edd8bff1","title":"MXM-CLR: A Unified Framework for Contrastive Learning of Multifold Cross-Modal Representations"},{"paperId":"c5b2243baf88a00db2d4e4f9edb33cde08eb153f","title":"eP-ALM: Efficient Perceptual Augmentation of Language Models"},{"paperId":"4396e30f28eb49bb07c63cf62ca90415ebbe43d4","title":"IRGen: Generative Modeling for Image Retrieval"},{"paperId":"6c5cfc5a8debff3dab55764b6b6b534d7c47ce72","title":"Enhancing the Role of Context in Region-Word Alignment for Object Detection"},{"paperId":"bb8075a3ac5375566bab20a244da74a2d10b1352","title":"Dual-Path Adaptation from Image to Video Transformers"},{"paperId":"04fb3ce57e2205b4b2918df462961a0a644b4fdc","title":"An Automatic Key-points Detection and Style Transfer based Method of Articulatory Animation Generation"},{"paperId":"a9f6649a6885a66251ea3cccfb90f35dc35317df","title":"Investigating the Role of Attribute Context in Vision-Language Models for Object Recognition and Detection"},{"paperId":"f818e71f883aa8eb515331b01e24ba3530968664","title":"Cross-Modal Causal Intervention for Medical Report Generation"},{"paperId":"01cd6565acfe7b32290fc87980f469489135a6b0","title":"Reinforce Data, Multiply Impact: Improved Model Accuracy and Robustness with Dataset Reinforcement"},{"paperId":"9d12916dd46df7a6446cbec0bc4d054f7dafcdab","title":"Scaling Vision-Language Models with Sparse Mixture of Experts"},{"paperId":"530bee65ee844ed794d98b1120e4cf2738558316","title":"ViM: Vision Middleware for Unified Downstream Transferring"},{"paperId":"e8b7b212d448ef3e08423e26bd224aa7ccf6dec1","title":"Align and Attend: Multimodal Summarization with Dual Contrastive Losses"},{"paperId":"8f3138f7ee5127faab265793be8ae278bc49d9b1","title":"PMC-CLIP: Contrastive Language-Image Pre-training using Biomedical Documents"},{"paperId":"6827e87642874d9bf69f0f1548d79a164aaa5e1e","title":"One Transformer Fits All Distributions in Multi-Modal Diffusion at Scale"},{"paperId":"69cfdc8df16ae63b7acba4ac6f727f78b86893c3","title":"ChatGPT Asks, BLIP-2 Answers: Automatic Questioning Towards Enriched Visual Descriptions"},{"paperId":"6721b137c216ae15bbeba641292a63e32ab159a8","title":"Tag2Text: Guiding Vision-Language Model via Image Tagging"},{"paperId":"374dc9612e3507d1d3517492589c177a73be8e21","title":"Understanding and Constructing Latent Modality Structures in Multi-Modal Representation Learning"},{"paperId":"980668424b68c02271f39b3f71dc416eb0439cc5","title":"HumanBench: Towards General Human-Centric Perception with Projector Assisted Pretraining"},{"paperId":"7ad627f426691601aa4ffea2092cb51c2f37ed8a","title":"Exploiting the Textual Potential from Vision-Language Pre-training for Text-based Person Search"},{"paperId":"36291f1e302d95fa5558442d61a2d64e538ea206","title":"Bootstrap The Original Latent: Learning a Private Model from a Black-box Model"},{"paperId":"a935ba7ce7fd44fe372c6860504fbc164f012f03","title":"HiCLIP: Contrastive Language-Image Pretraining with Hierarchy-aware Attention"},{"paperId":"0a0acae0a9466f24d17d447a575f0efa5b90ee0e","title":"FAME-ViL: Multi-Tasking Vision-Language Model for Heterogeneous Fashion Tasks"},{"paperId":"e64dc184ec4ef8c6f3e0046e25e20b5dbe043ff4","title":"Visual Atoms: Pre-Training Vision Transformers with Sinusoidal Waves"},{"paperId":"a339df8024808b006f8f91db613961dc1c346afe","title":"DejaVu: Conditional Regenerative Learning to Enhance Dense Prediction"},{"paperId":"4d4a96708fc67403176bb2b891b564af7a20c148","title":"RAMM: Retrieval-augmented Biomedical Visual Question Answering with Multi-modal Pre-training"},{"paperId":"fbfef4723d8c8467d7bd523e1d0b703cce0e0f9c","title":"Language Is Not All You Need: Aligning Perception with Language Models"},{"paperId":"facc16fa1226124cca9cc8f91411c3d6303e53d0","title":"Few-shot Multimodal Multitask Multilingual Learning"},{"paperId":"da9579539385daedd33a0de0f814e2977ad0d1f5","title":"Towards Unifying Medical Vision-and-Language Pre-training via Soft Prompts"},{"paperId":"97fa699cd5403f6a1fed6f79e02af4ae37f15c4d","title":"UniAdapter: Unified Parameter-Efficient Transfer Learning for Cross-modal Modeling"},{"paperId":"ea8134912282c4f8743800051050889ff7f38ff8","title":"Less is More: Selective Layer Finetuning with SubTuning"},{"paperId":"0e57006711cc83095eeee02c12e371a5d991a2e7","title":"Analyzing Multimodal Objectives Through the Lens of Generative Diffusion Guidance"},{"paperId":"717d4cc5188e06791ef2043045e6e570ae764091","title":"Re-ViLM: Retrieval-Augmented Visual Language Model for Zero and Few-Shot Image Captioning"},{"paperId":"f2b528e716f09f9e4bd6a45847c95814c47722c9","title":"SimCon Loss with Multiple Views for Text Supervised Semantic Segmentation"},{"paperId":"fac388b8c24044dea06cc8c7b03dd1d99c8439a0","title":"AIM: Adapting Image Models for Efficient Video Action Recognition"},{"paperId":"64caaab51d8339f1b99874d3bddb79debbe661ca","title":"mPLUG-2: A Modularized Multi-modal Foundation Model Across Text, Image and Video"},{"paperId":"6d7534a41fc933f4f6a99e039f585dc57a370a29","title":"ADAPT: Action-aware Driving Caption Transformer"},{"paperId":"463910f5a3abaa8d41dbbeeedd49d5746c1ab6b8","title":"UPop: Unified and Progressive Pruning for Compressing Vision-Language Transformers"},{"paperId":"3f5b31c4f7350dc88002c121aecbdc82f86eb5bb","title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models"},{"paperId":"81620597ffafb4368cf0fe4fab7b7cd4506e09cd","title":"Advancing Radiograph Representation Learning with Masked Record Modeling"},{"paperId":"af34181ae916e01c72513f915f984a1d2c7febab","title":"Augmented Behavioral Annotation Tools, with Application to Multimodal Datasets and Models: A Systematic Review"},{"paperId":"874deb5f06f35e52ae13a921b23611eec4abd1da","title":"ClimaX: A foundation model for weather and climate"},{"paperId":"fcaafd1064c6aa32d6517002ff7c5ebd878c90ed","title":"Masked Autoencoding Does Not Help Natural Language Supervision at Scale"},{"paperId":"b759f3fcf2459013c710bc0b000c46c8e70f9bf8","title":"PRUDEX-Compass: Towards Systematic Evaluation of Reinforcement Learning in Financial Markets"},{"paperId":"0b2134e5ae6f62d66686d5ca9bbbaadc1ddce61e","title":"A Survey on Self-supervised Learning: Algorithms, Applications, and Future Trends"},{"paperId":"74e7edf7c6436bb4992ca9c9df9476c9ccf31919","title":"Toward Building General Foundation Models for Language, Vision, and Vision-Language Understanding Tasks"},{"paperId":"d16ac1cc0036ffda0d44383304df8bd4f8e38c95","title":"Vision Transformers are Good Mask Auto-Labelers"},{"paperId":"e0b63fd4dd74239a7eb1b75e0108ca55bcad782d","title":"All in Tokens: Unifying Output Space of Visual Tasks via Soft Token"},{"paperId":"aa5645b4896acb72aa4893d174af765d962aa708","title":"Policy Pre-training for Autonomous Driving via Self-supervised Geometric Modeling"},{"paperId":"6f6c19b44c1e82e24d2b34683669e93277539021","title":"Disjoint Masking With Joint Distillation for Efficient Masked Image Modeling"},{"paperId":"7096894fe8b91ff05f9969cff559097b15069245","title":"On Realization of Intelligent Decision-Making in the Real World: A Foundation Decision Model Perspective"},{"paperId":"82e849a32601090fbf820f5d381dba43b52a8ed5","title":"Do DALL-E and Flamingo Understand Each Other?"},{"paperId":"007323e9a19faa7be415eb2122dd331b11a54989","title":"Reversible Column Networks"},{"paperId":"0c0300f53c01ae609c97395c98de4c9d85d92876","title":"MultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning"},{"paperId":"967907503b24423b9b74621051811fcf684e3957","title":"Generalized Decoding for Pixel, Image, and Language"},{"paperId":"9575afb5702bc33d7df14c48feeee5901ea00369","title":"A Length-Extrapolatable Transformer"},{"paperId":"8165c92e8794cc197b5f9909487b79d1bcf2c0b2","title":"Position-Guided Text Prompt for Vision-Language Pre-Training"},{"paperId":"fbed623ca22abaa493081f7d97be51b1c317d437","title":"Transferring General Multimodal Pretrained Models to Text Recognition"},{"paperId":"89a1dbbfd4c96d90b769f5d3427bd970b082898e","title":"BEATs: Audio Pre-Training with Acoustic Tokenizers"},{"paperId":"22471140ae31b15dd55241e4be0c8bb851961ddc","title":"Autoencoders as Cross-Modal Teachers: Can Pretrained 2D Image Transformers Help 3D Representation Learning?"},{"paperId":"30279ffe74bc5eccbb37bf7082056e7065727bc4","title":"On Human Visual Contrast Sensitivity and Machine Vision Robustness: A Comparative Study"},{"paperId":"1b31dbf44e68b698120552366df03e6e35a1e428","title":"Objaverse: A Universe of Annotated 3D Objects"},{"paperId":"ec8f75e22ffbb5ad7e2f9cfc20a7780eed45715b","title":"CLIPPO: Image-and-Language Understanding from Pixels Only"},{"paperId":"497a1accfd0be6cad1be4f2b6fa88078dae7414a","title":"Quant 4.0: Engineering Quantitative Investment with Automated, Explainable and Knowledge-driven Artificial Intelligence"},{"paperId":"fe34137e5cc07235eae65ce53a54cd226b9f8b23","title":"MAGVIT: Masked Generative Video Transformer"},{"paperId":"3e8251f259dc529b3aa2366fc68c1516b202cfb9","title":"Reveal: Retrieval-Augmented Visual-Language Pre-Training with Multi-Source Multimodal Knowledge Memory"},{"paperId":"8ca316a10a2749e4c6bf3d0284e8cce2f56a4543","title":"Open Vocabulary Semantic Segmentation with Patch Aligned Contrastive Learning"},{"paperId":"c8dbf43fc20160814b9506de32be86ada91fa725","title":"VindLU: A Recipe for Effective Video-and-Language Pretraining"},{"paperId":"f5c853861fcde704a7100e24e963c5262e625229","title":"VideoCoCa: Video-Text Modeling with Zero-Shot Transfer from Contrastive Captioners"},{"paperId":"933b37b21e9d61139660088adb032ff3fdf56d86","title":"Learning Video Representations from Large Language Models"},{"paperId":"d232d97761490828f20e9b77d2c91a135a7270ee","title":"OFASys: A Multi-Modal Multi-Task Learning System for Building Generalist Models"},{"paperId":"508d9b43832790b4d35f4ae1fa76e9712859d6aa","title":"Vision and Structured-Language Pretraining for Cross-Modal Food Retrieval"},{"paperId":"458e3d2be80c401fb47e562d9d57012bd63da1c3","title":"Talking Head Generation with Probabilistic Audio-to-Visual Diffusion Priors"},{"paperId":"325d8e9501af05e594bd668b6cd6d43ed42c8b4d","title":"InternVideo: General Video Foundation Models via Generative and Discriminative Learning"},{"paperId":"17066da1e298a997c123f551bf0515daccc2b7b5","title":"Unifying Vision, Text, and Layout for Universal Document Processing"},{"paperId":"36306b2de6e9b2b11d53b029e754b03977de6072","title":"Compound Tokens: Channel Fusion for Vision-Language Representation Learning"},{"paperId":"1db3db7e00fd53290f0c0d07f22937ed5fedfabf","title":"Masked Contrastive Pre-Training for Efficient Video-Text Retrieval"},{"paperId":"040fdeabc5e934f13eb7b05c1907568b7d7efe81","title":"Knowledge Helps Pretrained Model: An Ensemble Model of Knowledge Model and CLIP for Zero-shot Image Recognition"},{"paperId":"3dc7cb6c14cec8b02a150bfb8ce95e8e3e8a01f2","title":"Synaptic Dynamics Realize First-order Adaptive Learning and Weight Symmetry"},{"paperId":"91694fc5f0bae350157f4fc565d0207ae12f7eb9","title":"FusionBrain: Research Project in Multimodal and Multitask Learning"},{"paperId":"4e6a2d863aeaafed82a8411f01be6e5a9f801b44","title":"SLAN: Self-Locator Aided Network for Cross-Modal Understanding"},{"paperId":"f64111aa1a5695e9209bca131469b1dc184d91d0","title":"Seeing What You Miss: Vision-Language Pre-training with Semantic Completion Learning"},{"paperId":"e7dc75aa8bb2f4f0bab54a49a9e04db3bf63cf15","title":"Fast-iTPN: Integrally Pre-Trained Transformer Pyramid Network with Token Migration"},{"paperId":"3d3a76a2fedccea741f34303b06358440dc2a212","title":"DETRs with Collaborative Hybrid Assignments Training"},{"paperId":"be1e92c38ad95fe6deba06d70d1f967131f2b19a","title":"X2-VLM: All-In-One Pre-trained Model For Vision-Language Tasks"},{"paperId":"00b3421e147da7a0c1b60c15c878532cfc93ece6","title":"VATLM: Visual-Audio-Text Pre-Training with Unified Masked Prediction for Speech Representation Learning"},{"paperId":"fd8c1b8741163d8737652fbcd3507bcd7d6225c7","title":"Multitask Vision-Language Prompt Tuning"},{"paperId":"33ef78737ba57ecc1ff98c22369a8e17ed90eb98","title":"Synthesizing Coherent Story with Auto-Regressive Latent Diffusion Models"},{"paperId":"ee96ec926f06ff2f3ce3d131cffcbfe63af39f0c","title":"Towards All-in-One Pre-Training via Maximizing Multi-Modal Mutual Information"},{"paperId":"30a3731f09e7a391e79a28fa736fa6bdd8331866","title":"Uni-Perceiver v2: A Generalist Model for Large-Scale Vision and Vision-Language Tasks"},{"paperId":"96282f6456b10da5acfb8268632ef86645f4b339","title":"Cross-Modal Adapter for Text-Video Retrieval"},{"paperId":"b2286da2b293b50644e5dc8ddd75eb8651e8b257","title":"UniFormerV2: Spatiotemporal Learning by Arming Image ViTs with Video UniFormer"},{"paperId":"78281482c1fdad8e167bab39cc9955c73d58ae8f","title":"EVA: Exploring the Limits of Masked Visual Representation Learning at Scale"},{"paperId":"db3b99c407ff8a06bdc96151dfae1328fadfb858","title":"Grafting Pre-trained Models for Multimodal Headline Generation"},{"paperId":"26c80bd65baa90f5b18157de4951f4eb0b62ab69","title":"InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions"},{"paperId":"6a993404e07687b7edb7fb9a05092213a9419859","title":"OneFormer: One Transformer to Rule Universal Image Segmentation"},{"paperId":"b839b60ffcdedf3f0dfa43da3eefe843307679f3","title":"Towards Reasoning-Aware Explainable VQA"},{"paperId":"c90a33f1f0049d524e9b5b3174d35611fd9a8096","title":"Pretraining in Deep Reinforcement Learning: A Survey"},{"paperId":"58a18a0937fc199e87fbd455af3a53b157462217","title":"Group DETR v2: Strong Object Detector with Encoder-Decoder Pretraining"},{"paperId":"3526c4f136b70b0b8c050a5e2e2926103da1c871","title":"Semantic Segmentation Algorithms for Ground AGV and UAV Medical Transport Scenes"},{"paperId":"3c2b12824b0027edb49b68300cbeab02cfc49ca8","title":"Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese"},{"paperId":"82a867d6d699231bf4de20dcac8efb293d11e7df","title":"State-of-the-art Models for Object Detection in Various Fields of Application"},{"paperId":"c3d38dcba5b954d7b9919eb3f6f90afd095f1801","title":"Behavioral Intention Prediction in Driving Scenes: A Survey"},{"paperId":"9a6d83c836ce6389b526b941d971eee775aa573e","title":"ERNIE-ViLG 2.0: Improving Text-to-Image Diffusion Model with Knowledge-Enhanced Mixture-of-Denoising-Experts"},{"paperId":"30477855d76058a9b542cabea3058aad1a837d51","title":"A Case for Business Process-Specific Foundation Models"},{"paperId":"aeaf6966d460f28db97609e9baa45276395d05d5","title":"Less is More: Learning Simplicity in Datacenter Scheduling"},{"paperId":"eba51c023f3ae9eeca783893b973db60e7a99a6c","title":"A Unified View of Masked Image Modeling"},{"paperId":"07099fe26ee8850c9ccba6fe2ee139d67289b67c","title":"Foundation Transformers"},{"paperId":"6540916e3ebaeaccefeaa303ba94c50bd581ff2a","title":"Like a bilingual baby: The advantage of visually grounding a bilingual language model"},{"paperId":"29c2d3d77b6d6f24f4356d5ba20c1a6ab4229c76","title":"Open-Vocabulary Semantic Segmentation with Mask-adapted CLIP"},{"paperId":"bdfa20099220821ab66c6ecf2cac237006f5c1df","title":"MAMO: Fine-Grained Vision-Language Representations Learning with Masked Multimodal Modeling"},{"paperId":"0ee11b28a9ce49d3030cab11f1178fa5abae9c3b","title":"VoLTA: Vision-Language Transformer with Weakly-Supervised Local-Feature Alignment"},{"paperId":"25425e299101b13ec2872417a14f961f4f8aa18e","title":"VIMA: General Robot Manipulation with Multimodal Prompts"},{"paperId":"10667c1ae4b49808772b5a377c5b52196701267f","title":"When and why vision-language models behave like bags-of-words, and what to do about it?"},{"paperId":"836ca61c0226fd5f763335ad4c13acc784251343","title":"Towards a Unified View on Visual Parameter-Efficient Transfer Learning"},{"paperId":"13d78bde4dc7059ab941871048ffa91d556584c8","title":"Where Should I Spend My FLOPS? Efficiency Evaluations of Visual Pre-training Methods"},{"paperId":"9b9fb973e5d3b413baa90648d9eb0743bd889747","title":"Spotlight: Mobile UI Understanding using Vision-Language Models with a Focus"},{"paperId":"2b6e343fb16848c5d94f8723512dcfb266ed3162","title":"Correlation Information Bottleneck: Towards Adapting Pretrained Multimodal Models for Robust Visual Question Answering"},{"paperId":"28630034bb29760df01ab033b743e30b37f336ae","title":"PaLI: A Jointly-Scaled Multilingual Language-Image Model"},{"paperId":"0afc96eca8b94d19a4c98fdd78e5fd9c68f6859a","title":"Statistical Foundation Behind Machine Learning and Its Impact on Computer Vision"},{"paperId":"12c679ac59289ad8f65fd645211897a1a3f8e0d0","title":"Design of the topology for contrastive visual-textual alignment"},{"paperId":"599be9043ef3571f65758cf36e184c9dc1781baf","title":"BEiT v2: Masked Image Modeling with Vector-Quantized Visual Tokenizers"},{"paperId":"620369d6ed3ed68c3e4374d6ddf282e0b036d2f8","title":"Masked Vision and Language Modeling for Multi-modal Representation Learning"},{"paperId":"add7c86e21051a5d9f8509876a0b26b9905af765","title":"Group DETR: Fast DETR Training with Group-Wise One-to-Many Assignment"},{"paperId":"8b5eab31e1c5689312fff3181a75bfbf5c13e51c","title":"Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks"},{"paperId":"4c2668b3ae22fa592716480ec56012775b139f52","title":"Bridge-Tower: Building Bridges Between Encoders in Vision-Language Representation Learning"},{"paperId":"35fccd11326e799ebf724f4150acef12a6538953","title":"TransVG++: End-to-End Visual Grounding With Language Conditioned Vision Transformer"},{"paperId":"622428f5122ad12a40229e1768ecb929fd747ee7","title":"Multimodal Learning With Transformers: A Survey"},{"paperId":"53ae1072fd04080e4fc2c9205ebcbc2683d7264c","title":"Sparse Mixture-of-Experts are Domain Generalizable Learners"},{"paperId":"5598c4ece8ffc69a7eb584d16f6de6629044e76a","title":"Vision Transformer Adapter for Dense Predictions"},{"paperId":"c26bb68806a992bf4fc85b5639e1657a445c4781","title":"On the Representation Collapse of Sparse Mixture of Experts"},{"paperId":"fa717a2e31f0cef4e26921f3b147a98644d2e64c","title":"Focal Modulation Networks"},{"paperId":"3def68bd0f856886d34272840a7f81588f2bc082","title":"Survey of Hallucination in Natural Language Generation"},{"paperId":"3387e9dedb7accc3c248d194b012cab0ab5ab0b8","title":"Context Autoencoder for Self-Supervised Representation Learning"},{"paperId":"42876a29331590d40355e670755a0f823ca3a9eb","title":"SeekNet: Improved Human Instance Segmentation and Tracking via Reinforcement Learning Based Optimized Robot Relocation"},{"paperId":"fda8e7fafbf34cbba357beaf44e15e8351e2b390","title":"Magneto: A Foundation Transformer"},{"paperId":"12fc935f612776c449d0308054df6af4fecd521b","title":"MULTIWAY-ADAPTER: ADAPTING MULTIMODAL LARGE LANGUAGE MODELS FOR SCALABLE IMAGE-TEXT RETRIEVAL"},{"paperId":"f920895447be7eef1e53e415d7f0b7d69a9e8551","title":"CLIP-VG: Self-paced Curriculum Adapting of CLIP via Exploiting Pseudo-Language Labels for Visual Grounding"},{"paperId":"13b5b69355555e0c8b702261c5de3b4172ba653c","title":"The Art of SOCRATIC QUESTIONING: Zero-shot Multimodal Reasoning with Recursive Thinking and Self-Questioning"},{"paperId":"9b5a11d9bb3790dbbb02725231b290f67579469a","title":"A Survey of Self-Supervised Learning from Multiple Perspectives: Algorithms, Theory, Applications and Future Trends"},{"paperId":"14c840a7faa15a7e42e5664b5e896878d91dd8ae","title":"Self Supervision Does Not Help Natural Language Supervision at Scale"},{"paperId":"240906480b02f4903d74d8b30477f45683599b95","title":"SubTuning: Efficient Finetuning for Multi-Task Learning"},{"paperId":"e39e5601bde42fcc3ef770dc03e4607f42963bbe","title":"Bootstrap The Original Latent: Freeze-and-thaw Adapter for Back-Propagated Black-Box Adaptation"},{"paperId":"379b4d871cebf431208154adaaff7ea946b2bb38","title":"Structure-CLIP: Enhance Multi-modal Language Representations with Structure Knowledge"},{"paperId":"14cee97a36e2baca80317d94665bbf3508736fcf","title":"Object Detection and X-Ray Security Imaging: A Survey"},{"paperId":"22851d98d76886308bc7cf817efe3fc1bc4f041a","title":"Self-Supervised Pretraining via Multimodality Images With Transformer for Change Detection"},{"paperId":"40a342a5e48be59a674aa8a6d980133a1fc138b3","title":"LabelBench: A Comprehensive Framework for Benchmarking Label-Efficient Learning"},{"paperId":"b7b7b5bd3dbc917fc55e11a1d08dcc5e7f80d7e0","title":"RS5M: A Large Scale Vision-Language Dataset for Remote Sensing Vision-Language Foundation Model"},{"paperId":"7bf7172cb53c1c009bfed7c4b6e6f6898519168a","title":"M ODALITY -A WARE A DAPTATION OF C ONTRASTIVE L ANGUAGE -I MAGE M ODELS"},{"paperId":"ef1f21278ac55c4ef37c8a6a11cfebbfa6539d69","title":"Unifying Cross-Lingual and Cross-Modal Modeling Towards Weakly Supervised Multilingual Vision-Language Pre-training"},{"paperId":"290400d20170480edb36399cafe0f2fad510c635","title":"S POTLIGHT : M OBILE UI U NDERSTANDING USING V ISION -L ANGUAGE M ODELS WITH A F OCUS"},{"paperId":"110ccbe25193fa3df9ae073205dee9da8b05cd01","title":"Compositional Reasoning in Vision-Language Models"},{"paperId":"7fd1c7ccb7e1fc1a68c7cdf0f3ca94c53bbadd38","title":"Appendix for OneFormer: One Transformer to Rule Universal Image Segmentation"},{"paperId":"2cb9d167e9ddc9d48b931520248bdc1b7e818703","title":"Robustness in Multimodal Learning under Train-Test Modality Mismatch"},{"paperId":"5b06056345034e1559ef8680190cdccc79a2196d","title":"VIMA: Robot Manipulation with Multimodal Prompts"},{"paperId":"e045d5d7788c18c838697a8e92bdf562d6cd2d46","title":"Embodied Symbiotic Assistants that See, Act, Infer and Chat"},{"paperId":"c13a4cb912ef5202e99feff0f5dd01f183668d59","title":"Unleashing the Power of Large Models: Exploring Human-Machine Conversations"},{"paperId":"69aa33276ed9e4a1b121a48a7ea6269037671aae","title":"KnowledgeBot: Improving Assistive Robot for Task Completion and Live Interaction via Neuro-Symbolic Reasoning"},{"paperId":"6d3dc39d90419736fa44f65dccce651d9efd0992","title":"Self-Supervised Cluster-Contrast Distillation Hashing Network for Cross-Modal Retrieval"},{"paperId":"46ef35499354ddc14245bec849ed21dde482f5a6","title":"Semantically Consistent Visual Representation for Adversarial Robustness"},{"paperId":"fd394c65850c5028d8004b37110d1ca8a615e937","title":"Prompt Tuning for Unified Multimodal Pretrained Models"},{"paperId":"85cc71e7dab7ac1f851faca15dcd73c5584b4b05","title":"Deep-Learning-Based Segmentation of Individual Tooth and Bone With Periodontal Ligament Interface Details for Simulation Purposes"},{"paperId":"25e1194c09c8ecd769a004ac14d6ffe74e0a3f8e","title":"Irregularly Spatial Seismic Missing Data Reconstruction Using Transformer With Periodic Skip Connection"},{"paperId":"5ddb51ae85deca14dc7fc8adc07305c22a1ebe0a","title":"Qwen-VL: A Frontier Large Vision-Language Model with Versatile Abilities"},{"paperId":"b5b136ac59f1a75bdb1273f398526062ca223359","title":"New Audio Representations Image Gan Generation from BriVL"},{"paperId":"7bef12e957bbb1b62126a6f372191f3af24cc3d2","title":"Using Self-Supervised Dual Constraint Contrastive Learning for Cross-Modal Retrieval"},{"paperId":"3a5f9968e462109dfd79e15f1ea8b5e80f9a0a4f","title":"Towards Captioning an Image Collection from a Combined Scene Graph Representation Approach"},{"paperId":"3aa387e836339c42f46942b14727f01875bb771a","title":"UniBriVL: Robust Audio Representation and Generation of Audio Driven Diffusion Models"},{"paperId":"0d0269f8533a33c3c310fd0a59815aa16a0c47ff","title":"Perception Test : A Diagnostic Benchmark for Multimodal Models"},{"paperId":"2096246df0649fbbd9adc7895d2a50beeb46b6b7","title":"Structured Vision-Language Pretraining for Computational Cooking"},{"paperId":"abb205e2d1a20acc3088b93d92de1ee754e37e1e","title":"C ONNECTING REPRESENTATION AND GENERATION VIA MASKED VISION - LANGUAGE TRANSFORMER"},{"paperId":"26a2a15c16c78f586169e4768720187c1ef14f8a","title":"UP OP : U NIFIED AND P ROGRESSIVE P RUNING FOR C OMPRESSING V ISION -L ANGUAGE T RANSFORMERS"},{"paperId":"608b4fed95d2a134c1c283ec36b917ee4fe14127","title":"Video-Text Modeling with Zero-Shot Transfer from Contrastive Captioners"},{"paperId":"c2c9e5ecdbf10eda554c5d20b5395035ff913c20","title":"Master Computer Science"},{"paperId":"9d0ab1aad741cdd8aca079e7a372af0dda85734d","title":"eP-ALM: Eﬀicient Perceptual Augmentation of Language Models"}],"references":[{"paperId":"47aece805c34fc9b29e0b9bcd0966703e5c128fd","title":"Reward Shaping Study for Sub-Goal Based Robotic Tasks"},{"paperId":"599be9043ef3571f65758cf36e184c9dc1781baf","title":"BEiT v2: Masked Image Modeling with Vector-Quantized Visual Tokenizers"},{"paperId":"3e448df5aa191f7a3945d0fd609c8bc5966a2333","title":"HorNet: Efficient High-Order Spatial Interactions with Recursive Gated Convolutions"},{"paperId":"a8fd9c1625011741f74401ff9bdc1c584e25c86d","title":"Language Models are General-Purpose Interfaces"},{"paperId":"49b5ffebdbcbd683010a2558a19eaa9b21cd8c34","title":"GLIPv2: Unifying Localization and Vision-Language Understanding"},{"paperId":"eb940c4169b83d2a204aec4b8547d1b1a8d0491c","title":"Mask DINO: Towards A Unified Transformer-based Framework for Object Detection and Segmentation"},{"paperId":"0aa3cd5ab502b3dd7f23cf4781cd44305a642bea","title":"VL-BEiT: Generative Vision-Language Pretraining"},{"paperId":"5c4f8de98525eebd762773093d149ba459cef290","title":"Contrastive Learning Rivals Masked Image Modeling in Fine-tuning via Feature Distillation"},{"paperId":"5598c4ece8ffc69a7eb584d16f6de6629044e76a","title":"Vision Transformer Adapter for Dense Predictions"},{"paperId":"a26a7a74f1e5fd562be95c3611a0680759fbdf84","title":"CoCa: Contrastive Captioners are Image-Text Foundation Models"},{"paperId":"26218bdcc3945c7edae7aa2adbfba4cd820a2df3","title":"Flamingo: a Visual Language Model for Few-Shot Learning"},{"paperId":"2ad12a7be5eaf339a98c4defd8669e11fe726acc","title":"MaxViT: Multi-Axis Vision Transformer"},{"paperId":"a09cbcaac305884f043810afc4fa4053099b5970","title":"Exploring Plain Vision Transformer Backbones for Object Detection"},{"paperId":"54020e5fe48ebb250f27d744e20a63cac2988a84","title":"Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time"},{"paperId":"9dc481ec44178e797466bbad968071917842156b","title":"DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection"},{"paperId":"a3b42a83669998f65df60d7c065a70d07ca95e99","title":"BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation"},{"paperId":"2fd6f77540c1cc8e70b96208ccf9971b4251fc02","title":"FLAVA: A Foundational Language And Vision Alignment Model"},{"paperId":"5341b412383c43f4a693ad63ec4489e3ec7688c8","title":"Grounded Language-Image Pre-training"},{"paperId":"658a017302d29e4acf4ca789cb5d9f27983717ff","title":"Masked-attention Mask Transformer for Universal Image Segmentation"},{"paperId":"9137efc758f80dd22bb56f82cca5c94f78a5db3e","title":"MViTv2: Improved Multiscale Vision Transformers for Classification and Detection"},{"paperId":"21ec90872abd986c12afe39bebe807732ffa70c9","title":"Florence: A New Foundation Model for Computer Vision"},{"paperId":"be0fbb810583930c071d0b9b2c5187fe260783f5","title":"Swin Transformer V2: Scaling Up Capacity and Resolution"},{"paperId":"f675c62abfa788ea0be85d3124eba15a14d5e9d6","title":"FILIP: Fine-grained Interactive Language-Image Pre-Training"},{"paperId":"cf7c2e0e4fb2af689aaf4b7a7cddf7b1f4d5e3f0","title":"VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts"},{"paperId":"633d88b3b9c2dd578d9cfa90aa0abecd6ac86789","title":"s2s-ft: Fine-Tuning Pretrained Transformer Encoders for Sequence-to-Sequence Learning"},{"paperId":"5e00596fa946670d894b1bdaeff5a98e3867ef13","title":"SimVLM: Simple Visual Language Model Pretraining with Weak Supervision"},{"paperId":"b82c5f9efdb2ae56baa084ca41aeddd8a665c1d1","title":"Align before Fuse: Vision and Language Representation Learning with Momentum Distillation"},{"paperId":"fd51da088c5fe89eba0e363edad746bb3c2407d1","title":"End-to-End Semi-Supervised Object Detection with Soft Teacher"},{"paperId":"722ad6ac92286507437b31486f47987d6ece05c9","title":"BEiT: BERT Pre-Training of Image Transformers"},{"paperId":"9f4b69762ffb1ba42b573fd4ced996f3153e21c0","title":"CoAtNet: Marrying Convolution and Attention for All Data Sizes"},{"paperId":"2a805d0e1b067444a554c5169d189fa1f649f411","title":"Scaling Vision Transformers"},{"paperId":"1ee1160b8c7c70ded02e786c184a6da651e88bed","title":"Dynamic Head: Unifying Object Detection Heads with Attentions"},{"paperId":"63c74d15940af1af9b386b5762e4445e54c73719","title":"VinVL: Revisiting Visual Representations in Vision-Language Models"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"394be105b87e9bfe72c20efe6338de10604e1a11","title":"Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts"},{"paperId":"141a5033d9994242b18bb3b217e79582f1ee9306","title":"Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision"},{"paperId":"0839722fb5369c0abaff8515bfc08299efc790a1","title":"ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision"},{"paperId":"268d347e8a55b5eb82fb5e7d2f800e33c75ab18a","title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"},{"paperId":"2f5f81bc516a6d085d39479378af1fc27104f91e","title":"Large-Scale Adversarial Training for Vision-and-Language Representation Learning"},{"paperId":"818e5cbc337e4e1b98e65a2d7c2d6d2a0318cd57","title":"Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks"},{"paperId":"f64e1d6bc13aae99aab5449fc9ae742a9ba7761e","title":"UniLMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training"},{"paperId":"c5ff974a69fd0c760b4855b819e61e89f31cfffe","title":"Objects365: A Large-Scale, High-Quality Dataset for Object Detection"},{"paperId":"dfc7b58b67c31932b48586b3e23a43cc94695290","title":"UNITER: UNiversal Image-TExt Representation Learning"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"bc626a52664e948a0ffb2b95d0e1e6377a01171a","title":"Cascade R-CNN: High Quality Object Detection and Instance Segmentation"},{"paperId":"f4327b978dec52f16b089c222c43543f8ecf4717","title":"arXiv"},{"paperId":"1c71771c701aadfd72c5866170a9f5d71464bb88","title":"Unified Language Model Pre-training for Natural Language Understanding and Generation"},{"paperId":"cf336d272a30d6ad6141db67faa64deb8791cd61","title":"A Corpus for Reasoning about Natural Language Grounded in Photographs"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"b4df354db88a70183a64dbc9e56cf14e7669a6c0","title":"Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning"},{"paperId":"d7b6753a2d4a2b286c396854063bde3a91b75535","title":"A Simple Method for Commonsense Reasoning"},{"paperId":"155b7782dbd713982a4133df3aee7adfd0b6b304","title":"Unsupervised Feature Learning via Non-parametric Instance Discrimination"},{"paperId":"d07284a6811f1b2745d91bdb06b040b57f226882","title":"Decoupled Weight Decay Regularization"},{"paperId":"a82c1d1ccaa3a3d1d6ee6677de0eed2e93ddb6e8","title":"Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"53c0aa8d33d240197caff824a6225fb223c1181c","title":"Soft-NMS — Improving Object Detection with One Line of Code"},{"paperId":"7e232313a59d735ef7c8a9f4cc7bc980a29deb5e","title":"Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering"},{"paperId":"88512be44744615f4baa8e14f600f036db4c2433","title":"Semantic Understanding of Scenes Through the ADE20K Dataset"},{"paperId":"51db1f3c8dfc7d4077da39c96bb90a6358128111","title":"Deep Networks with Stochastic Depth"},{"paperId":"afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d","title":"Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations"},{"paperId":"0e6824e137847be0599bb0032e37042ed2ef5045","title":"Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books"},{"paperId":"11c9c31dff70de92ada9160c78ff8bb46b2912d6","title":"Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models"},{"paperId":"55e022fb7581bb9e1fce678d21fb25ffbb3fbb88","title":"Deep visual-semantic alignments for generating image descriptions"},{"paperId":"e74f9b7f8eec6ba4704c206b93bc8079af3da4bd","title":"ImageNet Large Scale Visual Recognition Challenge"},{"paperId":"71b7178df5d2b112d07e45038cb5637208659ff7","title":"Microsoft COCO: Common Objects in Context"},{"paperId":"8e080b98efbe65c02a116439205ca2344b9f7cd4","title":"Im2Text: Describing Images Using 1 Million Captioned Photographs"},{"paperId":"bc6dff14a130c57a91d5a21339c23471faf1d46f","title":"Et al"},{"paperId":"ecce44df1956db4ec486539c6543345344809958","title":"Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework"},{"paperId":null,"title":"In 2021 IEEE/CVF International Conference on Computer Vision"},{"paperId":null,"title":"In 7th International Conference on Learning Representations"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","title":"Improving Language Understanding by Generative Pre-Training"},{"paperId":null,"title":"and Ilya Sutskever"},{"paperId":null,"title":"In 2018 IEEE Conference on Computer Vision and Pattern Recognition"},{"paperId":null,"title":"editors"}],"id":"02251886950770e82b3d68564d60cdfe15e73199","summary":"This work introduces a general-purpose multimodal foundation model BEiT-3, which achieves state-of-the-art transfer performance on both vision and vision-language tasks and introduces Multiway Transformers for general- Purpose modeling, where the modular architecture enables both deep fusion and modality-specific encoding."},{"url":"https://www.semanticscholar.org/paper/18f9a6045ba01cb079c4fa49a630d71bbd27cd92","title":"A dataset of clinically generated visual questions and answers about radiology images","venue":"Scientific Data","year":2018,"referenceCount":14,"citationCount":150,"influentialCitationCount":31,"publicationDate":"20/11/2018","authors":"J. Lau,Soumya Gayen,Asma Ben Abacha,Dina Demner-Fushman","citations":[{"paperId":"a3d418b4e35a02e4306505ab660a6bcd44c3c752","title":"Asclepius: A Spectrum Evaluation Benchmark for Medical Multi-Modal Large Language Models"},{"paperId":"bc431ad78e91a73a69580b7d256d18777dcda313","title":"Prompt-based Personalized Federated Learning for Medical Visual Question Answering"},{"paperId":"7580327ffc9bd5daef83fe8285c0476ca074051d","title":"OmniMedVQA: A New Large-Scale Comprehensive Evaluation Benchmark for Medical LVLM"},{"paperId":"0fd27cba73c1af107a32b3c6138b643f22cf8749","title":"MultiMedEval: A Benchmark and a Toolkit for Evaluating Medical Vision-Language Models"},{"paperId":"61cadcfa555cbef120df7c017ef02e87f19900b7","title":"Free Form Medical Visual Question Answering in Radiology"},{"paperId":"6ed96d6822a06ad9a735bc09e301bf41df61c534","title":"CheXagent: Towards a Foundation Model for Chest X-Ray Interpretation"},{"paperId":"63de69245502d9a22de04581a4b5c0168d596aa3","title":"Generalizing Visual Question Answering from Synthetic to Human-Written Questions via a Chain of QA with a Large Language Model"},{"paperId":"1e0d21dc2caf7b58342ddc8609fb30cdc1e27cd5","title":"MISS: A Generative Pretraining and Finetuning Approach for Med-VQA"},{"paperId":"420087f314633a381e61e6c5cd73ccc2070a749e","title":"PeFoMed: Parameter Efficient Fine-tuning on Multimodal Large Language Models for Medical Visual Question Answering"},{"paperId":"accf39ab42d8d1157e5541ae7dcd63ea2be7b32d","title":"From image to language: A critical analysis of Visual Question Answering (VQA) approaches, challenges, and opportunities"},{"paperId":"1e1230ef1de1ba9c4f6cb4789184a295133afac0","title":"Visual Instruction Tuning towards General-Purpose Multimodal Model: A Survey"},{"paperId":"352252231462c24440bc0016638ea5fe8d4c6f7e","title":"UniDCP: Unifying Multiple Medical Vision-language Tasks via Dynamic Cross-modal Learnable Prompts"},{"paperId":"d48fa3ed73817563130ef217d85011ce1fbe7470","title":"BESTMVQA: A Benchmark Evaluation System for Medical Visual Question Answering"},{"paperId":"2c7e346aa311fec4dda04bdf3a214ce2026d8807","title":"Medical Vision Language Pretraining: A survey"},{"paperId":"cb5e4157bc37affe59d105fb14e5581ca18d5caf","title":"A Weak Supervision-based Robust Pretraining Method for Medical Visual Question Answering"},{"paperId":"290c8a0dde6fede48afcabb6de3b34fca5b370a2","title":"Complex Organ Mask Guided Radiology Report Generation"},{"paperId":"8d2709ed1788a67e64425fb410bb49f3ee49e088","title":"Large Language Models Illuminate a Progressive Pathway to Artificial Healthcare Assistant: A Review"},{"paperId":"749104d1a207f5bc192c7d95a12856b5e7f84d1f","title":"Mapping medical image-text to a joint space via masked modeling"},{"paperId":"18c48b42941d16eeaf053ae18b1fe671934af134","title":"Scaling-up medical vision-and-language representation learning with federated learning"},{"paperId":"88bddfb7d1e0462be8fe99fdbd71c658140cb17b","title":"From Image to Language: A Critical Analysis of Visual Question Answering (VQA) Approaches, Challenges, and Opportunities"},{"paperId":"1f5e1a036b24b9dd34c006ba3bb61119624f4fdb","title":"A Comprehensive Study of GPT-4V's Multimodal Capabilities in Medical Imaging"},{"paperId":"2010e5fb3a804ac376412b4fa65ee83f34d5e1d9","title":"A Systematic Evaluation of GPT-4V's Multimodal Capability for Medical Image Analysis"},{"paperId":"ad083672bc110b2c86d1461bb4cda3cd3eededee","title":"Multimodal ChatGPT for Medical Applications: an Experimental Study of GPT-4V"},{"paperId":"cb886bc9674d689d3a1f23713826374279894557","title":"EHRXQA: A Multi-Modal Question Answering Dataset for Electronic Health Records with Chest X-ray Images"},{"paperId":"da9134f694959b68027c33c8e998ffb3d41305da","title":"Exploring Question Decomposition for Zero-Shot VQA"},{"paperId":"181cd4973db6170d72a438db54db8e52ccbbe87a","title":"Multi-modal multi-head self-attention for medical VQA"},{"paperId":"c7492913370b5726eaa6ced163a60de6c9d4bb7f","title":"A Survey of Large Language Models for Healthcare: from Data, Technology, and Applications to Accountability and Ethics"},{"paperId":"ecd29b6980c52b5c9af25063dec46221931dff5e","title":"FGCVQA: Fine-Grained Cross-Attention for Medical VQA"},{"paperId":"8946891e94831adc8cddb0d32311cce2445c96d2","title":"MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts"},{"paperId":"910e82a2a825d3d1939908094c01134c2cf30dcc","title":"MITER: Medical Image-TExt joint adaptive pretRaining with multi-level contrastive learning"},{"paperId":"837b03421a6e0314b46ed24a5ae889aaafe2ec2a","title":"Event-Oriented Visual Question Answering: The E-VQA Dataset and Benchmark"},{"paperId":"a0476578761e983d5ab2083abab07b81236c1d58","title":"Asymmetric cross-modal attention network with multimodal augmented mixup for medical visual question answering"},{"paperId":"f0950a3f27c0fefffba60ae1c9a8ee360d5eb55f","title":"Instruction Tuning for Large Language Models: A Survey"},{"paperId":"df0ddb588a200d095743e9d26fc4a9318619766e","title":"Towards Generalist Foundation Model for Radiology"},{"paperId":"f660250f9fcd465acdf2e727d309acf1cc64c780","title":"ELIXR: Towards a general purpose X-ray artificial intelligence system through alignment of large language models and radiology vision encoders"},{"paperId":"9b6779fe8805abae18dcafcdf80f45109d61a7d7","title":"Medical visual question answering with symmetric interaction attention and cross-modal gating"},{"paperId":"304f8b4edea01fdb5a2f7f8b998c83188deeccff","title":"Towards Generalist Biomedical AI"},{"paperId":"8e51781ef23930913d1d6b9157fb310103aa9f82","title":"Expert Knowledge-Aware Image Difference Graph Representation Learning for Difference-Aware Medical Visual Question Answering"},{"paperId":"a4e3f9a3f8c875679c7806d846853b4530843799","title":"A Medical Domain Visual Question Generation Model via Large Language Model"},{"paperId":"baa1dc079d98ca76b0173c8d653fed759fd0a371","title":"A scoping review on multimodal deep learning in biomedical images and texts"},{"paperId":"bf40c9e7832e1b2887cbf5798455f91705ea11ba","title":"Masked Vision and Language Pre-training with Unimodal and Multimodal Contrastive Losses for Medical Visual Question Answering"},{"paperId":"e0e9ba0c01d441e1fdcb8628d3f743d387b0b017","title":"UIT-Saviors at MEDVQA-GI 2023: Improving Multimodal Learning with Image Enhancement for Gastrointestinal Visual Question Answering"},{"paperId":"534675abb9d72fc0c08d080d4f73335ceb75902c","title":"Multimodal Prompt Retrieval for Generative Visual Question Answering"},{"paperId":"ebedc4d7a2356090904baba4104ef0832bc236df","title":"A Survey on Multimodal Large Language Models"},{"paperId":"64fa56962dd0f4bbe206be6142fbe0315c4e7c2f","title":"Multi-modal Pre-training for Medical Vision-language Understanding and Generation: An Empirical Study with A New Benchmark"},{"paperId":"8213492345c67d2b0e692b6bb5c814d4f1aef8d2","title":"Benchmarking Robustness of Adaptation Methods on Pre-trained Vision-Language Models"},{"paperId":"f22d71c7ce9720ba1f717a4f1181488200e78198","title":"LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day"},{"paperId":"a563f823a887b75dd61adc96c556a8bd83c6e4c3","title":"HaVQA: A Dataset for Visual Question Answering and Multimodal Research in Hausa Language"},{"paperId":"07d45ce7de598ef03b400f8ddba7d2e055e77a08","title":"BiomedGPT: A Unified and Generalist Biomedical Generative Pre-trained Transformer for Vision, Language, and Multimodal Tasks"},{"paperId":"f4793adffd6f67ffcb93ccfc5672ab301b8a2b96","title":"PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering"},{"paperId":"ad2ad450f1ee6a0df46bc6fe6916a797c90b68f1","title":"Multi-task Paired Masking with Alignment Modeling for Medical Vision-Language Pre-training"},{"paperId":"c5bcc78ae708b29edb03481e12213eca53c28963","title":"A multi-modal model based on transformers for medical visual question answering"},{"paperId":"ac4d13b6a4f9fb67337099f4602135a0351f5c99","title":"Towards Medical Artificial General Intelligence via Knowledge-Enhanced Multimodal Pretraining"},{"paperId":"2f9b344158e40d4af8391fc7e79d400bebba39ce","title":"Assertiveness-based Agent Communication for a Personalized Medicine on Medical Imaging Diagnosis"},{"paperId":"2587cb7b1c02fde76e3c23c13f1bd40d6a199c57","title":"Q2ATransformer: Improving Medical VQA via an Answer Querying Decoder"},{"paperId":"46a0f57d5376ad1b9fb94b894931a5419d67dbf5","title":"A reinforcement learning approach for VQA validation: An application to diabetic macular edema grading"},{"paperId":"f7ea746cd2cc25628a7a553ac27d228198be42cb","title":"Pre-trained multilevel fuse network based on vision-conditioned reasoning and bilinear attentions for medical image visual question answering"},{"paperId":"9e8936a131ee0c765a6749d93bc58ad9522b7d6d","title":"Logical Implications for Visual Question Answering Consistency"},{"paperId":"8f3138f7ee5127faab265793be8ae278bc49d9b1","title":"PMC-CLIP: Contrastive Language-Image Pre-training using Biomedical Documents"},{"paperId":"17ca48ad1b944c897863f04ba9ffa72674dce1ce","title":"Parallel multi-head attention and term-weighted question embedding for medical visual question answering"},{"paperId":"5814bd146b37e13115af4330caf3a751159a156f","title":"BiomedCLIP: a multimodal biomedical foundation model pretrained from fifteen million scientific image-text pairs"},{"paperId":"20fb06a4aa4010470d388098618af5d1bea224ad","title":"Vision–Language Model for Visual Question Answering in Medical Imagery"},{"paperId":"8200be2e8b9af243ee72a9d919a4f7fbe82a17d2","title":"Medical knowledge-based network for Patient-oriented Visual Question Answering"},{"paperId":"4d4a96708fc67403176bb2b891b564af7a20c148","title":"RAMM: Retrieval-augmented Biomedical Visual Question Answering with Multi-modal Pre-training"},{"paperId":"ffeeb60b76d18e1e36dee0f87c95bab1bc65aa79","title":"Medical visual question answering using joint self-supervised learning"},{"paperId":"9259c41695c4451f1ca3e6bdc9829623b43f9a69","title":"Interpretable Medical Image Visual Question Answering via Multi-Modal Relationship Graph Learning"},{"paperId":"da9579539385daedd33a0de0f814e2977ad0d1f5","title":"Towards Unifying Medical Vision-and-Language Pre-training via Soft Prompts"},{"paperId":"f4b9ec310ef9aff69066e3e482cc113c8ae8d9dd","title":"PathNarratives: Data annotation for pathological human-AI collaborative diagnosis"},{"paperId":"940f303c2530a52c5fd3c52c9c64ceea4b53ab05","title":"Diversity Learning Based on Multi-Latent Space for Medical Image Visual Question Generation"},{"paperId":"e0b4ca7bffb64b4bbd95c9f5ee7a610e35fe95d8","title":"A comprehensive interpretation for medical VQA: Datasets, techniques, and challenges"},{"paperId":"2580d3fc39fed3989f10665559a955b847b7eb7f","title":"Medical Visual Question Answering via Conditional Reasoning and Contrastive Learning"},{"paperId":"90cd86b3c157e40cbaf1076f69cbd38d9c0781b9","title":"UnICLAM: Contrastive Representation Learning with Adversarial Masking for Unified and Interpretable Medical Vision Question Answering"},{"paperId":"83caa8f9ec66a9ebae68d0e963ac7ca2396c94c2","title":"Is Unimodal Bias Always Bad for Visual Question Answering? A Medical Domain Study with Dynamic Attention"},{"paperId":"5942335fdd35d1651aaabd7af4db129a29ed2a85","title":"How Well Apply Multimodal Mixup and Simple MLPs Backbone to Medical Visual Question Answering?"},{"paperId":"56d8d9fff399f798da97a69e891de4eeb4568d4f","title":"MHKD-MVQA: Multimodal Hierarchical Knowledge Distillation for Medical Visual Question Answering"},{"paperId":"b60711d89c34d8902d4b2768f01770473cf0adfc","title":"Medical image enhancement strategy based on morphologically processing of residuals using a special kernel"},{"paperId":"170667a96f04adf3b3b83526f75fe8d1063e0f7a","title":"Self-Supervised Vision-Language Pretraining for Medial Visual Question Answering"},{"paperId":"9d79f3601b0d73a2b64784cad2738c0fcd030824","title":"MF2-MVQA: A Multi-Stage Feature Fusion Method for Medical Visual Question Answering"},{"paperId":"6ae700c89a9a9a3da7e55dd51c4710b5ed8c8d4e","title":"Caption-Aware Medical VQA via Semantic Focusing and Progressive Cross-Modality Comprehension"},{"paperId":"d4d07180764fc30cf31261ddd072175a4daee10b","title":"A Dual-Attention Learning Network With Word and Sentence Embedding for Medical Visual Question Answering"},{"paperId":"8f93076f8e060eec0c058edb3de05f62886fffdf","title":"RepsNet: Combining Vision with Language for Automated Medical Reports"},{"paperId":"28ff0816f19a5e3e37eac5569de41872fd262f0a","title":"Align, Reason and Learn: Enhancing Medical Vision-and-Language Pre-training with Knowledge"},{"paperId":"81adb80e390f25d4a2d764b1063eeaa2c334d441","title":"Multi-modal Masked Autoencoders for Medical Vision-and-Language Pre-training"},{"paperId":"8cee548aa6a8d31dcac830695e4b72960ff45ecb","title":"MMCN: Multi-Modal Co-attention Network for Medical Visual Question Answering"},{"paperId":"0cbd644254462341a897d4bfa0134637662c3ab5","title":"A Transformer-based Medical Visual Question Answering Model"},{"paperId":"0976f5e6e7c0481f5f44c981d9f676e9ea7fa4d0","title":"AMAM: An Attention-based Multimodal Alignment Model for Medical Visual Question Answering"},{"paperId":"ef7dd87e8bfd11878e88ec3f0795ebda8aaf1690","title":"A Bi-level representation learning model for medical visual question answering"},{"paperId":"ef2edea434e487f288d4eed6f9b1dc480b917211","title":"Adversarial Learning to Improve Question Image Embedding in Medical Visual Question Answering"},{"paperId":"2ac3bacbbee520b701707ebcf7b9ca7a3f233129","title":"Medical visual question answering via corresponding feature fusion combined with semantic attention."},{"paperId":"67f992f43cc777a3e1aedc14cf3a11582ccfa570","title":"OVQA: A Clinically Generated Visual Question Answering Dataset"},{"paperId":"c98eafbef6fa40010fc3b78d96a04c41699e2c1b","title":"EBMs vs. CL: Exploring Self-Supervised Visual Pretraining for Visual Question Answering"},{"paperId":"6e7763ec04906726377953cc85f31a1a0c889001","title":"Anomaly Matters: An Anomaly-Oriented Model for Medical Visual Question Answering"},{"paperId":"e9480d62e216f77d5556b7eda769daa4c92d004d","title":"VQAMix: Conditional Triplet Mixup for Medical Visual Question Answering"},{"paperId":"d1f25ff0b282486acf6ae225e5fd18a82673eb35","title":"Multi-Modal Alignment of Visual Question Answering Based on Multi-Hop Attention Mechanism"},{"paperId":"8c9a9a1bbba2a3e3bab34bce533b3b2acfda32b0","title":"Medical visual question answering based on question-type reasoning and semantic space constraint"},{"paperId":"38cbcaab9387c9c08df2f89fe93792c3dfe46a01","title":"BreastScreening-AI: Evaluating medical intelligent agents for human-AI interactions"},{"paperId":"4ef3d9e492479e28fa57d107e52acc6a0c803de2","title":"Does CLIP Benefit Visual Question Answering in the Medical Domain as Much as it Does in the General Domain?"},{"paperId":"e34b699cef0a711a8cb9c39ecea20ac2df1578f5","title":"Medical Visual Question Answering: A Survey"},{"paperId":"39528ef1de5a6c1b4fba44071591e9f12167769c","title":"MVQAS: A Medical Visual Question Answering System"},{"paperId":"681b16ed7258bf28622f9c835cfe94f195bb5395","title":"MedFuseNet: An attention-based multimodal deep learning model for visual question answering in the medical domain"},{"paperId":"933242d263859a12c058979163f58035047d78b5","title":"Fine-grained Hand Gesture Recognition in Multi-viewpoint Hand Hygiene"},{"paperId":"ecf3163157d477d1a2188a3f8cf75c697a303708","title":"Goal-Driven Visual Question Generation from Radiology Images"},{"paperId":"e4f99837e02e7fbcccec1bf15cececacaaabbe32","title":"MuVAM: A Multi-View Attention-based Model for Medical Visual Question Answering"},{"paperId":"3795b18bb223ee70c6c4347ea371b28fffb671e8","title":"Automatic Generation of Structured Radiology Reports for Volumetric Computed Tomography Images Using Question-Specific Deep Feature Extraction and Learning"},{"paperId":"5bd42c29a5ba8a6c39547db89023d879e98a6b32","title":"Multiple Meta-model Quantifying for Medical Visual Question Answering"},{"paperId":"3c83f80f06633ff4598d33c2959f8e4cdcad3e93","title":"Cross-Modal Self-Attention with Multi-Task Pre-Training for Medical Visual Question Answering"},{"paperId":"17c2bb358169541f2d0a769f80779f46d1cd3d37","title":"MMBERT: Multimodal BERT Pretraining for Improved Medical VQA"},{"paperId":"a4d21d620a6cb7a8e6f06b996463172478562a0a","title":"Visual Question Answering using Data Mining Techniques for Skeletal Scintigraphy in medical domain - VQADMSS"},{"paperId":"3fb9e014d52a2082141acefdeaddbd81fd422b24","title":"Visual Question Answering: which investigated applications?"},{"paperId":"93b6b79b4ef6c345f31722ce7c829385c6dce0d6","title":"Slake: A Semantically-Labeled Knowledge-Enhanced Dataset For Medical Visual Question Answering"},{"paperId":"a627232a97a7a63f8399d157f0b022eb1ccd547c","title":"Biomedical Question Answering: A Survey of Approaches and Challenges"},{"paperId":"b8d5b853f2212cbb48a43f1edec9b96d76d388ec","title":"Medical Visual Question Answering via Conditional Reasoning"},{"paperId":"72e0dccf59f126a64f970fe9f4712b3221a3be8c","title":"Pathological Visual Question Answering"},{"paperId":"9fe3eeafbe022de014aeb54d0b55502e2a2e46fe","title":"Hierarchical Deep Multi-modal Network for Medical Visual Question Answering"},{"paperId":"ed2a06388dd14b052f33bac5e3bfc0fa26243b55","title":"A Comparison of Pre-trained Vision-and-Language Models for Multimodal Representation Learning across Medical Images and Reports"},{"paperId":"d77a71c94e688d92a3fa10fb7f7feda2c306b9dc","title":"Visual Question Generation from Radiology Images"},{"paperId":"6609489a0f800a9ef411efdcfca4c014c4e86aa8","title":"Towards Visual Dialog for Radiology"},{"paperId":"30f86e15b4fd7936b9812d476976a6ff579b9036","title":"Toward General Scene Graph: Integration of Visual Semantic Knowledge with Entity Synset Alignment"},{"paperId":"ed6ce80789889c0fd56c8117f85079c1c31fe426","title":"CGMVQA: A New Classification and Generative Model for Medical Visual Question Answering"},{"paperId":"fc0b46a0f3720e6c29c1a913aaa3de4a0699f713","title":"PathVQA: 30000+ Questions for Medical Visual Question Answering"},{"paperId":"099d7a1b4f80318c4b6090bfba4b60d1ff81220f","title":"A database for using machine learning and data mining techniques for coronary artery disease diagnosis"},{"paperId":"e11ec81062d591b8bd97362d671db80394e391f6","title":"MoBVQA: A Modality based Medical Image Visual Question Answering System"},{"paperId":"33301b25a297b701bdc287e985c006375cb7bb21","title":"Overcoming Data Limitation in Medical Visual Question Answering"},{"paperId":"f59ae732612ce8c42035adfb47bd5739c6288ad6","title":"Answering Questions about Data Visualizations using Efficient Bimodal Fusion"},{"paperId":"46699dd04d40efd34ae9088f945f672e50f9ec62","title":"Concept-Centric Visual Turing Tests for Method Validation"},{"paperId":"821bb93c7638bf722bea8444059502cbc4674a04","title":"Artificial intelligence, regenerative surgery, robotics? What is realistic for the future of surgery?"},{"paperId":"9ae30ac3609a90b487df3beec10eeface021c7c5","title":"Machine Learning in Computer Vision: A Review"},{"paperId":"b88f6aa65a4e1faf963494a76d28cc12112c9543","title":"A Critical Analysis of Benchmarks, Techniques, and Models in Medical Visual Question Answering"},{"paperId":"61c0b6a5e7aea48a1376b61a4a737137d602b242","title":"PubMedCLIP: How Much Does CLIP Benefit Visual Question Answering in the Medical Domain?"},{"paperId":"31a7d8c4a5ab6bab522494b57270249105c8748e","title":"BiomedGPT: A Unified and Generalist Biomedical Generative Pre-trained Transformer for Vision, Language, and Multimodal Tasks"},{"paperId":"76f8bad5aff7c21cc0049e8d97ff53c34e8311f8","title":"A Dual of Stacked Attention Networks (SAN's) and VGG-16 Model-Based Visual Question Answering Evaluation"},{"paperId":"b047b3b7d76b79958e23b0fcab985be22b1ce42d","title":"Alternating Cross-attention Vision-Language Model for Efficient Learning with Medical Image and Report without Curation"},{"paperId":"79478a2ac67b9fdbeadcde13faa2d84eb239e080","title":"Vision-Language Pretraining Enables Radiographs and Reports to be Learned without Curation"},{"paperId":"fae23fc97a31bf66563dd033faf311eaaaa05911","title":"Exploratory analysis of different metaheuristic optimization methods for medical image enhancement"},{"paperId":"2441230bd2f3cca924d597b3044ad63aaff269ec","title":"Self-supervised Co-learning of Uncurated Images and Reports Enables Oversight AI in Radiology"},{"paperId":"1f96539c083d60fa83f7548bc6996cdede1026ee","title":"Biomedical Question Answering: A Comprehensive Review"},{"paperId":"2551990a1ccdffb1a4d1d9040b2d493ba6d26dd1","title":"Towards Visual Question Answering on Pathology Images"},{"paperId":"411c7a1fb951a1420013c0af56f9d142565112aa","title":"Chabbiimen at VQA-Med 2021: Visual Generation of Relevant Natural Language Questions from Radiology Images for Anomaly Detection"},{"paperId":"357fc385caf2e5b9898c9140fa3ac9955e6bb3c6","title":"TeamS at VQA-Med 2021: BBN-Orchestra for Long-tailed Medical Visual Question Answering"},{"paperId":"31acfba3a19f780a3239925ff12a7a4047d6a705","title":"MLEC-QA: A Chinese Multi-Choice Biomedical Question Answering Dataset"},{"paperId":"519799a9e2a72b808d81c6bcba5de263503de053","title":"Contrastive Pre-training and Representation Distillation for Medical Visual Question Answering Based on Radiology Images"},{"paperId":"f2ff49a399468c005a7cf689ff882b4198587b1b","title":"MedFuseNet: An attention-based multimodal deep learning model for visual question answering in the medical domain"},{"paperId":"83fabf18e4b67f148cc5f85fda417f6a98d31bf4","title":"Overview of the VQA-Med Task at ImageCLEF 2021: Visual Question Answering and Generation in the Medical Domain"},{"paperId":"e05e4ab677e93939adf901945987ef7493dfbe37","title":"The Inception Team at VQA-Med 2020: Pretrained VGG with Data Augmentation for Medical VQA and VQG"},{"paperId":"cc71a905ca132999a158857823606cd979b9080e","title":"Visual Dialog for Radiology: Data Curation and FirstSteps"},{"paperId":"b7c9e854c1e9b964c8abe0cb80a744e2739ddf40","title":"Tlemcen University at ImageCLEF 2019 Visual Question Answering Task"},{"paperId":"1526501f1939311106f72c128a189bbb6487ca6a","title":"SMAC: An Interpretable Reasoning Network for Visual Question Answering"},{"paperId":"9eeeb23546d3d2bbc73959bffc6819f2335f3c83","title":"VQA-Med: Overview of the Medical Visual Question Answering Task at ImageCLEF 2019"},{"paperId":"4634bf44a0c994e2bed89686225f8cef601a0224","title":"NLM at ImageCLEF 2018 Visual Question Answering in the Medical Domain"},{"paperId":"a7930f1ce6085f5bb6301cad958fd6501002eda0","title":"Medical Image Analysis"}],"references":[{"paperId":"7e4b638e028498e900747b600f46cd723f1f231e","title":"Data Augmentation for Visual Question Answering"},{"paperId":"6ff909c6fe089fc8ebfc64eca0f0c3cc34ba277f","title":"A survey on deep learning in medical image analysis"},{"paperId":"7e232313a59d735ef7c8a9f4cc7bc980a29deb5e","title":"Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering"},{"paperId":"12f7de07f9b00315418e381b2bd797d21f12b419","title":"Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding"},{"paperId":"0460d3497490fa8332c5ff2ecdab88fb7dff4755","title":"Learning to Read Chest X-Rays: Recurrent Neural Cascade Model for Automated Image Annotation"},{"paperId":"afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d","title":"Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations"},{"paperId":"2c03df8b48bf3fa39054345bafabfeff15bfd11d","title":"Deep Residual Learning for Image Recognition"},{"paperId":"def584565d05d6a8ba94de6621adab9e301d375d","title":"Visual7W: Grounded Question Answering in Images"},{"paperId":"2c1890864c1c2b750f48316dc8b650ba4772adc5","title":"Stacked Attention Networks for Image Question Answering"},{"paperId":"580062407427236ced45253a2ff7df2e147a81e2","title":"The Multimodal Brain Tumor Image Segmentation Benchmark (BRATS)"},{"paperId":"2fcd5cff2b4743ea640c4af68bf4143f4a2cccb1","title":"Are You Talking to a Machine? Dataset and Methods for Multilingual Image Question"},{"paperId":"97ad70a9fa3f99adf18030e5e38ebe3d90daa2db","title":"VQA: Visual Question Answering"},{"paperId":"71b7178df5d2b112d07e45038cb5637208659ff7","title":"Microsoft COCO: Common Objects in Context"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"}],"id":"18f9a6045ba01cb079c4fa49a630d71bbd27cd92","summary":"This work introduces VQA-RAD, the first manually constructed dataset where clinicians asked naturally occurring questions about radiology images and provided reference answers and demonstrates the rich quality of this dataset over other automatically constructed ones."},{"url":"https://www.semanticscholar.org/paper/4ef3d9e492479e28fa57d107e52acc6a0c803de2","title":"Does CLIP Benefit Visual Question Answering in the Medical Domain as Much as it Does in the General Domain?","venue":"arXiv.org","year":2021,"referenceCount":31,"citationCount":58,"influentialCitationCount":7,"publicationDate":"27/12/2021","authors":"Sedigheh Eslami,Gerard de Melo,C. Meinel","citations":[{"paperId":"61cadcfa555cbef120df7c017ef02e87f19900b7","title":"Free Form Medical Visual Question Answering in Radiology"},{"paperId":"1e0d21dc2caf7b58342ddc8609fb30cdc1e27cd5","title":"MISS: A Generative Pretraining and Finetuning Approach for Med-VQA"},{"paperId":"93886752191db25efd096a65af7b09df5c0a64e0","title":"Data-Centric Foundation Models in Computational Healthcare: A Survey"},{"paperId":"15da03bb202a9504d13c9e106d3b630d6392cacc","title":"VALD-MD: Visual Attribution via Latent Diffusion for Medical Diagnostics"},{"paperId":"352252231462c24440bc0016638ea5fe8d4c6f7e","title":"UniDCP: Unifying Multiple Medical Vision-language Tasks via Dynamic Cross-modal Learnable Prompts"},{"paperId":"b1721374889899950994f67029fe899de257c140","title":"A Foundational Multimodal Vision Language AI Assistant for Human Pathology"},{"paperId":"d48fa3ed73817563130ef217d85011ce1fbe7470","title":"BESTMVQA: A Benchmark Evaluation System for Medical Visual Question Answering"},{"paperId":"6bdfffbf92d01c8b543088d40d46233610e469a8","title":"CLIP in Medical Imaging: A Comprehensive Survey"},{"paperId":"60b460c7939f828e156508dcf6b5f532589bc6cf","title":"MedXChat: Bridging CXR Modalities with a Unified Multimodal Large Model"},{"paperId":"1f5e1a036b24b9dd34c006ba3bb61119624f4fdb","title":"A Comprehensive Study of GPT-4V's Multimodal Capabilities in Medical Imaging"},{"paperId":"2010e5fb3a804ac376412b4fa65ee83f34d5e1d9","title":"A Systematic Evaluation of GPT-4V's Multimodal Capability for Medical Image Analysis"},{"paperId":"e845135f29b23505400ff00bfbecea517a7effa4","title":"A Multimodal Transfer Learning Approach for Medical Image Classification"},{"paperId":"27659e165ee529cbf6710f2aa5732332fce20a50","title":"Causal Reasoning through Two Layers of Cognition for Improving Generalization in Visual Question Answering"},{"paperId":"8e743a2841e26ea60d8d341ebaf50d2138b8bae3","title":"Dual Learning with Dynamic Knowledge Distillation for Partially Relevant Video Retrieval"},{"paperId":"5ca34d383136623bf8d851b63bc43a5c9674feac","title":"SDA-CLIP: surgical visual domain adaptation using video and text labels"},{"paperId":"823b4b5403500e5020ba38f345d48803f33f8a35","title":"An Empirical Analysis for Zero-Shot Multi-Label Classification on COVID-19 CT Scans and Uncurated Reports"},{"paperId":"50da73ae5d7f2dae59a3fc5b2a00d160b8bd437d","title":"Multimodal Foundation Models For Echocardiogram Interpretation"},{"paperId":"125081b729bee38708e218cba101f21bb5288c8a","title":"Exploring the Transfer Learning Capabilities of CLIP in Domain Generalization for Diabetic Retinopathy"},{"paperId":"9a89ba37542df3bf48d8098cb893dbd91e82a75c","title":"Variational Information Pursuit with Large Language and Multimodal Models for Interpretable Predictions"},{"paperId":"e9f0223f8dce8b04d37d1f56e6c976b5d0cb5956","title":"A Foundation LAnguage-Image model of the Retina (FLAIR): Encoding expert knowledge in text supervision"},{"paperId":"304f8b4edea01fdb5a2f7f8b998c83188deeccff","title":"Towards Generalist Biomedical AI"},{"paperId":"bf40c9e7832e1b2887cbf5798455f91705ea11ba","title":"Masked Vision and Language Pre-training with Unimodal and Multimodal Contrastive Losses for Medical Visual Question Answering"},{"paperId":"f89b8e79a1b4b9a2febd9b8ab3f7933c89e1c3e0","title":"A ChatGPT Aided Explainable Framework for Zero-Shot Medical Image Diagnosis"},{"paperId":"534675abb9d72fc0c08d080d4f73335ceb75902c","title":"Multimodal Prompt Retrieval for Generative Visual Question Answering"},{"paperId":"14b9561062faeaee8bc62c1a618f04b105f7b291","title":"Dental CLAIRES: Contrastive LAnguage Image REtrieval Search for Dental Research"},{"paperId":"e17e34fc53ce5e1d039d5dce056cb9c7691eb568","title":"Quilt-1M: One Million Image-Text Pairs for Histopathology"},{"paperId":"d92c797f587ce7f1b001920ab9e6b7d31960bd77","title":"RemoteCLIP: A Vision Language Foundation Model for Remote Sensing"},{"paperId":"cb5f53cd7d43ac6f48abf4dde6fc58e53a083b0e","title":"Accelerating the integration of ChatGPT and other large‐scale AI models into biomedical research and healthcare"},{"paperId":"c5bcc78ae708b29edb03481e12213eca53c28963","title":"A multi-modal model based on transformers for medical visual question answering"},{"paperId":"2587cb7b1c02fde76e3c23c13f1bd40d6a199c57","title":"Q2ATransformer: Improving Medical VQA via an Answer Querying Decoder"},{"paperId":"61b877c3ef91286fe3fca51c899bf6c857ea4add","title":"Leveraging medical Twitter to build a visual–language foundation model for pathology AI"},{"paperId":"f7ea746cd2cc25628a7a553ac27d228198be42cb","title":"Pre-trained multilevel fuse network based on vision-conditioned reasoning and bilinear attentions for medical image visual question answering"},{"paperId":"ce865a1d2ad7ac6850bfc72edcea9e6cf3930976","title":"Increasing Textual Context Size Boosts Medical Image-Text Matching"},{"paperId":"a5bc3c0bce8d105a6b95f999fed4ea59c342cb1d","title":"Multi-Modal Bias: Introducing a Framework for Stereotypical Bias Assessment beyond Gender and Race in Vision–Language Models"},{"paperId":"785650a805851c7e945523e495c5a523c60f72a4","title":"Open-Ended Medical Visual Question Answering Through Prefix Tuning of Language Models"},{"paperId":"5814bd146b37e13115af4330caf3a751159a156f","title":"BiomedCLIP: a multimodal biomedical foundation model pretrained from fifteen million scientific image-text pairs"},{"paperId":"4d4a96708fc67403176bb2b891b564af7a20c148","title":"RAMM: Retrieval-augmented Biomedical Visual Question Answering with Multi-modal Pre-training"},{"paperId":"c011a59b155965caf2ab5cdf9e61e6544142a981","title":"X-TRA: Improving Chest X-ray Tasks with Cross-Modal Retrieval Augmentation"},{"paperId":"125632627bfad80c2c688bcbed7f3ee915de7359","title":"CLIP-Driven Universal Model for Organ Segmentation and Tumor Detection"},{"paperId":"90cd86b3c157e40cbaf1076f69cbd38d9c0781b9","title":"UnICLAM: Contrastive Representation Learning with Adversarial Masking for Unified and Interpretable Medical Vision Question Answering"},{"paperId":"56d8d9fff399f798da97a69e891de4eeb4568d4f","title":"MHKD-MVQA: Multimodal Hierarchical Knowledge Distillation for Medical Visual Question Answering"},{"paperId":"170667a96f04adf3b3b83526f75fe8d1063e0f7a","title":"Self-Supervised Vision-Language Pretraining for Medial Visual Question Answering"},{"paperId":"4be48792dfdb5876023fb0523cd25d4b89083ef4","title":"Remote sensing visual question answering with a self-attention multi-modal encoder"},{"paperId":"51d2032637f97fd0a10d01d80ced906d0b6e6444","title":"Transfer Learning for Medical Image Classification on Multiple Datasets using PubMedCLIP"},{"paperId":"6ae700c89a9a9a3da7e55dd51c4710b5ed8c8d4e","title":"Caption-Aware Medical VQA via Semantic Focusing and Progressive Cross-Modality Comprehension"},{"paperId":"5d8fd04c436367b18b35e28332ee8e452a477f3f","title":"Medical Image Understanding with Pretrained Vision Language Models: A Comprehensive Study"},{"paperId":"28ff0816f19a5e3e37eac5569de41872fd262f0a","title":"Align, Reason and Learn: Enhancing Medical Vision-and-Language Pre-training with Knowledge"},{"paperId":"24aa57dae649b6683d8f5bc8deaf2ff549cdacc4","title":"Transformers in Medical Imaging: A Survey"},{"paperId":"6dd9f99cecd38504b667d320eb2a6267a9fee35d","title":"Contrastive Learning of Medical Visual Representations from Paired Images and Text"},{"paperId":"e710fc044e79d2df28f405952f3eeee579f28a73","title":"IUST_NLPLAB at ImageCLEFmedical Caption Tasks 2023"},{"paperId":"c202fc756bbbe0b47b3021d806ee37b924a7db39","title":"Causal Reasoning through Two Cognition Layers for Improving Generalization in Visual Question Answering"},{"paperId":"31a7d8c4a5ab6bab522494b57270249105c8748e","title":"BiomedGPT: A Unified and Generalist Biomedical Generative Pre-trained Transformer for Vision, Language, and Multimodal Tasks"},{"paperId":"b047b3b7d76b79958e23b0fcab985be22b1ce42d","title":"Alternating Cross-attention Vision-Language Model for Efficient Learning with Medical Image and Report without Curation"},{"paperId":"79478a2ac67b9fdbeadcde13faa2d84eb239e080","title":"Vision-Language Pretraining Enables Radiographs and Reports to be Learned without Curation"},{"paperId":"2441230bd2f3cca924d597b3044ad63aaff269ec","title":"Self-supervised Co-learning of Uncurated Images and Reports Enables Oversight AI in Radiology"},{"paperId":"49b43e98c4ffc4ee0383f15940c97e9540a64c9f","title":"Language over Labels: Contrastive Language Supervision Exceeds Purely Label-Supervised Classification Performance on Chest X-Rays"},{"paperId":"0a8b6fafc045642e4502d74d899cdaeeabf02f92","title":"Image-Text Re-Matching with Zero-shot and Finetuning of CLIP"},{"paperId":"a7930f1ce6085f5bb6301cad958fd6501002eda0","title":"Medical Image Analysis"}],"references":[{"paperId":"8f167ec1149921fac63b1ea855443de109bb013a","title":"How Much Can CLIP Benefit Vision-and-Language Tasks?"},{"paperId":"e4f99837e02e7fbcccec1bf15cececacaaabbe32","title":"MuVAM: A Multi-View Attention-based Model for Medical Visual Question Answering"},{"paperId":"ac74a160e0ca53d3ffb15f79f0b9d3911df2fc28","title":"Glance-and-Gaze Vision Transformer"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"93b6b79b4ef6c345f31722ce7c829385c6dce0d6","title":"Slake: A Semantically-Labeled Knowledge-Enhanced Dataset For Medical Visual Question Answering"},{"paperId":"cb596bffc5c5042c254058b62317a57fa156fea4","title":"Unifying Vision-and-Language Tasks via Text Generation"},{"paperId":"268d347e8a55b5eb82fb5e7d2f800e33c75ab18a","title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"},{"paperId":"b8d5b853f2212cbb48a43f1edec9b96d76d388ec","title":"Medical Visual Question Answering via Conditional Reasoning"},{"paperId":"6dd9f99cecd38504b667d320eb2a6267a9fee35d","title":"Contrastive Learning of Medical Visual Representations from Paired Images and Text"},{"paperId":"1e5fb8003f866f9beeb1003f0be9a129d480ae75","title":"Problems and Opportunities in Training Deep Learning Software Systems: An Analysis of Variance"},{"paperId":"6adb61121ca4560915ade532910acde56440b88f","title":"A Question-Centric Model for Visual Question Answering in Medical Imaging"},{"paperId":"3c8a456509e6c0805354bd40a35e3f2dbf8069b1","title":"PyTorch: An Imperative Style, High-Performance Deep Learning Library"},{"paperId":"33301b25a297b701bdc287e985c006375cb7bb21","title":"Overcoming Data Limitation in Medical Visual Question Answering"},{"paperId":"4aa6298b606941a282d735fa3143da293199d2ca","title":"VL-BERT: Pre-training of Generic Visual-Linguistic Representations"},{"paperId":"a564fabf130ff6e2742cfba90c7a4018937d764d","title":"Radiology Objects in COntext (ROCO): A Multimodal Image Dataset"},{"paperId":"a5d10341717c0519cf63151b496a6d2ed67aa05f","title":"Bilinear Attention Networks"},{"paperId":"b14a60a1c3e6bb45baddd754a1cfe83ffc1bbb81","title":"Tips and Tricks for Visual Question Answering: Learnings from the 2017 Challenge"},{"paperId":"c889d6f98e6d79b89c3a6adf8a921f88fa6ba518","title":"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks"},{"paperId":"5582bebed97947a41e3ddd9bd1f284b73f1648c2","title":"Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization"},{"paperId":"2c03df8b48bf3fa39054345bafabfeff15bfd11d","title":"Deep Residual Learning for Image Recognition"},{"paperId":"2c1890864c1c2b750f48316dc8b650ba4772adc5","title":"Stacked Attention Networks for Image Question Answering"},{"paperId":"97ad70a9fa3f99adf18030e5e38ebe3d90daa2db","title":"VQA: Visual Question Answering"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"f37e1b62a767a307c046404ca96bc140b3e68cb5","title":"GloVe: Global Vectors for Word Representation"},{"paperId":"1c6d990c80e60aa0b0059415444cdf94b3574f0f","title":"Stacked Convolutional Auto-Encoders for Hierarchical Feature Extraction"},{"paperId":"519799a9e2a72b808d81c6bcba5de263503de053","title":"Contrastive Pre-training and Representation Distillation for Medical Visual Question Answering Based on Radiology Images"},{"paperId":"357fc385caf2e5b9898c9140fa3ac9955e6bb3c6","title":"TeamS at VQA-Med 2021: BBN-Orchestra for Long-tailed Medical Visual Question Answering"},{"paperId":"13e7212d5af59137ad770f42712d762247ebd3ed","title":"SYSU-HCP at VQA-Med 2021: A Data-centric Model with Efficient Training Methodology for Medical Visual Question Answering"},{"paperId":"39dbb2e49fb33351044a9b8c152a173b31f4c405","title":"Overview of the VQA-Med Task at ImageCLEF 2020: Visual Question Answering and Generation in the Medical Domain"},{"paperId":"18f9a6045ba01cb079c4fa49a630d71bbd27cd92","title":"Descriptor : A dataset of clinically generated visual questions and answers about radiology images"},{"paperId":null,"title":"Does CLIP beneﬁt visual question answering in the medical domain as much as it does in the general domain?"}],"id":"4ef3d9e492479e28fa57d107e52acc6a0c803de2","summary":"A fine-tuned version of CLIP for the medical domain based on PubMed articles is presented, which leads to noticeable improvements for MedVQA and fundamental performance differences of VQA in general versus medical domains are witnessed."},{"url":"https://www.semanticscholar.org/paper/6edcb09a09c8df43cb62119133df9bb2eb75e5cf","title":"From Images to Textual Prompts: Zero-shot VQA with Frozen Large Language Models","venue":"arXiv.org","year":2022,"referenceCount":67,"citationCount":33,"influentialCitationCount":0,"publicationDate":"21/12/2022","authors":"Jiaxian Guo,Junnan Li,Dongxu Li,A. M. H. Tiong,Boyang Li,Dacheng Tao,Steven Hoi","citations":[{"paperId":"fed3376de52d70ba83050182e79466dddde45746","title":"On the Safety Concerns of Deploying LLMs/VLMs in Robotics: Highlighting the Risks and Vulnerabilities"},{"paperId":"3ca5d2f2483e658c2810d57d5ee00d85d00aa3db","title":"Large Language Models for Captioning and Retrieving Remote Sensing Images"},{"paperId":"deb1f198eea208fa637a8b4d3934f9ad4c8ed1b6","title":"CIC: A framework for Culturally-aware Image Captioning"},{"paperId":"0be1c71b1710f01fb5d321e9b1459a7d2a7cdaf2","title":"Q&A Prompts: Discovering Rich Visual Clues through Mining Question-Answer Prompts for VQA requiring Diverse World Knowledge"},{"paperId":"8b55f7386a6f68df678cd791b29a23ab6c5515e4","title":"Teach Large Language Models to Forget Privacy"},{"paperId":"cd6402234500f37286e52811dbdbfb87b557a437","title":"Large Scale Foundation Models for Intelligent Manufacturing Applications: A Survey"},{"paperId":"4c92bd73698cf25e7a16c694f8c278ab3f9bfd08","title":"Effectively Fine-tune to Improve Large Multimodal Models for Radiology Report Generation"},{"paperId":"52941cadbd340344f3e0a6f50719fe55b3de5088","title":"Multimodal Large Language Models: A Survey"},{"paperId":"13d12b26db345f62e8e512db181b96a7f8763b47","title":"An Embodied Generalist Agent in 3D World"},{"paperId":"88bddfb7d1e0462be8fe99fdbd71c658140cb17b","title":"From Image to Language: A Critical Analysis of Visual Question Answering (VQA) Approaches, Challenges, and Opportunities"},{"paperId":"589e34ea39a418b7d2e291f66e379de4b55a7edd","title":"Harvest Video Foundation Models via Efficient Post-Pretraining"},{"paperId":"696ec1f8d25de41a082ff6c059e296c8fc6d9af2","title":"Challenges and Opportunities in Neuro-Symbolic Composition of Foundation Models"},{"paperId":"2b6a3cad4e4cbc2f2ae2a9f2d5b9e349071f24c2","title":"Open Visual Knowledge Extraction via Relation-Oriented Multimodality Model Prompting"},{"paperId":"96d104dfe727f78a35faaafe81481f3672b485ee","title":"Large Language Models are Visual Reasoning Coordinators"},{"paperId":"ac2e5bf716aed246ca8914a6816ef73e00286099","title":"Beyond Segmentation: Road Network Generation with Multi-Modal LLMs"},{"paperId":"28fbbf98bac1bb941162df553ca034d600cb59a6","title":"Rephrase, Augment, Reason: Visual Grounding of Questions for Vision-Language Models"},{"paperId":"61bbdbf481a6d3519c22513ebe8d6c3cd381851e","title":"Language Models as Knowledge Bases for Visual Word Sense Disambiguation"},{"paperId":"a9f016b31daf2896510ada413e54e62440d61ed0","title":"Learning to Prompt CLIP for Monocular Depth Estimation: Exploring the Limits of Human Language"},{"paperId":"0b9ab24684d275c355248c54bcac5d44cf6b1999","title":"LANCAR: Leveraging Language for Context-Aware Robot Locomotion in Unstructured Environments"},{"paperId":"93183f050e0ce0aff8ba0aa850c8353e24fb169d","title":"Fine-grained Late-interaction Multi-modal Retrieval for Retrieval Augmented Visual Question Answering"},{"paperId":"c74e9642ec71c6dfaadd3b8638c110d4048ff53e","title":"Suspicion-Agent: Playing Imperfect Information Games with Theory of Mind Aware GPT-4"},{"paperId":"a8f05b5ef3d60fb310f8366aa775118df5b700c3","title":"Tackling VQA with Pretrained Foundation Models without Further Training"},{"paperId":"838422b5ddaaa5637ba86056d1e964409bb2f016","title":"Robust Visual Question Answering: Datasets, Methods, and Future Challenges"},{"paperId":"16ea13d8a88b3d65596f36aac22561dd663bec22","title":"ClipSitu: Effectively Leveraging CLIP for Conditional Predictions in Situation Recognition"},{"paperId":"efc694164312006c543ef745611348ef64e68dda","title":"Towards Language Models That Can See: Computer Vision Through the LENS of Natural Language"},{"paperId":"8efc20988021ce3b4b05dd44b13e27260ee9b99b","title":"Investigating Prompting Techniques for Zero- and Few-Shot Visual Question Answering"},{"paperId":"3b508a48a4b48d2a16dd790a2a04ffcf51c0b4a6","title":"SUR-adapter: Enhancing Text-to-Image Pre-trained Diffusion Models with Large Language Models"},{"paperId":"20fcc01d12a50f1da2af71d85f0a269b3ba48b77","title":"LMEye: An Interactive Perception Network for Large Language Models"},{"paperId":"95430a76264a9be7d64633e56831c60041fb2948","title":"SurgicalGPT: End-to-End Language-Vision GPT for Visual Question Answering in Surgery"},{"paperId":"197022486b2e2584302bd9b6442e44d15bf3e351","title":"ICL-D3IE: In-Context Learning with Diverse Demonstrations Updating for Document Information Extraction"},{"paperId":"0f19e94f30b99d6c4b349900057cdae9262034f9","title":"The Contribution of Knowledge in Visiolinguistic Learning: A Survey on Tasks and Challenges"},{"paperId":"3f5b31c4f7350dc88002c121aecbdc82f86eb5bb","title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models"},{"paperId":"2859de53b8309a1389715f54b50bc84bab9893d3","title":"Towards Explainable Automatic Knowledge Graph Construction with Human-in-the-Loop"}],"references":[{"paperId":"bb15f3727f827a3cb88b5d3ca48415c09b40a88f","title":"What Language Model to Train if You Have One Million GPU Hours?"},{"paperId":"26fd105d0b5a458979c012cddb3ba2de943388c4","title":"Plug-and-Play VQA: Zero-shot VQA by Conjoining Large Pretrained Models with Zero Training"},{"paperId":"dac3a172b504f4e33c029655e9befb3386e5f63a","title":"Emergent Abilities of Large Language Models"},{"paperId":"47a67e76ed84260ff19f7a948d764005d1edf1c9","title":"A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge"},{"paperId":"57c64f233a0db4d17e0e750c12516364ca009fb2","title":"REVIVE: Regional Visual Representation Matters in Knowledge-Based Visual Question Answering"},{"paperId":"5437e8adab596d7294124c0e798708e050e25321","title":"Least-to-Most Prompting Enables Complex Reasoning in Large Language Models"},{"paperId":"7f5170b8ec68629164a98f8dfa1d2cbef5bbe5f5","title":"All You May Need for VQA are Image Captions"},{"paperId":"13a0d8bb38f739990c8cd65a44061c6534f17221","title":"OPT: Open Pre-trained Transformer Language Models"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"de20efa062cb54ce06beab24ac70be9501423f6a","title":"Pseudo-Q: Generating Pseudo Language Queries for Visual Grounding"},{"paperId":"7f71875f8214dffa4f3276da123c4990a6d437cc","title":"Enabling Multimodal Generation on CLIP via Vision-Language Knowledge Distillation"},{"paperId":"79956ac2a4164c298387546fc10139c3d5192842","title":"Webly Supervised Concept Expansion for General Purpose Vision Models"},{"paperId":"a3b42a83669998f65df60d7c065a70d07ca95e99","title":"BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation"},{"paperId":"1b6e810ce0afd0dd093f789d2b2742d047e316d5","title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"ab8ee8d06ed581c876da3a2e7a5fdb1cbec21a45","title":"KAT: A Knowledge Augmented Transformer for Vision-and-Language"},{"paperId":"2fd6f77540c1cc8e70b96208ccf9971b4251fc02","title":"FLAVA: A Foundational Language And Vision Alignment Model"},{"paperId":"21ec90872abd986c12afe39bebe807732ffa70c9","title":"Florence: A New Foundation Model for Computer Vision"},{"paperId":"a7aa150b55d64d339b1c154d6d88455fc3cbc44f","title":"ClipCap: CLIP Prefix for Image Captioning"},{"paperId":"118962f61df9ab6c8310d5a3eb0ab61f22802360","title":"Language bias in Visual Question Answering: A Survey and Taxonomy"},{"paperId":"32d59ab951be74be351f9777da2cbc71bb68c3c1","title":"A Good Prompt Is Worth Millions of Parameters: Low-resource Prompt-based Learning for Vision-Language Models"},{"paperId":"467e5a2164cf78c5be70c91129e1c6e843685fb3","title":"Discovering the Unknown Knowns: Turning Implicit Knowledge in the Dataset into Explicit Training Examples for Visual Question Answering"},{"paperId":"2672777d25562c9df6fc13b653181db62d39bece","title":"An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA"},{"paperId":"4e92fec0a61972ae076707d0630d1333affccdfc","title":"Weakly-Supervised Visual-Retriever-Reader for Knowledge-based Question Answering"},{"paperId":"f5a76442659066434b1bdf480cf11f4f549411ab","title":"QACE: Asking Questions to Evaluate an Image Caption"},{"paperId":"5e00596fa946670d894b1bdaeff5a98e3867ef13","title":"SimVLM: Simple Visual Language Model Pretraining with Weak Supervision"},{"paperId":"b82c5f9efdb2ae56baa084ca41aeddd8a665c1d1","title":"Align before Fuse: Vision and Language Representation Learning with Momentum Distillation"},{"paperId":"01b5412f3d17e90e09226d7c40ad4d4468a1414d","title":"Multimodal Few-Shot Learning with Frozen Language Models"},{"paperId":"63c74d15940af1af9b386b5762e4445e54c73719","title":"VinVL: Revisiting Visual Representations in Vision-Language Models"},{"paperId":"57ed901be5d1b4d853d4f8998dadc1b60e2151f9","title":"On Attention Redundancy: A Comprehensive Study"},{"paperId":"8dce342a435034fa0521b24b61393397df95c095","title":"Multi-Modal Answer Validation for Knowledge-Based VQA"},{"paperId":"7e5008713c404445dd8786753526f1a45b93de12","title":"GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow"},{"paperId":"cb596bffc5c5042c254058b62317a57fa156fea4","title":"Unifying Vision-and-Language Tasks via Text Generation"},{"paperId":"1a9015e511ec3da873f6114eeb542905a92d7d62","title":"KRISP: Integrating Implicit and Symbolic Knowledge for Open-Domain Knowledge-Based VQA"},{"paperId":"1a575075ba357723009a9a8905d5dccf9115ae6c","title":"WeaQA: Weak Supervision via Captions for Visual Question Answering"},{"paperId":"9958887e8dd5f84595818c50fb734b566996541a","title":"ConceptBert: Concept-Aware Representation for Visual Question Answering"},{"paperId":"0030605bfa0a11e7474a8c5ff5b00f3ccdb22b22","title":"Boosting Visual Question Answering with Context-aware Knowledge Aggregation"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"ad5970584754cc7a1d91c95ab84a1e210258183a","title":"UnifiedQA: Crossing Format Boundaries With a Single QA System"},{"paperId":"64a548ca02c8d647358cac809d9c059d34dc4f3a","title":"Radial Graph Convolutional Network for Visual Question Generation"},{"paperId":"818e5cbc337e4e1b98e65a2d7c2d6d2a0318cd57","title":"Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks"},{"paperId":"6548a60a6bcdf6c402d9de1c05ba7afe4f49fee9","title":"12-in-1: Multi-Task Vision and Language Representation Learning"},{"paperId":"6c4b76232bb72897685d19b3d264c6ee3005bc2b","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"79c93274429d6355959f1e4374c2147bb81ea649","title":"LXMERT: Learning Cross-Modality Encoder Representations from Transformers"},{"paperId":"65a9c7b0800c86a196bc14e7621ff895cc6ab287","title":"ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks"},{"paperId":"9ecd13dff7ce478d36b2390e28bbf3990a24c751","title":"Generating Question Relevant Captions to Aid Visual Question Answering"},{"paperId":"28ad018c39d1578bea84e7cedf94459e3dbe1e70","title":"OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge"},{"paperId":"1536e8958697c5364f68b2e2448905dbbeb3a0ca","title":"Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering"},{"paperId":"36c3972569a6949ecca90bfa6f8e99883e092845","title":"Pythia v0.1: the Winning Entry to the VQA Challenge 2018"},{"paperId":"4d1c856275744c0284312a3a50efb6ca9dc4cd4c","title":"Know What You Don’t Know: Unanswerable Questions for SQuAD"},{"paperId":"99ad0533f84c110da2d0713d5798e6e14080b159","title":"Looking Beyond the Surface: A Challenge Set for Reading Comprehension over Multiple Sentences"},{"paperId":"29de7c0fb3c09eaf55b20619bceaeafe72fd87a6","title":"Hierarchical Neural Story Generation"},{"paperId":"90873a97aa9a43775e5aeea01b03aea54b28bfbd","title":"Don't Just Assume; Look and Answer: Overcoming Priors for Visual Question Answering"},{"paperId":"7e4b638e028498e900747b600f46cd723f1f231e","title":"Data Augmentation for Visual Question Answering"},{"paperId":"a82c1d1ccaa3a3d1d6ee6677de0eed2e93ddb6e8","title":"Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering"},{"paperId":"915b5b12f9bdebc321e970ecd713458c3479d70e","title":"An Analysis of Visual Question Answering Algorithms"},{"paperId":"26aa6fe2028b5eefbaa40ab54ef725bbbe7d9810","title":"ConceptNet 5.5: An Open Multilingual Graph of General Knowledge"},{"paperId":"7e232313a59d735ef7c8a9f4cc7bc980a29deb5e","title":"Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering"},{"paperId":"5582bebed97947a41e3ddd9bd1f284b73f1648c2","title":"Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization"},{"paperId":"2c1890864c1c2b750f48316dc8b650ba4772adc5","title":"Stacked Attention Networks for Image Question Answering"},{"paperId":"97ad70a9fa3f99adf18030e5e38ebe3d90daa2db","title":"VQA: Visual Question Answering"},{"paperId":"e6a6b66eeb506dc326e3c3f7f49a1f260469c281","title":"VC-GPT: Visual Conditioned GPT for End-to-End Generative Vision-and-Language Pre-training"},{"paperId":"4593c88fb33023bec84f8f443d16262810b9047a","title":"CrossVQA: Scalably Generating Benchmarks for Systematically Testing VQA Generalization"},{"paperId":null,"title":"GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"c21a4d70d83e0f6eb2a9e1c41d034842dd561e47","title":"CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge"},{"paperId":null,"title":"Socialiqa: Commonsense reasoning about social interactions"},{"paperId":null,"title":"Success case analysis for VQAv2. Green color indicates answer cues and correct prediction"}],"id":"6edcb09a09c8df43cb62119133df9bb2eb75e5cf","summary":"Img2Prompt is a plug-and-play module that provides the prompts that can bridge the aforementioned modality and task disconnections, so that LLMs can perform zero-shot VQA tasks without end-to-end training."},{"url":"https://www.semanticscholar.org/paper/1e43c7084bdcb6b3102afaf301cce10faead2702","title":"BioBERT: a pre-trained biomedical language representation model for biomedical text mining","venue":"Bioinform.","year":2019,"referenceCount":45,"citationCount":3926,"influentialCitationCount":536,"publicationDate":"25/01/2019","authors":"Jinhyuk Lee,Wonjin Yoon,Sungdong Kim,Donghyeon Kim,Sunkyu Kim,Chan Ho So,Jaewoo Kang","citations":[{"paperId":"d31819413e4eb1b743bf087b61d7914812e66986","title":"A Token-based transition-aware joint framework for multi-span question answering"},{"paperId":"5220bc52d5bec334315724f059106b6b9e364fd3","title":"Information bottleneck based knowledge selection for commonsense reasoning"},{"paperId":"5fb0f16d1a661c3997d3ed3f71fc95f8b8161f1e","title":"How Important is Domain Specificity in Language Models and Instruction Finetuning for Biomedical Relation Extraction?"},{"paperId":"d4cc89b1c38e014b8889ec5eb44734728ffc1784","title":"LocalTweets to LocalHealth: A Mental Health Surveillance Framework Based on Twitter Data"},{"paperId":"b26f66c9bb59811c7d67cdab91a0d65d15364836","title":"An Evaluation of Large Language Models in Bioinformatics Research"},{"paperId":"2d968cbfbf8d5bd40041dc50b9d490642ce47e98","title":"On Leveraging Encoder-only Pre-trained Language Models for Effective Keyphrase Generation"},{"paperId":"b798cf6af813638fab09a8af6ad0f3df6c241485","title":"Benchmarking Retrieval-Augmented Generation for Medicine"},{"paperId":"2db51d392ba762af3703240325dc74df2c112b8f","title":"BiMediX: Bilingual Medical Mixture of Experts LLM"},{"paperId":"0a1b6d53f66733b584c95885564bfd6aacf22529","title":"InMD-X: Large Language Models for Internal Medicine Doctors"},{"paperId":"3b494b3b7b0b2013c0a036c9aa1868af807f8714","title":"HunFlair2 in a cross-corpus evaluation of biomedical named entity recognition and normalization tools"},{"paperId":"97cabfed8f83f49df8e524fa718d70fa661c3658","title":"A simple but effective span-level tagging method for discontinuous named entity recognition"},{"paperId":"73dd39b7bab172fc01980c966bb2746597bb012d","title":"Efficiency at Scale: Investigating the Performance of Diminutive Language Models in Clinical Tasks"},{"paperId":"13b8934468665ecb586f491d7f9f6c460cb095e5","title":"BioMistral: A Collection of Open-Source Pretrained Large Language Models for Medical Domains"},{"paperId":"fed3376de52d70ba83050182e79466dddde45746","title":"On the Safety Concerns of Deploying LLMs/VLMs in Robotics: Highlighting the Risks and Vulnerabilities"},{"paperId":"3ed112214b8345738d6cb7146c3198f911eb2620","title":"Evaluating Knowledge Fusion Models on Detecting Adverse Drug Events in Text"},{"paperId":"15aad9b72ad278397a467a2f7a2576fff6557c5f","title":"Chinese MentalBERT: Domain-Adaptive Pre-training on Social Media for Chinese Mental Health Text Analysis"},{"paperId":"ee9eb03d95158bc405945abb7070ab45ecc647d7","title":"Integrating ChatGPT into Secure Hospital Networks: A Case Study on Improving Radiology Report Analysis"},{"paperId":"e99766a0aefbac7c9b8b998584c2ca86883055f9","title":"Academic Surgery in the Era of Large Language Models: A Review."},{"paperId":"208f3df65812326b983abe3bf12c84799e4c1709","title":"Named Entity Recognition of Pharmacokinetic parameters in the scientific literature"},{"paperId":"727b058bf0c3c3b5d18f1937783e8c7bbddcd03d","title":"Utilizing BERT for Information Retrieval: Survey, Applications, Resources, and Challenges"},{"paperId":"eca8ea2ed71ea0a2851881a748b0fd9a8540a692","title":"Artificial Intelligence for Literature Reviews: Opportunities and Challenges"},{"paperId":"2dc2d0273903f47e592eb9e693b763c13b2f8818","title":"Deep learning for drug‐drug interaction prediction: A comprehensive review"},{"paperId":"0c2f30064341bd59f25ece5bf6562722a01412e4","title":"A bibliometric analysis of autism spectrum disorder signaling pathways research in the past decade"},{"paperId":"7566981189099487b9a78db4496eacab177eaf27","title":"Crop-GPA: an integrated platform of crop gene-phenotype associations"},{"paperId":"f8b12781543b4d6ade2a16739c87d5b9afce0bd6","title":"A Combined Manual Annotation and Deep-Learning Natural Language Processing Study on Accurate Entity Extraction in Hereditary Disease Related Biomedical Literature."},{"paperId":"310a858da26bca61bc78a9f860304731e63604a2","title":"DAEDRA: A language model for predicting outcomes in passive pharmacovigilance reporting"},{"paperId":"e5481d7b936569040220867f533696af46017b3a","title":"Comparison of Prompt Engineering and Fine-Tuning Strategies in Large Language Models in the Classification of Clinical Notes"},{"paperId":"e708a73ea5f54cd944fb1e4ab36d947bc35aca2c","title":"Pretrained Generative Language Models as General Learning Frameworks for Sequence-Based Tasks"},{"paperId":"88e7ef0fe62ccf6c92d3c3bc8b5f5f66767e2a84","title":"Text-to-Code Generation with Modality-relative Pre-training"},{"paperId":"f2d2220ee6d015bfac03dabd1d8e4d19ec088d85","title":"Advances in materials informatics: a review"},{"paperId":"604200318cc7337ad0f6978ad4976603ce35e7ef","title":"Exploring the performance and explainability of fine-tuned BERT models for neuroradiology protocol assignment"},{"paperId":"ad3c7ec457de8ed0bc1144a8460cc3b2dd9ae323","title":"Drug-drug interaction relation extraction based on deep learning: A review"},{"paperId":"877226d28578f2e7a01bf1ad6c617f919b273d51","title":"Navigating Data Privacy and Analytics: The Role of Large Language Models in Masking conversational data in data platforms"},{"paperId":"3dd1c7e4d30999a51188b7ef960c2380021c2547","title":"Progress and Opportunities of Foundation Models in Bioinformatics"},{"paperId":"3cb5827e5e613fd0a3cd4cc515c2149743a8df79","title":"Leveraging Semantic Text Analysis to Improve the Performance of Transformer-Based Relation Extraction"},{"paperId":"186c7e80de21eda4272687c2ee1542af0e66f20a","title":"KnowLog: Knowledge Enhanced Pre-trained Language Model for Log Understanding"},{"paperId":"361e9549940fc75daeeef3d9b31183cc1cf417da","title":"A Survey on Challenges and Advances in Natural Language Processing with a Focus on Legal Informatics and Low-Resource Languages"},{"paperId":"a8e1c215722f46d3af27310c2819ea509e9db6bc","title":"Large language models assisted multi-effect variants mining on cerebral cavernous malformation familial whole genome sequencing"},{"paperId":"43b8600d11d98449bf7d81fa6b3f8f1c8beb8320","title":"EHR-KnowGen: Knowledge-enhanced multimodal learning for disease diagnosis generation"},{"paperId":"f3edc5a87051be2b4b249c4896c8ab14462c659c","title":"Knowledge-based dynamic prompt learning for multi-label disease diagnosis"},{"paperId":"3c68185557e2c649c7bf4805bad358946f6d11fc","title":"MED-Prompt: A novel prompt engineering framework for medicine prediction on free-text clinical notes"},{"paperId":"2848dfa7d0ae090fcfe4bda208c84510654c4296","title":"SA-MDKIF: A Scalable and Adaptable Medical Domain Knowledge Injection Framework for Large Language Models"},{"paperId":"76b48eb9719e8efa8b1215c6b04d0e57de7de72c","title":"Overview of temporal action detection based on deep learning"},{"paperId":"7ce9de3c54a725f7ed5b3eaa7d8d25d67a68dbda","title":"PolyAMiner-Bulk is a deep learning-based algorithm that decodes alternative polyadenylation dynamics from bulk RNA-seq data"},{"paperId":"3ddd2ac706eceeafe791bfe11dbc0a6ce74e8f11","title":"Natural Language Processing for Radiation Oncology: Personalizing Treatment Pathways"},{"paperId":"1d9784418f2b0786015286d494ff200f451e2a28","title":"Modelling long medical documents and code associations for explainable automatic ICD coding"},{"paperId":"3957d4c46e0949907c86d9ef4486ffebbd95fd69","title":"TCGA-Reports: A machine-readable pathology report resource for benchmarking text-based AI models"},{"paperId":"1b7cf9e153955d3aaf5a78323c24be8c6d3bef29","title":"FEUDA: Frustratingly Easy Prompt Based Unsupervised Domain Adaptation"},{"paperId":"f6ad16768ea5db9de78bd08a373c3295c016aabd","title":"Are my answers medically accurate? Exploiting medical knowledge graphs for medical question answering"},{"paperId":"bdcc5b4701af025652dec2feab2fe67c3530e2b6","title":"NNOSE: Nearest Neighbor Occupational Skill Extraction"},{"paperId":"8e1c1ef7743f624515fa59528a1dc8bf020c2162","title":"NanoNER: Named Entity Recognition for Nanobiology Using Experts’ Knowledge and Distant Supervision"},{"paperId":"af782b96b72cda6ff057a0d2ef4218676177eb87","title":"Knowledge-Aware Code Generation with Large Language Models"},{"paperId":"ef91db28bc3c301e3d8ef91b361178dbad54c1c1","title":"Credit Risk Meets Large Language Models: Building a Risk Indicator from Loan Descriptions in P2P Lending"},{"paperId":"16625e774006dc55d733c8b6660a01cf6a6ace9f","title":"Mitophagy in Alzheimer’s Disease: A Bibliometric Analysis from 2007 to 2022"},{"paperId":"89abfcec27ae484c368c25b0a1441aaa3bf6e408","title":"What is the Consumer Attitude toward Healthcare Services? A Transfer Learning Approach for Detecting Emotions from Consumer Feedback"},{"paperId":"392024c233bc1ca3a2cf66a2e5185b3d4554060e","title":"The Impact of Snippet Reliability on Misinformation in Online Health Search"},{"paperId":"f4478d04731b4323f6db7f143188f18f2087929e","title":"CERM: Context-Aware Literature-Based Discovery via Sentiment Analysis"},{"paperId":"edb3f38e434b33c360c6c1e1b419f4ecab016c5a","title":"Transfer Learning for the Prediction of Entity Modifiers in Clinical Text: Application to Opioid Use Disorder Case Detection"},{"paperId":"1f98af8fdca3bc61da466ef88fdc64a86de6d422","title":"Almanac - Retrieval-Augmented Language Models for Clinical Medicine."},{"paperId":"0adafc770a0497832fbdd8a5e02aba6f9c984c1b","title":"KIMedQA: towards building knowledge-enhanced medical QA models"},{"paperId":"a0e5697f4250a20d79fd8f033ef105512d4bbd32","title":"Question answering systems for health professionals at the point of care - a systematic review"},{"paperId":"61cadcfa555cbef120df7c017ef02e87f19900b7","title":"Free Form Medical Visual Question Answering in Radiology"},{"paperId":"d376872de9d2420ce1f4a2d4a5a472334d26bdd2","title":"Clinical Information Retrieval: A Literature Review"},{"paperId":"67335a676ae3b4e17a4494edf19b2101484cd5b4","title":"AI-assisted Blockchain-enabled Smart and Secure E-prescription Management Framework"},{"paperId":"d034845314490c8a30ce8d801f87e00c853098fc","title":"Quality of Answers of Generative Large Language Models vs Peer Patients for Interpreting Lab Test Results for Lay Patients: Evaluation Study"},{"paperId":"6ed96d6822a06ad9a735bc09e301bf41df61c534","title":"CheXagent: Towards a Foundation Model for Chest X-Ray Interpretation"},{"paperId":"ec65cc0c0f3186c43ee839e61a4f1ae30acd50e3","title":"Generative AI-Driven Human Digital Twin in IoT-Healthcare: A Comprehensive Survey"},{"paperId":"2fd7d9923014c12477d1673f0f85c144d9502ba9","title":"Evaluating the ChatGPT family of models for biomedical reasoning and classification."},{"paperId":"1c10df8351a6efee2b87b16d405d73ab373f210f","title":"Automated stratification of trauma injury severity across multiple body regions using multi-modal, multi-class machine learning models"},{"paperId":"331c0d54e02b03b72e70b5058c86969ca392e71b","title":"MedLM: Exploring Language Models for Medical Question Answering Systems"},{"paperId":"b2e393581361fb47c6859be832ada7391dc8d4aa","title":"Dynamic Q&A of Clinical Documents with Large Language Models"},{"paperId":"c458eff1b087caa53feaf5c86b912e61fc0f2a97","title":"CrossU-Net: Dual-modality cross-attention U-Net for segmentation of precancerous lesions in gastric cancer."},{"paperId":"ede7ad7eba55bc0e292619e058097040784f8e8d","title":"NeighBERT: Medical Entity Linking Using Relation-Induced Dense Retrieval"},{"paperId":"6202d2a94b29d6602707eac98a17490eb50b2f89","title":"BibSonomy Meets ChatLLMs for Publication Management: From Chat to Publication Management: Organizing your related work using BibSonomy & LLMs"},{"paperId":"cc82790342e20f943f0b47f80175e0e3100a5503","title":"Optimizing classification of diseases through language model analysis of symptoms"},{"paperId":"6c273bde12952c8d58de56ba0a43405c0681bfbc","title":"Unfolding the Transitions in Sustainability Reporting"},{"paperId":"d47a0adfeee0c83299391d6bc205a1adb70d792e","title":"Leveraging External Knowledge Resources to Enable Domain-Specific Comprehension"},{"paperId":"c1046a57e416f6785069f5fa4f7b24b02cd66f7a","title":"Taec: a Manually annotated text dataset for trait and phenotype extraction and entity linking in wheat breeding literature"},{"paperId":"0c281160b7cfba2d652fea297b68317c7539cd37","title":"Assessing Large Language Models in Mechanical Engineering Education: A Study on Mechanics-Focused Conceptual Understanding"},{"paperId":"4be46ae53206440cfa6cab48f7523df5a701a65f","title":"Zero-shot Generative Large Language Models for Systematic Review Screening Automation"},{"paperId":"505aea68b6b9438d98ce0dfa228f69c84bcb9757","title":"Knowledge-Informed Machine Learning for Cancer Diagnosis and Prognosis: A review"},{"paperId":"94fd434cd601d842893af02c68c911c9dd312a5b","title":"Lightweight transformers for clinical natural language processing"},{"paperId":"2a0295009215387aa85add9b629b5ec2e3b9c7d6","title":"Hybrid architecture based intelligent diagnosis assistant for GP"},{"paperId":"07e6b56936fbbc83675344f2ec85ed6442e7433c","title":"TwinBooster: Synergising Large Language Models with Barlow Twins and Gradient Boosting for Enhanced Molecular Property Prediction"},{"paperId":"8551dc1842deb251f183fc9ebe38a726b1f1665b","title":"Empirical Analysis of Efficient Fine-Tuning Methods for Large Pre-Trained Language Models"},{"paperId":"fcb232cc3c7b4b1aaee9cce538f5ca8247d4df17","title":"A Span-based Model for Extracting Overlapping PICO Entities from RCT Publications"},{"paperId":"e6590f833cbccded3002467f8f0ec8479062bead","title":"Taxonomy Mining from a Smart City CMS using the Multidimensional Knowledge Representation Approach"},{"paperId":"18d9b13e3383d98c181f4d7a2b3ca1503ed707a0","title":"No-boundary thinking for artificial intelligence in bioinformatics and education"},{"paperId":"c9c322288f06976b11879baf566a8de040cf5e35","title":"German Text Embedding Clustering Benchmark"},{"paperId":"1347323c30180b5d5528abace9d36ba0d2dbf3c1","title":"ACP-ESM: A novel framework for classification of anticancer peptides using protein-oriented transformer approach"},{"paperId":"00b68a19a60a0611df19da97365b2cf40c6224b0","title":"BLINKtextsubscriptLSTM: BioLinkBERT and LSTM based approach for extraction of PICO frame from Clinical Trial Text"},{"paperId":"a76fdc630ccd8275ee48475c0224f307c48a7aba","title":"A patient safety knowledge graph supporting vaccine product development"},{"paperId":"93886752191db25efd096a65af7b09df5c0a64e0","title":"Data-Centric Foundation Models in Computational Healthcare: A Survey"},{"paperId":"a1c04034df40cbe6f0d0004d456fdab807ae63ec","title":"Generalist embedding models are better at short-context clinical semantic search than specialized embedding models"},{"paperId":"053487dd273f9bd03ab42afd8d713db579dcbeee","title":"Contextual Word Embedding for Biomedical Knowledge Extraction: a Rapid Review and Case Study"},{"paperId":"7db68d2282d03690c28a24004f0b5a88f23d4de7","title":"Freeze the backbones: A Parameter-Efficient Contrastive Approach to Robust Medical Vision-Language Pre-training"},{"paperId":"15da03bb202a9504d13c9e106d3b630d6392cacc","title":"VALD-MD: Visual Attribution via Latent Diffusion for Medical Diagnostics"},{"paperId":"595dd3553e3233f461b8ef4f71d4023f723f3376","title":"Artificial intelligence generated content (AIGC) in medicine: A narrative review."},{"paperId":"938dde7822cc914610c2f8681396c2c90bf3a081","title":"Entity Recognition from Colloquial Text"},{"paperId":"0f02e3c514a57bdeb224d1a24d206f7b3cc27885","title":"A hierarchical convolutional model for biomedical relation extraction"},{"paperId":"78a67ff8654a08a9c36e66190bb1474234e4e151","title":"Revealing the technology development of natural language processing: A Scientific entity-centric perspective"},{"paperId":"23170f794747145d3c56baddd9584e061e474f8b","title":"A study on pharmaceutical text relationship extraction based on heterogeneous graph neural networks."},{"paperId":"0d9bfeab1a38aa770b4237ebf89ba76bf9206ca3","title":"Predicting Anti-microbial Resistance using Large Language Models"},{"paperId":"f815ad5532d79a6bd0126826a09055750fec4447","title":"BactInt: A domain driven transfer learning approach for extracting inter-bacterial associations from biomedical text."},{"paperId":"6b3acd49f16b2a5d68337b7160955e2d226fbb0b","title":"An empirical assessment of different word embedding and deep learning models for bug assignment"},{"paperId":"e5d7159adb4458df938181d0e40edc1cbeea4551","title":"A topical review on AI-interlinked biodomain sensors for multi-purpose applications"},{"paperId":"ab0eb6cab97ee63ca1ab16a2e468cd1326449088","title":"Industry-sensitive language modeling for business"},{"paperId":"518f9b3aba34bb08a4f20b42423e16403cc1bf02","title":"Semantics-enabled Biomedical Literature Analytics."},{"paperId":"50cf2ce0ba9b33fbe3bee22921ceefcc98ab4c12","title":"T4SEpp: A pipeline integrating protein language models to predict bacterial type IV secreted effectors"},{"paperId":"5fdd837b9ee9e7a6507dc30a0bc0d6d4b8d4d7c1","title":"Child-Sum (N2E2N)Tree-LSTMs: An Interactive Child-Sum Tree-LSTMs to Extract Biomedical Event"},{"paperId":"f47125522490eb5dfe38d4b35289af27b4f9c9a1","title":"Validation of a Zero-shot Learning Natural Language Processing Tool to Facilitate Data Abstraction for Urologic Research."},{"paperId":"032f11733df3d4aa517911fd4e74d0611feda846","title":"Do Managers Always Make Optimal Investment Decisions: Relationship between Managerial Optimism Measured by BERT and Investment Decision-making"},{"paperId":"3fad6e45cf9d3c063079cd4042d698448b383cf7","title":"HSC-GPT: A Large Language Model for Human Settlements Construction"},{"paperId":"a4715887bd5a4c328ebf1d6ebb18ab94e71ee8d8","title":"MosaicBERT: A Bidirectional Encoder Optimized for Fast Pretraining"},{"paperId":"73757c0e4b2f28eaa73ac036694468fd6c811e22","title":"One Model to Rule them All: Towards Universal Segmentation for Medical Images with Text Prompts"},{"paperId":"c21c59740ad75e0d8164fa4355ba0317f9c65530","title":"Empowering Transformers for Evidence-Based Medicine"},{"paperId":"d93f1475212f14bd3c778b9f7f82f9dc0ee6c740","title":"T cell receptor binding prediction: A machine learning revolution"},{"paperId":"e2960506f5e69fe2b8e5bbf67025580e54431e39","title":"Safeguarding Mail-Order DNA Synthesis in the Age of Artificial Intelligence"},{"paperId":"fba6329fc9b1da747892efae8b89f4ef6d69fe21","title":"Mining literature and pathway data to explore the relations of ketamine with neurotransmitters and gut microbiota using a knowledge-graph"},{"paperId":"efd916a6e1f320a11715b2fbadd1dd7b9abcc9f7","title":"Improved Weighting in the Automated Texts Classification using Fuzzy Method"},{"paperId":"92aabfa1e0e174686c1c5190518136eb4d40f1e2","title":"Inference of Dependency Knowledge Graph for Electronic Health Records"},{"paperId":"11df06751c4d03c5cb70faccc4c3c25f415d6900","title":"Multi-level biomedical NER through multi-granularity embeddings and enhanced labeling"},{"paperId":"9b996cdf31e1078cc38dedb4980f42a5ca8fd10a","title":"Robust Knowledge Extraction from Large Language Models using Social Choice Theory"},{"paperId":"c7809b89c648012f28b272ed1bb91175f08426c8","title":"Collaborative Synthesis of Patient Records through Multi-Visit Health State Inference"},{"paperId":"99973c3830ba8fc8f4915f6fc4859e024c189a68","title":"The Influence of Teacher Professional Competence on Student Learning Motivation in PAI Subjects"},{"paperId":"fb8a1050c3a281fcaf20eb4526918503a6da5b0f","title":"Cross-Lingual Sentiment Analysis: Comparative Study of Opinion Expression Across Different Languages"},{"paperId":"97a868f12c123d637f667eb99633591db189b2d9","title":"Diversifying Knowledge Enhancement of Biomedical Language Models using Adapter Modules and Knowledge Graphs"},{"paperId":"8596a700705502563f5549fa005014cfff2b675d","title":"Domain-Specific Code Language Models: Unraveling the Potential for HPC Codes and Tasks"},{"paperId":"0a0b121dcc127be734c5199d121946bbe8b1ae5d","title":"Quantifying confidence shifts in a BERT-based question answering system evaluated on perturbed instances"},{"paperId":"744e0fa91081389e1249ccd04ce0d6289a6dd25f","title":"BioEGRE: a linguistic topology enhanced method for biomedical relation extraction based on BioELECTRA and graph pointer neural network"},{"paperId":"b87906b53933a8e442c54bd448821f9a869c6332","title":"PowerPulse: Power energy chat model with LLaMA model fine‐tuned on Chinese and power sector domain knowledge"},{"paperId":"352252231462c24440bc0016638ea5fe8d4c6f7e","title":"UniDCP: Unifying Multiple Medical Vision-language Tasks via Dynamic Cross-modal Learnable Prompts"},{"paperId":"4739e96fb40091b0d15bd22750b143e1fcef05c7","title":"Using BERT to Identify Causal Structure in Students’ Scientific Explanations"},{"paperId":"8bbcc26b4b8edf730f1b19997b318e9e63e80159","title":"LLM Instruction-Example Adaptive Prompting (LEAP) Framework for Clinical Relation Extraction"},{"paperId":"b667b389647f3bbccaedfa6a60b1bc50af246795","title":"A compressed large language model embedding dataset of ICD 10 CM descriptions"},{"paperId":"9d0cd8d210b5f9032837b04b173975397a1a31b2","title":"ConfliBERT-Spanish: A Pre-trained Spanish Language Model for Political Conflict and Violence"},{"paperId":"6cb3e86b3b518fb179887e49ae7e5b73153c17c3","title":"Enabling Dataspaces Using Foundation Models: Technical, Legal and Ethical Considerations and Future Trends"},{"paperId":"43a7ed002d1a1b822a0f3a0361cdc37d0d4c9a8e","title":"A study of deep active learning methods to reduce labelling efforts in biomedical relation extraction"},{"paperId":"0f49334fc5de373dfdb22d12a5c62a7732c5b511","title":"BUILD-KG: Integrating Heterogeneous Data Into Analytics-Enabling Knowledge Graphs"},{"paperId":"a6c8af2e21439ce62efc3b21e2b0c05bf178db62","title":"TreeBERT: Advanced Representation Learning for Relation Extraction"},{"paperId":"931623f332af66b84527f92648dbb5cd0a334a8b","title":"Towards automatic identification of self-reported COVID-19 tweets: Introducing a multilingual manually annotated dataset, baseline systems and exploratory evaluations"},{"paperId":"a3039c1780bc06c259637190e630025c6a87532e","title":"Integrating a PICO Clinical Questioning to the QL4POMR Framework for Building Evidence-Based Clinical Case Reports"},{"paperId":"9bdb2c918b91500356898a2432b0746b017f4613","title":"Achieving Seamless Semantic Interoperability and Enhancing Text Embedding in Healthcare IoT: A Deep Learning Approach with Survey"},{"paperId":"d9403061e3a3392652d037138532fab111c845c1","title":"Large language models in healthcare and medical domain: A review"},{"paperId":"33ac04c55ebcfa7a6dbc47514922bfb5cfeecbab","title":"Large Language Models are Clinical Reasoners: Reasoning-Aware Diagnosis Framework with Prompt-Generated Rationales"},{"paperId":"6bdfffbf92d01c8b543088d40d46233610e469a8","title":"CLIP in Medical Imaging: A Comprehensive Survey"},{"paperId":"21b3df7a067ed40c2f325df020846ed42bfd7ffc","title":"GWAS From Spoken Phenotypic Descriptions: A Proof of Concept From Maize Field Studies"},{"paperId":"9ba61be98dfbd865dc0a1bb501b55c152a46570d","title":"BioKG: a comprehensive, large-scale biomedical knowledge graph for AI-powered, data-driven biomedical research"},{"paperId":"8081cadfdd6c34627ec7584f8718a11a0ddf499b","title":"Bidirectional Encoder Representations from Transformers-like large language models in patient safety and pharmacovigilance: A comprehensive assessment of causal inference implications."},{"paperId":"b09cfb6f7c3cfa67c96578bc6577e628d4f17b37","title":"STRING-ing together protein complexes: extracting physical protein interactions from the literature"},{"paperId":"91353067c8f4e0ef81a031ab8e8c66bb8eef0080","title":"Improving dictionary-based named entity recognition with deep learning"},{"paperId":"338de438ce922fc154163d26ae761194855f69a3","title":"Ensemble Text Summarization Model for COVID-19-Associated Datasets"},{"paperId":"eb5a139538e6e74591be9d613ab8b625c7562ee6","title":"Knowledge-Based Intelligent Text Simplification for Biological Relation Extraction"},{"paperId":"fa427de71ff8602c9f4b766320fd8e340fbc1553","title":"Advancing Clinical Text Summarization through Extractive Methods using BERT-Based Models on the NBME Dataset"},{"paperId":"baa0ebbe528e035e05b1bb4bc620ef1e8b234353","title":"Integrating PubMed Label Hierarchy Knowledge into a Complex Hierarchical Deep Neural Network"},{"paperId":"f18560a6aee793bb9b4e4a1974cf4d061cedbbaf","title":"Labrador: Exploring the Limits of Masked Language Modeling for Laboratory Data"},{"paperId":"1bd7b90c9d92b3714e77d5bb1bcc9a190757afb2","title":"Dictionary-based matching graph network for biomedical named entity recognition"},{"paperId":"7e55d8701785818776323b4147cb13354c820469","title":"PaperQA: Retrieval-Augmented Generative Agent for Scientific Research"},{"paperId":"7ae88d10358347e4258269184f78991d18cad71d","title":"Deep Multimodal Fusion for Surgical Feedback Classification"},{"paperId":"1aa49b3622668d0500aa783e9a28bfb08e4b9ce6","title":"OpenDeID Pipeline for Unstructured Electronic Health Record Text Notes Based on Rules and Transformers: Deidentification Algorithm Development and Validation Study"},{"paperId":"413880f17759ffefbc15f0fdf81a83ffa15aa684","title":"Corporate Bankruptcy Prediction with BERT Model"},{"paperId":"c40717f5527c82acb9eb63634ce3aa67e2c9ad96","title":"Medical Extractive Question-Answering Based on Fusion of Hierarchical Features"},{"paperId":"e8915fb8c2d3e06b705e0ee5c3afe61420e529b8","title":"Fine-Grained and Complex Food Entity Recognition Benchmark for Ingredient Substitution"},{"paperId":"67239d6e9c2c5f8a6d19cb35154e5aa7eaa00f51","title":"Large Language Models on Graphs: A Comprehensive Survey"},{"paperId":"539ef391f944818b1158ea0e3f839ba43ccdccd2","title":"Topic-BiGRU-U-Net for Document-level Relation Extraction from Biomedical Literature"},{"paperId":"544ff0ab5255a1b4dfdbe2c3bf80faeec676adc9","title":"EGDE: A Framework for Bridging the Gap in Medical Zero-shot Relation Triplet Extraction"},{"paperId":"c07a58f82e6b3ee99af1e253fb1b69fa1b512c35","title":"Multimodal reasoning for nutrition and human health via knowledge graph embedding"},{"paperId":"ec3ef9bbbdf11d08f27fab35d1487acb6c2bc221","title":"Cascade Decoding for Antibiotic Resistance Event Extraction Based on Contrastive Learning"},{"paperId":"ceec4e2f08777997ba41132a5c6b397f54c3a569","title":"Joint Learning-based Multiple Documents Heterogeneous Graph Inference for Biomedical Entity Linking"},{"paperId":"20ae8ff81d9d87b7842272c6bff4233515167b4b","title":"Radiology Report Generation via Structured Knowledge-Enhanced Multi-modal Attention and Contrastive Learning"},{"paperId":"6eab13697a1a6f6c584d65c3c772d935d27f2b11","title":"Mining disease-associated genes based on heterogeneous graph transformer"},{"paperId":"397a11d4f5b31424d1ac5cf49732521c0b66d620","title":"Fine-tuning a pre-trained Transformers-based model for gene name entity recognition in biomedical text using a customized dataset: case of Desulfovibrio vulgaris Hildenborough"},{"paperId":"0dfbd9adbef108ab33e244f1ebc168835db2b235","title":"Machine learning based system for the automation of systematic literature reviews"},{"paperId":"9bddee9564053de6ab0a547fc46afc10eb7fe035","title":"Enhancing Clinical Outcome Predictions through Auxiliary Loss and Sentence-Level Self-Attention"},{"paperId":"6c09cb19cf424673911bc97bdaf6a17eda4012a2","title":"Biomedical Causal Relation Extraction via Data Augmentation and Multi-source Knowledge Fusion"},{"paperId":"6c8c36a790d2dded8b0cf8e1ecb8b44403204333","title":"An Effective Microbial–drug Relation Extraction Model Based on Dual Graph Convolutional Networks"},{"paperId":"bb4d212304221089b49612bdc4b1b9d898f77996","title":"A Tree-structured Neural Network Model for Joint Extraction of Adverse Drug Events"},{"paperId":"979ffb184d0b2945d1dfedff6157e531e98e7abe","title":"Survey and Experiments on Biomedical Pre-Trained Language Models for Named Entity Recognition"},{"paperId":"f87e82627e4c683fd49fec5c158e716722b9dfa7","title":"An Early Depression Detection Model on Social Media using Emotional and Causal Features"},{"paperId":"d0c05cc8cc062eae3d4671361d3762bbf41052f5","title":"Disease Diagnosis based on Multiple Semantic Relationship Prompt Subgraph"},{"paperId":"f5cc6df1a7d00a5ed93bee3db2e5df00c338a936","title":"LLMs Accelerate Annotation for Medical Information Extraction"},{"paperId":"f2e5d96c745b29e5d062a89b6c43810bdcb69969","title":"A medical multimodal large language model for future pandemics"},{"paperId":"68b9033e42088b58c4ef85e68ac62db2e9b4cb02","title":"Beyond rating scales: With targeted evaluation, large language models are poised for psychological assessment"},{"paperId":"47e674257870f781d918d3c59cc172d1c86e7c97","title":"Deep learning for report generation on chest X-ray images"},{"paperId":"8c882a0c55a87ff4a120b13c052608868f3b9f28","title":"Enhancing phenotype recognition in clinical notes using large language models: PhenoBCBERT and PhenoGPT"},{"paperId":"45f0808ef12ef299038be04d4c45cc2589baa52d","title":"A natural language processing system for the efficient updating of highly curated pathophysiology mechanism knowledge graphs"},{"paperId":"ab8ee4af2f0f30fe1ca080278d2a333ae1c7d3f2","title":"Automated clinical knowledge graph generation framework for evidence based medicine"},{"paperId":"2bf0b1caa891803b899859248970f0a48f6b2c19","title":"HALD, a human aging and longevity knowledge graph for precision gerontology and geroscience analyses"},{"paperId":"51fb6598a3ebe36b371b096b4824d718e6e527fb","title":"The Efficiency Spectrum of Large Language Models: An Algorithmic Survey"},{"paperId":"a4f95e1bcb0f7fe57e02dae86b74a8af2da64656","title":"BERT based natural language processing for triage of adverse drug reaction reports shows close to human-level performance"},{"paperId":"e1da20e6da3786931e9b0084813967d954c6508a","title":"Context is not key: Detecting Alzheimer’s disease with both classical and transformer-based neural language models"},{"paperId":"db7f52834908e3cac56e8a010cf775b69bc7a08a","title":"Explanatory Argument Extraction of Correct Answers in Resident Medical Exams"},{"paperId":"f900b983ca0eb1540fff1cba37d6b80d0743db13","title":"Entity recognition method for airborne products metrological traceability knowledge graph construction"},{"paperId":"ebb5dccb2f00c918ec5a165de5f1c09413a84935","title":"SOAP classifier for free-text clinical notes with domain-specific pre-trained language models"},{"paperId":"52bc4893339de8e68daa29492d2500946646d535","title":"EMoDi: Entity-Enhanced Momentum-Difference Contrastive Learning for Semantic-Aware Verification of Scientific Information"},{"paperId":"11f7383957a274f0c4878099f5780014b481ceaa","title":"Building knowledge graphs from technical documents using named entity recognition and edge weight updating neural network with triplet loss for entity normalization"},{"paperId":"5faa163cbd1651e5325cacba93f6b4d5fbcda6f0","title":"Biomedical knowledge graph-enhanced prompt generation for large language models"},{"paperId":"0e733bbb87f4406800c13b2b5439641368937750","title":"Gene-MOE: A sparsely gated prognosis and classification framework exploiting pan-cancer genomic information"},{"paperId":"cde42399a756854ad0997ac39f9f84231533efdc","title":"Ascle: A Python Natural Language Processing Toolkit for Medical Text Generation"},{"paperId":"6481d1a88ff3a180758c619ec69366f75fda62d7","title":"A machine learning-enabled open biodata resource inventory from the scientific literature"},{"paperId":"2b3554a8fea6f123fc04bd3e120f2293f227e1b2","title":"InstructMol: Multi-Modal Integration for Building a Versatile and Reliable Molecular Assistant in Drug Discovery"},{"paperId":"99dd3d58f0074aa1221b660927327b345935e60a","title":"SCREENER: Streamlined collaborative learning of NER and RE model for discovering gene-disease relations"},{"paperId":"4ed3854943390b9cf991bd877496b3822a71c59d","title":"A survey of consumer health question answering systems"},{"paperId":"82009c0a288eb86eae72a4c1c14b72b3cba1afa4","title":"Leveraging deep active learning to identify low-resource mobility functioning information in public clinical notes"},{"paperId":"ff5f0c5b6905a8c4b361a625b450e9ab417fa854","title":"MEDITRON-70B: Scaling Medical Pretraining for Large Language Models"},{"paperId":"e9e47833edacd4dbb5d3b3272fee4c11c5d41aaf","title":"Solving the Right Problem is Key for Translational NLP: A Case Study in UMLS Vocabulary Insertion"},{"paperId":"9c586fe742c3820667a04e613a8b81430a6859a8","title":"AptaBERT: Predicting aptamer binding interactions"},{"paperId":"460be63bf6876b947168a7024d3cda73e4cf5459","title":"BIR: Biomedical Information Retrieval System for Cancer Treatment in Electronic Health Record Using Transformers"},{"paperId":"eab81517f393f0e9515f89b164910d8482cdd196","title":"Identifying the potential miRNA biomarkers based on multi-view networks and reinforcement learning for diseases"},{"paperId":"d733c2d6e08f5b59ccc0af9188f1f86d0aa7a4c5","title":"nach0: Multimodal Natural and Chemical Languages Foundation Model"},{"paperId":"4a6c1ee10c448840f598a281374496f6ebe11b5c","title":"Adapt in Contexts: Retrieval-Augmented Domain Adaptation via In-Context Learning"},{"paperId":"29fa031268ffe0a80f7a4e3fb3a135db7fcd0b8d","title":"KBioXLM: A Knowledge-anchored Biomedical Multilingual Pretrained Language Model"},{"paperId":"cd370a4b149a3ffcebc8f2973952e82f723a973b","title":"Unifying Corroborative and Contributive Attributions in Large Language Models"},{"paperId":"ed9dfde95319125922166a6ea64fdcde226e09d3","title":"Automated classification of lay health articles using natural language processing: a case study on pregnancy health and postpartum depression"},{"paperId":"0c8a630657a2cf5dea41472a9b5e20544ce2bd56","title":"Taiyi: A Bilingual Fine-Tuned Large Language Model for Diverse Biomedical Tasks"},{"paperId":"31a748f9edfc69054f879a94a63a2b350ee42231","title":"TarBase-v9.0 extends experimentally supported miRNA–gene interactions to cell-types and virally encoded miRNAs"},{"paperId":"d7b4420226d01fa05a72a9d4f813f94a7032576d","title":"Leveraging Generative AI for Clinical Evidence Summarization Needs to Ensure Trustworthiness"},{"paperId":"6a149970c5f5b69ff777dd5f5a8d632d94d88091","title":"Prompting Meta-Learned Hierarchical Graph Network for Molecular Property Prediction"},{"paperId":"6f77613b4ecfa5c96942ede6a9a38f8580df831a","title":"A Knowledge-Enhanced Medical Named Entity Recognition Method that Integrates Pre-Trained Language Models"},{"paperId":"aae2e3a53ec7cdcff9898b11a7f73859206b89a1","title":"Zero-Shot Construction of Chinese Medical Knowledge Graph with ChatGPT"},{"paperId":"5853a640b2e1453661e978072892f4602eb3f48f","title":"Advancing COVID-19 Inquiry Responses Through Transfer Learning-Based Question Entailment Methodology"},{"paperId":"55cc92db1633e9155ea70c88264bd7a5bf35a5f5","title":"ClotCatcher: a novel natural language model to accurately adjudicate venous thromboembolism from radiology reports"},{"paperId":"eaa55bc8b654b038ecec15c2dad64ce32ae99806","title":"Source Prompt: Coordinated Pre-training of Language Models on Diverse Corpora from Multiple Sources"},{"paperId":"91ae1eadf9336fb613cb6a27ac0a2bc6046dbd41","title":"German FinBERT: A German Pre-trained Language Model"},{"paperId":"a817263affeed3fc11937b8e3e3f3ef7f04680c8","title":"GO2Sum: Generating Human Readable Functional Summary of Proteins from GO Terms"},{"paperId":"99be42f807311159efeb19f89fe0f0a6c3e97861","title":"An Eye on Clinical BERT: Investigating Language Model Generalization for Diabetic Eye Disease Phenotyping"},{"paperId":"74fe68e7a89390fdbb29a8546139ef9341aa979c","title":"An Entity Extraction pipeline for Medical Text Records Utilizing Large Language Models: An Analytical Study (Preprint)"},{"paperId":"3f5b5613de10ec59c87cc3c7b2b512e8ff3a772e","title":"Natural Language Processing for Financial Regulation"},{"paperId":"ed3682e646acb412823d60f0b7c736398ecb9b38","title":"PolyIE: A Dataset of Information Extraction from Polymer Material Scientific Literature"},{"paperId":"d3f96583a2f441f627b372e9452466eafa89656f","title":"Learning Knowledge-Enhanced Contextual Language Representations for Domain Natural Language Understanding"},{"paperId":"0d2e5c45f7bc8c9f58f4bc48d5295665a2417e56","title":"ADHD-KG: a knowledge graph of attention deficit hyperactivity disorder"},{"paperId":"24c3375fb3e18faf2356e0f2234f18c029ec1188","title":"PECoP: Parameter Efficient Continual Pretraining for Action Quality Assessment"},{"paperId":"aef5bea83f7f000e7bbdd81483220baeb5393014","title":"Complementary and Integrative Health Information in the literature: its lexicon and named entity recognition"},{"paperId":"cf90f4d2bea741281db659946598842e7c344a92","title":"Automatic Report Generation for Histopathology images using pre-trained Vision Transformers"},{"paperId":"58cf5b3f5b6cef67f0f0a2a909eb2efcb09e27ac","title":"Biomedical Named Entity Recognition Based on Residual Network and Global Context Mechanism"},{"paperId":"4bf576ff688724c7d2fc0ee759969a1dc37eb9de","title":"Labor Space: A Unifying Representation of the Labor Market via Large Language Models"},{"paperId":"04c3f1debbba3a856f26c58edc585d8688139740","title":"Legal-HNet: Mixing Legal Long-Context Tokens with Hartley Transform"},{"paperId":"0a898a2772c7c2941b83a737f1bef806710b047a","title":"Evaluating multiple large language models in pediatric ophthalmology"},{"paperId":"ade0ab81403cd1dbe2a49d76c0393dbd615fd979","title":"Principles from Clinical Research for NLP Model Generalization"},{"paperId":"65f22498157efa395c6feef7d88f948519c1a9d2","title":"Adapting Pre-trained Generative Models for Extractive Question Answering"},{"paperId":"b26a44eac3fb2a583846c99e594d841525bacbe7","title":"Injecting Categorical Labels and Syntactic Information into Biomedical NER"},{"paperId":"d587027861ae2ed5049b3efc71f1a08eeeca93f9","title":"Spoken Dialogue System for Medical Prescription Acquisition on Smartphone: Development, Corpus and Evaluation"},{"paperId":"f8c86413b528815e0749d0d7f6791fb31a60ba0c","title":"Using natural language processing to extract plant functional traits from unstructured text"},{"paperId":"283fad39ca746f10d2bcc444c1d80c9bd2a4132c","title":"Fine-tuning Strategies for Domain Specific Question Answering under Low Annotation Budget Constraints"},{"paperId":"e15007216ee8c4c490c84e11b046b3efbf6e4b92","title":"Automatic literature screening using the PAJO deep-learning model for clinical practice guidelines"},{"paperId":"b74be6891cd7f6d81e346852494c08528dbffdc4","title":"TCM-GPT: Efficient Pre-training of Large Language Models for Domain Adaptation in Traditional Chinese Medicine"},{"paperId":"c33416dfb9a00224b30db3faa6f61a03c9119cdf","title":"Investigating Deep-Learning NLP for Automating the Extraction of Oncology Efficacy Endpoints from Scientific Literature"},{"paperId":"6cb53afdc2b21236c3957efd4bbeb2aa65338c50","title":"An Interdisciplinary Outlook on Large Language Models for Scientific Research"},{"paperId":"ba6da0b434a0ea8f6e8ff73ba03d83933e1e2478","title":"Heterogeneous deep graph convolutional network with iterative deep graph learning for Covid-19 inline recommendation"},{"paperId":"9c93f65f5bffad8362537b61310babbc5f1af3ac","title":"DeepSA: a deep-learning driven predictor of compound synthesis accessibility"},{"paperId":"4a2a300aee196aff6f490c148a3966eb9a1b20d4","title":"Better with Less: A Data-Active Perspective on Pre-Training Graph Neural Networks"},{"paperId":"18d44e0bad89b512de308dfedf3105d5a0775a9f","title":"pathCLIP: Detection of Genes and Gene Relations from Biological Pathway Figures through Image-Text Contrastive Learning"},{"paperId":"441dc2cf846ab6c9bc75e6e7e8de92c62735e754","title":"Preserving the knowledge of long clinical texts using aggregated ensembles of large language models"},{"paperId":"944bca64fdd8fd068bf59ecf5f99e692cbfa5ba1","title":"Low-Resource Named Entity Recognition: Can One-vs-All AUC Maximization Help?"},{"paperId":"74bab08a1aff7e0e55f6da47535774e079a232ef","title":"Hierarchical Classification System for Breast Cancer Specimen Report (HCSBC) - an end-to-end model for characterizing severity and diagnosis"},{"paperId":"7d1c642f4de2cbe5a43d31d2b1961b91239cd1be","title":"A survey of the recent trends in deep learning for literature based discovery in the biomedical domain"},{"paperId":"4d5e44c257d92c49c51edd8bddff2c624c926fcf","title":"Deep learning-enabled natural language processing to identify directional pharmacokinetic drug–drug interactions"},{"paperId":"f5f8b3f79193d1302a3c7aa58696fd4d507d6cb3","title":"IDPpub: Illuminating the Dark Phosphoproteome Through PubMed Mining"},{"paperId":"ac3754287e8d2718d01f99ef83c48fb5a1331633","title":"Serial KinderMiner (SKiM) discovers and annotates biomedical knowledge using co-occurrence and transformer models"},{"paperId":"ed12a60f8c58319fc0a5536f62f193ac50e1ed84","title":"Deep purified feature mining model for joint named entity recognition and relation extraction"},{"paperId":"df22e98223d9c8adddfc7eee061c33042214cef9","title":"Multilingual bi‐encoder models for biomedical entity linking"},{"paperId":"a6432dca42d8c3d9627a65b3fc337f043e865746","title":"AdaSent: Efficient Domain-Adapted Sentence Embeddings for Few-Shot Classification"},{"paperId":"24928fa6efa495cc8828c015bc87c14f32ad6452","title":"Prediction of High-Risk Donors for Kidney Discard and Nonrecovery Using Structured Donor Characteristics and Unstructured Donor Narratives."},{"paperId":"77fc0ed29a5d9dfc99c00401c9ac920c4b73e07d","title":"A cross-modal clinical prediction system for intensive care unit patient outcome"},{"paperId":"ddd0e8917ee5ab020c6b63a1d55e09043b154adb","title":"One Hot (Up)Take: Vision, Language, and Domain Knowledge in PET/CT Reporting."},{"paperId":"63d2268d1c1bab53f59a378d0e63b113ee9d373a","title":"Integration of multiple terminology bases: a multi-view alignment method using the hierarchical structure"},{"paperId":"3e99d92eb2193a5bd94ab39fb2be9af709e88544","title":"BioBERT-based SNP-trait Associations Extraction from Biomedical Literature"},{"paperId":"1422ccf1d62a9c1192436e89487ae5d350a2a2fa","title":"Enhancing pre-trained contextual embeddings with triplet loss as an effective fine-tuning method for extracting clinical features from electronic health record derived mental health clinical notes"},{"paperId":"8b8b4ca8141f78f09962aa8a5a40691bd5acf4c8","title":"The SourceData-NLP dataset: integrating curation into scientific publishing for training large language models"},{"paperId":"49823523acaafbab64391a105a945045f1b0efd5","title":"Evidence-based clinical engineering: Health information technology adverse events identification and classification with natural language processing"},{"paperId":"8dd5ef8f047a2c74a30cc21d12b82c80232aa41d","title":"Interactive Multi-fidelity Learning for Cost-effective Adaptation of Language Model with Sparse Human Supervision"},{"paperId":"8a42f1d55656d1e8df9527747fefc4eacd6904ca","title":"Learning Meta Soft Prompt for Few-Shot Language Models"},{"paperId":"b7db959f3b7e6d0feffc0332b91400f0ac6cacb8","title":"Split-NER: Named Entity Recognition via Two Question-Answering-based Classifications"},{"paperId":"458915f659dd77f0306a53e6c5c7be30d1352f62","title":"Taking Off with AI: Lessons from Aviation for Healthcare"},{"paperId":"defe18a0a862b2024f1ea1671edd8b086a053c98","title":"BioInstruct: Instruction Tuning of Large Language Models for Biomedical Natural Language Processing"},{"paperId":"0340d6e882bb47674500d2304696f287bf89786a","title":"Semantic harmonization of Alzheimer’s disease datasets using AD-Mapper"},{"paperId":"d6bf6d570283423b968f195c7faa3fb80b62e405","title":"Sentence Bag Graph Formulation for Biomedical Distant Supervision Relation Extraction"},{"paperId":"ddbb30487dede4836aa9fd62acbee3247e2a260a","title":"Artificial Intelligence for Surface-Enhanced Raman Spectroscopy."},{"paperId":"14f18bed093405e9c80a920d679702c67a93d698","title":"Prediction of coronary heart disease risk based on multimodal EHRs"},{"paperId":"b763f84fb29243120ecca5a5c9fb092da6d430c0","title":"Constructing a finer-grained representation of clinical trial results from ClinicalTrials.gov"},{"paperId":"be4a8ca1fb43fdbc1db1c2bc2695c5cc05a0a5ff","title":"Joint Constrained Learning for Causal Event-Event Relation Extraction of Brain Connectome"},{"paperId":"40183351b32f1432841d1dfaa262f827d759cadc","title":"Show from Tell: Audio-Visual Modelling in Clinical Settings"},{"paperId":"9ea8105a7bc03dbcde05bf953f3b90f1db61f6bd","title":"URL-BERT: Training Webpage Representations via Social Media Engagements"},{"paperId":"980c7ee1df549f97b998ed156307e22ec3a7de29","title":"ENQUIRE RECONSTRUCTS AND EXPANDS GENE AND MESH CO-OCCURRENCE NETWORKS FROM CONTEXT-SPECIFIC LITERATURE"},{"paperId":"9c5609baff6175b0a2e436bb69e89737c4be3cf4","title":"Improving Biomedical Abstractive Summarisation with Knowledge Aggregation from Citation Papers"},{"paperId":"116eba970643613f5284b490f8c3c85319053763","title":"WojoodNER 2023: The First Arabic Named Entity Recognition Shared Task"},{"paperId":"426ac33636fd56ba80153e60fc59ef7f1db2d3af","title":"Multimodal Graph Learning for Modeling Emerging Pandemics with Big Data"},{"paperId":"468fc94845b52c6e96ba1f3c3884d0653d5421b4","title":"GeoLM: Empowering Language Models for Geospatially Grounded Language Understanding"},{"paperId":"876bfbe25cabe31d337e38d7e600b4d445581023","title":"Bi-Encoders based Species Normalization - Pairwise Sentence Learning to Rank"},{"paperId":"51232f0037e8ca83f30ba46b9f87e3565494063a","title":"NERetrieve: Dataset for Next Generation Named Entity Recognition and Retrieval"},{"paperId":"73325a6e15d940de861b115395f3c453608a4e50","title":"Continually-Adaptive Representation Learning Framework for Time-Sensitive Healthcare Applications"},{"paperId":"c6aab2f8dea7aaf650fc7df6a7fa067dac135e68","title":"PetBERT: automated ICD-11 syndromic disease coding for outbreak detection in first opinion veterinary electronic health records"},{"paperId":"39abce3268f309d5655247de0c442a28219df390","title":"MedEval: A Multi-Level, Multi-Task, and Multi-Domain Medical Benchmark for Language Model Evaluation"},{"paperId":"852355b71817c508a64593b65dc76bdf3f792dd5","title":"A Hybrid Named Entity Recognition System for Aviation Text"},{"paperId":"ec00637804494c881c3b208a6655665d127f7b71","title":"Exploring the Impact of Corpus Diversity on Financial Pretrained Language Models"},{"paperId":"2bd6d70978e92e960544329ef81cbe70668d7ffd","title":"CLIFT: Analysing Natural Distribution Shift on Question Answering Models in Clinical Domain"},{"paperId":"7dd4b7e6f5f1588eae707df8334589ffd503bc54","title":"A Predictive Factor Analysis of Social Biases and Task-Performance in Pretrained Masked Language Models"},{"paperId":"23f96db82ae02c5c3c0a861571e7aa8d27c91bc9","title":"Data Augmentations for Improved (Large) Language Model Generalization"},{"paperId":"53f1715758830e49722de332be18bc3184422ee9","title":"OphNER: Named Entity Recognition for Ophthalmology Newspapers"},{"paperId":"cee600d8e99fded496437a0358755b9fd2111c28","title":"Thyroidkeeper: a healthcare management system for patients with thyroid diseases"},{"paperId":"b44fbb613573f78d444361d7670c8ab130ef9174","title":"ViSoBERT: A Pre-Trained Language Model for Vietnamese Social Media Text Processing"},{"paperId":"468e84b71e8d71742a89bce870ee1727b94fd695","title":"Medical Specialty Classification Based on Semiadversarial Data Augmentation"},{"paperId":"b0a73d18abcdb250885d02d9f2149b9a65c2f3f5","title":"The landscape of biomedical research"},{"paperId":"b185e04bf51a50d23640160ae7c74b9b1a5747da","title":"Research on NER model for coal mine safety hazards based on BERT-CNN-BiGRUs-CRF"},{"paperId":"185f9b5d79cae6a2e20bbf0d935a26724192924f","title":"Semantic Template-based Convolutional Neural Network for Text Classification"},{"paperId":"5b038c1a93967072cc76689fd805e756f804cc42","title":"Large Models for Time Series and Spatio-Temporal Data: A Survey and Outlook"},{"paperId":"1efb48995732f58dbf2e251bfdf8571545033db9","title":"BioPlanner: Automatic Evaluation of LLMs on Protocol Planning in Biology"},{"paperId":"99bd4e2e25a0088c59342ba8047778fd9a8f9c22","title":"Learning entity-oriented representation for biomedical relation extraction"},{"paperId":"c2a707f093a58beb050892db9417e7cffd722213","title":"Interpretable Disease Prediction from Clinical Text by Leveraging Pattern Disentanglement"},{"paperId":"34f237a2cc90417e33ae70dd13636f60f056b8c5","title":"SiaKey: A Method for Improving Few-shot Learning with Clinical Domain Information"},{"paperId":"fff2a52ea684a50aae8b2d6e1077ce129be89498","title":"Surveying the Landscape of Text Summarization with Deep Learning: A Comprehensive Review"},{"paperId":"e95c17a6788dcd8aec1f4a99c1740ff0ee824243","title":"Prediction Of User Ratings For Drug Side Effects Using Deep Neural Network With Contextual Co-occurrence Based Word-Embedding Vector"},{"paperId":"d8605b80bd617a7b7b8ec9df625dd7972a9a2efa","title":"From Large Language Models to Knowledge Graphs for Biomarker Discovery in Cancer"},{"paperId":"d9ef89eab7f18bdd41588066b1987a4f91131fe0","title":"Question Answering for Electronic Health Records: A Scoping Review of datasets and models"},{"paperId":"9c4ebdcf3bdfed0ed9b2f0fea5ba6f8fea49c632","title":"Large Language Models for Scientific Synthesis, Inference and Explanation"},{"paperId":"bccc22f9f2f08e3e8490e341aea58b551592fedf","title":"MProto: Multi-Prototype Network with Denoised Optimal Transport for Distantly Supervised Named Entity Recognition"},{"paperId":"7fcb9ffcea0c6af04a2e7cfe7d0e750e77f8d311","title":"A Scientific Document Retrieval and Reordering Method by Incorporating HFS and LSD"},{"paperId":"d551073f0897fceccea43280d898bfcea6fbf0fc","title":"A Survey of Heterogeneous Transfer Learning"},{"paperId":"ddcc5bdd28355652e29393ee76d17601e5983b3f","title":"Effects of Human Adversarial and Affable Samples on BERT Generalization"},{"paperId":"b4a01f2c43481f4c956b1da384beedd90dc5e9e3","title":"DKEC: Domain Knowledge Enhanced Multi-Label Classification for Electronic Health Records"},{"paperId":"615c34193a26b8a5dc04407d777ea0ff81114fb5","title":"An Analysis on Large Language Models in Healthcare: A Case Study of BioBERT"},{"paperId":"b1ece182d4934dfc629ca409ef78e56c1762965c","title":"On the Impact of Cross-Domain Data on German Language Models"},{"paperId":"5bda9200b6466da0779116385a277be6dbc746f4","title":"Hierarchical Pretraining on Multimodal Electronic Health Records"},{"paperId":"c3382fd533b9dd7f8ed7ba7766159079bc1d3935","title":"BioT5: Enriching Cross-modal Integration in Biology with Chemical Knowledge and Natural Language Associations"},{"paperId":"23097f2d2deda9b92544fc2294d0c2f7d57cf12a","title":"Rethinking Model Selection and Decoding for Keyphrase Generation with Pre-trained Sequence-to-Sequence Models"},{"paperId":"72b06aef94f798ad9035b5775c186fd2fd6a8f38","title":"Multi-domain improves out-of-distribution and data-limited scenarios for medical image analysis"},{"paperId":"282c568302701bc163d454702eae10e43ca784a3","title":"The future landscape of large language models in medicine"},{"paperId":"b1edb49d90a1a652821084435ea1ec750aafcc54","title":"Domain-Specific Language Model Post-Training for Indonesian Financial NLP"},{"paperId":"732b1e2e57351342be4456b486bdfc39ed348df0","title":"Early Prediction of Sepsis Using Time Series Forecasting"},{"paperId":"da7bc9c47bc06ed2742142540cc94b918c1fe723","title":"Are Large Language Models Post Hoc Explainers?"},{"paperId":"99e6c90c0ed18030164a731250d547b5e5735055","title":"LLM for SoC Security: A Paradigm Shift"},{"paperId":"c7492913370b5726eaa6ced163a60de6c9d4bb7f","title":"A Survey of Large Language Models for Healthcare: from Data, Technology, and Applications to Accountability and Ethics"},{"paperId":"c957c50ead898bd76944a554f0bf02bdf59c67e5","title":"ShellGPT: Generative Pre-trained Transformer Model for Shell Language Understanding"},{"paperId":"a922ce01d2fadcad1bf7cd4bbf91cb8effadd3d4","title":"Recent advancement in targeted therapy and role of emerging technologies to treat cancer"},{"paperId":"e2a2ae2b97e838b905b1a928605e3a887ca7133a","title":"CnGeoPLM: Contextual knowledge selection and embedding with pretrained language representation model for the geoscience domain"},{"paperId":"7b5b7aefc727bdf1e73efdd65af539d79628230a","title":"A Comprehensive Evaluation of Large Language Models on Benchmark Biomedical Text Processing Tasks"},{"paperId":"02d7fe5ffc543b8d16e87fdd37bedcd00716117d","title":"Evaluation of a prototype machine learning tool to semi-automate data extraction for systematic literature reviews"},{"paperId":"020e370bf90c142b9ba5e83173986f1b9dc09628","title":"Automated Extraction and Classification of Drug Prescriptions in Electronic Health Records: Introducing the PRESNER Pipeline"},{"paperId":"b7f379f67f5e0777aa7a15d3d61950b519bdc589","title":"Deep Representations of First-person Pronouns for Prediction of Depression Symptom Severity"},{"paperId":"864c896c686d36ba84de69cbb646a56bfb1b7d44","title":"Know2BIO: A Comprehensive Dual-View Benchmark for Evolving Biomedical Knowledge Graphs"},{"paperId":"ad76cd056ce580d24cda52764fe06b926edfee41","title":"ChatGPT makes medicine easy to swallow: an exploratory case study on simplified radiology reports."},{"paperId":"0c924d41033eca9ea19e6418c8fcfcbf35a32a12","title":"A Multilabel Text Classifier of Cancer Literature at the Publication Level: Methods Study of Medical Text Classification"},{"paperId":"d4b44f9a783911c9222fe6975d0dc6ad99a4cb81","title":"Literature Based Discovery (LBD): Towards Hypothesis Generation and Knowledge Discovery in Biomedical Text Mining"},{"paperId":"d52e65ef8e909eb15c544a9e0ba0b551f768b578","title":"The Use of ICT as a Resource and Media for Modern 21st Century Learning in Primary Schools"},{"paperId":"fa874e7b66a5b936469872054986c8f340701146","title":"A Survey of GPT-3 Family Large Language Models Including ChatGPT and GPT-4"},{"paperId":"a8aef5a15dc2b486b3bf01205d2687a1140a41bb","title":"Conversational Health Agents: A Personalized LLM-Powered Agent Framework"},{"paperId":"2784e89fb178fdb13e47166a1e961eebd2b72a4a","title":"Evolution of research topics and paradigms in plant sciences"},{"paperId":"bd6d7122b871aa80dd22ff18985d435004851b5a","title":"Extraction of Medication and Temporal Relation from Clinical Text using Neural Language Models"},{"paperId":"9ee833ef54724cd292122e9901862af797cfe15a","title":"Large Language Models Trained on Equipment Maintenance Text"},{"paperId":"7c6b857a8c8c9e0874fd29e1a605a1edb6028d50","title":"Schema matching based on energy domain pre-trained language model"},{"paperId":"29a5c700b5844df30cd8c0ff01b59bd25b94c477","title":"GDPRxiv: Establishing the State of the Art in GDPR Enforcement"},{"paperId":"748194e591fa7689940d7d6a8766f1a4afa2dac0","title":"A literature-mining method of integrating text and table extraction for materials science publications"},{"paperId":"a0476578761e983d5ab2083abab07b81236c1d58","title":"Asymmetric cross-modal attention network with multimodal augmented mixup for medical visual question answering"},{"paperId":"bf176a1da55b30cc81ad309808c3d8c90b16456a","title":"Artificial Intelligence and Infectious Disease Imaging."},{"paperId":"ff0bbf1867f9f6b494ee79443c403d062fa23fd1","title":"Data augmentation via context similarity: An application to biomedical Named Entity Recognition"},{"paperId":"061ec64edce2632870bdf72d33ddc5c046193e0a","title":"SUSIE: Pharmaceutical CMC ontology-based information extraction for drug development using machine learning"},{"paperId":"a73f8fe4095cb0fe3408a15cd4f937094d02b1ab","title":"GERNERMED++: Semantic annotation in German medical NLP through transfer-learning, translation and word alignment"},{"paperId":"932be70db1e3d6fbe5fe4a305593f22a5f058049","title":"T 2 -NER: A Two-Stage Span-Based Framework for Unified Named Entity Recognition with Templates"},{"paperId":"e0aba21dc7f4cffdcda0a772153fdbc139681e5a","title":"Exploring named entity recognition and relation extraction for ontology and medical records integration"},{"paperId":"243e325812d4fc02dd380c5561b7a72954ba0834","title":"On the instability of further pre-training: Does a single sentence matter to BERT?"},{"paperId":"01a2fe143a05b9faeb481f86261dd1f0db3a57fa","title":"Multi-ontology embeddings approach on human-aligned multi-ontologies representation for gene-disease associations prediction"},{"paperId":"399cbcf0187197c8c371fcca1bd78cd3e529621c","title":"Named Entity Recognition in Electronic Health Records: A Methodological Review"},{"paperId":"7bc7f0929632b5b130d5626efd78ebdba28b2753","title":"Multi-Scale Bidirectional Recurrent Network with Hybrid Correlation for Point Cloud Based Scene Flow Estimation"},{"paperId":"1c0b7518e219d71ae682a838744c5c5bed8fce74","title":"A Small Claims Court for the NLP: Judging Legal Text Classification Strategies With Small Datasets"},{"paperId":"9d2a314b46ead6332ff2926f857df9537c839509","title":"Question-Answering Model for Schizophrenia Symptoms and Their Impact on Daily Life using Mental Health Forums Data"},{"paperId":"8c132843ba216536e8b0a1e13b212e6c1d429075","title":"How to Improve Student Understanding in Learning Science by Regulating Strategy in Language Education? Definition, Factors for Enhancing Students Comprehension, and Computational Bibliometric Review Analysis"},{"paperId":"0c75cda2bb0812217bf0e5460e910212ad512944","title":"An evaluation of GPT models for phenotype concept recognition"},{"paperId":"e94b1b868bf57f0243e42d4f51042bd1f1e621b3","title":"Medical Foundation Models are Susceptible to Targeted Misinformation Attacks"},{"paperId":"513cd3699a0518ba2119f36691d23ccd6de67c0f","title":"Knowledge Graphs for the Life Sciences: Recent Developments, Challenges and Opportunities"},{"paperId":"2380ea65a9e5d34df3ea2ee84a487e8263663e12","title":"Using Weak Supervision and Data Augmentation in Question Answering"},{"paperId":"3ba45f22238c9902557cae0dbb381cc2e562b1ad","title":"Deeply integrating unsupervised semantics and syntax into heterogeneous graphs for inductive text classification"},{"paperId":"c234381686e782987a556e44aed061aaedd8c2de","title":"MedEdit: Model Editing for Medical Question Answering with External Knowledge Bases"},{"paperId":"04ec6f22741b3d0d3e695486957d5e6cf94b3cfc","title":"Mapping Vaccine Names in Clinical Trials to Vaccine Ontology using Cascaded Fine-Tuned Domain-Specific Language Models"},{"paperId":"133b5d9959b7f1c2b094934eab36cdaa0e27f387","title":"CONORM: Context-Aware Entity Normalization for Adverse Drug Event Detection"},{"paperId":"504218f47f24a89aba246243c228fd3fa85b9643","title":"Global trends in PANoptosis research: bibliometrics and knowledge graph analysis"},{"paperId":"10eb81d069f3654fa234f769852173a9ddadfabd","title":"Comprehensive Overview of Named Entity Recognition: Models, Domain-Specific Applications and Challenges"},{"paperId":"c5fa173e6f52dbc2e5156e2e7c9e6d3c0f1c4aa0","title":"DRG-LLaMA : tuning LLaMA model to predict diagnosis-related group for hospitalized patients"},{"paperId":"784c4f49e4b70b8907f75fd9c5606f62913e40db","title":"To BERT or not to BERT: advancing non-invasive prediction of tumor biomarkers using transformer-based natural language processing (NLP)"},{"paperId":"1295a68542f23696f3fd02ffbf39a5fe107d6227","title":"Hybrid medical named entity recognition using document structure and surrounding context"},{"paperId":"e3b0944a445706addd7e12617308245034db0456","title":"Nested Event Extraction upon Pivot Element Recogniton"},{"paperId":"a54493bdca9b63c63468714e9b60fe89d56cd265","title":"Large Language Models and Control Mechanisms Improve Text Readability of Biomedical Abstracts"},{"paperId":"3e7c564b9da8ee4f2b1ed437f347aed59a68c529","title":"Improving VTE Identification through Adaptive NLP Model Selection and Clinical Expert Rule-based Classifier from Radiology Reports"},{"paperId":"f993edb9aca0239decf07bdbec0cef8ba4ca7c83","title":"Inspire the Large Language Model by External Knowledge on BioMedical Named Entity Recognition"},{"paperId":"20cb4e0bd8871d33d82fc72ea82a0aa1dd922810","title":"Foundation Metrics: Quantifying Effectiveness of Healthcare Conversations powered by Generative AI"},{"paperId":"61b48fb871bf184ab2e8137275e74639fdbd3b36","title":"A short review on recent developments in the computational techniques (2021) to mitigate SARS-CoV-19 disease"},{"paperId":"18a2295063d02204375488da5a926287cdcb77d0","title":"Ottoman Turkey the Sorrowful Period of Islam's Political Journey"},{"paperId":"991409eec5acfcad720073c91ae06441c438d14e","title":"Visual Question Answering in the Medical Domain"},{"paperId":"7aef8e81883e9ded80076b8d2002e29ec5555564","title":"BioBERTurk: Exploring Turkish Biomedical Language Model Development Strategies in Low-Resource Setting"},{"paperId":"e07e9a54f71c598e01f3d868bd02cfec7362eaa4","title":"Investigating Zero- and Few-shot Generalization in Fact Verification"},{"paperId":"6d180d3d54fdfb82149b03dd0cb28e808be957c2","title":"Prediction of hot spots towards drug discovery by protein sequence embedding with 1D convolutional neural network"},{"paperId":"df5a3ab06204449c314c2b7f45a41c2b08a82bfe","title":"Natural Language Processing to Classify Caregiver Strategies Supporting Participation Among Children and Youth with Craniofacial Microsomia and Other Childhood-Onset Disabilities"},{"paperId":"bed13c309108041de2d46321423146ac819ef610","title":"Adding Linguistic Information to Transformer Models Improves Biomedical Event Detection?"},{"paperId":"a94cd92bb3b0cca139198c4eedd494f9ece74c2a","title":"Reranking for a Polish Medical Search Engine"},{"paperId":"ea9735ac5ddf69823c44e360a7a68b5e5e71ad46","title":"A study of BERT-based methods for formal citation identification of scientific data"},{"paperId":"756a5a2387ae9c09704efc4757765d656576df07","title":"Analyzing research diversity of scholars based on multi-dimensional calculation of knowledge entities"},{"paperId":"c51fef8d716cbb7811fe0253e21ef8dac62cc78a","title":"HealthFC: A Dataset of Health Claims for Evidence-Based Medical Fact-Checking"},{"paperId":"e8c16838f342eb36d06a67ceece672f4879bd03a","title":"Discovering research data management trends from job advertisements using a text-mining approach"},{"paperId":"59fc922aac0f177a517d5656868c9c4334d863ef","title":"MAPLE: Mobile App Prediction Leveraging Large Language model Embeddings"},{"paperId":"9ff38398b5808fe3d26fbfa19f77eebbad177dc2","title":"Text Classification of Cancer Clinical Trial Eligibility Criteria"},{"paperId":"bcea96cdff75b703c366c8bb8f42e30d4f8a9f3b","title":"Benchmarking ChatGPT-4 on a radiation oncology in-training exam and Red Journal Gray Zone cases: potentials and challenges for ai-assisted medical education and decision making in radiation oncology"},{"paperId":"7ca7d39a9023dccab0925613cfe8c98927adcb92","title":"Sequence Labeling for Disambiguating Medical Abbreviations"},{"paperId":"ebcc994b32959bcdc2d091781178ca2872a9cbef","title":"Advanced Privacy Preserving Model for Smart Healthcare Using Deep Learning"},{"paperId":"22a52b4a74334bd17ae75ef9f0618e75c69e8986","title":"Hypert: hypernymy-aware BERT with Hearst pattern exploitation for hypernym discovery"},{"paperId":"0502ad3507b437af48afb3cd8bb4c2d1875bcbff","title":"Content Reduction, Surprisal and Information Density Estimation for Long Documents"},{"paperId":"5ca6e9a5029e4b2d5d718840c20d5853a8c5719e","title":"Leveraging Large Language Models and Weak Supervision for Social Media data annotation: an evaluation using COVID-19 self-reported vaccination tweets"},{"paperId":"41a9aab56f219dbfea39558e0ffc1d700914e902","title":"Discovering social determinants of health from case reports using natural language processing: algorithmic development and validation"},{"paperId":"3aed81250dcd924b601835aadff50cc6dfc0984e","title":"Collaborative and privacy-preserving workflows on a clinical data warehouse: an example developing natural language processing pipelines to detect medical conditions"},{"paperId":"211ee44814d9f5df4ffe86640cb2017a496d8641","title":"CrisisTransformers: Pre-trained language models and sentence encoders for crisis-related social media texts"},{"paperId":"6ecdea4b8c07a539092650be017a940138a9d8b2","title":"BugSigDB captures patterns of differential abundance across a broad range of host-associated microbial signatures."},{"paperId":"59018c9baafb9de3906e9fc413cba529179e46c6","title":"Evaluation of text summarization techniques in healthcare domain: Pharmaceutical drug feedback"},{"paperId":"aebc595bf89c201428c69d878b8fdc57784582fb","title":"IFM-RCNN: a hybrid text classifier with enhanced performance of binary drug classification from tweets using improved faster mask-recurrent convolutional neural network"},{"paperId":"1c3170932d55d61756916c5c33fedb51cdf3bb7e","title":"A large-scale evaluation of NLP-derived chemical-gene/protein relationships from the scientific literature: Implications for knowledge graph construction"},{"paperId":"1fb2bde5c2f3a3c4d7b810b29ec3f21f60e75d35","title":"Knowledge-tuning Large Language Models with Structured Medical Knowledge Bases for Reliable Response Generation in Chinese"},{"paperId":"46bdc5cf1e1472fb6f58687a93fca19370e823ff","title":"Extracting Mutant-Affected Protein-Protein Interactions via Gaussian-Enhanced Representation and Contrastive Learning"},{"paperId":"a570d3d8f1b66a8ab3fff7876dc9bba3bcdc789b","title":"Aligning Large Language Models for Clinical Tasks"},{"paperId":"82736c46c788744b8cf3428441e27b86bb8480f6","title":"Fine-Tuning Pre-Trained Language Model for Urgency Classification on Food Safety Feedback"},{"paperId":"35408ebad5f1acb9d6e9dd8591c62bf75bffd6b1","title":"Predictive Food Safety Risk Monitoring"},{"paperId":"2aa5b00af1431abc1fdedd3f0bdf136d7c055c92","title":"A multi-stage recognizer for nested named entity with weakly labeled data"},{"paperId":"5e761e9f5cd9672a181b256299cd2916a8079461","title":"Augmenting Black-box LLMs with Medical Textbooks for Clinical Question Answering"},{"paperId":"ccab431bf6a154c7c4048eed723a46aef93891fc","title":"DiscoverPath: A Knowledge Refinement and Retrieval System for Interdisciplinarity on Biomedical Research"},{"paperId":"1af7ae5c0a476fa9ee7b8ae0d76012a73c47c63f","title":"Incorporating Dictionaries into a Neural Network Architecture to Extract COVID-19 Medical Concepts From Social Media"},{"paperId":"2acd6f474567fec34cd2e46f7dd049f8ff2dc9dd","title":"Efficient Machine Reading Comprehension for Healthcare Applications: A Context Extraction Approach (Preprint)"},{"paperId":"d0929ad54e151b4bd7f0577a78bbba6c25292459","title":"Into the Single Cell Multiverse: an End-to-End Dataset for Procedural Knowledge Extraction in Biomedical Texts"},{"paperId":"8f968ce745d65f8929aaf809b4a7bc252a65598d","title":"Large-scale Dataset and Effective Model for Variant-Disease Associations Extraction"},{"paperId":"41135453dd1d09ec7d2942b649d5145ecb9b9113","title":"Exploring Pair-Aware Triangular Attention for Biomedical Relation Extraction"},{"paperId":"cbd7b827c541ed6a5d944c21c2bff2887c273413","title":"Knowledge Graph Embeddings for Multi-Lingual Structured Representations of Radiology Reports"},{"paperId":"1a66c290beef2d992fc55137f9e8fd5a67cf09ae","title":"Empirical evaluation of language modeling to ascertain cancer outcomes from clinical text reports"},{"paperId":"01ca6da9b3756c178e43358e31528a6aea472eba","title":"Improving Log-Based Anomaly Detection by Pre-Training Hierarchical Transformers"},{"paperId":"f9fc7e1d942900a5c589602c9cefefacdbafb05f","title":"Datasets Construction and Development of QSAR Models for Predicting Micronucleus In Vitro and In Vivo Assay Outcomes"},{"paperId":"bffbfd88a9c4465d56d2535683f5ca1cbff26588","title":"Disambiguation of medical abbreviations for knowledge organization"},{"paperId":"735035035a8b81cffb7b982d88d5ff753517fbce","title":"SPS-LCNN: A Significant Point Sampling-based Lightweight Convolutional Neural Network for point cloud processing"},{"paperId":"9812785093330fb6882e243e1c5051a3be1f9ee6","title":"Weak-PMLC: A large-scale framework for multi-label policy classification based on extremely weak supervision"},{"paperId":"3e5e5c2098de36a144728e766c58eed3e77a1eff","title":"A survey on Relation Extraction"},{"paperId":"2bc4c5d853225ba1194c45d2c84bac822f81b6d0","title":"Automatic classification of experimental models in biomedical literature to support searching for alternative methods to animal experiments"},{"paperId":"937a6c992dff0b02dd0d0e30ebf2ef49d6989d4f","title":"TGRA-P: Task-driven model predicts 90-day mortality from ICU clinical notes on mechanical ventilation"},{"paperId":"5878a3a90c9c31f68defabaf9cff49acbe7b797d","title":"Biomaterials Text Mining: A Hands-On Comparative Study of Methods on Polydioxanone Biocompatibility."},{"paperId":"28f462b3241751446edc8915868c446476103612","title":"Reconstruction of temperature field in nanofluid-filled annular receiver with fins using deep hybrid transformer-convolutional neural network"},{"paperId":"d07ce3d87e98ce804c0aebf430f402bc8c67806d","title":"Generative Artificial Intelligence Through ChatGPT and Other Large Language Models in Ophthalmology"},{"paperId":"37008d6aadecd12aa52f330fe00bfe0c7e655d2c","title":"An interpretable deep learning framework for predicting liver metastases in postoperative colorectal cancer patients using natural language processing and clinical data integration"},{"paperId":"386c6e609bb700587fb8d675f0f53a3b3ebd31e4","title":"Towards precise PICO extraction from abstracts of randomized controlled trials using a section-specific learning approach"},{"paperId":"d859cf170e60ffa7f109e4f9e3a88fd2519f5b73","title":"A self-supervised language model selection strategy for biomedical question answering"},{"paperId":"b8bb40305a6432af86048e93929aeb28c187f306","title":"Literature-based predictions of Mendelian disease therapies."},{"paperId":"5289937ca9bbd4dfefa5e90e5b5857f404a09ffc","title":"DEED: DEep Evidential Doctor"},{"paperId":"e20766f84713eef324cfcd6e3ddf9e6c47915836","title":"Tell me your position: Distantly supervised biomedical entity relation extraction using entity position marker"},{"paperId":"2ccdfdbf3bb408a1c74dde2dbb2cf8492b3591db","title":"Large language models in medicine: the potentials and pitfalls"},{"paperId":"55c5b37a2089781a264553ec7d1b96fe89675132","title":"Can ChatGPT explain it? Use of artificial intelligence in multiple sclerosis communication"},{"paperId":"84fe94bad3f2adff4b021e11641ba44cffb835d8","title":"Automatic retrieval of health case reports for public needs using deep learning techniques"},{"paperId":"437b0c6218bd06a0730e1b29829fa299e72b4a0b","title":"Towards More Generalizable and Accurate Sentence Classification in Medical Abstracts with Less Data"},{"paperId":"50da73ae5d7f2dae59a3fc5b2a00d160b8bd437d","title":"Multimodal Foundation Models For Echocardiogram Interpretation"},{"paperId":"236a88ec400bcbc91c235cd56638e13e31c03fbb","title":"A decision support system in precision medicine: contrastive multimodal learning for patient stratification"},{"paperId":"125081b729bee38708e218cba101f21bb5288c8a","title":"Exploring the Transfer Learning Capabilities of CLIP in Domain Generalization for Diabetic Retinopathy"},{"paperId":"f97ec1d8b221bf548d314a71257197f642e1eedb","title":"T4SEpp: a pipeline integrated with protein language models effectively predicting bacterial type IV secreted effectors"},{"paperId":"f190aeb5ff68a51d963680403c3d160a72e47e63","title":"Multimodal Data Hybrid Fusion and Natural Language Processing for Clinical Prediction Models"},{"paperId":"23efe9b99b5f0e79d7dbd4e3bfcf1c2d8b23c1ff","title":"Marie and BERT—A Knowledge Graph Embedding Based Question Answering System for Chemistry"},{"paperId":"13f8afd11e7267f3c59ee0f9b1b58bfe50d5a85f","title":"Improving Image Classification of Knee Radiographs: An Automated Image Labeling Approach"},{"paperId":"b6d64d2f3464fea548731693ed194edfcf58af77","title":"Knowledge-injected Prompt Learning for Chinese Biomedical Entity Normalization"},{"paperId":"b9e0fdb5af49a2219ec8743d5daab7df9946ce1b","title":"BELB: a biomedical entity linking benchmark"},{"paperId":"35d2e90278212678dc0ee66b08f4bce01bb221ed","title":"An extensible point-based method for data chart value detection"},{"paperId":"8beaf2a71e44e0c9e6be254f50decc574733c4a2","title":"Joint representation learning for retrieval and annotation of genomic interval sets"},{"paperId":"1c486c97e49c53dd0350131da6d962a4ab43845d","title":"Identifying COVID-19 cases and extracting patient reported symptoms from Reddit using natural language processing"},{"paperId":"591d15dadf10e60b3cb94586879663ac6d74612d","title":"Leveraging Language Models for Inpatient Diagnosis Coding"},{"paperId":"116eef204491b82b4bfa23345f1645935faedbad","title":"Software Entity Recognition with Noise-Robust Learning"},{"paperId":"2cfda2581748c2a47b3460c7255685b8c07740bf","title":"Unlocking Hardware Security Assurance: The Potential of LLMs"},{"paperId":"e2d9af5557b69a84fdfb3fa845f4fec75f5f2c4b","title":"Biomedical Named Entity Recognition from Malaria Literature using BioBERT"},{"paperId":"73c948bf410db6f4482214a4e39abb18745236c5","title":"OralMedNER: A Named Entity Recognition System for Oral Medicine and Radiology"},{"paperId":"a6cbafb46fb6a58321c440829c359675a2c892ae","title":"Named Entity Recognition in Power Marketing Domain Based on Whole Word Masking and Dual Feature Extraction"},{"paperId":"308db6f428a20fbef0a72f9ee73798d60fe9ff50","title":"BIOptimus: Pre-training an Optimal Biomedical Language Model with Curriculum Learning for Named Entity Recognition"},{"paperId":"479018820d4ddeab58297224f0462497668f57bd","title":"Multi-label topic classification model of COVID-19 literature"},{"paperId":"98db49e5fbda7198f7946da1d6fda2829bdfa603","title":"Leveraging Explainable AI to Analyze Researchers' Aspect-Based Sentiment about ChatGPT"},{"paperId":"0db5edaaacb1f3b240b2eea596297419ca9cb88e","title":"Finding Stakeholder-Material Information from 10-K Reports using Fine-Tuned BERT and LSTM Models"},{"paperId":"3958152c43888a0a9461de2b952df4a39491442b","title":"Informed Named Entity Recognition Decoding for Generative Language Models"},{"paperId":"a232471542bf3f6c7a5a23c8d96acf065b830ce7","title":"Advanced intelligent health advice with informative summaries to facilitate treatment decision-making"},{"paperId":"6ddef11311fc716b3d87e4d6814202b3df985cb5","title":"Artificial intelligence can dynamically adjust strategies for auxiliary diagnosing respiratory diseases and analyzing potential pathological relationships"},{"paperId":"ccf993557d41fed8b92a17bbd84a7a8a3247706b","title":"KBMQA: medical question and answering model based on Knowledge Graph and BERT"},{"paperId":"64e802ea8e9dbe247c31fb06184c04dbf9e55e4e","title":"EcomGPT: Instruction-tuning Large Language Models with Chain-of-Task Tasks for E-commerce"},{"paperId":"745e015bbe917a7a3dbd5f50c2198a027d67475d","title":"MC-DRE: Multi-Aspect Cross Integration for Drug Event/Entity Extraction"},{"paperId":"3eff0e1187dbd60f12dd06c5f3291b1eb6858c1a","title":"Bio-SIEVE: Exploring Instruction Tuning Large Language Models for Systematic Review Automation"},{"paperId":"70980c12cdb463caedd0441f061bd9103e5a039f","title":"Demonstration-based learning for few-shot biomedical named entity recognition under machine reading comprehension"},{"paperId":"bd1b4a8181b75574f7afa11a1f3404232e1ebee8","title":"Natural language processing to predict isocitrate dehydrogenase genotype in diffuse glioma using MR radiology reports"},{"paperId":"cee0fabddbb9d84ff2f5aa139bc57055082525c1","title":"Enhancing Phenotype Recognition in Clinical Notes Using Large Language Models: PhenoBCBERT and PhenoGPT"},{"paperId":"4fc9c2ff8aef5d75b3e3ab7b7897c71fd2fe9332","title":"DEBBIE: The Open Access Database of Experimental Scaffolds and Biomaterials Built Using an Automated Text Mining Pipeline"},{"paperId":"e965883e850d4a9599f541bf0cdedb9785973d0d","title":"RadGraph2: Modeling Disease Progression in Radiology Reports via Hierarchical Information Extraction"},{"paperId":"72cd71582f212002ad73d0bdfd6319114d4cb0e4","title":"MediBERT: A Medical Chatbot Built Using KeyBERT, BioBERT and GPT-2"},{"paperId":"cd9a1e785f3f2ec85256decec0986a2e24fd4db1","title":"A framework for multi-faceted content analysis of social media chatter regarding non-medical use of prescription medications"},{"paperId":"2e783a3fe650c8eeff81cc867fa9921729422c1d","title":"PaniniQA: Enhancing Patient Education Through Interactive Question Answering"},{"paperId":"ce3a4278ec0459f5186b2d8c5b5fd8d86b5847e9","title":"Decision Knowledge Graphs: Construction of and Usage in Question Answering for Clinical Practice Guidelines"},{"paperId":"53947b7f42b7bf6c59d2f72bab2829342154d2ac","title":"A Survey of Spanish Clinical Language Models"},{"paperId":"28cf32dfe7f5f4532d02ed0b864e9ecbc9c09111","title":"Improving Biomedical Claim Detection using Prompt Learning Approaches"},{"paperId":"a3a43f60f004b698306a0ef398ae8a34bb9c1748","title":"BioBERT Based SNP-traits Associations Extraction from Biomedical Literature"},{"paperId":"2090f208a05344fd15a995fcbc9d539508841347","title":"Local Large Language Models for Complex Structured Medical Tasks"},{"paperId":"81a2d4ee13955a4fdbfc178d6df64cf9e786ff52","title":"Toward a stable and low-resource PLM-based medical diagnostic system via prompt tuning and MoE structure"},{"paperId":"b7ced95a2f6984bd0bc4ae90420a59f55151c834","title":"Searching for explanations of black-box classifiers in the space of semantic queries"},{"paperId":"2feaff53b1aa498d09dc5dba2f08ab6a60385e9c","title":"Can ChatGPT provide intelligent diagnoses? A comparative study between predictive models and ChatGPT to define a new medical diagnostic bot"},{"paperId":"210ecef8b8add99b90191db37206c6811f6475fd","title":"Few-shot biomedical named entity recognition via knowledge-guided instance generation and prompt contrastive learning"},{"paperId":"2147a8b64f406945d0d670a6dbab99b536d6e45f","title":"Document-level relation extraction with multi-layer heterogeneous graph attention network"},{"paperId":"68469d83ebd14069e8173e5bf830ed925d13bea2","title":"Control, coordination, and adaptation functions in construction contracts: A machine-coding model"},{"paperId":"08063c57e0ff13dfff0f895ca67a37b52953e0e6","title":"Extracting and structuring information from the electronic medical text: state of the art and trendy directions"},{"paperId":"d2bd31f8034c86da3598d94a05e878dcd2374f6b","title":"Recycle-BERT: Extracting Knowledge about Plastic Waste Recycling by Natural Language Processing"},{"paperId":"5ca39d6d6ae510609f9608e8d49b421bd522326d","title":"Supplementing domain knowledge to BERT with semi-structured information of documents"},{"paperId":"368fe791d80d954400fa52cd630f745adfe02e6b","title":"Systematic review of natural language processing for recurrent cancer detection from electronic medical records"},{"paperId":"648a2ef5c1e3be331c77e86e6b99b70c2909153b","title":"Associating biological context with protein-protein interactions through text mining at PubMed scale"},{"paperId":"639207980804bae07bc6651da27acbe7bd5c7015","title":"ChatGPT and Artificial Intelligence in Medical Writing: Concerns and Ethical Considerations"},{"paperId":"8a71f4c8f6c3a1780a4aa8f6f1f2f7f5ca05e109","title":"MOOC-BERT: Automatically Identifying Learner Cognitive Presence From MOOC Discussion Data"},{"paperId":"3c71960195b00baa4b77b9ed464155736e25be77","title":"ODEE: A One-Stage Object Detection Framework for Overlapping and Nested Event Extraction"},{"paperId":"c2200f2a69ae1e7665adfd74ec617443558b51c5","title":"FedBFPT: An Efficient Federated Learning Framework for Bert Further Pre-training"},{"paperId":"f6a2087b7b6dbf051d53e1b330b497026124e36d","title":"Using transfer learning-based causality extraction to mine latent factors for Sjögren's syndrome from biomedical literature"},{"paperId":"8aceb1e4a5ebac9dd42a83456909a9a67dd57b5e","title":"An efficient circRNA-miRNA interaction prediction model by combining biological text mining and wavelet diffusion-based sparse network structure embedding"},{"paperId":"c95290d74d01cb29e8eee3ac238739513b9551fb","title":"Year 2022 in Medical Natural Language Processing: Availability of Language Models as a Step in the Democratization of NLP in the Biomedical Area"},{"paperId":"6bd919086940f84a60046f4bfe45c195eaa8168e","title":"AsdKB: A Chinese Knowledge Base for the Early Screening and Diagnosis of Autism Spectrum Disorder"},{"paperId":"05fac7020ef3782ea91191fb6b77d6db00d8840f","title":"Prompt Learning with Structured Semantic Knowledge Makes Pre-Trained Language Models Better"},{"paperId":"21737406f448844220dde7c711565bd44b0c27f0","title":"ChatHome: Development and Evaluation of a Domain-Specific Language Model for Home Renovation"},{"paperId":"6eb3dd2b64db74f9743802acbb9875bdffb9d246","title":"Text-guided Foundation Model Adaptation for Pathological Image Classification"},{"paperId":"90bba6971cb04ee035e869e77fb471016868ab35","title":"Matching Patients to Clinical Trials with Large Language Models"},{"paperId":"c9dbdae8146b9f97e254f5d26fd6efde96eaa703","title":"Med-Flamingo: a Multimodal Medical Few-shot Learner"},{"paperId":"7a97e7bd7c7be5331bfe66f673a5f086d1954750","title":"Mining drug-target interactions from biomedical literature using chemical and gene descriptions based ensemble transformer model"},{"paperId":"152d9bb7be8721578e2d6c4b3ebe9641924295eb","title":"An Improved Model for Medical Forum Question Classification Based on CNN and BiLSTM"},{"paperId":"3b59dc8c98371458d4a5ab6a04e8381858552edb","title":"Biomedical relation extraction with knowledge base–refined weak supervision"},{"paperId":"de7e5fee8cf03bd485b1104d3e40e8ab45d76c0a","title":"Prot2Text: Multimodal Protein's Function Generation with GNNs and Transformers"},{"paperId":"bacd25f6d593fc65edf8287d79bfb1318f5df913","title":"Knowledge-Grounded Dialogue Generation for Medical Conversations: A Survey"},{"paperId":"e87ebf5f69f349b861af0aa0b967f6bd4c039188","title":"DeepGATGO: A Hierarchical Pretraining-Based Graph-Attention Model for Automatic Protein Function Prediction"},{"paperId":"85f8d6fe82b9e98bc5e40c9d640856d7fc59729c","title":"An attention‐based deep learning model for credibility assessment of online health information"},{"paperId":"33112b58e3eb4a6506fa537d892dc6742c5e794d","title":"Large language models in health care: Development, applications, and challenges"},{"paperId":"089f6328085066263fedc083952624ca121ebbf3","title":"CohortGPT: An Enhanced GPT for Participant Recruitment in Clinical Study"},{"paperId":"4623a828f776c029dfa26b77c75cc21379461259","title":"GIST: Generating Image-Specific Text for Fine-grained Object Classification"},{"paperId":"3c98209db032328c5eed95e923783cff09314156","title":"An Evidential Classifier with Multiple Pre-trained Language Models for Nested Named Entity Recognition"},{"paperId":"3d915391a46593524eb47189b47d52334f13a9a4","title":"UMLS-KGI-BERT: Data-Centric Knowledge Integration in Transformers for Biomedical Entity Recognition"},{"paperId":"9f4017de7deded49c032a83d7844efcd9ea1aa21","title":"Embroid: Unsupervised Prediction Smoothing Can Improve Few-Shot Classification"},{"paperId":"6def4b34c904ccd59589229584489d291252fe01","title":"An In-Depth Evaluation of Federated Learning on Biomedical Natural Language Processing"},{"paperId":"d0edff5402edeab721b7681643e1ff7c2354de4a","title":"Leveraging pre-trained language models for mining microbiome-disease relationships"},{"paperId":"233015efe02ceaf3fe97d5893cdf210b05b36861","title":"Improving the Reusability of Pre-trained Language Models in Real-world Applications"},{"paperId":"42555e01ba20ea0ef970ff7d5d6266992e50e233","title":"Investigating drug translational research using PubMed articles"},{"paperId":"d210cb3f64ab2f5fa106fac60f2f5a94387a3b29","title":"RegEMR: a natural language processing system to automatically identify premature ovarian decline from Chinese electronic medical records"},{"paperId":"0379cfb16c1678bde9b889bb1c0ca39db2cb564a","title":"Few-shot Named Entity Recognition: Definition, Taxonomy and Research Directions"},{"paperId":"91c5e43f6ed8d13057eaead21c4f6dd0ed8a1112","title":"Multimodal Machine Learning for Extraction of Theorems and Proofs in the Scientific Literature"},{"paperId":"f11d58e16c3eee7d85ccf5ae99b774894341aaa0","title":"SciMine: An Efficient Systematic Prioritization Model Based on Richer Semantic Information"},{"paperId":"8c881afce8ba9ff0eb430d7d6018965453b60435","title":"BioSift: A Dataset for Filtering Biomedical Abstracts for Drug Repurposing and Clinical Meta-Analysis"},{"paperId":"5b97c7ba8dc762d917af5a486ca76e352c8f2a33","title":"Assessment of the E3C corpus for the recognition of disorders in clinical texts"},{"paperId":"94ce1d5924e05e8d75e43ce70044293ddcef850a","title":"Large language models in medicine"},{"paperId":"5d7bef7691811a080a38b7852f2bed942a42797e","title":"Improved prediction of drug-induced liver injury literature using natural language processing and machine learning methods"},{"paperId":"04626331422e2d0480cbf88ec9ad087e47b9ee9c","title":"Generalizable and explainable prediction of potential miRNA-disease associations based on heterogeneous graph learning"},{"paperId":"4b84cb46a704cc3a978758d8bf09fff25ed71a5a","title":"Inferring cancer disease response from radiology reports using large language models with data augmentation and prompting"},{"paperId":"7502ce27b86263dafd791ff9d4934fafce7bd02d","title":"Do not Mask Randomly: Effective Domain-adaptive Pre-training by Masking In-domain Keywords"},{"paperId":"29954a8c7e43b9d96bf968298c8dbd37aedbb887","title":"Leveraging artificial intelligence in the fight against infectious diseases"},{"paperId":"7e2e093009d63e3cd1f3152fa11d8a75bf937c31","title":"Generalizability of machine learning methods in detecting adverse drug events from clinical narratives in electronic medical records"},{"paperId":"2003dcf311ccab2426818113964e59ca0cfde2c9","title":"FDAPT: Federated Domain-adaptive Pre-training for Language Models"},{"paperId":"313e928295fcf9a33186155c53a9acda98e4f945","title":"Artificial Intelligence in Orthopaedic Surgery: Can a Large Language Model \"Write\" a Believable Orthopaedic Journal Article?"},{"paperId":"8a1c61b9364f0ae19331b1ea753ff892c635c59b","title":"Enhancing Biomedical Text Summarization and Question-Answering: On the Utility of Domain-Specific Pre-Training"},{"paperId":"8dd053ea38e09cd8d311b6b40fbc3d99420e6c5e","title":"Sentiment Analysis with the Use of Transformers and BERT"},{"paperId":"9bf8cb128b5e5a13122152bb34617a393520d02c","title":"Advancements in Scientific Controllable Text Generation Methods"},{"paperId":"6651eb8205e3d90c420fbdf8a2740c74e590e545","title":"Parameter-Efficient Fine-Tuning of LLaMA for the Clinical Domain"},{"paperId":"e0e9ba0c01d441e1fdcb8628d3f743d387b0b017","title":"UIT-Saviors at MEDVQA-GI 2023: Improving Multimodal Learning with Image Enhancement for Gastrointestinal Visual Question Answering"},{"paperId":"a2d765c0563aaf802cf3b70bb69fe6361bb9e316","title":"DeepOnto: A Python Package for Ontology Engineering with Deep Learning"},{"paperId":"0734964fbeaab1da5b992fd5e88a6fe1b8c6e97a","title":"Enhancing multiple-choice question answering through sequential fine-tuning and Curriculum Learning strategies"},{"paperId":"ed26016878ec049ae8b3179cc7a5fdd896a69d08","title":"ChatGPT, a powerful language model and its potential uses in bioinformatics"},{"paperId":"5651bcb92660a2a376b36a08647549b0305c7c58","title":"ODD: A Benchmark Dataset for the NLP-based Opioid Related Aberrant Behavior Detection"},{"paperId":"1e3ef48abeef882e12f9553a1baf8944f3782c88","title":"Several categories of Large Language Models (LLMs): A Short Survey"},{"paperId":"1b8cdff9e5eefc614cd8940c386b26ce215d32b6","title":"DeBEIR: A Python Package for Dense Bi-Encoder Information Retrieval"},{"paperId":"9ecf184dd657640cbd1c4cc0f3c801ebd9d53162","title":"ReactIE: Enhancing Chemical Reaction Extraction with Weak Supervision"},{"paperId":"fd42031baa3fe8690a00767c2fdf52dbcf945713","title":"Exploring the In-context Learning Ability of Large Language Model for Biomedical Concept Linking"},{"paperId":"bffe2951b1ec9ebde3677a7aadc8b91212a9fdfc","title":"BioCPT: Contrastive Pre-trained Transformers with Large-scale PubMed Search Logs for Zero-shot Biomedical Information Retrieval"},{"paperId":"5e791fa8bd5ebe8b31957345112cb03ceec23b9e","title":"Automated Recognition of Visual Acuity Measurements in Ophthalmology Clinical Notes Using Deep Learning"},{"paperId":"316a845b449b64bed6f2aeab94045e5d77c25a8b","title":"Biomedical extractive question answering based on dynamic routing and answer voting"},{"paperId":"ec23e0a536e0c2d09cfb115e11842fc4575043a2","title":"A social media event detection framework based on transformers and swarm optimization for public notification of crises and emergency management"},{"paperId":"3ec26a84129f1dfd758563d116a4c2b8976732e2","title":"Counterfactual can be strong in medical question and answering"},{"paperId":"e9388e699a7ef4d12ae425e341ff610c67cbf64b","title":"How far is Language Model from 100% Few-shot Named Entity Recognition in Medical Domain"},{"paperId":"d935f18e9942bc6ecaefad2c739a5c4a83f3b121","title":"SenRev: Measurement of Personal Information Disclosure in Online Health Communities"},{"paperId":"cb3fc8d828f5c7628e5bc5f7427dc567a4043788","title":"ShortMail: An email summarizer system"},{"paperId":"7d9fbd1ce90c2e6b64ba054c9f78cc5c45756cac","title":"Self-prediction of relations in GO facilitates its quality auditing"},{"paperId":"d798b65bb57f54c62e6f54b074c580fb7bc360d3","title":"RDKG-115: Assisting drug repurposing and discovery for rare diseases by trimodal knowledge graph embedding"},{"paperId":"bc67c4134b772e0af693d33cd27d7fd033d61253","title":"SPBERE: Boosting span-based pipeline biomedical entity and relation extraction via entity information."},{"paperId":"77bb65ee2e7d5cb0515c6d77a565c15e06b10f0c","title":"MEGACare: Knowledge-guided multi-view hypergraph predictive framework for healthcare"},{"paperId":"d71b646b9d159996866b9436a75a85c31a7ffce0","title":"Biomedical document relation extraction with prompt learning and KNN"},{"paperId":"f6974a11315fa698dc584e9f09316d61f414e952","title":"Learning Representations on Logs for AIOps"},{"paperId":"2fa9f26c26ae7872cd9a6e5833e563b80b25d5fe","title":"Transfer Learning Improves Unsupervised Assignment of ICD codes with Clinical Notes"},{"paperId":"94a443efe70955abf9831e0afd6cc44b18e9af39","title":"Information Flow in Graph Neural Networks: A Clinical Triage Use Case"},{"paperId":"be6cf83660d71b6ee6dd7c231fc57dd6fffb774d","title":"MIKA: Manager for Intelligent Knowledge Access Toolkit for Engineering Knowledge Discovery and Information Retrieval"},{"paperId":"92eab493a403adb72cca77ef645b9173d5cc0523","title":"Hybrid approach combining deep learning and a rule based expert system for concept extraction from prescriptions"},{"paperId":"b39c7419f505ba9f2c801dd3ea46a83d5b77c541","title":"ChatGPT for phenotypes extraction: one model to rule them all?"},{"paperId":"b44865672b3896e249b81a39cbe850286f8140c0","title":"Transformers in Healthcare: A Survey"},{"paperId":"30f36f68265823c7f9945f902451fe0b1fac790b","title":"Biomedical Language Models are Robust to Sub-optimal Tokenization"},{"paperId":"ebb3d299213bae89b5d302cc3dfc36573ec83956","title":"SummQA at MEDIQA-Chat 2023: In-Context Learning with GPT-4 for Medical Summarization"},{"paperId":"ee29656917619452360fe5dd848251a5081944bf","title":"Machine learning for potion development at Hogwarts"},{"paperId":"439c2a5c4883b421ca316617b1306583cc1d706c","title":"Automated Extraction and Visualization of Metabolic Networks from Biomedical Literature Using a Large Language Model"},{"paperId":"4f20a597685bb1554c318d0b75cf71f608708166","title":"Knowledge and skills extraction from the job requirements texts"},{"paperId":"06c235c27db3e6f43413ccf99490f9b22cc83aaf","title":"Is ChatGPT a Biomedical Expert?"},{"paperId":"ce6c1514a75619996e9fd686e53ff1f480b87d51","title":"Biomedical Entity Recognition by Detection and Matching"},{"paperId":"668eb1a3c4878363b56037af0beb262d9e08e09d","title":"CamemBERT-bio: a Tasty French Language Model Better for your Health"},{"paperId":"d8b8d0109f9b02442b366b10517e99f15e08a277","title":"Nested Named Entity Recognition as Building Local Hypergraphs"},{"paperId":"a1306e40b8d0c89150967c6b63b4bf4cbf67c8a3","title":"EASAL: Entity-Aware Subsequence-Based Active Learning for Named Entity Recognition"},{"paperId":"593434904a5fe302aa876d7736a737c86e795724","title":"A Simple Yet Effective Subsequence-Enhanced Approach for Cross-Domain NER"},{"paperId":"2a8fe3f0c6d39efcd5e0c1a8d186fb736b0ab414","title":"Extracting Periodontitis Diagnosis in Clinical Notes with RoBERTa and Regular Expression"},{"paperId":"fe3aed731150fc0a2ad2019050de88e0311381dd","title":"Identifying Major Depressive Disorder From Clinical Notes Using Neural Language Models with Distant Supervision"},{"paperId":"a28b42541034d0e18d925f03b1958a247787c322","title":"Annotate French Clinical Data Using Large Language Model Predictions"},{"paperId":"bf758359f5f9bbf3201ec1cf2444fd231f56d01a","title":"Classification of Patient Portal Messages with BERT-based Language Models"},{"paperId":"920953238e203dbfc507ec6a1d103ff49ddabf09","title":"Multimodal Deep Learning Methods on Image and Textual Data to Predict Radiotherapy Structure Names"},{"paperId":"0c4a1d3bb96df512037c6f0ae7630aa696e09180","title":"Vocabulary Matters: An Annotation Pipeline and Two Deep Learning Algorithms for Enzyme Named Entity Recognition"},{"paperId":"ba95eca1bc4b99c1b95246d09efd8bf46de2a6e9","title":"Stress Testing BERT Anaphora Resolution Models for Reaction Extraction in Chemical Patents"},{"paperId":"234f618b5ac7ecae8228009280e95c2f74a113dd","title":"A Literature Mining Method Based on Curve Graph Information in Literature"},{"paperId":"554b6817197821d006fd60aa8515712f30c71e75","title":"Supervised Text Classification System Detects Fontan Patients in Electronic Records With Higher Accuracy Than ICD Codes"},{"paperId":"aed522d7ea6a05349d0f6e5365b9284d463b8a52","title":"Explainable online health information truthfulness in Consumer Health Search"},{"paperId":"e96d3f85aa56f027e028189346e043e346f3acea","title":"LMFlow: An Extensible Toolkit for Finetuning and Inference of Large Foundation Models"},{"paperId":"1d633934bb4164c48f1c29bf2632492fe31b629b","title":"Bidirectional End-to-End Learning of Retriever-Reader Paradigm for Entity Linking"},{"paperId":"c7288139fa83a54c6bbc3680535256371678ff1e","title":"Exploring New Frontiers in Agricultural NLP: Investigating the Potential of Large Language Models for Food Applications"},{"paperId":"4ca6b4cd66c418478461f6e741b5195dd9aae763","title":"Provision and Characterization of a Corpus for Pharmaceutical, Biomedical Named Entity Recognition for Pharmacovigilance: Evaluation of Language Registers and Training Data Sufficiency"},{"paperId":"e623fc12e03d599de94d6384601723416bedd41c","title":"Enhancing Documents with Multidimensional Relevance Statements in Cross-encoder Re-ranking"},{"paperId":"a6418c9f0135a4d1489aa4cf9e67564c57d14706","title":"BioREx: Improving Biomedical Relation Extraction by Leveraging Heterogeneous Datasets"},{"paperId":"e36b3a9b6071a237ab110219d61107f14c7275e2","title":"No Labels?: No Problem! Experiments with active learning strategies for multi-class classification in imbalanced low-resource settings"},{"paperId":"3d50bc44a45a5d9ee98104e45c71c4f072a1d2cd","title":"Legal Holding Extraction from Italian Case Documents using Italian-LEGAL-BERT Text Summarization"},{"paperId":"b0984b2572cfe393d842114e559da1496fc8ce6f","title":"Uncovering Trauma in Genocide Tribunals: An NLP Approach Using the Genocide Transcript Corpus"},{"paperId":"13b4243fa5658b363c75d8faffaab8021bcb1c18","title":"EMSAssist: An End-to-End Mobile Voice Assistant at the Edge for Emergency Medical Services"},{"paperId":"26def038af58e714b23b1fb875c53f09ce388692","title":"A Joint Entity and Relation Extraction Model based on Efficient Sampling and Explicit Interaction"},{"paperId":"8453a04c5a7b41675fd4c6748ce9bc3ae16ecdb9","title":"Automatic Extraction of Comprehensive Drug Safety Information from Adverse Drug Event Narratives in the Korea Adverse Event Reporting System Using Natural Language Processing Techniques"},{"paperId":"67c6295d6af7fe79b5f7e0309e85cfc76495f123","title":"SCALE: Scaling up the Complexity for Advanced Language Model Evaluation"},{"paperId":"aff0fe00ed7892d1185e8c5b66d318d3892abe6e","title":"Opportunities and Challenges for ChatGPT and Large Language Models in Biomedicine and Health"},{"paperId":"d2c4d1b7643e16ccfc0f6975ec84d0ca86a8d791","title":"SoyDNGP: A Web-Accessible Deep Learning Framework for Genomic Prediction in Soybean Breeding"},{"paperId":"ace34d08c8600ddd364d09347295f4529d662a30","title":"Acute stroke CDS: automatic retrieval of thrombolysis contraindications from unstructured clinical letters"},{"paperId":"be26d2b940eac33a9989d1a6bf2316934c6e1837","title":"Building a Corpus for Biomedical Relation Extraction of Species Mentions"},{"paperId":"405c420a5c5753d0d7168e67809b67096c862a0a","title":"Large Language Models and Medical Education: Preparing for a Rapid Transformation in How Trainees Will Learn to Be Doctors"},{"paperId":"e92d9b002849d9f9350b2337194836ec64cef3af","title":"Learning to Summarize Chinese Radiology Findings With a Pre-Trained Encoder"},{"paperId":"9f3778d51275a912a46bd05ace20b101c3f8f0d7","title":"A detailed library perspective on nearly unsupervised information extraction workflows in digital libraries"},{"paperId":"86804135859c64b37b0dac7e41fe4b9a0e8b6023","title":"Contextualized medication event extraction with striding NER and multi-turn QA"},{"paperId":"1bfe35520140dbd1a21508ed3ff814e7ae218464","title":"Weakly supervised information extraction from inscrutable handwritten document images"},{"paperId":"b17e827549b75376a2e56e4324b28e96bb13e576","title":"EriBERTa: A Bilingual Pre-Trained Language Model for Clinical Natural Language Processing"},{"paperId":"6294f078e79828cac21e717813e8f3d02b18a97c","title":"The importance of resource awareness in artificial intelligence for healthcare"},{"paperId":"09734bb2b7da3f7dfc0eb1c093a949e855794d6a","title":"MedKPL: A heterogeneous knowledge enhanced prompt learning framework for transferable diagnosis"},{"paperId":"116c19f5cdf2bf7884fd25ff2a7683ede6eaaa8a","title":"QUERT: Continual Pre-training of Language Model for Query Understanding in Travel Domain Search"},{"paperId":"a866be3528023e70ba4a6e3abf4c1151d3a7eff5","title":"ECGBERT: Understanding Hidden Language of ECGs with Self-Supervised Representation Learning"},{"paperId":"b239648a07d44d71185e48ab79bf5a2bc347e52f","title":"Medical Data Augmentation via ChatGPT: A Case Study on Medication Identification and Medication Event Classification"},{"paperId":"1da7013f004bc2107d58d094cb9e868152b255c9","title":"Using BERT models for breast cancer diagnosis from Turkish radiology reports"},{"paperId":"30987ef379e52b8bf3d942a2458d6774b8580853","title":"$FastDoc$: Domain-Specific Fast Pre-training Technique using Document-Level Metadata and Taxonomy"},{"paperId":"542308ad5c1c0b5ad88e76e6c8d941a6d08ccd01","title":"Interpretable Medical Diagnostics with Structured Data Extraction by Large Language Models"},{"paperId":"bb17c5fb339e758feeed1bd080bf49ba1f097900","title":"Comprehensive evaluation of deep and graph learning on drug-drug interactions prediction"},{"paperId":"db1aa71314016e12e115fbe449a688f523e52e77","title":"CUED at ProbSum 2023: Hierarchical Ensemble of Summarization Models"},{"paperId":"9618aa98729670f74418d2087f5e47ab137856b4","title":"Mixture-of-Domain-Adapters: Decoupling and Injecting Domain Knowledge to Pre-trained Language Models’ Memories"},{"paperId":"5bec96fe13fb1ff9bb860b326bab076fdd1f52e9","title":"Aviation-BERT: A Preliminary Aviation-Specific Natural Language Model"},{"paperId":"23de28dfa8173679f78cd068a860c9c102eaa2ec","title":"Advancing Italian biomedical information extraction with transformers-based models: Methodological insights and multicenter practical application"},{"paperId":"a3f5fa0897ba260aa3d89ba6fc358f742f379a55","title":"Multi-task bioassay pre-training for protein-ligand binding affinity prediction"},{"paperId":"9821928eee1ed1f36a2f5f935a5d31a71eede6b7","title":"Leveraging Knowledge Graph Embeddings to Enhance Contextual Representations for Relation Extraction"},{"paperId":"ea7e6df5b48f36dd13c838fd56744aae6189ee8b","title":"Evaluation of ChatGPT on Biomedical Tasks: A Zero-Shot Comparison with Fine-Tuned Generative Transformers"},{"paperId":"9b11f5e8b40b109cb774e29e5cf5a5baa8beeed8","title":"Generative Text-Guided 3D Vision-Language Pretraining for Unified Medical Image Segmentation"},{"paperId":"9121b7e4cb8d8d241574e7066a8aadcda93b4828","title":"ModuleFormer: Modularity Emerges from Mixture-of-Experts"},{"paperId":"28b07dd3c43238b2de2d7aaa64cb06d0c88411b4","title":"PyTrial: Machine Learning Software and Benchmark for Clinical Trial Applications"},{"paperId":"a7f55cf136f6a24eaa0772006de92979acfc3a58","title":"BioBLP: a modular framework for learning on multimodal biomedical knowledge graphs"},{"paperId":"3846d296a938dcc0a92784da76f9ef90d9de2e29","title":"Integrating domain knowledge for biomedical text analysis into deep learning: A survey"},{"paperId":"7c8254f6d95863fffeef2ba3d0d07f42b0f72e21","title":"MolFM: A Multimodal Molecular Foundation Model"},{"paperId":"f63a02601c7c3fdabcfff118d98e815697c42e0f","title":"shs-nlp at RadSum23: Domain-Adaptive Pre-training of Instruction-tuned LLMs for Radiology Report Impression Generation"},{"paperId":"264fd0af8b0906e3deb5df7f0ec0a7845b8ba213","title":"CoSiNES: Contrastive Siamese Network for Entity Standardization"},{"paperId":"8d277d9264fb6f8bf7cc99f8d91f2e64c12091aa","title":"When does Continuous Learning for BERT make sense?"},{"paperId":"4791aacf07f8ed425003d32ad3138416ffa44ae2","title":"Meta Learning for Domain Agnostic Soft Prompt"},{"paperId":"3f18227b9b23115253e582610b41d864264fe7f7","title":"RadLing: Towards Efficient Radiology Report Understanding"},{"paperId":"93856a907207c770a68f5c91c67854b366b3c1f7","title":"Impact of translation on biomedical information extraction from real-life clinical notes"},{"paperId":"362d4e00506f9bb39d42185a0b128f8602e139a8","title":"Utilizing ChatGPT to Enhance Clinical Trial Enrollment"},{"paperId":"b4f0059c9a706f963e0ab6d82163a7762dafb176","title":"Aci-bench: a Novel Ambient Clinical Intelligence Dataset for Benchmarking Automatic Visit Note Generation"},{"paperId":"4b78ad9179428a746908d05313a160d220e97750","title":"A Comprehensive Survey on Deep Learning for Relation Extraction: Recent Advances and New Frontiers"},{"paperId":"80cee5037d01470edc7fbd20c564f2e1fc2c6b85","title":"MultiLegalPile: A 689GB Multilingual Legal Corpus"},{"paperId":"edb428f00899810457892faaecdbcfbd04a4b42f","title":"Contextual Representation in NLP to Improve Success in Accident Classification of Mine Safety Narratives"},{"paperId":"cd9c33a2b221d9af7a2da8cd05655db3ca3d18a2","title":"Impact of translation on biomedical information extraction: an experiment on real-life clinical notes (Preprint)"},{"paperId":"4a4511d367113da3d3febf813bcb857023665533","title":"An analysis of entity normalization evaluation biases in specialized domains"},{"paperId":"7de4761f76af92762f88e6fa877ce5ea70db445c","title":"HealthE: Recognizing Health Advice & Entities in Online Health Communities"},{"paperId":"120c3c98818ce29dbb9847f221050b2a2a82d4ed","title":"Extensive Evaluation of Transformer-based Architectures for Adverse Drug Events Extraction"},{"paperId":"ac9d0362acd4cb37a0226cc29cedde336a740bb3","title":"EMS-BERT: A Pre-Trained Language Representation Model for the Emergency Medical Services (EMS) Domain"},{"paperId":"34b1220687a8bf25ece402aa3a1053bce959c49c","title":"CMTN: A Convolutional Multi-Level Transformer to Identify Suicidal Behaviors Using Clinical Notes"},{"paperId":"1681b374734559fe476aed69afaa2887f3576ad9","title":"BERT-based natural language processing analysis of French CT reports: Application to the measurement of the positivity rate for pulmonary embolism"},{"paperId":"08d5d3f67cb783ebe6fcb4274116335607a4b3ba","title":"Performance Comparison of Transformer-Based Models on Twitter Health Mention Classification"},{"paperId":"ea537354fc2e2e9c53253aad1dc752f7bf715805","title":"CAISA at SemEval-2023 Task 8: Counterfactual Data Augmentation for Mitigating Class Imbalance in Causal Claim Identification"},{"paperId":"f22d71c7ce9720ba1f717a4f1181488200e78198","title":"LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day"},{"paperId":"ea4cf43b0c4990a2a262cf86c62e3a522e60861f","title":"MTMG: A multi-task model with multi-granularity information for drug-drug interaction extraction"},{"paperId":"dd6ed104420b314ac3b4f3556f35fd07274bf453","title":"Quality of word and concept embeddings in targetted biomedical domains"},{"paperId":"e6c1422edf59b77da143c44e29f0020352e85d65","title":"Applications of cutting-edge artificial intelligence technologies in biomedical literature and document mining"},{"paperId":"c0f1ca1ed0f031ffaaf91a00eb16049917d395af","title":"Probing the EHR for Standardized Nursing Data"},{"paperId":"b5f437115d6c763218f186962811185613d64908","title":"Large-scale neural biomedical entity linking with layer overwriting"},{"paperId":"ee28be89eea46d2d46ea39efc670f08089241bf2","title":"Pre-Training MLM Using Bert for the Albanian Language"},{"paperId":"8952bdaac612090c99a8430e79b26c7059f97350","title":"FindZebra online search delving into rare disease case reports using natural language processing"},{"paperId":"858be2842122005b5a1aabce844676423ab6ca01","title":"CardioBERTpt: Transformer-based Models for Cardiology Language Representation in Portuguese"},{"paperId":"718ab6f71db78fd9ce756901b3321f27cb27d437","title":"Transformer-based Automatic Mapping of Clinical Notes to Specific Clinical Concepts"},{"paperId":"103f454cbcb1b54c77f27b38662138d85983e7fc","title":"ChatGPT in glioma adjuvant therapy decision making: ready to assume the role of a doctor in the tumour board?"},{"paperId":"ee390b0a29d65373aa0f4c64558ed8b02925bec6","title":"A Novel Method for Medical Semantic Word Sense Disambiguation by Using Graph Neural Network"},{"paperId":"1a1f7d3376cb4773aff852bf6bd7f6df229bd5f3","title":"Research on the System of Blockchain Data Sharing and Early-warning Decision for Public Health Emergency"},{"paperId":"6b38963d45e15a83244aafec08438bb9bf8f3831","title":"Serial KinderMiner (SKiM) Discovers and Annotates Biomedical Knowledge Using Co-Occurrence and Transformer Models"},{"paperId":"53707e7d7c5c95495b68fed893069c3457509630","title":"Named Entity Recognition and Normalization for Alzheimer’s Disease Eligibility Criteria"},{"paperId":"6e69fe3f69249dffc90ef2336b842ca7f4724c82","title":"MedNgage: A Dataset for Understanding Engagement in Patient-Nurse Conversations"},{"paperId":"96f006da556061e74751a598c5ff185999efa240","title":"DyGen: Learning from Noisy Labels via Dynamics-Enhanced Generative Modeling"},{"paperId":"8b5d8d852a0b924fe285ed70c3add4a8ff14713c","title":"Shuo Wen Jie Zi: Rethinking Dictionaries and Glyphs for Chinese Language Pre-training"},{"paperId":"5f1d51ce3bc9823e000af9c4ebfdd5d993a53e50","title":"W-procer: Weighted Prototypical Contrastive Learning for Medical Few-Shot Named Entity Recognition"},{"paperId":"d3060876d9ad4e4e50e1c88a8c04186df00f24e2","title":"A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets"},{"paperId":"55971443f3ca7f26056682ecb3da4491a1e4810d","title":"Understanding Breast Cancer Survival: Using Causality and Language Models on Multi-omics Data"},{"paperId":"ebf3a59aacdd9982283d7f41229ee2a93800d6ef","title":"Knowledge-Augmented Reasoning Distillation for Small Language Models in Knowledge-Intensive Tasks"},{"paperId":"04b62536270e27efe280f904f7e27e394f6d0192","title":"Complementary and Integrative Health Lexicon (CIHLex) and Entity Recognition in the Literature"},{"paperId":"a893a42e32651a0d6d1fee327c285765216809ae","title":"Diagnosing Transformers: Illuminating Feature Spaces for Clinical Decision-Making"},{"paperId":"2def8709b2a75ed533663ec7676601ceebdd23c3","title":"Constructing a disease database and using natural language processing to capture and standardize free text clinical information"},{"paperId":"0133c1128f2036ecb6b65ab15c562b71bf4f18a0","title":"Scientific Fact-Checking: A Survey of Resources and Approaches"},{"paperId":"f45bee9da1655320b7fc290d2abc20903bd12545","title":"Leveraging Domain Knowledge for Inclusive and Bias-aware Humanitarian Response Entry Classification"},{"paperId":"51b169701290cd129e0781fc9f3a9918604c89b5","title":"Improving accuracy of GPT-3/4 results on biomedical data using a retrieval-augmented language model"},{"paperId":"26de21346c5b88566a873fa76325673c917ff3a7","title":"W&G-BERT: A Pretrained Language Model for Automotive Warranty and Goodwill Text"},{"paperId":"0cd9a76d89ad2ffdd8ecaa314e141c2b66d05e5e","title":"Extracting Textual Information from Website Using Mixed Rules"},{"paperId":"07d45ce7de598ef03b400f8ddba7d2e055e77a08","title":"BiomedGPT: A Unified and Generalist Biomedical Generative Pre-trained Transformer for Vision, Language, and Multimodal Tasks"},{"paperId":"d8eb6b0535ae6c96921bfc2c08318902c08a3e63","title":"Efficient Document Embeddings via Self-Contrastive Bregman Divergence Learning"},{"paperId":"bb07ac7a94ab5cefc6d40df46fe20b71382ef09d","title":"Neural Natural Language Processing for Long Texts: A Survey of the State-of-the-Art"},{"paperId":"0fbf7ea1a3bd1754ed9aa12ed25906b731ece589","title":"Training Data Extraction From Pre-trained Language Models: A Survey"},{"paperId":"06091944b864d6dc473cab63321a95fb9c4067cc","title":"ChatCAD+: Towards a Universal and Reliable Interactive CAD using LLMs"},{"paperId":"f31b89d216f3c60773e11228fd0b60c37ccfefdf","title":"Ensemble of deep learning language models to support the creation of living systematic reviews for the COVID-19 literature"},{"paperId":"30fd5cad61b20dd39a49fb50c3dbc300d146c049","title":"Injecting Knowledge into Biomedical Pre-trained Models via Polymorphism and Synonymous Substitution"},{"paperId":"4fd05237af737c58c60e4ca8a745f85013681604","title":"Lawyer LLaMA Technical Report"},{"paperId":"180a5bfb5459538127f90db0a44445353175b052","title":"A publication-wide association study (PWAS), historical language models to prioritise novel therapeutic drug targets"},{"paperId":"29bf34941a5f8c33f2262356cb18ffe4f555fbc5","title":"A-BBL: A Risk Prediction Model for Patient Readmission based on Electronic Medical Records"},{"paperId":"3fabdd81cae936132d996c5d7534cc26c9b250cc","title":"Unleashing the Power of NLP and Transformers: A Game-Changer in Medical Research and Clinical Practice and a revolution of Medical Text Analysis.: Case Study: Cancer report classification by priority"},{"paperId":"ca4c88f57a1914024ccfd2e98d59e343c340fb01","title":"Challenges to sharing sample metadata in computational genomics"},{"paperId":"b17969990b3745a494f8fcadae6c8cd0426dc3ec","title":"Pre-training Multi-task Contrastive Learning Models for Scientific Literature Understanding"},{"paperId":"124a3c12d35c4f40d5ce4fb48355cd17af1aca89","title":"BAND: Biomedical Alert News Dataset"},{"paperId":"8d0c37eee7162f33178979b4183f0211e2dcae0d","title":"Difference-Masking: Choosing What to Mask in Continued Pretraining"},{"paperId":"3351c60442e8bd9e109fe2d9cd7fbf806a42dd33","title":"Partial Annotation Learning for Biomedical Entity Recognition"},{"paperId":"12d4cbeea33a6a61d8461648d070fe358c2ac879","title":"Biomedical Named Entity Recognition via Dictionary-based Synonym Generalization"},{"paperId":"1e4c49c9c93678dec95326ce25715fd2a1e64192","title":"Farewell to Aimless Large-scale Pretraining: Influential Subset Selection for Language Model"},{"paperId":"9bd30b7626fb4c229abe41c2c0d19c90dd87e168","title":"Drug–disease association prediction with literature based multi-feature fusion"},{"paperId":"d62ddd64f841cd5c263003a426adb3909b6311bc","title":"Evaluating Prompt-based Question Answering for Object Prediction in the Open Research Knowledge Graph"},{"paperId":"74b94891f8f7ac8d73d9df817b6720e1cb792bcc","title":"Enhancing Small Medical Learners with Privacy-preserving Contextual Prompting"},{"paperId":"de24a9888816de03dfabd9215ceb3443981b8e7e","title":"TADA: Efficient Task-Agnostic Domain Adaptation for Transformers"},{"paperId":"508730e9b10bf7fb048677248b53d82144d63666","title":"ARCH: Large-scale Knowledge Graph via Aggregated Narrative Codified Health Records Analysis"},{"paperId":"fe090a804a6d229034823dde035e4c0655b6665c","title":"Understanding the Effect of Data Augmentation on Knowledge Distillation"},{"paperId":"21c3343148290cf3f3d7739ca6d6f191fc66f613","title":"Gene Set Summarization using Large Language Models"},{"paperId":"4eeade5b60ad6495dfe9f4fb4cd6183a861520af","title":"EduNER: a Chinese named entity recognition dataset for education research"},{"paperId":"c07618042c9ad4ae4b296cc307f21d6b28d3dcdd","title":"ESCOXLM-R: Multilingual Taxonomy-driven Pre-training for the Job Market Domain"},{"paperId":"60b7c6913bead7636cba9aec55b1428c466771e1","title":"MediTab: Scaling Medical Tabular Data Predictors via Data Consolidation, Enrichment, and Refinement"},{"paperId":"8949f157d5e4483f1e9ca29b6ea80f3fe1ad699d","title":"Word Embedding of Dimensionality Reduction for Document Clustering"},{"paperId":"4280acb444242ab708e36c817d4d6682dad70373","title":"PlugMed: Improving Specificity in Patient-Centered Medical Dialogue Generation using In-Context Learning"},{"paperId":"784335a19e41dc0cedc5e030cba85b74ba142eff","title":"Appraising the Potential Uses and Harms of LLMs for Medical Systematic Reviews"},{"paperId":"6783b17fe4328f48403f57009a73f784de09f645","title":"XuanYuan 2.0: A Large Chinese Financial Chat Model with Hundreds of Billions Parameters"},{"paperId":"b396232b0b32212a94d46b9d30253eab3e97dfc5","title":"Decouple knowledge from parameters for plug-and-play language modeling"},{"paperId":"5ecfadb0211dd89a81d066a35a0e2312e991bc4c","title":"BioAug: Conditional Generation based Data Augmentation for Low-Resource Biomedical NER"},{"paperId":"cb5f53cd7d43ac6f48abf4dde6fc58e53a083b0e","title":"Accelerating the integration of ChatGPT and other large‐scale AI models into biomedical research and healthcare"},{"paperId":"cd72278471f49a3e667d6eab56ba9b538b42403b","title":"A domain semantics-enhanced relation extraction model for identifying the railway safety risk"},{"paperId":"a03b0ca43b5b687a6c38789157c3b803c9e02694","title":"Echoes of Biases: How Stigmatizing Language Affects AI Performance"},{"paperId":"80e972c82df37c60970552fb262c68cc24114964","title":"Review on Query-focused Multi-document Summarization (QMDS) with Comparative Analysis"},{"paperId":"393262e1345e71fbd6eb454a4f76eecf2da6634f","title":"Can Publicly Available Models Understand NATO Language? A Named Entity Recognition Case Study"},{"paperId":"dff0efaa388f546249976e8f5c6cee8eb4f12633","title":"An answer recommendation framework for an online cancer community forum"},{"paperId":"44cf14ea05ea9e7bc7d92de1d2228f514671bc44","title":"Use of Machine Learning Tools in Evidence Synthesis of Tobacco Use Among Sexual and Gender Diverse Populations: Algorithm Development and Validation"},{"paperId":"963cc6bddbdd140aa972327be5c04e2b451e45d5","title":"Question-Answering System Extracts Information on Injection Drug Use from Clinical Notes"},{"paperId":"fded78529a6c853bb97ce7755000ff217756bd0d","title":"From language models to large-scale food and biomedical knowledge graphs"},{"paperId":"049288e68caeadf7842df6977e140b47a8a2f89d","title":"MatSci-NLP: Evaluating Scientific Language Models on Materials Science Language Tasks Using Text-to-Schema Modeling"},{"paperId":"2e4e00164e5fb1a6786cc45d39ce95063dca2e70","title":"Deployment of machine learning algorithms to predict sepsis: systematic review and application of the SALIENT clinical AI implementation framework"},{"paperId":"0f7e4790f0b225c4b677e0424631f015cebe364b","title":"Understanding the Clinical Context of Medication Change Events in Clinical Narratives using Pre-trained Clinical Language Models"},{"paperId":"33d9ceeb9bf8e6e8a7ed724e8f2f98d05fc9d941","title":"Optimizing Medical Service Request Processes through Language Modeling and Semantic Search"},{"paperId":"891fed52e58af8150de46f0b7abbcbad4e6f9dcb","title":"INGENIOUS: Using Informative Data Subsets for Efficient Pre-Training of Language Models"},{"paperId":"39831c8c222a8d78fff4d67e7e56f5eeb90fdd7f","title":"Generative Pre-trained Transformer: A Comprehensive Review on Enabling Technologies, Potential Applications, Emerging Challenges, and Future Directions"},{"paperId":"3f0212d72d1f35ef69e18bfaf580fd7456114879","title":"Applications of the Natural Language Processing Tool ChatGPT in Clinical Practice: Comparative Study and Augmented Systematic Review"},{"paperId":"4c4722d3767dae6bc00b7de3e3fa160caaffe483","title":"Privacy-Preserving Prompt Tuning for Large Language Model Services"},{"paperId":"fc19a3b7364cc9ee1092df99cb74426843d92af1","title":"Large language models in biomedical natural language processing: benchmarks, baselines, and recommendations"},{"paperId":"9dc245029868ef9f63c07a2cac57791f2f055ecd","title":"Applying BioBERT to Extract Germline Gene-Disease Associations for Building a Knowledge Graph from the Biomedical Literature"},{"paperId":"c7d4f0e2cd91ea8b66563e43152fa82808b00443","title":"Representation Learning for Person or Entity-Centric Knowledge Graphs: An Application in Healthcare"},{"paperId":"ce7d9f1fab20e8121a6a86107eccbf0064405003","title":"Large language models for oncological applications"},{"paperId":"7445ca3b53cf597b1c81b347b3e954e70b71dee9","title":"GersteinLab at MEDIQA-Chat 2023: Clinical Note Summarization from Doctor-Patient Conversations through Fine-tuning and In-context Learning"},{"paperId":"bf09ea12856f2e055d24a1e41612c2a70e4e24f2","title":"Enhancing Knowledge Management in Healthcare: An Embedding Fusion Approach to Business Rule Representation"},{"paperId":"aa95fa8f677b07d32446de78dca300aae13d0a36","title":"MIReAD: Simple Method for Learning High-quality Representations from Scientific Documents"},{"paperId":"9e005b62ac48fae400028bcc00378d9bb1c04a7b","title":"NER-to-MRC: Named-Entity Recognition Completely Solving as Machine Reading Comprehension"},{"paperId":"86a55cd501409c129441456c4425547f3b981134","title":"SANTA: Separate Strategies for Inaccurate and Incomplete Annotation Noise in Distantly-Supervised Named Entity Recognition"},{"paperId":"802c74851cfd607fde830932f4b46d58f2b79e85","title":"Algorithmic Bias, Generalist Models, and Clinical Medicine"},{"paperId":"76c26e1a30d665c62fe78b7e9c31ed8358915dcc","title":"From Zero to Hero: Harnessing Transformers for Biomedical Named Entity Recognition in Zero- and Few-shot Contexts"},{"paperId":"35d2276749c2c31290d2ff410a305112e742da71","title":"Low-Resource Multi-Granularity Academic Function Recognition Based on Multiple Prompt Knowledge"},{"paperId":"3922365b7b40a447ecc57e027530cc90131e171e","title":"NLI4CT: Multi-Evidence Natural Language Inference for Clinical Trial Reports"},{"paperId":"42b6c3f3ab62311d503e6d30233aa96beac689fe","title":"Harnessing the Power of BERT in the Turkish Clinical Domain: Pretraining Approaches for Limited Data Scenarios"},{"paperId":"f11ea0fe307ce40fee1f18dfc3eef946a1c7a769","title":"SemEval-2023 Task 7: Multi-Evidence Natural Language Inference for Clinical Trial Data"},{"paperId":"75b04f49dd5685fc639c4511a4793a8d7b416abb","title":"Learning Missing Modal Electronic Health Records with Unified Multi-modal Data Embedding and Modality-Aware Attention"},{"paperId":"452fa2baafdfedaa93c6cbe9a28d3927864b185d","title":"SnorkelPlus: A Novel Approach for Identifying Relationships Among Biomedical Entities Within Abstracts"},{"paperId":"ae15f13135748fae111c8cfdeb18b35eb9239cc6","title":"Creating an Ignorance-Base: Exploring Known Unknowns in the Scientific Literature"},{"paperId":"c3b516c152c02bee1896ba6205d71f852ec9b236","title":"Heart disease risk factors detection from electronic health records using advanced NLP and deep learning techniques"},{"paperId":"fcd1d26d443a982ea79e1351bfaf791209e7c74d","title":"Europe PMC annotated full-text corpus for gene/proteins, diseases and organisms"},{"paperId":"60d309178cd324de7980008d768707ba1682b8ea","title":"Siamese BERT Architecture Model with attention mechanism for Textual Semantic Similarity"},{"paperId":"1c8703b93f28db2c748627e176f280b138354c12","title":"Knowledge-graph-enabled biomedical entity linking: a survey"},{"paperId":"28751205d16e6058798a6b2a2a5cf2d63f76f93e","title":"Bloom’s Taxonomy-based exam question classification: The outcome of CNN and optimal pre-trained word embedding technique"},{"paperId":"912ef861b37cfd95d631daefb57a21832efb8380","title":"Biomedical Relation Extraction Using Dependency Graph and Decoder-Enhanced Transformer Model"},{"paperId":"8ff8b3173c6b684d361209d7493568c3588d3daa","title":"Affect Analysis in Arabic Text: Further Pre-Training Language Models for Sentiment and Emotion"},{"paperId":"1227c8f40b62153c7068e4a134711f6ebb402e64","title":"Exploring the knowledge certainty shift: Metaknowledge analysis on drugs via assertion uncertainty burstiness"},{"paperId":"e5d51e257299cc3996b4dc10a3ca55f4934a38df","title":"Deep learning to refine the identification of high-quality clinical research articles from the biomedical literature: Performance evaluation"},{"paperId":"e4d6df552ec096000e459cecfe936ab4d6781c3b","title":"Enhancing early autism prediction based on electronic records using clinical narratives"},{"paperId":"69e3769724b91aa19f852786c04a9d079097ae64","title":"Active learning with feature matching for clinical named entity recognition"},{"paperId":"033cd044b41868c4d1713d917b6ff73f919783c5","title":"KEBLM: Knowledge-Enhanced Biomedical Language Models"},{"paperId":"cd5a1c434fc47892f68a4d0f58d88dc3b6218e05","title":"FooDis: A food-disease relation mining pipeline"},{"paperId":"6486f0b6e443cb864639d4a85277d71cf69f78e0","title":"Embracing Large Language Models for Medical Applications: Opportunities and Challenges"},{"paperId":"618f9c581f6003a79eff5ca2410e3be70bf4abd1","title":"Towards electronic health record-based medical knowledge graph construction, completion, and applications: A literature study"},{"paperId":"662a0ffbbb108e6d51882c788a949e73ff56fe7f","title":"A co-adaptive duality-aware framework for biomedical relation extraction"},{"paperId":"b9f7947227210e44012f806c28510dbf761dacc0","title":"A Joint Extraction System Based on Conditional Layer Normalization for Health Monitoring"},{"paperId":"df8740034b68e4250d0ceefa9fcbdf42c83af25d","title":"A Comparative Study of Cross-Sentence Features for Named Entity Recognition"},{"paperId":"ae495ee34fa97614c79949d528de9dec182f5365","title":"RepresentThemAll: A Universal Learning Representation of Bug Reports"},{"paperId":"e18590ebcd56755fb19d8c1ff3f8ed46a66fe1a8","title":"An integrated explicit and implicit offensive language taxonomy"},{"paperId":"d2685ecfb612493b44b6eaecb5533e8da7b4b7c3","title":"Lessons learned to enable question answering on knowledge graphs extracted from scientific publications: A case study on the coronavirus literature"},{"paperId":"afccd7c756ef9616af588bbd04e1946dbcfdae85","title":"MSQ-BioBERT: Ambiguity Resolution to Enhance BioBERT Medical Question-Answering"},{"paperId":"309275a127536f4efca4a5cda1d47ee7bb0368c3","title":"Contextual Response Interpretation for Automated Structured Interviews: A Case Study in Market Research"},{"paperId":"a0dca1c35f698b7b7af91449427ed035d9e4e049","title":"A Review of ChatGPT Applications in Education, Marketing, Software Engineering, and Healthcare: Benefits, Drawbacks, and Research Directions"},{"paperId":"39c486150cc8179e30ac8bab5935c7990167b5c0","title":"BactInt: A domain driven transfer learning approach and a corpus for extracting inter-bacterial interactions from biomedical text"},{"paperId":"04ee9597be4d6d2457214334e495e591000b5542","title":"PMC-LLaMA: Towards Building Open-source Language Models for Medicine"},{"paperId":"131c6f328c11706de2c43cd16e0b7c5d5e610b6a","title":"Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond"},{"paperId":"37ebfbc27e9a9a9dd772a472c94c2ae664152508","title":"MasonNLP+ at SemEval-2023 Task 8: Extracting Medical Questions, Experiences and Claims from Social Media using Knowledge-Augmented Pre-trained Language Models"},{"paperId":"32fdf76e571089ac463e356694a90599b99c6214","title":"Extracting social determinants of health from clinical note text with classification and sequence-to-sequence approaches"},{"paperId":"d15009785d3449d16244b50ef570922e463ba3ba","title":"A Compressed Language Model Embedding Dataset of ICD 10 CM Descriptions"},{"paperId":"12594b6afe01461384d2856d2bf44f1cf8533e3e","title":"ChatGPT and the rise of large language models: the new AI-driven infodemic threat in public health"},{"paperId":"6377f7a529549af725908e8eaee9d24aef9879cc","title":"Sebis at SemEval-2023 Task 7: A Joint System for Natural Language Inference and Evidence Retrieval from Clinical Trial Reports"},{"paperId":"7480bf28ce20a03f8296925ec3eb6e71ee71935b","title":"Information Extraction Network Based on Multi-Granularity Attention and Multi-Scale Self-Learning"},{"paperId":"dbd1162ad79cf153bad482a5dc63b9430b6e593e","title":"FineEHR: Refine Clinical Note Representations to Improve Mortality Prediction"},{"paperId":"da226d6e7007b39e7f5f0878894419858e3133cb","title":"Improving Model Transferability for Clinical Note Section Classification Models Using Continued Pretraining"},{"paperId":"1a45858d84a3baece50bb690235323ed8579b4c7","title":"Benchmarking ChatGPT-4 on ACR Radiation Oncology In-Training (TXIT) Exam and Red Journal Gray Zone Cases: Potentials and Challenges for AI-Assisted Medical Education and Decision Making in Radiation Oncology"},{"paperId":"89cbb16aea55469e56a7b5051196e14055511a00","title":"DDI-MuG: Multi-aspect graphs for drug-drug interaction extraction"},{"paperId":"286756b2b02d6a7bc49a7ad66686f30831f26c25","title":"Differentiating ChatGPT-Generated and Human-Written Medical Texts: Quantitative Study"},{"paperId":"2d31cf90218fbf3ce4b220ab3bdff74e17d1b4f5","title":"Web Interface of NER and RE with BERT for Biomedical Text Mining"},{"paperId":"ccbc308ff7775af21c3922b6a92faf3fe21f0261","title":"CPK-Adapter: Infusing Medical Knowledge into K-Adapter with Continuous Prompt"},{"paperId":"40ae90005ba612cff567a96b7d0cedeca0d2635a","title":"On the Use of Transformer-Based Models for Intent Detection Using Clustering Algorithms"},{"paperId":"7e8ce82692f8f32c584de37ff2294f3bc7a1acfa","title":"Extracting Drug-Drug Interactions from Biomedical Texts Using BioBERT with Improved Focal Loss"},{"paperId":"a564daa5ffdd4ea4cbb28b6ea459da9f9f65428d","title":"Faithful AI in Medicine: A Systematic Review with Large Language Models and Beyond"},{"paperId":"a0ec9f110ea172aa863862929cc0338934ff93c6","title":"Harnessing Biomedical Literature to Calibrate Clinicians’ Trust in AI Decision Support Systems"},{"paperId":"8e4558c878ecdac1091486727634c1ed5c8c38a8","title":"A Systematic survey on automated text generation tools and techniques: application, evaluation, and challenges"},{"paperId":"be545c9ec5e757513988a11cb7024e026616d8b4","title":"Multi-task learning for few-shot biomedical relation extraction"},{"paperId":"258605dc5b00fe66b72091f947642a554e472aee","title":"Exploring the Trade-Offs: Unified Large Language Models vs Local Fine-Tuned Models for Highly-Specific Radiology NLI Task"},{"paperId":"b3c1fad1f5f8f0213b6d3f3458fa86205a3434f7","title":"A Survey for Biomedical Text Summarization: From Pre-trained to Large Language Models"},{"paperId":"a7f8fd45fbcdd81449cb7a1a6a2b2c18b38f8151","title":"ImpressionGPT: An Iterative Optimizing Framework for Radiology Report Summarization with ChatGPT"},{"paperId":"7e97c05a1374082c49b69c8a19461490d6452efa","title":"EasyNER: A Customizable Easy-to-Use Pipeline for Deep Learning- and Dictionary-based Named Entity Recognition from Medical Text"},{"paperId":"9ec07ad77267af6c304bdf0c2b9c914d296b468b","title":"EDAD: Efficient Domain Adaptive Distillation by Integrating Domain Adaptation and Knowledge Distillation"},{"paperId":"c7705944c22a3d95413bc1a1950521662a25f1b3","title":"Bridging the Gap between Medical Tabular Data and NLP Predictive Models: A Fuzzy-Logic-Based Textualization Approach"},{"paperId":"965e0d4bfe8097baab1947fc23263ae790620e23","title":"AGI for Agriculture"},{"paperId":"90616bb932b345c83b5b70dffc76a75b14805315","title":"From benchmark to bedside: transfer learning from social media to patient-provider text messages for suicide risk prediction"},{"paperId":"30fec23437cf9aaf3e9cb7d0c076483c14893abf","title":"An NLP approach to identify SDoH-related circumstance and suicide crisis from death investigation narratives"},{"paperId":"22c6d46e88dd289d700be2767622cc9e9391a0aa","title":"Large-Scale Biomedical Relation Extraction Across Diverse Relation Types: Model Development and Usability Study on COVID-19"},{"paperId":"b8c9bcec47ac62105e32549e77aac979df8ad481","title":"DisGeReExT: a knowledge discovery system for exploration of disease–gene associations through large-scale literature-wide analysis study"},{"paperId":"989c337316e25f8e5dadf3847f8bac36d4ed0e3c","title":"Drug–drug interaction extraction‐based system: An\n natural language processing\n approach"},{"paperId":"42780f9c7f73d73d7a887e2f787af0e079703d40","title":"Are Large Language Models Ready for Healthcare? A Comparative Study on Clinical Language Understanding"},{"paperId":"4ce68478791bd4cfcdf883d75fa31fc1ebc6c7cc","title":"FrenchMedMCQA: A French Multiple-Choice Question Answering Dataset for Medical domain"},{"paperId":"994b335b0e42955df63c3e963de47655f5aa8b8d","title":"How Does Imperfect Automatic Indexing Affect Semantic Search Performance?"},{"paperId":"744cbd5c2e6edff0584104d196ef45128ac12800","title":"Transfer Learning Approach to Multilabel Biomedical Literature Classification using Transformer Models"},{"paperId":"0940155bec999067aba536d80e37f720ce91c4d0","title":"Automatic ICD-10 Code Association: A Challenging Task on French Clinical Texts"},{"paperId":"1774405ae834c0e3c1f7af2b1e8f963fc23bd4a1","title":"Machine learning for synergistic network pharmacology: a comprehensive overview"},{"paperId":"020e473d8c987dcfb03fcfffeb87b17812447031","title":"Evaluation of ChatGPT Family of Models for Biomedical Reasoning and Classification"},{"paperId":"8f4e198467de15fdbb305d0982ff6f15565ab601","title":"To Asymmetry and Beyond: Structured Pruning of Sequence to Sequence Models for Improved Inference Efficiency"},{"paperId":"4d1d673b0184d910f9b3d182825fc1ae256800ff","title":"G2PTL: A Pre-trained Model for Delivery Address and its Applications in Logistics System"},{"paperId":"677529c13cfebc273e13ae036c09658033b0afee","title":"DrBERT: A Robust Pre-trained Model in French for Biomedical and Clinical domains"},{"paperId":"bce55193d9a887ad00774a9134df08cd521a85ae","title":"DoctorGLM: Fine-tuning your Chinese Doctor is not a Herculean Task"},{"paperId":"538680e08812fadc22ac4a7eefa6b40ae9179b28","title":"Identifying Mentions of Pain in Mental Health Records Text: A Natural Language Processing Approach"},{"paperId":"f822aa4c0a026e67566689e6012b0446b58ab7cd","title":"A marker-based neural network system for extracting social determinants of health"},{"paperId":"771921b829b04d149bb0e06112a731e4a43ce666","title":"Using language models and ontology topology to perform semantic mapping of traits between biomedical datasets"},{"paperId":"a41552d69ca566bfc5c7f1b7e0e97fe61cadb7af","title":"Robust Identification of Figurative Language in Personal Health Mentions on Twitter"},{"paperId":"304b640c1b2559ac1442ac9be54853ac80ec248c","title":"Multimodal data fusion for cancer biomarker discovery with deep learning"},{"paperId":"2da5b68b89d1d53373d122ac8e6fb2d23668c22f","title":"Toward structuring real-world data: Deep learning for extracting oncology information from clinical text with patient-level supervision"},{"paperId":"80652b8e0c03ebfabb6255f097a9704dcb2b79ff","title":"Disto-TRP: An approach for identifying transient receptor potential (TRP) channels using structural information generated by AlphaFold."},{"paperId":"07bfaf2d713040efe69d0c61bcdfd9870bfbaf5a","title":"A novel self-attention enriching mechanism for biomedical question answering"},{"paperId":"4283021777631cbdc0e9a84218d37d2fe0e9f828","title":"Vision-knowledge fusion model for multi-domain medical report generation"},{"paperId":"1cdce64d3832e465adb1151044153c687f8c819d","title":"Analysis of ‘One in a Million’ primary care consultation conversations using natural language processing"},{"paperId":"b56c53936a436ebb7d9e79a0e0da1760184cd3c5","title":"Artificial Intelligence in Pharmaceutical Sciences"},{"paperId":"3b99f8c18dfe1ba61144cfacaf8c22357455024c","title":"Review: A Roadmap to Use Nonstructured Data to Discover Multitarget Cancer Therapies."},{"paperId":"e68d738c6c769fc28d790bfad57b9ce804961f18","title":"Modeling semantic business trajectories of territories for multidisciplinary studies through controlled vocabularies"},{"paperId":"f9ccd94fbe3946b47e07540d232b084ef805fcef","title":"The Future of AI in Ovarian Cancer Research: The Large Language Models Perspective"},{"paperId":"f88e3426f752c0712b87af29aa1bfc811adff2b5","title":"RoBERTa-Assisted Outcome Prediction in Ovarian Cancer Cytoreductive Surgery Using Operative Notes"},{"paperId":"4be694b101230da39a035c0cf4ebb90aa569879c","title":"Quick Dense Retrievers Consume KALE: Post Training KullbackLeibler Alignment of Embeddings for Asymmetrical dual encoders"},{"paperId":"00366a22d89a1c580ecd21108d44e9d8042fd9d8","title":"GPT-4 can pass the Korean National Licensing Examination for Korean Medicine Doctors"},{"paperId":"e3048287564e965203d22837ad7b2c488c19d451","title":"Identifying Functional Status Impairment in People Living With Dementia Through Natural Language Processing of Clinical Documents: Cross-Sectional Study."},{"paperId":"19cd2250f419666d4df441bae7ade1dd9a2f6bf9","title":"ChatGPT in Healthcare: A Taxonomy and Systematic Review"},{"paperId":"83edcfbb206ddad38a971d605da09390604248ea","title":"BloombergGPT: A Large Language Model for Finance"},{"paperId":"1ad9a295ea841599383e5ae3e88381438d4a7db3","title":"Evaluation of GPT and BERT-based models on identifying protein-protein interactions in biomedical text"},{"paperId":"1f78eb8e03774209cff65dba4bfd3b95aef77ded","title":"oBERTa: Improving Sparse Transfer Learning via improved initialization, distillation, and pruning regimes"},{"paperId":"06cf8da98925503cd2f4186dc48d8ef454d5a6f6","title":"Methods of extracting biomedical information from patents and scientific publications (on the example of chemical compounds)"},{"paperId":"de6fd09ed7783b95af7e5f4088a70d5b0244f5aa","title":"Improving large language models for clinical named entity recognition via prompt engineering."},{"paperId":"eaadcb852955ac2b664f7b7cd111a73661aa90df","title":"Biomedical named entity recognition based on fusion multi-features embedding"},{"paperId":"0d7a0b53f65929776b851933d869fa753798bca3","title":"Carolina: a General Corpus of Contemporary Brazilian Portuguese with Provenance, Typology and Versioning Information"},{"paperId":"df6ce09e914f278a95c5c2b6dfaecfaf6e81c8c7","title":"Identifying Reasons for Statin Nonuse in Patients With Diabetes Using Deep Learning of Electronic Health Records"},{"paperId":"a7e80501734132b784aa8866ce1994d56611acd8","title":"Enhancing Biomedical ReQA With Adversarial Hard In-Batch Negative Samples"},{"paperId":"0f8f20ef4bc90a2b210bc5be08a7f326a214556f","title":"An Information Extraction Study: Take In Mind the Tokenization!"},{"paperId":"02634d754f4898ffd68623f8ff6f7861e700ef88","title":"Accurate and Reliable Classification of Unstructured Reports on Their Diagnostic Goal Using BERT Models"},{"paperId":"620facb14edcd4897c00e335569932392894778f","title":"A Disease-Prediction Protocol Integrating Triage Priority and BERT-Based Transfer Learning for Intelligent Triage"},{"paperId":"ea2504a6ca0a520af5ea0c96d00fb28cccc5d410","title":"A Biomedical Entity Extraction Pipeline for Oncology Health Records in Portuguese"},{"paperId":"42c2507cf28070785b92342804aed1eba4380400","title":"Bias Amplification in Intersectional Subpopulations for Clinical Phenotyping by Large Language Models"},{"paperId":"3494e10a099ffef4e87f0a84d64af8f1a527b80c","title":"ChatGPT in glioma patient adjuvant therapy decision making: ready to assume the role of a doctor in the tumour board?"},{"paperId":"812b6f4dd78edb52959a660c1ac3cdcf5f8e13c6","title":"Bat4RCT: A suite of benchmark data and baseline methods for text classification of randomized controlled trials"},{"paperId":"2d0b9af28c80cfa5c87c08a249af7393c6b4695f","title":"Enabling Early Health Care Intervention by Detecting Depression in Users of Web-Based Forums using Language Models: Longitudinal Analysis and Evaluation"},{"paperId":"9305fd6d87007c7b90d2e0db579ff40467352969","title":"Natural language processing to automatically extract the presence and severity of esophagitis in notes of patients undergoing radiotherapy"},{"paperId":"ec13360ba5820b228333bc21d12f4871250546e8","title":"Lay Text Summarisation Using Natural Language Processing: A Narrative Literature Review"},{"paperId":"44caf26949b4799e21f7b0754fe61315e8b71542","title":"Compositional Zero-Shot Domain Transfer with Text-to-Text Models"},{"paperId":"3f11e89e7f19e931adf31b91f15302d7539c809d","title":"A Joint Domain-Specific Pre-Training Method Based on Data Enhancement"},{"paperId":"92bf19978212365fd372d99e946ae86f36fb20b0","title":"Deep Learning Based Structural Reliability Finite Element Analysis Surrogate for Hydro-Generator Lower Frame"},{"paperId":"256979852e0e0a5fe5cc8ddbf54fa1af2a843722","title":"ChatGPT for shaping the future of dentistry: the potential of multi-modal large language model"},{"paperId":"6fdcc152422c64de86c859b53669c0548261ec09","title":"GrapeQA: GRaph Augmentation and Pruning to Enhance Question-Answering"},{"paperId":"e9e52a94f1cd46da9ae47e40ba2981ba87cb92ab","title":"Analyzing the Generalizability of Deep Contextualized Language Representations For Text Classification"},{"paperId":"443d898928eb8e32d2e6f8f287beaa63f5b00eb9","title":"JaCoText: A Pretrained Model for Java Code-Text Generation"},{"paperId":"0096f13f307a5bfae8cb92f722f9811e10d16ee5","title":"ExBEHRT: Extended Transformer for Electronic Health Records to Predict Disease Subtypes & Progressions"},{"paperId":"23684a07517870cffd1f97fafbaae16ba22bd2b7","title":"Large AI Models in Health Informatics: Applications, Challenges, and the Future"},{"paperId":"3611674ccd820efc0b59981038bc4161d46b3add","title":"A Systematic Literature Review of the Use of Computational Text Analysis Methods in Intimate Partner Violence Research"},{"paperId":"cff26bda86237d113ed01c812ad8bedd0afbe070","title":"DeID-GPT: Zero-shot Medical Text De-Identification by GPT-4"},{"paperId":"79545d9d30b924df293ee103e46f78aaf3249e51","title":"Leveraging Foundation Models for Clinical Text Analysis"},{"paperId":"9f105fdbe301eb23371f35f697164a19e6c45ed5","title":"OpticalBERT and OpticalTable-SQA: Text- and Table-Based Language Models for the Optical-Materials Domain"},{"paperId":"01caa1fbe78ee2234ade9228c772cbb6d5e47458","title":"Exploring Partial Knowledge Base Inference in Biomedical Entity Linking"},{"paperId":"689e0dfcec660611c1f84490b3055b020b7bd0e1","title":"Public Awareness and Sentiment Analysis of COVID-Related Discussions Using BERT-Based Infoveillance"},{"paperId":"701e61977155143529e44264ccdb8443f07c4660","title":"IK-DDI: a novel framework based on instance position embedding and key external text for DDI extraction"},{"paperId":"0a438980ac42451d6d32dd2ad8ead7b55520408d","title":"A Systematic Review of Transformer-Based Pre-Trained Language Models through Self-Supervised Learning"},{"paperId":"eea77daf238730dd7e3686b33cf31bb771f058ff","title":"B-LBConA: a medical entity disambiguation model based on Bio-LinkBERT and context-aware mechanism"},{"paperId":"0429e9343424ded011eaa46547780c5c17f66fec","title":"A cross-modal deep metric learning model for disease diagnosis based on chest x-ray images"},{"paperId":"171598987de38aeac08ffa338df9e0bbd78d58ca","title":"Applying unsupervised keyphrase methods on concepts extracted from discharge sheets"},{"paperId":"8641c70c106c4f7485e613888b91a58e9812a5a7","title":"MEDBERT.de: A Comprehensive German BERT Model for the Medical Domain"},{"paperId":"c7ccb92677afc2738868cf95d38af79a1adf7e9d","title":"Potential of natural language processing for metadata extraction from environmental scientific publications"},{"paperId":"7d66ec6870d6773d559df20642bb2f30b106edc0","title":"Input-length-shortening and text generation via attention values"},{"paperId":"1fcd70f19c05c37f75bbf856cbb3b8bbef73373a","title":"Contextualized Medication Information Extraction Using Transformer-based Deep Learning Architectures"},{"paperId":"6e96773bac534c87cf0eeaf11c5ba2a596b3380e","title":"Attention is not all you need: the complicated case of ethically using large language models in healthcare and medicine"},{"paperId":"d3d9021293fd0716466306b4a4ec9dbbd1932886","title":"Self-Supervised Learning-Based General Laboratory Progress Pretrained Model for Cardiovascular Event Detection"},{"paperId":"9556d7ad415cadb2c2d9b34e5fa5d9c2192a26b7","title":"Generating multiple-choice questions for medical question answering with distractors and cue-masking"},{"paperId":"17ca48ad1b944c897863f04ba9ffa72674dce1ce","title":"Parallel multi-head attention and term-weighted question embedding for medical visual question answering"},{"paperId":"656e8c5f8bced540425c12d854b2911dddefff14","title":"Multimodal Data Integration for Oncology in the Era of Deep Neural Networks: A Review"},{"paperId":"3440687e1fc734baeab1abee4a86c81347d1422a","title":"aeroBERT-Classifier: Classification of Aerospace Requirements Using BERT"},{"paperId":"fa9482793ab5f9ad6f7429476db6e11f502e2440","title":"Math Function Recognition with Fine-Tuning Pre-Trained Models"},{"paperId":"e5174aeda1baa67c17f4ac630ae2e44453954cc3","title":"Personalisation within bounds: A risk taxonomy and policy framework for the alignment of large language models with personalised feedback"},{"paperId":"758e94d65a10783c2a64e478a2103224d295ab13","title":"Enhanced disease-disease association with information enriched disease representation."},{"paperId":"1475905e3687c21428ef3cad902d465093072fd1","title":"Deep multi-modal intermediate fusion of clinical record and time series data in mortality prediction"},{"paperId":"bdf7bf9e81a6c12e22323d0402885b2ba62f623e","title":"Does Synthetic Data Generation of LLMs Help Clinical Text Mining?"},{"paperId":"38311d8f52e4fef86a121dd91923a1df798f79fd","title":"A Study of Text Summarization in the Medical Domain using BERT and its Variants"},{"paperId":"0606bb9a541ce7e57bd78ac680a7df0225ece30c","title":"Can large language models build causal graphs?"},{"paperId":"914f807de0eaf055aded977419d5d22bb6078d90","title":"Document-level Relation Extraction with Cross-sentence Reasoning Graph"},{"paperId":"b388002a68143f94a6efb12ea75a2d18af64da0d","title":"The named entity recognition of vessel power equipment fault using the multi-details embedding model"},{"paperId":"30809168fff23c852867ad359baaebfae532f0a7","title":"Enhancing Activity Prediction Models in Drug Discovery with the Ability to Understand Human Language"},{"paperId":"62245ea7a34afb2613b7fbffbc3578e12e459f3c","title":"Hybrid Graph Convolutional Network With Online Masked Autoencoder for Robust Multimodal Cancer Survival Prediction"},{"paperId":"7f193c6302b2eed4779354f6763fa1217b123d05","title":"Rad-Former: Structuring Radiology Reports using Transformers*"},{"paperId":"883a5338dee175ae61f323202a5aba80b2458e0f","title":"Deep Learning Based Code Generation Methods: A Literature Review"},{"paperId":"31f32b26e6b997f9d5691a8edc6ea86e434865d8","title":"Constructing and analyzing domain-specific language model for financial text mining"},{"paperId":"1419dea18f214029959097c265024e9e9bc598f3","title":"WeakMeSH: Leveraging provenance information for weakly supervised classification of biomedical articles with emerging MeSH descriptors"},{"paperId":"4d4a96708fc67403176bb2b891b564af7a20c148","title":"RAMM: Retrieval-augmented Biomedical Visual Question Answering with Multi-modal Pre-training"},{"paperId":"a68ba8496d7c7125ac470057f7b2e8fb13845e3a","title":"Domain-adapted large language models for classifying nuclear medicine reports"},{"paperId":"57c14d250f231bf56b3c68441aaa36d389281b0d","title":"Assessment of Natural Language Processing of Electronic Health Records to Measure Goals-of-Care Discussions as a Clinical Trial Outcome"},{"paperId":"6c48f1a429e8371ebdac412602629cb6472db1a8","title":"Domain Word Extension Using Curriculum Learning"},{"paperId":"b169f2ce55e14584d2db6f64eeb5ad2702d39d40","title":"Artificial intelligence foundation and pre-trained models: Fundamentals, applications, opportunities, and social impacts"},{"paperId":"9991fe75b5c3d47699d7aa3b44f2a26a9b29eecd","title":"Extraction and analysis of risk factors from Chinese Chemical Accident Reports"},{"paperId":"10916b9df058a076238f6520435d0961419a4308","title":"Knowledge graph assisted end-to-end medical dialog generation"},{"paperId":"eb78b34409a323fb36af93b5c252ee99a9c036b5","title":"Planarized sentence representation for nested named entity recognition"},{"paperId":"430aa6966c15c4a20a4fb2d8383e136b9cb6cde7","title":"Almanac: Retrieval-Augmented Language Models for Clinical Medicine"},{"paperId":"efa9fc7d5b6b244d8ae3a9c3db98418ec39aa7a3","title":"Precision information extraction for rare disease epidemiology at scale"},{"paperId":"d8035d652a26bc96ceb9d0ba89460d11d4850e76","title":"Knowledge grounded medical dialogue generation using augmented graphs"},{"paperId":"8abee896e893dcf230c9e02de2bb435e33ecba76","title":"Survey on the Biomedical Text Summarization Techniques with an Emphasis on Databases, Techniques, Semantic Approaches, Classification Techniques, and Similarity Measures"},{"paperId":"236445f0a3b1e30b2542e5e64616ff6a8af7e3ea","title":"Language Models are Few-shot Learners for Prognostic Prediction"},{"paperId":"7480d1e33e14ff113413de8dc09b7664dad1c0da","title":"Improving Clinical Decision Making with a Two-Stage Recommender System Based on Language Models: A Case Study on MIMIC-III Dataset"},{"paperId":"fefe6c2eb25da9f9f7982b8718f3abc1de2ada03","title":"Modelling Temporal Document Sequences for Clinical ICD Coding"},{"paperId":"5527c8aca3e9f30b2f6382ab66a83eec2757051e","title":"Construction of Knowledge Graphs: State and Challenges"},{"paperId":"114655441607fbf58f5b174f2905a006b3853d91","title":"FiTs: Fine-grained Two-stage Training for Knowledge-aware Question Answering"},{"paperId":"ef959f7212091d0e9aa7502d15ef7d87dd70b902","title":"Identification of Thermophilic Proteins Based on Sequence-Based Bidirectional Representations from Transformer-Embedding Features"},{"paperId":"5c7353fac22a8fdc43fc2f5c006b5d6902c47e75","title":"On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective"},{"paperId":"b48adefb1d6a2da907f602c1d572704f4599792e","title":"S1000: a better taxonomic name corpus for biomedical information extraction"},{"paperId":"460dea7c62ca0fdc27f671f50b76f477942dea12","title":"Improving text mining in plant health domain with GAN and/or pre-trained language model"},{"paperId":"534be0d0603866e71f873d9940e1281a5a3045fb","title":"Boosting classification reliability of NLP transformer models in the long run"},{"paperId":"692f49d243a23c220b40cbd2b6b26b773d9b31c4","title":"Hashtag-Guided Low-Resource Tweet Classification"},{"paperId":"b58c2110655e950c36bc533fa81f143397a5fe2e","title":"Exploring the Limits of Transfer Learning with Unified Model in the Cybersecurity Domain"},{"paperId":"f221280799e5cc9f4e43f2a26754ff802e22a3f0","title":"Question Answering Chatbots for Biomedical Research Using Transformers"},{"paperId":"6b6ea46e57d026c546e45cbe25ea4be55523a6b6","title":"On the Use of Knowledge Transfer Techniques for Biomedical Named Entity Recognition"},{"paperId":"48de1a31cca6631bd73a5d0854acfda5e5195d66","title":"BORD: A Biomedical Ontology based method for concept Recognition using Distant supervision: Application to Phenotypes and Diseases"},{"paperId":"d0c5f901868f6e2cb126fd51b155f631372a9669","title":"Biomedical Text Classification Using Augmented Word Representation Based on Distributional and Relational Contexts"},{"paperId":"99ecb1ffe691f5414d737c4cb8e824f513c0bb31","title":"Uni-Fold MuSSe: De Novo Protein Complex Prediction with Protein Language Models"},{"paperId":"ef91c31d8aab9fe95fec29149e2fe4568ab2fb32","title":"SwitchPrompt: Learning Domain-Specific Gated Soft Prompts for Classification in Low-Resource Domains"},{"paperId":"fab384a1f667b75a1277244acdbc22625d20caf8","title":"Reveal the Unknown: Out-of-Knowledge-Base Mention Discovery with Entity Linking"},{"paperId":"629bc57782bb4326a3eb5f89314e350729c5f417","title":"AdapterSoup: Weight Averaging to Improve Generalization of Pretrained Language Models"},{"paperId":"5ef821267fa68d3231ed8135ff8ec09f25bb1398","title":"ChatCAD: Interactive Computer-Aided Diagnosis on Medical Image using Large Language Models"},{"paperId":"1929c070964bac55a1d57d13bcaae44b28eb97fb","title":"BLIAM: Literature-based Data Synthesis for Synergistic Drug Combination Prediction"},{"paperId":"83c3bef2a3d24c31bccb8cf638dcdea630567089","title":"Expediting Distributed DNN Training With Device Topology-Aware Graph Deployment"},{"paperId":"ce6f2d68b1a4029ff4a838fcf12d5ad1d47f0e68","title":"Multilingual translation for zero-shot biomedical classification using BioTranslator"},{"paperId":"dd4238d23fea6fc8e9232ca0fec4cad728f213ea","title":"Span-based Named Entity Recognition by Generating and Compressing Information"},{"paperId":"597c60202acd32e4ad7e3fc9f1db993fa25290d6","title":"TaughtNet: Learning Multi-Task Biomedical Named Entity Recognition From Single-Task Teachers"},{"paperId":"d7047f54c65fee3c61288b3c490a862e95ae5092","title":"Lightweight Transformers for Clinical Natural Language Processing"},{"paperId":"1e2839669f61fd99c524690e238f6cbe505e5ffd","title":"Real-Time Visual Feedback to Guide Benchmark Creation: A Human-and-Metric-in-the-Loop Workflow"},{"paperId":"292c3ca299362db1435ae8ea6a35929b430bdb17","title":"Zero-Shot Learning for Requirements Classification: An Exploratory Study"},{"paperId":"4562c122c523f7ea2b7c36ee524a47f59d7e74b2","title":"A Biomedical Knowledge Graph for Biomarker Discovery in Cancer"},{"paperId":"9d379e0e77a57bf0c9e33576c465afcedd13ec89","title":"A prefix and attention map discrimination fusion guided attention for biomedical named entity recognition"},{"paperId":"b3cbce144f18ba1bfbaacb17d6284ba8bb4dbb28","title":"A Review on Clinical Named Entity Recognition"},{"paperId":"fed2fab877ba1af72470d3dc061747d0ea9879d0","title":"Deep learning approach to detection of colonoscopic information from unstructured reports"},{"paperId":"2ad818c34b63aa2260542ac619b7098fb7745bac","title":"Machine learning and deep learning in medicine and neuroimaging"},{"paperId":"10c5079d2baa5a287054d9ddd7806a4c16fd7531","title":"UDAPTER - Efficient Domain Adaptation Using Adapters"},{"paperId":"e7dcdfb7734d59b97f825cce8b3105a2d9b14d10","title":"The Effect of Metadata on Scientific Literature Tagging: A Cross-Field Cross-Model Study"},{"paperId":"e3ec55e9e6720194a0ed5d4033d93a941c8a4f99","title":"Continual Pre-training of Language Models"},{"paperId":"2c6a6eb161c04d1f4149b38321b23878d24f2da3","title":"A survey on Transformers for long sentences and its application to medical notes"},{"paperId":"3b0424149731d10829015cb4ab6299d18162128e","title":"LIQUID: A Framework for List Question Answering Dataset Generation"},{"paperId":"627b6f7687e122b5578f095221f66583850f0ea5","title":"GLADIS: A General and Large Acronym Disambiguation Benchmark"},{"paperId":"4cfec8ec51a0ecd7efbc6e6622ae8f930935f714","title":"Bioformer: an efficient transformer language model for biomedical text mining"},{"paperId":"279cc657655eeb4e96a2eaf3d77f708edbf6a313","title":"Construction and evaluation of a domain-specific knowledge graph for knowledge discovery"},{"paperId":"47a1263ba21a72790334544f2a11b7c0ee4b5e76","title":"Multimodality Representation Learning: A Survey on Evolution, Pretraining and Its Applications"},{"paperId":"cb1a64200edaa038326a177538b6b0d5ba21558a","title":"Informing clinical assessment by contextualizing post-hoc explanations of risk prediction models in type-2 diabetes"},{"paperId":"c41146620c907535b5f21c1f50f981f4c724f3d5","title":"Improving Protein Function Prediction by Adaptively Fusing Information From Protein Sequences and Biomedical Literature"},{"paperId":"02e6bd66517850b1ff446fc34acb647ac72f3ba9","title":"A role distinguishing Bert model for medical dialogue system in sustainable smart city"},{"paperId":"1755d306187b5b87110ae5e0006dc762942554a4","title":"Deep representation learning of scientific paper reveals its potential scholarly impact"},{"paperId":"0acd41876736b1563d35588ab76cb4c2266052e9","title":"LFT-Net: Local Feature Transformer Network for Point Clouds Analysis"},{"paperId":"5e3ff15eba3770f479c1e44bc2fea54c4cab1384","title":"Transformer-based language models for mental health issues: A survey"},{"paperId":"3474e4bb497e5b5fc31a4c9757f55cd578678549","title":"Deep learning based classification of multi-label chest X-ray images via dual-weighted metric loss"},{"paperId":"f0b40e3bc7a4ed554e82905e6fb65cd8d3489f44","title":"ADPG: Biomedical entity recognition based on Automatic Dependency Parsing Graph"},{"paperId":"31605129071ea6fca8e22e622de164e1a2ae1549","title":"Neurofuzzy semantic similarity measurement"},{"paperId":"9bedc9b944b2480affea4832e38106ce5eccead5","title":"Classifying literature mentions of biological pathogens as experimentally studied using natural language processing"},{"paperId":"de7fa58930e86217498e6c3ac656366cfb39930a","title":"Supporting SNOMED CT postcoordination with knowledge graph embeddings"},{"paperId":"498f055c8a6c2231083e507a5c4f10f5dc1e0c60","title":"Large Language Models for Biomedical Knowledge Graph Construction: Information extraction from EMR notes"},{"paperId":"4ea1f64c13280ef13f506eef4b3dd2395d1cf171","title":"ProtST: Multi-Modality Learning of Protein Sequences and Biomedical Texts"},{"paperId":"71dc990592911c454714e6fbe680dadf0cae1e45","title":"Context Matters: A Strategy to Pre-train Language Model for Science Education"},{"paperId":"ca120bcaa4bc5e43f445bcfcc5e577d2112c5626","title":"TA-WHI: Text Analysis of Web-Based Health Information"},{"paperId":"1cfe781523b4b3469d822bc52a0721b1c70cee5f","title":"A rule-free workflow for the automated generation of databases from scientific literature"},{"paperId":"cde6b633703eda8c3fbd7f76a967e07b19b7623d","title":"A Multi-View Joint Learning Framework for Embedding Clinical Codes and Text Using Graph Neural Networks"},{"paperId":"5ab6fb406369148bd604fa5e0ecb841d45899225","title":"Entity and relation extraction from clinical case reports of COVID-19: a natural language processing approach"},{"paperId":"76bed13bac28090bd0498757cc0c02a0ddaaa28c","title":"Task formulation for Extracting Social Determinants of Health from Clinical Narratives"},{"paperId":"51c7ba41ab310f05f34d4e0b2bc777e554dd40e0","title":"A Survey on BERT and Its Applications"},{"paperId":"2c3c53aa7eca573d9e48e67fc87d98f8fc1ed908","title":"A Cross-Level Requirement Trace Link Update Model Based on Bidirectional Encoder Representations from Transformers"},{"paperId":"811bd3079580e93c5297c882d2c59ff39df308ac","title":"Knowledge-augmented Graph Neural Networks with Concept-aware Attention for Adverse Drug Event Detection"},{"paperId":"ad8991af165bb1ecd83251867aba392adfaa0033","title":"Cross-lingual Argument Mining in the Medical Domain"},{"paperId":"eb3f0b9a7ca8882d17328363f2eb182efda9a529","title":"Cross-lingual German Biomedical Information Extraction: from Zero-shot to Human-in-the-Loop"},{"paperId":"223a498fd673c0fc83b4db6883052f57dbbc4278","title":"Semi-Automated Construction of Food Composition Knowledge Base"},{"paperId":"1226d0618689ac3a262721c4ca5cd4d964a4919f","title":"CARES: A Corpus for classification of Spanish Radiological reports"},{"paperId":"216f281e37fe48dbbaa99bb3e08bd8c47783156a","title":"SPEC5G: A Dataset for 5G Cellular Network Protocol Analysis"},{"paperId":"e3f839b01567ae73af822a3da5e160dac2fb4708","title":"Adapting a Language Model While Preserving its General Knowledge"},{"paperId":"121c3831752cb92e1222d8d1d8cc529016845247","title":"Information Retrieval: Recent Advances and Beyond"}],"references":[{"paperId":"31fe8afa6531400e3b76982a3984c7e2605074f8","title":"Document-level attention-based BiLSTM-CRF incorporating disease dictionary for disease named entity recognition"},{"paperId":"2a567ebd78939d0861d788f0fedff8d40ae62bf2","title":"Publicly Available Clinical BERT Embeddings"},{"paperId":"fa82a552e9001ec95432f63fe4f3172dbba3beab","title":"The cell line ontology-based representation, integration and analysis of cell lines used in China"},{"paperId":"d08be34cb90718b331aa6574dc80b0370ca5895f","title":"A Silver Standard Corpus of Human Phenotype-Gene Relations"},{"paperId":"e65715d20baac11d0a53fd7107de18cb7f67e775","title":"MedMentions: A Large Biomedical Corpus Annotated with UMLS Concepts"},{"paperId":"fc33b11d0cbc6ec891aa5faa88e471bfa3dee361","title":"Clinical Concept Extraction with Contextual Word Embedding"},{"paperId":"f8b901c330e7f946ef93453b24682f294b8764a1","title":"In-domain Context-aware Token Embeddings Improve Biomedical Named Entity Recognition"},{"paperId":"14ad9d060c1e8f0449e697ee189ac346353fbfbc","title":"CollaboNet: collaboration of deep neural networks for biomedical named entity recognition"},{"paperId":"cc09d65183d3b986d1477df8584c96ecfee2b184","title":"Automatic extraction of gene-disease associations from literature using joint ensemble learning"},{"paperId":"2966e82ec5f89f23ec7636acc00c9ee74d491968","title":"Chemical–gene relation extraction using recursive neural network"},{"paperId":"fa4e9eb4020ef7152adbd62906d605d63236ef37","title":"Transfer learning for biomedical named entity recognition with neural networks"},{"paperId":"204a327e58a8594f701e0526fcc12d2c36aaaf4c","title":"An attention‐based BiLSTM‐CRF approach to document‐level chemical named entity recognition"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","title":"Deep Contextualized Word Representations"},{"paperId":"418cd8e078ed9b91b08e7915ac6753ed811053e4","title":"Cross-type Biomedical Named Entity Recognition with Deep Multi-Task Learning"},{"paperId":"d5400c4cc068eda5fd4c6f7c7f4bfcea41a5a1f8","title":"A Pilot Study of Biomedical Text Comprehension using an Attention-Based Deep Neural Reader: Design and Experimental Analysis"},{"paperId":"91c035165ee5f251c4a0c0b2af67d2891b404316","title":"NSML: A Machine Learning Platform That Enables You to Focus on Your Models"},{"paperId":"9223c95f0e600aee2dcf476094a5102adc386e0f","title":"Effective Use of Bidirectional Language Modeling for Transfer Learning in Biomedical Named Entity Recognition"},{"paperId":"c54e8c7a4f9c2ebd8787aecafa4cfdb35bfd49e0","title":"Effective Use of Bidirectional Language Modeling for Medical Named Entity Recognition"},{"paperId":"bc8fa64625d9189f5801837e7b133e7fe3c581f7","title":"Learned in Translation: Contextualized Word Vectors"},{"paperId":"db8562bb8dc69a7628c49c92ac8e08b5da91130e","title":"A transition‐based joint model for disease named entity recognition and normalization"},{"paperId":"91d4498849fe1966a629cddb187d3cd62eedb2ca","title":"Deep learning with word embeddings improves biomedical named entity recognition"},{"paperId":"4c16a6fd7b4aad8c1331e4753b30701fdf6d12f4","title":"Neural Domain Adaptation for Biomedical Question Answering"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"f27c51137c40940e2facc0ec932cf560967e1f5a","title":"ChimerDB 3.0: an enhanced database for fusion genes from cancer transcriptome and literature data mining"},{"paperId":"c6850869aa5e78a107c378d2e8bfa39633158c0c","title":"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"},{"paperId":"05dd7254b632376973f3a1b4d39485da17814df5","title":"SQuAD: 100,000+ Questions for Machine Comprehension of Text"},{"paperId":"61322ec6cfc54fe9723d4637239b8fb9938dc501","title":"BioCreative V CDR task corpus: a resource for chemical disease relation extraction"},{"paperId":"c4dd9a19d822c965ce8cde55ab23b8a0b628278a","title":"An overview of the BIOASQ large-scale biomedical semantic indexing and question answering competition"},{"paperId":"932117813af889104cafb457ebbc52a38d8b64eb","title":"The CHEMDNER corpus of chemicals and drugs and its annotation principles"},{"paperId":"f37e1b62a767a307c046404ca96bc140b3e68cb5","title":"GloVe: Global Vectors for Word Representation"},{"paperId":"cfb4edb7541fafcf593b466320c63ae32d27f57e","title":"Extraction of relations between genes and diseases from text and large-scale data analysis: implications for translational research"},{"paperId":"696753d59185436ec95ecf3021c413f353be4874","title":"NCBI disease corpus: A resource for disease name recognition and concept normalization"},{"paperId":"87f40e6f3022adbc1f1905e3e506abad05a9964f","title":"Distributed Representations of Words and Phrases and their Compositionality"},{"paperId":"c83d05b15797ade0f8dffb9a311a859682d43a27","title":"The SPECIES and ORGANISMS Resources for Fast and Accurate Identification of Taxonomic Names in Text"},{"paperId":"506c7e333efa0b31823c1b1914b1180c346773ee","title":"The EU-ADR corpus: Annotated drugs, diseases, targets, and their relationships"},{"paperId":"5e095981ebf4d389e9356bd56e59e0ade1b42e88","title":"2010 i2b2/VA challenge on concepts, assertions, and relations in clinical text"},{"paperId":"e1039346942874c59d89028bb934b72524827b8c","title":"LINNAEUS: A species name identification system for biomedical literature"},{"paperId":"f5a0c6593ba95d23c025608ce9280848da8b929f","title":"Overview of BioCreative II gene mention recognition"},{"paperId":"3bd4d2de49d8a092abb295b845dba14874f8787d","title":"Introduction to the Bio-entity Recognition Task at JNLPBA"},{"paperId":"28209ce8d0ac1cf4ceea3eeddf4630e1032fa0ef","title":"A neural probabilistic language model"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"afc317b098cd6744611049ff16f351032ab14f83","title":"A BERT-based Universal Model for Both Within- and Cross-sentence Clinical Temporal Relation Extraction"},{"paperId":"eed781f498b563df5a9e8a241c67d63dd1d92ad5","title":"Overview of the BioCreative VI chemical-protein interaction Track"},{"paperId":"2913c2bf3f92b5ae369400a42b2d27cc5bc05ecb","title":"Deep Learning"},{"paperId":"e2f28568031e1902d4f8ee818261f0f2c20de6dd","title":"Distributional Semantics Resources for Biomedical Text Processing"}],"id":"1e43c7084bdcb6b3102afaf301cce10faead2702","summary":"This article introduces BioBERT (Bidirectional Encoder Representations from Transformers for Biomedical Text Mining), which is a domain-specific language representation model pre-trained on large-scale biomedical corpora that largely outperforms BERT and previous state-of-the-art models in a variety of biomedical text mining tasks when pre- trained on biomedical Corpora."},{"url":"https://www.semanticscholar.org/paper/336ce63b472a65f053f854d45851d6f0e896f05e","title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":47,"citationCount":1161,"influentialCitationCount":10,"publicationDate":"30/01/2023","authors":"Junnan Li,Dongxu Li,S. Savarese,Steven Hoi","citations":[{"paperId":"5e7274bcda47b704b6797bb14be8b7a61c047a61","title":"Uncertainty-Aware Evaluation for Vision-Language Models"},{"paperId":"4b48fa99d17e130da58b0f72c2c4b50f72d2e146","title":"Video ReCap: Recursive Captioning of Hour-Long Videos"},{"paperId":"c4a1d57011307c575bfa6f41d0afe9dc75fed10b","title":"A Touch, Vision, and Language Dataset for Multimodal Alignment"},{"paperId":"5f0bb5267aef221e99c05c49dd30d488f94636dd","title":"Open3DSG: Open-Vocabulary 3D Scene Graphs from Point Clouds with Queryable Objects and Open-Set Relationships"},{"paperId":"d9f198541267870ae71087f120ea543ebc38d1c6","title":"Robust CLIP: Unsupervised Adversarial Fine-Tuning of Vision Embeddings for Robust Large Vision-Language Models"},{"paperId":"8bb714e389f46192a33c0f475e94f573cc58a3f9","title":"AICAttack: Adversarial Image Captioning Attack with Attention-Based Optimization"},{"paperId":"758c2dc290c037a6f211ec503beee70abe2d1197","title":"DriveVLM: The Convergence of Autonomous Driving and Large Vision-Language Models"},{"paperId":"d006b5eb0c52e0f4af3ec604d3f9b16f5695a534","title":"Pushing Auto-regressive Models for 3D Shape Generation at Capacity and Scalability"},{"paperId":"ba6643367e9150b3f4c2f3946a08e7e4f9267c35","title":"ALLaVA: Harnessing GPT4V-synthesized Data for A Lite Vision-Language Model"},{"paperId":"a1714677252a39d1835824efb185beb0113ca189","title":"Efficient Multimodal Learning from Data-centric Perspective"},{"paperId":"e3ca1f399ed5ee21d0b73422f8837fba0985665f","title":"OpenFMNav: Towards Open-Set Zero-Shot Object Navigation via Vision-Language Foundation Models"},{"paperId":"7f254928af6718c80224199e9cd915e7372ac54d","title":"RAG-Driver: Generalisable Driving Explanations with Retrieval-Augmented In-Context Learning in Multi-Modal Large Language Model"},{"paperId":"83d201d503b863fec7d1225f00a141e722e03f17","title":"Using Left and Right Brains Together: Towards Vision and Language Planning"},{"paperId":"7580327ffc9bd5daef83fe8285c0476ca074051d","title":"OmniMedVQA: A New Large-Scale Comprehensive Evaluation Benchmark for Medical LVLM"},{"paperId":"656d96cf9c17293dd8c9c5282a0b406856357206","title":"Visual Question Answering Instruction: Unlocking Multimodal Large Language Model To Domain-Specific Visual Multitasks"},{"paperId":"77ff5c9276bcf30b524a7cc030e0365a33b686bf","title":"PIN: Positional Insert Unlocks Object Localisation Abilities in VLMs"},{"paperId":"9f12a20f62238f5206520e52e83e2ccd1da17f03","title":"Test-Time Backdoor Attacks on Multimodal Large Language Models"},{"paperId":"a082c9c93b5cc0d38e7ac14c6c9dfe186bb5c824","title":"Mitigating Object Hallucination in Large Vision-Language Models via Classifier-Free Guidance"},{"paperId":"22affb27874444f65c32055d4a27f1d36eabbc19","title":"ChatGPT vs LLaMA: Impact, Reliability, and Challenges in Stack Overflow Discussions"},{"paperId":"cc216ce7bab105b10f088c43eb624f4c8640222d","title":"An Empirical Study Into What Matters for Calibrating Vision-Language Models"},{"paperId":"fb7820b0df2a6d53a1b112c629bf0dd90429214f","title":"SLIT: Boosting Audio-Text Pre-Training via Multi-Stage Learning and Instruction Tuning"},{"paperId":"3f77a630fde20d75963fb7f91fe1de42e78ec1aa","title":"Open-ended VQA benchmarking of Vision-Language models by exploiting Classification datasets and their semantic hierarchy"},{"paperId":"388b15ff565daa492932800ef7cf6b7042c2d286","title":"The Bias of Harmful Label Associations in Vision-Language Models"},{"paperId":"bcc2a6593c7667d341a36e76a0d9d40b4efdf787","title":"TransGPT: Multi-modal Generative Pre-trained Transformer for Transportation"},{"paperId":"95134b9fed1c769593a189ed0af5068d513b29b2","title":"Domain Adaptable Fine-Tune Distillation Framework For Advancing Farm Surveillance"},{"paperId":"3ca5d2f2483e658c2810d57d5ee00d85d00aa3db","title":"Large Language Models for Captioning and Retrieving Remote Sensing Images"},{"paperId":"1cfb7fba7194860e8b8818eb5e87e0a8e14e518a","title":"ViGoR: Improving Visual Grounding of Large Vision Language Models with Fine-Grained Reward Modeling"},{"paperId":"b8dbdb9eb9141562f7d48bc3709fee2d23f5e5bd","title":"LLaVA-Docent: Instruction Tuning with Multimodal Large Language Model to Support Art Appreciation Education"},{"paperId":"2d421d94bc9eed935870088a6f3218244e36dc97","title":"Question Aware Vision Transformer for Multimodal Reasoning"},{"paperId":"1254a2c223accad9e13dd639c51c1f81698bcaae","title":"CREMA: Multimodal Compositional Video Reasoning via Efficient Modular Adaptation and Fusion"},{"paperId":"deb1f198eea208fa637a8b4d3934f9ad4c8ed1b6","title":"CIC: A framework for Culturally-aware Image Captioning"},{"paperId":"ec8e2b45c4601730015608a58e33409224a81228","title":"SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models"},{"paperId":"59f3cf13401b9cc45d0e5ad7ea525e1eec84cce1","title":"Real-World Robot Applications of Foundation Models: A Review"},{"paperId":"0ed1a023637cddb8af1f290cb9b63fe7312e551f","title":"λ-ECLIPSE: Multi-Concept Personalized Text-to-Image Diffusion Models by Leveraging CLIP Latent Space"},{"paperId":"5d50c81ddd0ed89086de72f344cfbb658703cc25","title":"Convincing Rationales for Visual Question Answering Reasoning"},{"paperId":"8ac6eb0273849f0fe3d29d193cc5cac731000d17","title":"Dual-View Visual Contextualization for Web Navigation"},{"paperId":"04558479b899c206bf056f2353078a19c874d67a","title":"CAT-SAM: Conditional Tuning Network for Few-Shot Adaptation of Segmentation Anything Model"},{"paperId":"956c34e071a4b7ec348e37f1aeeeaf909d2cd6a9","title":"EVA-CLIP-18B: Scaling CLIP to 18 Billion Parameters"},{"paperId":"a091bf215c716a146140f81c751712db628c8e20","title":"MobileVLM V2: Faster and Stronger Baseline for Vision Language Model"},{"paperId":"c7a66961ee07b0e9e792d3625c1b20d510f29429","title":"CogCoM: Train Large Vision-Language Models Diving into Details through Chain of Manipulations"},{"paperId":"aabfdbd9db5ce9b1d598eae44e0d6250e7f0fc00","title":"MOMENT: A Family of Open Time-series Foundation Models"},{"paperId":"2a1168abf9e5bf1c4a9cfc5b4aab681c1479b110","title":"Scientific Language Modeling: A Quantitative Review of Large Language Models in Molecular Science"},{"paperId":"a9f9358a66be329ce97d92405c7428dba2353ed8","title":"Time-, Memory- and Parameter-Efficient Visual Adaptation"},{"paperId":"2b32f8c25f15344e27d9decbe9f2f15e4946c76f","title":"Visual Text Meets Low-level Vision: A Comprehensive Survey on Visual Text Processing"},{"paperId":"83ee82e62f2eae18cc3472120eb9004109895a31","title":"Delving into Multi-modal Multi-task Foundation Models for Road Scene Understanding: From Learning Paradigm Perspectives"},{"paperId":"4b1278b2266ce5009e70f2efe85ccff87350de9c","title":"Vision-Language Models Provide Promptable Representations for Reinforcement Learning"},{"paperId":"4b7ad761c08a7ffb191577b3d4fd4ff2d66a8a38","title":"V-IRL: Grounding Virtual Intelligence in Real Life"},{"paperId":"f1e4c1f2f65598709848ab4f7f08b332403f3f5f","title":"InstanceDiffusion: Instance-level Control for Image Generation"},{"paperId":"1b05e2243134099efb6d6bde5f2d4944869596a3","title":"LHRS-Bot: Empowering Remote Sensing with VGI-Enhanced Large Multimodal Language Model"},{"paperId":"d57dea679fae7cd5bca45adb882f2d334b495cfb","title":"GeReA: Question-Aware Prompt Captions for Knowledge-based Visual Question Answering"},{"paperId":"52f3338b8969517629097e745f66cb9eac5f99c5","title":"Advancing Graph Representation Learning with Large Language Models: A Comprehensive Survey of Techniques"},{"paperId":"0681c7ad009859c22e7638d4636d13e74cca08c6","title":"Image Fusion via Vision-Language Model"},{"paperId":"b3fd9f9245584ee41c0ba005cb262fd8f93ff3b5","title":"Skip \\n: A Simple Method to Reduce Hallucination in Large Vision-Language Models"},{"paperId":"60316dcf0c41799928cd46b4579db8e74fd9bb90","title":"A Survey for Foundation Models in Autonomous Driving"},{"paperId":"13df472c3fe81bf1b615238fbd7884c8b45d8d1c","title":"Large Language Models for Time Series: A Survey"},{"paperId":"da68013fd293594ad21eb1c3d838d06e691a556b","title":"The Role of Foundation Models in Neuro-Symbolic Learning and Reasoning"},{"paperId":"4ee7a9a6c42281fc53030e12934a44b0cd99cfe9","title":"Explaining latent representations of generative models with large multimodal models"},{"paperId":"844c50db2438cedff6ff9af987f3569dadd7e006","title":"Multimodal Embodied Interactive Agent for Cafe Scene"},{"paperId":"ebe4e298853acbe82ed5957f01210e7e38d28b9b","title":"AnimateLCM: Accelerating the Animation of Personalized Diffusion Models and Adapters with Decoupled Consistency Learning"},{"paperId":"fc4c380102d6f72657d1ab54dffd6be536bb01c7","title":"A Survey on Hallucination in Large Vision-Language Models"},{"paperId":"c914d62120c30e6f2aa1bcd4dd8a2ccc1042e824","title":"Controllable Dense Captioner with Multimodal Embedding Bridging"},{"paperId":"9e9baf1a5b76e3e9e09a06b6e1dd2c7e0f0a8402","title":"Binding Touch to Everything: Learning Unified Multimodal Tactile Representations"},{"paperId":"1bd350197e85fb9f80d8325a4c64606a86995aa4","title":"SpeechComposer: Unifying Multiple Speech Tasks with Prompt Composition"},{"paperId":"b738b46b2deccd72f0dea5c93d33e9fbd69d51f7","title":"PVLR: Prompt-driven Visual-Linguistic Representation Learning for Multi-Label Image Recognition"},{"paperId":"8351e835cc70caf928f2059919a7b4018661faac","title":"M2-RAAP: A Multi-Modal Recipe for Advancing Adaptation-based Pre-training towards Effective and Efficient Zero-shot Video-text Retrieval"},{"paperId":"6b3d58d367b049532c0dd7a203eb0c17c94e734e","title":"Image Anything: Towards Reasoning-coherent and Training-free Multi-modal Image Generation"},{"paperId":"59f4322ff656c81e9767c78e8599dabccebb5b5c","title":"Are Generative AI systems Capable of Supporting Information Needs of Patients?"},{"paperId":"8c888fb89f1b2bbd66f95e39899a07cf2100fa38","title":"EarthGPT: A Universal Multi-modal Large Language Model for Multi-sensor Image Comprehension in Remote Sensing Domain"},{"paperId":"7b24bdef2b04139f99e0773e5298a55bc07df6c5","title":"Multi-granularity Correspondence Learning from Long-term Noisy Videos"},{"paperId":"ab003ca4c9479d1b155f8da9505160e8c07e83ce","title":"MouSi: Poly-Visual-Expert Vision-Language Models"},{"paperId":"16c816201eb629a758676a3c1fa4cead50e5f412","title":"When Large Language Models Meet Vector Databases: A Survey"},{"paperId":"5b6622e03e20ae1acbcd9390f98d7739c7fc6517","title":"VIALM: A Survey and Benchmark of Visually Impaired Assistance with Large Models"},{"paperId":"6d9609ccf84e0ecb2d59844b5135e77933939003","title":"TIP-Editor: An Accurate 3D Editor Following Both Text-Prompts And Image-Prompts"},{"paperId":"4d5262260022d8baff65242c6eb879d184447d5f","title":"CMMU: A Benchmark for Chinese Multi-modal Multi-type Question Understanding and Reasoning"},{"paperId":"be50b554fd2fb37982bae3d0e359f1520b8665f9","title":"Sketch2NeRF: Multi-view Sketch-guided Text-to-3D Generation"},{"paperId":"3238761b4e0b3afdb1655cd996285adeebbdd7af","title":"LanDA: Language-Guided Multi-Source Domain Adaptation"},{"paperId":"f58a49ea99e9a2754dd050927b9d830e3743f844","title":"Towards 3D Molecule-Text Interpretation in Language Models"},{"paperId":"3c3ed4f438930ee01b5d2812f3df578ceb36fa7f","title":"BootPIG: Bootstrapping Zero-shot Personalized Image Generation Capabilities in Pretrained Diffusion Models"},{"paperId":"f554b22d2ccf786a6d61d5858f43024ba9115e15","title":"VisualWebArena: Evaluating Multimodal Agents on Realistic Visual Web Tasks"},{"paperId":"1bdbb7b4d657a2fd3e9f64fc689a14a8c94ebfaf","title":"ChatterBox: Multi-round Multimodal Referring and Grounding"},{"paperId":"086168c2ad02930279fe481a52641a565b55c4a0","title":"Democratizing Fine-grained Visual Recognition with Large Language Models"},{"paperId":"41f12456780aecd204a210ce04b1a92d022b8c4c","title":"Small Language Model Meets with Reinforced Vision Vocabulary"},{"paperId":"c00a86523bcacf4db91d9970e1957e78152cb103","title":"Zero Shot Open-ended Video Inference"},{"paperId":"6ed96d6822a06ad9a735bc09e301bf41df61c534","title":"CheXagent: Towards a Foundation Model for Chest X-Ray Interpretation"},{"paperId":"1c097b50dd7601b5bf916973f5f6482bf3ddbf3e","title":"CoAVT: A Cognition-Inspired Unified Audio-Visual-Text Pre-Training Model for Multimodal Processing"},{"paperId":"fb60a8655b3b174aa34a5ffe0ee72ca732d287e4","title":"LLMRA: Multi-modal Large Language Model based Restoration Assistant"},{"paperId":"61ea0a87eab0029de9f4f6032108cb8d94cca3ac","title":"Inducing High Energy-Latency of Large Vision-Language Models with Verbose Images"},{"paperId":"1afb89d948f88f757825120cb53c4695518d2cdd","title":"UltrAvatar: A Realistic Animatable 3D Avatar Diffusion Model with Authenticity Guided Textures"},{"paperId":"1f8dccfe4a7a959e5e08e6be399f3aa2770790ca","title":"Toward Robust Multimodal Learning using Multimodal Foundational Models"},{"paperId":"333381155dfb30e3e05886835b1217c63f1a8100","title":"Weakly Supervised Gaussian Contrastive Grounding with Large Multimodal Models for Video Question Answering"},{"paperId":"0be1c71b1710f01fb5d321e9b1459a7d2a7cdaf2","title":"Q&A Prompts: Discovering Rich Visual Clues through Mining Question-Answer Prompts for VQA requiring Diverse World Knowledge"},{"paperId":"111359fbc0fe744f969cb0f1b66ae3e2bf4e8685","title":"Large Language Models are Efficient Learners of Noise-Robust Speech Recognition"},{"paperId":"4f2a56102bcbf0fe79379c4c27daecbccfb35a26","title":"MLLM-Tool: A Multimodal Large Language Model For Tool Agent Learning"},{"paperId":"616e98ba9e60f36c6ee226cc66c787610f0bbb62","title":"MM-Interleaved: Interleaved Image-Text Generative Modeling via Multi-modal Feature Synchronizer"},{"paperId":"9c20d8d5cfc60f5b9aa058ff2968563f2af33398","title":"Temporal Insight Enhancement: Mitigating Temporal Hallucination in Multimodal Large Language Models"},{"paperId":"0a8a776054a087118f4f9523994ef084b2b2469a","title":"Advancing Large Multi-modal Models with Explicit Chain-of-Reasoning and Visual Question Generation"},{"paperId":"14ed0a54cb784d939a142beddad63724fa052f36","title":"SkyEyeGPT: Unifying Remote Sensing Vision-Language Tasks via Instruction Tuning with Large Language Model"},{"paperId":"071df7085c03824246d4fbf8cbbf1c616b7058fb","title":"Supervised Fine-tuning in turn Improves Visual Foundation Models"},{"paperId":"38c48a1cd296d16dc9c56717495d6e44cc354444","title":"Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model"},{"paperId":"bd7271fd7f595f66c47eaee28d5f556731882a18","title":"Beyond Anti-Forgetting: Multimodal Continual Instruction Tuning with Positive Forward Transfer"},{"paperId":"eca69cf6fd3d439ce86e44a1a056452ac3e4ed5f","title":"SceneVerse: Scaling 3D Vision-Language Learning for Grounded Scene Understanding"},{"paperId":"b0eed53f6733150941fe18bc0f7ccd0f6b4e9f7a","title":"COCO is \"ALL\" You Need for Visual Instruction Fine-tuning"},{"paperId":"4a48d628e53f554eb6ef09a457ca855188b96171","title":"DoraemonGPT: Toward Understanding Dynamic Scenes with Large Language Models"},{"paperId":"dd319bce44a9ca0ef3309fe9d5191c1b59c1b4f5","title":"SCoFT: Self-Contrastive Fine-Tuning for Equitable Image Generation"},{"paperId":"fc44959f1259cbaf7c474f13a4c9f6889125b91d","title":"Self-Imagine: Effective Unimodal Reasoning with Multimodal Models using Self-Imagination"},{"paperId":"fe07897aa2af4d25e1847577c6198e9bc72f2f5c","title":"MultiPLY: A Multisensory Object-Centric Embodied Large Language Model in 3D World"},{"paperId":"bfc868a624f5539af7eb5ec8c02f8eaa1d2b562c","title":"VeCAF: VLM-empowered Collaborative Active Finetuning with Training Objective Awareness"},{"paperId":"c5a3b7f3d0203f001621e48a52e24c029963a2a3","title":"Towards A Better Metric for Text-to-Video Generation"},{"paperId":"79d8ee84273ff75842b44eab7cd2236f8f7d25f1","title":"FiGCLIP: Fine-Grained CLIP Adaptation via Densely Annotated Videos"},{"paperId":"0f9b66c9208b11369e9d94d85b7dc23bcc5115e9","title":"InstantID: Zero-shot Identity-Preserving Generation in Seconds"},{"paperId":"281f345ac786b79623e0c0ad81043a419463d971","title":"MapGPT: Map-Guided Prompting with Adaptive Path Planning for Vision-and-Language Navigation"},{"paperId":"45151ac2726bb3bc7e545441a458fd4afb1bba48","title":"UMG-CLIP: A Unified Multi-Granularity Vision Generalist for Open-World Understanding"},{"paperId":"46e32d4b037968a10a6a5f531731e4b514ae6cd2","title":"AffordanceLLM: Grounding Affordance from Vision Language Models"},{"paperId":"5502d769595981009e43344f8914e287acca2359","title":"ModaVerse: Efficiently Transforming Modalities with LLMs"},{"paperId":"8a83ea7c69a0b410f1983ab1d14d3532e4d1bfe3","title":"An Axiomatic Approach to Model-Agnostic Concept Explanations"},{"paperId":"1b959bafead388e9815305471b4e5b607c9f4b82","title":"Distilling Vision-Language Models on Millions of Videos"},{"paperId":"c1a28653b28b3d75de4dca940a33082564d3a2bf","title":"Video Anomaly Detection and Explanation via Large Language Models"},{"paperId":"c1a2e03ac42586d574b29f97ca3dfaa171f8961f","title":"REBUS: A Robust Evaluation Benchmark of Understanding Symbols"},{"paperId":"ca00f4056f9039d3c1a4c3a113f5ee0527149b66","title":"Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs"},{"paperId":"af5f256e9771bf9cd02451195e3a7ac693fde3ed","title":"Exploring the Reasoning Abilities of Multimodal Large Language Models (MLLMs): A Comprehensive Survey on Emerging Trends in Multimodal Reasoning"},{"paperId":"8c972787aae08796069e4fc6b2752f83a87fd198","title":"Generating Diverse and High-Quality Texts by Minimum Bayes Risk Decoding"},{"paperId":"dad2852ee397ae3edd28ad9d3c28e759a3bd93d2","title":"Diffusion Priors for Dynamic View Synthesis from Monocular Videos"},{"paperId":"3791dad1e3c1eec45a3833e5264574a308cc0014","title":"An EcoSage Assistant: Towards Building A Multimodal Plant Care Dialogue Assistant"},{"paperId":"5714bd69ffc2e377bcf370c23aec7e6991891762","title":"Uni3D-LLM: Unifying Point Cloud Perception, Generation and Editing with Large Language Models"},{"paperId":"02d91501d59fee66216aabfb64f800bd7dcaee16","title":"Low-Resource Vision Challenges for Foundation Models"},{"paperId":"7294c426b8a95975ca932eaf8f700acdd3d950b2","title":"Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models"},{"paperId":"cdb6423611354ee5f572f829bffd37a95e42c3f8","title":"Large-scale Generative AI Models Lack Visual Number Sense"},{"paperId":"3f19484f941f209f45a51b1b69160e24e9b9dc99","title":"GPT-4V(ision) is a Human-Aligned Evaluator for Text-to-3D Generation"},{"paperId":"27d1615f847b3429d947a07ea734426c958b2026","title":"Language-Conditioned Robotic Manipulation with Fast and Slow Thinking"},{"paperId":"002d2c4569d070a55fe69c25ebccad8e9ddae572","title":"Incorporating Visual Experts to Resolve the Information Loss in Multimodal Large Language Models"},{"paperId":"c21781a9ebb3da9dfc318259633f6eaea8e8447f","title":"CaMML: Context-Aware Multimodal Learner for Large Models"},{"paperId":"60afc279e2c6d1e82b5ef5399548e18270aa8f75","title":"Hyperparameter-Free Approach for Faster Minimum Bayes Risk Decoding"},{"paperId":"904cbad95be3cc3e2a7cf48b333a8a37a44a30e9","title":"Open-Vocabulary SAM: Segment and Recognize Twenty-thousand Classes Interactively"},{"paperId":"420087f314633a381e61e6c5cd73ccc2070a749e","title":"PeFoMed: Parameter Efficient Fine-tuning on Multimodal Large Language Models for Medical Visual Question Answering"},{"paperId":"e7e76e0af01c7f7cbf61c2544a3f0a58794bbf11","title":"ChartAssisstant: A Universal Chart Multimodal Language Model via Chart-to-Table Pre-training and Multitask Instruction Tuning"},{"paperId":"ece33ee67d74c29cd2a83c505e5bf0b818f9c2a1","title":"LLaVA-Phi: Efficient Multi-Modal Assistant with Small Language Model"},{"paperId":"0833d5b8c46fe7391e7decc3f11fea87e023d710","title":"SyCoCa: Symmetrizing Contrastive Captioners with Attentive Masking for Multimodal Alignment"},{"paperId":"2ee7301efe2829a8f34bbc70006e45f461092af8","title":"VASE: Object-Centric Appearance and Shape Manipulation of Real Videos"},{"paperId":"c844694387a89a477e7a8bbf918171cdc3b85672","title":"GPT-4V(ision) is a Generalist Web Agent, if Grounded"},{"paperId":"fc7feeaddc5a38c0d6f0d793737584e5f0bb7519","title":"Towards Truly Zero-shot Compositional Visual Reasoning with LLMs as Programmers"},{"paperId":"516dafd778d6dcce0ef04ef0539257976b897d24","title":"Incorporating Geo-Diverse Knowledge into Prompting for Increased Geographical Robustness in Object Recognition"},{"paperId":"cd49101103f73d88a4a3b368898066f03984c339","title":"Holistic Autonomous Driving Understanding by Bird's-Eye-View Injected Multi-Modal Large Models"},{"paperId":"9f994a4f02c7cc3512fd1a8c31ca3a5d1aa826f7","title":"BEV-CLIP: Multi-modal BEV Retrieval Methodology for Complex Scene in Autonomous Driving"},{"paperId":"65161d4c0a245aa40a23e9706b7a03afad1a7f63","title":"Enhancing Surveillance Systems: Integration of Object, Behavior, and Space Information in Captions for Advanced Risk Assessment"},{"paperId":"b1b29e274aa116826f8e82177415f40743ba0392","title":"COSMO: COntrastive Streamlined MultimOdal Model with Interleaved Pre-Training"},{"paperId":"ed40245e716164818131826f5f1ecdc089ba8e6a","title":"GD^2-NeRF: Generative Detail Compensation via GAN and Diffusion for One-shot Generalizable Neural Radiance Fields"},{"paperId":"449be2f55abca6ec62bdba9c28fc221e4abab281","title":"Images, Words, and Imagination: Accessible Descriptions to Support Blind and Low Vision Art Exploration and Engagement"},{"paperId":"356cc7533e562b4cc56fad93fade0e77e829d6c4","title":"E-chat: Emotion-sensitive Spoken Dialogue System with Large Language Models"},{"paperId":"64c00822dea4fe7180042d745109be30ce1e7c30","title":"Brain-Conditional Multimodal Synthesis: A Survey and Taxonomy"},{"paperId":"575f403261d5f99526f0b4dfc8644352d6c4467a","title":"DocLLM: A layout-aware generative language model for multimodal document understanding"},{"paperId":"5f58863dd6474d6f127be995b5871e7c60f2792f","title":"Video Understanding with Large Language Models: A Survey"},{"paperId":"061f91f1740ba6ee860d38f8637cb6b1b98bfb10","title":"Tracking with Human-Intent Reasoning"},{"paperId":"074ab652cf539d695aabd9d5fe07c69386deb8da","title":"MIVC: Multiple Instance Visual Component for Visual-Language Models"},{"paperId":"e36f79b2bfbf50cf6362cc563a6e5c261e3f0615","title":"MobileVLM : A Fast, Strong and Open Vision Language Assistant for Mobile Devices"},{"paperId":"4641fe56cd44144b6cabea583233ed952f97f4c0","title":"A Simple LLM Framework for Long-Range Video Question-Answering"},{"paperId":"7d63931765a950cca8aadc03a2f3d3c357158f91","title":"EFHQ: Multi-purpose ExtremePose-Face-HQ dataset"},{"paperId":"c6495e41d3974476e0be9d5137c3dda51125bcec","title":"Improving Image Restoration through Removing Degradations in Textual Representations"},{"paperId":"46f64681d7a0c7f80303bebb0d62a3b7acbcdcd9","title":"An Improved Baseline for Reasoning Segmentation with Large Language Model"},{"paperId":"f2b6323973955f9a1ebb9be76a616991de3d3a8f","title":"ChartBench: A Benchmark for Complex Visual Reasoning in Charts"},{"paperId":"93020bac4336453b694d589c41f3b7420baf7ecb","title":"Masked Contrastive Reconstruction for Cross-modal Medical Image-Report Retrieval"},{"paperId":"45251c730fc47bce93bc19f38a2918b4af2c1ead","title":"LeanVec: Search your vectors faster by making them fit"},{"paperId":"d24c6fbdf81824065eeaa866021bff4ce0c40b87","title":"SSR-Encoder: Encoding Selective Subject Representation for Subject-Driven Generation"},{"paperId":"5acbf917da5be89e4eebd7e98c81d87069450a5d","title":"Cloud-Device Collaborative Learning for Multimodal Large Language Models"},{"paperId":"82dc8edc49f9edf4a53056aedcdfb339be070166","title":"IQAGPT: Image Quality Assessment with Vision-language and ChatGPT Models"},{"paperId":"6a9517806d7e0f92cdb8d249081829aceae0b3e7","title":"Voila-A: Aligning Vision-Language Models with User's Gaze Attention"},{"paperId":"b137709522bc70b42b026cae192de2a45000b22e","title":"MetaAID 2.5: A Secure Framework for Developing Metaverse Applications via Large Language Models"},{"paperId":"466134a53cd65ded957ce2d102a10eed3170e0e7","title":"Harnessing Diffusion Models for Visual Perception with Meta Prompts"},{"paperId":"a5b4f03ae8602f563d0e76a7c9c43a2be77f770d","title":"Environment-Specific People"},{"paperId":"0ca80b1244ee5419ff9108fbf39aa59f24fdfc4c","title":"FoodLMM: A Versatile Food Assistant using Large Multi-modal Model"},{"paperId":"3c8cc9a5ee373d51e0bf71621b6eb6901c762e8f","title":"DriveLM: Driving with Graph Visual Question Answering"},{"paperId":"6a33e58ef961a3a0a5657518b2be86395eb7c8d0","title":"InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks"},{"paperId":"5edf706467dc76cd09319592d18db0ad4e1fb64d","title":"LiDAR-LLM: Exploring the Potential of Large Language Models for 3D LiDAR Understanding"},{"paperId":"c672ec79f55cef8f7a32cd8dddfa981b893f1567","title":"V*: Guided Visual Search as a Core Mechanism in Multimodal LLMs"},{"paperId":"185121b33189cfd5fa485418b2ebfe2491abf099","title":"LingoQA: Video Question Answering for Autonomous Driving"},{"paperId":"cfed151248243502fcb8c3b019f0f816a6109fa0","title":"Free-Editor: Zero-shot Text-driven 3D Scene Editing"},{"paperId":"fc199c9a84522feeb7017d2f769bfd2b437e5843","title":"A Strong Baseline for Temporal Video-Text Alignment"},{"paperId":"b2e650f7f3348ee4fa12798c0d8ecfcf479dc328","title":"DreamTuner: Single Image is Enough for Subject-Driven Generation"},{"paperId":"196d87448d10c58db59f62f29b1257888ace4cc9","title":"ZeroShape: Regression-based Zero-shot Shape Reconstruction"},{"paperId":"1c9bab7ab072c619133c936b5b85160e5373e638","title":"VCoder: Versatile Vision Encoders for Multimodal Large Language Models"},{"paperId":"5fb5b87d7a9a42a38afb3e942ce9ef7664b3eeab","title":"Text-Conditioned Resampler For Long Form Video Understanding"},{"paperId":"17a32c825bd746a2625eddc2728092171a9ef72a","title":"Jack of All Tasks, Master of Many: Designing General-purpose Coarse-to-Fine Vision-Language Model"},{"paperId":"2d4a853affeb0b164fc1134df612aea658f36459","title":"Mixture of Cluster-conditional LoRA Experts for Vision-language Instruction Tuning"},{"paperId":"dc6a7257c19be0b48d200bcb66c493c4ac07a632","title":"Mask Grounding for Referring Image Segmentation"},{"paperId":"25fdd7247b042f6976baeae258b69f910ecaf9d9","title":"VQA4CIR: Boosting Composed Image Retrieval with Visual Question Answering"},{"paperId":"fcf66b0af76ce5206139d5a994d18372b013b2ea","title":"Learning Object State Changes in Videos: An Open-World Perspective"},{"paperId":"61981d5de69fd148e0a06b6740b44ce886d6d4aa","title":"Atlantis: Enabling Underwater Depth Estimation with Stable Diffusion"},{"paperId":"bf532b2207542384c7b5740f258934d62ac8aa60","title":"The Good, The Bad, and Why: Unveiling Emotions in Generative AI"},{"paperId":"46f9f7b8f88f72e12cbdb21e3311f995eb6e65c5","title":"Retrieval-Augmented Generation for Large Language Models: A Survey"},{"paperId":"0825ef213b4fdeadc8b27fb5a6361bb84d57bc8b","title":"Tuning LayerNorm in Attention: Towards Efficient Multi-Modal LLM Finetuning"},{"paperId":"5f9f4f1ad5a6c3d4fdc3b6e7f2c2b9e100bb5fcb","title":"Unleashing the Power of CNN and Transformer for Balanced RGB-Event Video Recognition"},{"paperId":"998b2f12cd0f5f0cf57f3c3a81e885dd27cc7a85","title":"VolumeDiffusion: Flexible Text-to-3D Generation with Efficient Volumetric Encoder"},{"paperId":"c2f1a101eb1c54cab321c47ce4f3b4b416f4786a","title":"M3DBench: Let's Instruct Large Models with Multi-modal 3D Prompts"},{"paperId":"60d3ade5c0085f5de1f5ab944cc058c78706ac66","title":"StarVector: Generating Scalable Vector Graphics Code from Images"},{"paperId":"2b14d9e190022e388476ebb24eb1a84349ca0de4","title":"Silkie: Preference Distillation for Large Visual Language Models"},{"paperId":"493a931db01812489c04b04fec525e6bb44d83de","title":"M2ConceptBase: A Fine-grained Aligned Multi-modal Conceptual Knowledge Base"},{"paperId":"c42c25b81dfe9b6745a25b01396e3e2309ba4dae","title":"Shot2Story20K: A New Benchmark for Comprehensive Understanding of Multi-shot Videos"},{"paperId":"852ab98a42204ea905ba41b3b1354820be4f201c","title":"When Parameter-efficient Tuning Meets General-purpose Vision-language Models"},{"paperId":"3c5fa29b3c1c55cfd7a4ed87a4a484dd6a488eb2","title":"Data-Efficient Multimodal Fusion on a Single GPU"},{"paperId":"95d791ad14db2c779daa67ca7fdc3a75214c42eb","title":"3DAxiesPrompts: Unleashing the 3D Spatial Task Capabilities of GPT-4V"},{"paperId":"b5503967d39557a77c70076c308183e92d6d775a","title":"Osprey: Pixel Understanding with Visual Instruction Tuning"},{"paperId":"3cc0f0b40589f4e0f971280cad48919048d22a41","title":"GSVA: Generalized Segmentation via Multimodal Large Language Models"},{"paperId":"faee13d1bf76b43fafb641055b1c3d475e7f6c2d","title":"Customize-It-3D: High-Quality 3D Creation from A Single Image Using Subject-Specific Knowledge Prior"},{"paperId":"529880cc9d8ac13f63f4263932db704f1b9553e7","title":"WAVER: Writing-style Agnostic Text-Video Retrieval via Distilling Vision-Language Models Through Open-Vocabulary Knowledge"},{"paperId":"62ec2bbf7fa85a2a71838587cc59eb65dd22d40e","title":"Training-free Zero-shot Composed Image Retrieval with Local Concept Reranking"},{"paperId":"6768a6aeb61ad8522795d92bb0ca44f87a327a59","title":"Pixel Aligned Language Models"},{"paperId":"ea6982a936a2b263bbf46ff6eb27fc0b63fddaf7","title":"VL-GPT: A Generative Pre-trained Transformer for Vision and Language Understanding and Generation"},{"paperId":"6d2ab31aa75468f5458b9d96192c3f4a28f55d73","title":"DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving"},{"paperId":"6140211405f9917ded519da50f00eee989eabd7f","title":"Toward General-Purpose Robots via Foundation Models: A Survey and Meta-Analysis"},{"paperId":"a17fe25540a96782cd1f24d7be512f7516359a7f","title":"A Picture is Worth More Than 77 Text Tokens: Evaluating CLIP-Style Models on Dense Captions"},{"paperId":"81c34355f585f40e827f79acaccbcc885ed133a0","title":"Depicting Beyond Scores: Advancing Image Quality Assessment through Multi-modal Language Models"},{"paperId":"55c6d16b550c606d62dd85084f0d373d8f087966","title":"VLAP: Efficient Video-Language Alignment via Frame Prompting and Distilling for Video Question Answering"},{"paperId":"8380f2082dbe3effead38b7b32e9615d6c30b381","title":"Chat-3D v2: Bridging 3D Scene and Large Language Models with Object Identifiers"},{"paperId":"2a76a0dbb9bb9997536b2b3b42c28a01ad27e5cf","title":"Modality Plug-and-Play: Elastic Modality Adaptation in Multimodal LLMs for Embodied AI"},{"paperId":"36962295d8a69b08038282307b8f3f3bb90ad3f1","title":"EVP: Enhanced Visual Perception using Inverse Multi-Attentive Feature Refinement and Regularized Image-Text Alignment"},{"paperId":"559ec2f23e7b65825b614346bdbabdfd8b56a667","title":"Assessing GPT4-V on Structured Reasoning Tasks"},{"paperId":"10280c290825fc0b0c884e988f4f1dedb80e4e80","title":"ToViLaG: Your Visual-Language Generative Model is Also An Evildoer"},{"paperId":"2141ed804636a1cf339d606cd03fd3b3e9582133","title":"VILA: On Pre-training for Visual Language Models"},{"paperId":"edf9c0cfb6c1d08dceca0f988e5b1876e986dd26","title":"A Simple Recipe for Contrastively Pre-training Video-First Encoders Beyond 16 Frames"},{"paperId":"33c3dbf86dd6e338e2a49bba9c2e2193d1eca08a","title":"Vista-LLaMA: Reliable Video Narrator via Equal Distance to Visual Tokens"},{"paperId":"b5bb1ac9935f377a8f8ae8e6360879833e8caff8","title":"Interfacing Foundation Models' Embeddings"},{"paperId":"ffa7e52470437d61110b5498fcfa6d4d6e18b78c","title":"Turbo: Informativity-Driven Acceleration Plug-In for Vision-Language Models"},{"paperId":"d1f925c65d56ff4de5d317a54d47d6df34b17d4e","title":"Hallucination Augmented Contrastive Learning for Multimodal Large Language Model"},{"paperId":"e0b05e314372ed580d9612ef5f0ee672b17ad2e4","title":"LMDrive: Closed-Loop End-to-End Driving with Large Language Models"},{"paperId":"3f622f71276ffdeca771f0f3758f3974d3a18f28","title":"Remote Sensing Vision-Language Foundation Models without Annotations via Ground Remote Alignment"},{"paperId":"de4c362339afc7e070bd4250de3dcb06b32a46fc","title":"ThinkBot: Embodied Instruction Following with Thought Chain Reasoning"},{"paperId":"b240a1d8ec2860bdd7370daa3144268ce46ac018","title":"Vary: Scaling up the Vision Vocabulary for Large Vision-Language Models"},{"paperId":"593caf35212638a4bf865873d6af5c6b5ab6dccb","title":"Converting and Smoothing False Negatives for Vision-Language Pre-training"},{"paperId":"4f5654ec1dfc04478be42d03eee8e6db6bd9ca14","title":"Honeybee: Locality-enhanced Projector for Multimodal LLM"},{"paperId":"1c99cc6568c2038a539f3672b3f951dc16c47c5b","title":"AnyHome: Open-Vocabulary Generation of Structured and Textured 3D Homes"},{"paperId":"257eaf21e6e6926e9693e04e946ec2199b3afee1","title":"NuScenes-MQA: Integrated Evaluation of Captions and QA for Autonomous Driving Datasets using Markup Annotations"},{"paperId":"369b34826e23cb43bea9a91395e9603eacfa7420","title":"EgoPlan-Bench: Benchmarking Egocentric Embodied Planning with Multimodal Large Language Models"},{"paperId":"500bf7fbd979855fea47758197c05258a7d24fbd","title":"Compress&Align: Curating Image-Text Data with Human Knowledge"},{"paperId":"514f7237a57909aef36479f6bea8a727dd00b1b9","title":"InstructAny2Pix: Flexible Visual Editing via Multimodal Instruction Following"},{"paperId":"1ae9afce62c60fd0bfc9f5b57f8d8a1bbc3641eb","title":"Audio-Visual LLM for Video Understanding"},{"paperId":"388b0f44faf0a14cc402c2554ec36a868cf59129","title":"SmartEdit: Exploring Complex Instruction-based Image Editing with Multimodal Large Language Models"},{"paperId":"cb2295766b2f8f35524f6a9f93ae39d948d50bd4","title":"Genixer: Empowering Multimodal Large Language Models as a Powerful Data Generator"},{"paperId":"cd6402234500f37286e52811dbdbfb87b557a437","title":"Large Scale Foundation Models for Intelligent Manufacturing Applications: A Survey"},{"paperId":"6e8a92479981afec0610414d0501d8ef56c555dc","title":"NLLG Quarterly arXiv Report 09/23: What are the most influential current AI Papers?"},{"paperId":"e723e4a63687f9ee372b1e684a23cbd138bda5f1","title":"FT2TF: First-Person Statement Text-To-Talking Face Generation"},{"paperId":"4923e93f043091da51ac41b039be1698a1158d80","title":"Causal-CoG: A Causal-Effect Look at Context Generation for Boosting Multi-modal Language Models"},{"paperId":"1a4d681abbaa68366544b154104b7725b8fe8b6a","title":"MoVQA: A Benchmark of Versatile Question-Answering for Long-Form Movie Understanding"},{"paperId":"5c17fa02a4a4c0655b1873cf3e34fffbc1e7d601","title":"Localized Symbolic Knowledge Distillation for Visual Commonsense Models"},{"paperId":"d23f08611ea1e64f691ecddee1f7f48c8015eea6","title":"Lyrics: Boosting Fine-grained Language-Vision Alignment and Comprehension via Semantic-aware Visual Objects"},{"paperId":"89c3f28292c0f68960c6b1f3c8639768addc491a","title":"Retrieval-based Video Language Model for Efficient Long Video Question Answering"},{"paperId":"e7ceec6f384db0ddf8c2b387c5172dee28326567","title":"ControlRoom3D: Room Generation using Semantic Proxy Rooms"},{"paperId":"24df8cdc293ac8857b4a3afaeec0865b3d86a5fd","title":"User-Aware Prefix-Tuning is a Good Learner for Personalized Image Captioning"},{"paperId":"0a36008613d67fb3aac8345f847fc4787a0d69f3","title":"PixLore: A Dataset-driven Approach to Rich Image Captioning"},{"paperId":"21a2a2e96e5ba50140507cbf4d42b69756ae109b","title":"Improving Medical Report Generation with Adapter Tuning and Knowledge Enhancement in Vision-Language Foundation Models"},{"paperId":"68ca15f5f5a4254e243f630b7d03a529ff0c5b4b","title":"Style Transfer to Calvin and Hobbes comics using Stable Diffusion"},{"paperId":"87af368c458c43bb050550d05261a4195f821b63","title":"Scaling Laws of Synthetic Images for Model Training ... for Now"},{"paperId":"50dfd272a4a7da2b643f80064eec6192d0e7279c","title":"PhotoMaker: Customizing Realistic Human Photos via Stacked ID Embedding"},{"paperId":"ffc69246cc4b6c52efc02f22389dc8bff5ceaad0","title":"Dream2Real: Zero-Shot 3D Object Rearrangement with Vision-Language Models"},{"paperId":"fb064f8376eba221245c551cc028e4dbcb9f043e","title":"Prompt Highlighter: Interactive Control for Multi-Modal LLMs"},{"paperId":"205ada2927972cb3156e94247e29d454bf620399","title":"Towards Knowledge-driven Autonomous Driving"},{"paperId":"4d1e0e68577782e9b64449a146969f41abe21992","title":"Improved Visual Grounding through Self-Consistent Explanations"},{"paperId":"994a60cae4d2c4ad1c1a3d5727f96a07dc64316a","title":"Text-to-3D Generation with Bidirectional Diffusion using both 2D and 3D priors"},{"paperId":"4ece984342174620705c0ba1186ea4a97ff76052","title":"LifelongMemory: Leveraging LLMs for Answering Queries in Egocentric Videos"},{"paperId":"d198a5a1a0c6e31bd0ad70658c8c2a74b8753aed","title":"Alpha-CLIP: A CLIP Model Focusing on Wherever You Want"},{"paperId":"98de0a73fc32e04b58d76579aef964cf686b25da","title":"Reason2Drive: Towards Interpretable and Chain-based Reasoning for Autonomous Driving"},{"paperId":"53c3c3984649ca82a2f85629dae01087e9e72991","title":"OneLLM: One Framework to Align All Modalities with Language"},{"paperId":"44999d9bae3976d38e91b742c4541a21c7000260","title":"MotionCtrl: A Unified and Flexible Motion Controller for Video Generation"},{"paperId":"996fd7229e4aba2c9c33a6f2861e8020b8a2702e","title":"VLFM: Vision-Language Frontier Maps for Zero-Shot Semantic Navigation"},{"paperId":"8e125a0dd098c23e91abb0613ca42f9623edd3c3","title":"Language-Informed Visual Concept Learning"},{"paperId":"8f51370df2913eb4a83f496d932fd75b03794c90","title":"UFineBench: Towards Text-based Person Retrieval with Ultra-fine Granularity"},{"paperId":"c708931122d29d9a14df1640555c6eb6775ed95b","title":"Open-sourced Data Ecosystem in Autonomous Driving: the Present and Future"},{"paperId":"8a14b0a3633de9d9b3bed552c5d3b9d7dbd57bc4","title":"Context Diffusion: In-Context Aware Image Generation"},{"paperId":"91159f6d3d52e6cfed1e4d1c6e50d1b17086a910","title":"On the Robustness of Large Multimodal Models Against Image Adversarial Attacks"},{"paperId":"b92289123a94f6076505487adfb4513bd3495c1d","title":"LEGO: Learning EGOcentric Action Frame Generation via Visual Instruction Tuning"},{"paperId":"999b48ef5551e550c89fba97d7f66347efee8030","title":"A Task is Worth One Word: Learning with Task Prompts for High-Quality Versatile Image Inpainting"},{"paperId":"d9d3dfce3bfa12fd3f3fb747b2252d57c83ab0f2","title":"FoMo Rewards: Can we cast foundation models as reward functions?"},{"paperId":"2feb5d47fedbc360649746123c3cbaafff1863ff","title":"From Pixels to Explanations: Uncovering the Reasoning Process in Visual Question Answering"},{"paperId":"4ada01c0cc79ed4abe081bd59300247626501808","title":"GPT4Point: A Unified Framework for Point-Language Understanding and Generation"},{"paperId":"1eea88f53291103a6e999b2bf64484cd705300a4","title":"Lenna: Language Enhanced Reasoning Detection Assistant"},{"paperId":"0fd97918661df32752c1d5ec70531be8bc793d5e","title":"TPA3D: Triplane Attention for Fast Text-to-3D Generation"},{"paperId":"535682dc230c3814fa1163885f61d75ccdc4e59c","title":"EtC: Temporal Boundary Expand then Clarify for Weakly Supervised Video Grounding with Multimodal Large Language Model"},{"paperId":"adee8299464cbd308db9d6f2daf73c7dbf83977a","title":"UPOCR: Towards Unified Pixel-Level OCR Interface"},{"paperId":"769b794fe9f97268007676171f246d45e0631014","title":"Towards More Unified In-context Visual Understanding"},{"paperId":"03f692d89c480cc7ce4c757f1ede6bfcd8a46b1f","title":"Describing Differences in Image Sets with Natural Language"},{"paperId":"67239d6e9c2c5f8a6d19cb35154e5aa7eaa00f51","title":"Large Language Models on Graphs: A Comprehensive Survey"},{"paperId":"0ef006f9900185ac6ede39352d1de1ddee7923b6","title":"Let's Think Outside the Box: Exploring Leap-of-Thought in Large Language Models with Creative Humor Generation"},{"paperId":"efc7c8d72897585f9086795810eb986bb918651c","title":"Stable Diffusion Exposed: Gender Bias from Prompt to Image"},{"paperId":"9af04f2ef5c798655c987aaf76e35a103d2efc30","title":"LooseControl: Lifting ControlNet for Generalized Depth Conditioning"},{"paperId":"f32ea390686b1eee3ba5b53c7a85e9e9385d4b94","title":"Visual Program Distillation: Distilling Tools and Programmatic Reasoning into Vision-Language Models"},{"paperId":"257c2adecba9cfba7eaec8463f8d378f6886f550","title":"ViscoNet: Bridging and Harmonizing Visual and Textual Conditioning for ControlNet"},{"paperId":"5579a90486bc9d135e5967a6e9ea1ae352216cf0","title":"Training on Synthetic Data Beats Real Data in Multimodal Relation Extraction"},{"paperId":"5220687c42520855db240b67415a20e09c137004","title":"Mismatch Quest: Visual and Textual Feedback for Image-Text Misalignment"},{"paperId":"0e8dec431a62dea147139d7805ab3a0a97bf3857","title":"LLaRA: Aligning Large Language Models with Sequential Recommenders"},{"paperId":"39a58f65cb38e1f715ba11ca75d17a934d4ee8db","title":"Image Caption Generation Based on Image-Text Matching Schema in Deep Reinforcement Learning"},{"paperId":"529a3164a4ef5c227b6a775f73936866cb51d72f","title":"Object Recognition as Next Token Prediction"},{"paperId":"e49cb2ab3a7990e3d05042197ae8b3fd934453de","title":"StoryGPT-V: Large Language Models as Consistent Story Visualizers"},{"paperId":"b9401a81dca0bebfc1d602843ddc43ac1b4a3d1b","title":"InstructTA: Instruction-Tuned Targeted Attack for Large Vision-Language Models"},{"paperId":"22d45b7b4cd23162dd38c9b577749d86db34075b","title":"ArtAdapter: Text-to-Image Style Transfer using Multi-Level Style Encoder and Explicit Adaptation"},{"paperId":"eca8a3e6383e3618e0bc984382e08c09be3cca6c","title":"TimeChat: A Time-sensitive Multimodal Large Language Model for Long Video Understanding"},{"paperId":"24b489376adf63bfae2fd1852c7f9fe7227d154c","title":"Bootstrapping SparseFormers from Vision Foundation Models"},{"paperId":"a6fb97da5ccf224eb6ecaff10b7518ddcbb86ed8","title":"Semantics-aware Motion Retargeting with Vision-Language Models"},{"paperId":"ef4e4e4b52d4379ab5387d8dc53da87e561e78db","title":"Good Questions Help Zero-Shot Image Reasoning"},{"paperId":"c355d65e1fd49371cab805b32a8405e351dd2cdc","title":"How to Configure Good In-Context Sequence for Visual Question Answering"},{"paperId":"d3dbe1447d68a8cd3536a420cc4a6c5058934edb","title":"SequencePAR: Understanding Pedestrian Attributes via A Sequence Generation Paradigm"},{"paperId":"165e1e1cdd08a676f1c67f78e8ba81d731a0c18d","title":"Hulk: A Universal Knowledge Translator for Human-Centric Tasks"},{"paperId":"5ed465aa4a72af045a94943e150fa32815b4db21","title":"A Contrastive Compositional Benchmark for Text-to-Image Synthesis: A Study with Unified Text-to-Image Fidelity Metrics"},{"paperId":"a4815d80c63092a2b71384570facda6c61e47035","title":"VaQuitA: Enhancing Alignment in LLM-Assisted Video Understanding"},{"paperId":"81aebd5ebab8ea7d834ac8ba31d7028986e57fda","title":"Geometrically-driven Aggregation for Zero-shot 3D Point Cloud Understanding"},{"paperId":"adb969668f191839d273af5743948ed10b28c43c","title":"PixelLM: Pixel Reasoning with Large Multimodal Model"},{"paperId":"4c92bd73698cf25e7a16c694f8c278ab3f9bfd08","title":"Effectively Fine-tune to Improve Large Multimodal Models for Radiology Report Generation"},{"paperId":"57427ef02d111ba8dfe270f0db970f56eeb8f826","title":"Language-driven All-in-one Adverse Weather Removal"},{"paperId":"a8898778e9ea9b9d653a5d341379a237ff05b58d","title":"ViVid-1-to-3: Novel View Synthesis with Video Diffusion Models"},{"paperId":"94602d45d2e8f8bb85cb61836381b67c5e705a71","title":"Bootstrapping Interactive Image–Text Alignment for Remote Sensing Image Captioning"},{"paperId":"0f9a3c5c6a54fca6be2afa0fd5fd34eed96a31e8","title":"RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from Fine-grained Correctional Human Feedback"},{"paperId":"3a2bb5b4f1f724df9295de5e0e1235cc30769b82","title":"Omni-SMoLA: Boosting Generalist Multimodal Models with Soft Mixture of Low-rank Experts"},{"paperId":"339ec34efdccdf2bf43bb817ef7cab5058bfa2e7","title":"Segment and Caption Anything"},{"paperId":"246017780386eba39d6cda760a1c2c70356baa50","title":"VIoTGPT: Learning to Schedule Vision Tools towards Intelligent Video Internet of Things"},{"paperId":"c95c4fb96868d6512c32988632a7b101a42c455d","title":"Dolphins: Multimodal Language Model for Driving"},{"paperId":"b013c9eb1284554ae696fba02bd4d7fc599890b6","title":"StyleCrafter: Enhancing Stylized Text-to-Video Generation with Style Adapter"},{"paperId":"cc9627c4475b9ad1c7e4105f3bceb1e7b59b7cab","title":"Zero-Shot Video Question Answering with Procedural Programs"},{"paperId":"70ea292c2312afdbe474e3c095dfa620b2cd46cc","title":"Video Summarization: Towards Entity-Aware Captions"},{"paperId":"e263e08a20080a2543d0ca29d3d63c4717a8beb6","title":"X-InstructBLIP: A Framework for aligning X-Modal instruction-aware representations to LLMs and Emergent Cross-modal Reasoning"},{"paperId":"0f0071aa8a04b2414e7856656f2d6a49e84ade89","title":"RaDialog: A Large Vision-Language Model for Radiology Report Generation and Conversational Assistance"},{"paperId":"fc53f8f3a84f1fc4993689d8f98cf6551d07a22d","title":"LL3DA: Visual Interactive Instruction Tuning for Omni-3D Understanding, Reasoning, and Planning"},{"paperId":"9e2bac2777eebe603a39f69221689493609d4149","title":"MLLMs-Augmented Visual-Language Representation Learning"},{"paperId":"7216573998589c342a35e69b26df22868c660680","title":"Spacewalk-18: A Benchmark for Multimodal and Long-form Procedural Video Understanding in Novel Domains"},{"paperId":"17ff6a0844afe74796022e7aaf372553e9303d72","title":"VTimeLLM: Empower LLM to Grasp Video Moments"},{"paperId":"e24241a8188d7c41d841cd6413da936adc247bb4","title":"VIDiff: Translating Videos via Multi-Modal Instructions with Diffusion Models"},{"paperId":"42f16fc51db7862e9f0ddb1547102d034302fc0f","title":"InstructSeq: Unifying Vision Tasks with Instruction-conditioned Multi-modal Sequence Generation"},{"paperId":"fe799bcd7f4190714ca1cd6d90617e3468aa1bee","title":"HiFi Tuner: High-Fidelity Subject-Driven Fine-Tuning for Diffusion Models"},{"paperId":"98ef472da617c1a4f51da54b59b5ae1b07cea978","title":"Acoustic Prompt Tuning: Empowering Large Language Models with Audition Capabilities"},{"paperId":"40de3296157b9d7a7882b61f967e37b3cc93f197","title":"Merlin: Empowering Multimodal LLMs with Foresight Minds"},{"paperId":"e2e09cc3c99324908186d465087edb3a6f8d748e","title":"Adaptive Multi-Modality Prompt Learning"},{"paperId":"6dd6101b2d7709f2a05f17510d98a01424672e1e","title":"Knowledge Transfer from Vision Foundation Models for Efficient Training of Small Task-specific Models"},{"paperId":"a35b7855d6dba196dd45db329afcbdec698ae718","title":"Symbol-LLM: Leverage Language Models for Symbolic System in Visual Human Activity Reasoning"},{"paperId":"2b41b3e23d6b8b84445abb77870aa3b9e71c75d8","title":"Contrastive Vision-Language Alignment Makes Efficient Instruction Learner"},{"paperId":"fb5a1a764c3fe0c12b82bcb2f9667dc7c7d8c4d8","title":"LALM: Long-Term Action Anticipation with Language Models"},{"paperId":"7450c20d844ca05e643ece2461ff7aa2f381e22a","title":"Towards Top-Down Reasoning: An Explainable Multi-Agent Approach for Visual Question Answering"},{"paperId":"f247bf010dc3b73c887bed82fcb4613a2ef7ab21","title":"VideoAssembler: Identity-Consistent Video Generation with Reference Entities using Diffusion Model"},{"paperId":"6c2b0947d537903a9d06ae83c5e5a4677060e3ff","title":"GenZI: Zero-Shot 3D Human-Scene Interaction Generation"},{"paperId":"09157a8c0e7d7263ac035690118ddcbe295cee5c","title":"ShapeGPT: 3D Shape Generation with A Unified Multi-modal Language Model"},{"paperId":"d4ba81bda42b408cd6d48205a593826060efc1ed","title":"Evaluating VLMs for Score-Based, Multi-Probe Annotation of 3D Objects"},{"paperId":"aa709983e1d74a3bc7737b9d8735c0c35bea0175","title":"When StyleGAN Meets Stable Diffusion: a W+ Adapter for Personalized Image Generation"},{"paperId":"679105b4343f316cab0c2e1ce3be0ed498341b86","title":"VITATECS: A Diagnostic Dataset for Temporal Concept Understanding of Video-Language Models"},{"paperId":"49b79d61ffc2db6dce8c2cd9cda06e1876ed8b4c","title":"OPERA: Alleviating Hallucination in Multi-Modal Large Language Models via Over-Trust Penalty and Retrospection-Allocation"},{"paperId":"d16f72b7be526dee5eb49e5afffeea2bddba5e66","title":"DreamSync: Aligning Text-to-Image Generation with Image Understanding Feedback"},{"paperId":"22d55c52f43f59634586ab95fefbb7dba8c8b190","title":"ChatIllusion: Efficient-Aligning Interleaved Generation ability with Visual Instruction Model"},{"paperId":"117bfefd48f9fb5a4f2d2d73f4b52274738079e4","title":"Rethinking Image Editing Detection in the Era of Generative AI Revolution"},{"paperId":"498decc50ccea9293f63a98c30d7c3439be074b7","title":"Look Before You Leap: Unveiling the Power of GPT-4V in Robotic Vision-Language Planning"},{"paperId":"5aea65d0d712360bb3357cbc23d43707ec27c461","title":"MM-Narrator: Narrating Long-form Videos with Multimodal In-Context Learning"},{"paperId":"0ab10b06f3a35f27b0fd5c9e1ee3a802d141478b","title":"Efficient In-Context Learning in Vision-Language Models for Egocentric Videos"},{"paperId":"ea3448eb86a233189631d914721e587d45931b64","title":"MVBench: A Comprehensive Multi-modal Video Understanding Benchmark"},{"paperId":"3d5ea21a305a31d67430ef5d4bce7262297f00b4","title":"HumanRef: Single Image to 3D Human Generation via Reference-Guided Diffusion"},{"paperId":"0a95f7d0165671c922446663d3d4de45ea87ef2e","title":"LLaFS: When Large-Language Models Meet Few-Shot Segmentation"},{"paperId":"486c2df78cbb770a90a55f7fa3fe19102fba2c24","title":"LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models"},{"paperId":"425f1edd88fe3539c40ddd93c3e07c95de67ba00","title":"Embodied Multi-Modal Agent trained by an LLM from a Parallel TextWorld"},{"paperId":"9037c23cd2b25eea8f146e73ba6ad7985549d1a6","title":"UniIR: Training and Benchmarking Universal Multimodal Information Retrievers"},{"paperId":"c3bb138fc5187a0949f5cee93f5004e1a78b5796","title":"PEA-Diffusion: Parameter-Efficient Adapter with Knowledge Distillation in non-English Text-to-Image Generation"},{"paperId":"154cc4e8a9e8ad24d4f9c9440b187d06b9ba57bd","title":"SEED-Bench-2: Benchmarking Multimodal Large Language Models"},{"paperId":"4df34ebf827d21decc8de2742daa10accf5e6168","title":"Plug-and-Play, Dense-Label-Free Extraction of Open-Vocabulary Semantic Segmentation from Vision-Language Models"},{"paperId":"c34952e5e073cf645fd1f18bc4f3663c6dedcaf4","title":"TransNeXt: Robust Foveal Visual Perception for Vision Transformers"},{"paperId":"8ddd7750389dc18efbf6437916f3e94258081faf","title":"Shadows Don't Lie and Lines Can't Bend! Generative Models don't know Projective Geometry...for now"},{"paperId":"012e24f7c4dc92f26b41cacca501c0d4355b02fd","title":"Towards Vision Enhancing LLMs: Empowering Multimodal Knowledge Storage and Sharing in LLMs"},{"paperId":"4223ff010b2a91a62a131f12d14e7a8eb684e71d","title":"Fully Authentic Visual Question Answering Dataset from Online Communities"},{"paperId":"038c5d91b952464d413defcd7a3b695a1306f650","title":"CoSeR: Bridging Image and Language for Cognitive Super-Resolution"},{"paperId":"1c9060b7246bae6f5000ddd3041b54619243e2bf","title":"EVCap: Retrieval-Augmented Image Captioning with External Visual-Name Memory for Open-World Comprehension"},{"paperId":"fe92a35c51ebba91ed99f7da0e0124434229a469","title":"Continual Instruction Tuning for Large Multimodal Models"},{"paperId":"ab342050a0eb26fb4a0af5fff028e1a8ca54f575","title":"GaussianEditor: Editing 3D Gaussians Delicately with Text Instructions"},{"paperId":"596bd637e6bab16058759ebc28aba2dbc5e19fed","title":"Reinforcement Learning from Diffusion Feedback: Q* for Image Search"},{"paperId":"6cc00fe6d7a78d166327c95e5ad4ebe4654b71d2","title":"Can Vision-Language Models Think from a First-Person Perspective?"},{"paperId":"85fcab7f42a3b1513f3b7b8e34e1164b77c3c29d","title":"AerialBooth: Mutual Information Guidance for Text Controlled Aerial View Synthesis from a Single Image"},{"paperId":"25905977ce577f1e637ed9be53695fde8888ca09","title":"Instruct2Attack: Language-Guided Semantic Adversarial Attacks"},{"paperId":"4f94e78bcf7ed0fbedae7d5337bda6f90ea0ffd6","title":"RO-LLaMA: Generalist LLM for Radiation Oncology via Noise Augmentation and Consistency Regularization"},{"paperId":"73f082fc7df9f2b9f3bf7dafb7c4422bb7aae968","title":"How Many Unicorns Are in This Image? A Safety Evaluation Benchmark for Vision LLMs"},{"paperId":"ed2a6cebe1d0ebfe4dfe3fe42fd6bb5d8206a2fa","title":"ViT-Lens-2: Gateway to Omni-modal Intelligence"},{"paperId":"2b3554a8fea6f123fc04bd3e120f2293f227e1b2","title":"InstructMol: Multi-Modal Integration for Building a Versatile and Reliable Molecular Assistant in Drug Discovery"},{"paperId":"769a924d0af014acec326f50c15c5d70d258a969","title":"LLMGA: Multimodal Large Language Model based Generation Assistant"},{"paperId":"7c261866e9d8ddc42f3c1f0b1c2c882182d47fc9","title":"Mitigating Hallucination in Visual Language Models with Visual Supervision"},{"paperId":"40cd34f260d5596263654caf9d911d4355bf4f4e","title":"ChartLlama: A Multimodal LLM for Chart Understanding and Generation"},{"paperId":"5eea245cc12c55905d4df827d0c9776c5ddfa743","title":"Compositional Chain-of-Thought Prompting for Large Multimodal Models"},{"paperId":"be974844cd1e5a441fcfebcff62f72e48af46f63","title":"Knowledge Unlearning for LLMs: Tasks, Methods, and Challenges"},{"paperId":"a756b584f8f8b4307e52895ae2120bc339580ad8","title":"See and Think: Embodied Agent in Virtual Environment"},{"paperId":"7b0a186b0140ee91fb13991c9c7187f3dc3b0670","title":"Visual Programming for Zero-shot Open-Vocabulary 3D Visual Grounding"},{"paperId":"cb76f7fc35ff289fdf1cf50d9cfe1493342a0dec","title":"AutoEval-Video: An Automatic Benchmark for Assessing Large Vision Language Models in Open-Ended Video Question Answering"},{"paperId":"437c7836d32c7f221aad466047130075c7cb5336","title":"Point Cloud Pre-training with Diffusion Models"},{"paperId":"4d51f4698725ad73be4d8e096ea755b5ed9eb45e","title":"Mug-STAN: Adapting Image-Language Pretrained Models for General Video Understanding"},{"paperId":"b754ff078bb45131aef0b8bd7aa1bebd238560da","title":"Large Language Models as Automated Aligners for benchmarking Vision-Language Models"},{"paperId":"76803ea9ccc93a4f277aae5d4714ea79a99e55d6","title":"Griffon: Spelling out All Object Locations at Any Granularity with Large Language Models"},{"paperId":"ee2c769943f9e46c3bbee117d1ecf14566b7bf1f","title":"Boosting the Power of Small Multimodal Reasoning Models to Match Larger Models with Self-Consistency Training"},{"paperId":"d4aac80473473eac22d8b5371110e0788e8e037a","title":"Perceptual Image Compression with Cooperative Cross-Modal Side Information"},{"paperId":"fb01eb4411e9563ae90fde80d24d3118e54df5e4","title":"Towards Improving Document Understanding: An Exploration on Text-Grounding via MLLMs"},{"paperId":"52941cadbd340344f3e0a6f50719fe55b3de5088","title":"Multimodal Large Language Models: A Survey"},{"paperId":"3039e5c8bd6147b6ee08f0f50d52047cc3be2372","title":"ADriver-I: A General World Model for Autonomous Driving"},{"paperId":"5c3a1b509b4f2851305e049b06ffe40dfaab87ca","title":"T-Rex: Counting by Visual Prompting"},{"paperId":"9711a5846444270bb05da8f58ebb355dd8d8f04e","title":"Boosting3D: High-Fidelity Image-to-3D by Boosting 2D Diffusion Prior to 3D Prior with Progressive Learning"},{"paperId":"1e838bd5fa2f5bca805493d0f672d03514b36869","title":"Vamos: Versatile Action Models for Video Understanding"},{"paperId":"cf193b5b34178a444cb9bd9f51beb4124b753935","title":"HalluciDoctor: Mitigating Hallucinatory Toxicity in Visual Instruction Data"},{"paperId":"48491cef7a2d462b3c6e23574929d4e4f766a64f","title":"ViLaM: A Vision-Language Model with Enhanced Visual Grounding and Generalization Capability"},{"paperId":"f68f6f2a057c4e6e5a3c91fc8563533d9bf6e560","title":"ShareGPT4V: Improving Large Multi-Modal Models with Better Captions"},{"paperId":"451539c0d0f5f5785ff58d09ca5e67a5f129f9de","title":"A Survey on Multimodal Large Language Models for Autonomous Driving"},{"paperId":"8362a7a4d1f93adda2859d2ed9da738e58c93a5b","title":"ALPHA: AnomaLous Physiological Health Assessment Using Large Language Models"},{"paperId":"7f807249c0ef0fe07d5e9c810684cd5daba0edc5","title":"De-fine: Decomposing and Refining Visual Programs with Auto-Feedback"},{"paperId":"54630cd92c0c6696a422c3b2aa986c1f75df70b3","title":"A Survey of Graph Meets Large Language Model: Progress and Future Directions"},{"paperId":"91e3906550821c4624146e6e87db36c3296e773a","title":"Applications of Large Scale Foundation Models for Autonomous Driving"},{"paperId":"e679c3e3ba2dd98a8895b95a93acafc7c032a425","title":"Cut-and-Paste: Subject-Driven Video Editing with Attention Control"},{"paperId":"3dee0acc8728655b3458704ca90778ec0b28758b","title":"LLMs as Visual Explainers: Advancing Image Classification with Evolving Visual Descriptions"},{"paperId":"6fa0677731184444df0e1fc8070938419cd6da47","title":"Igniting Language Intelligence: The Hitchhiker's Guide From Chain-of-Thought Reasoning to Language Agents"},{"paperId":"5f370f52e24d185ae44bb0ea18cbd4be2aab0d15","title":"VLM-Eval: A General Evaluation on Video Large Language Models"},{"paperId":"98b69e478d2d4e4cf1a0befcdb27c4f220fc0a4b","title":"LION : Empowering Multimodal Large Language Model with Dual-Level Visual Knowledge"},{"paperId":"0c7ca22dfbe1ce02cb0f0658292499457db8ec6e","title":"InfiMM-Eval: Complex Open-Ended Reasoning Evaluation For Multi-Modal Large Language Models"},{"paperId":"de3d1bdf8931285abacc2e0bee6f9eb7f662af94","title":"Quality and Quantity: Unveiling a Million High-Quality Images for Text-to-Image Synthesis in Fashion Design"},{"paperId":"1e84d7c45f70038574fcdb7bc1b20da9b348a092","title":"M2UGen: Multi-modal Music Understanding and Generation with the Power of Large Language Models"},{"paperId":"56025f5034f7aebe1b7292284d33d3d0e3317614","title":"Deep Tensor Network"},{"paperId":"8ece28f449ee8f1109ce0bae8f3375ef9e08b4b0","title":"Behavior Optimized Image Generation"},{"paperId":"dcd40f8c4828e125591a74d9d00dff81f1bfe90d","title":"Active Prompt Learning in Vision Language Models"},{"paperId":"13d12b26db345f62e8e512db181b96a7f8763b47","title":"An Embodied Generalist Agent in 3D World"},{"paperId":"4fc7dd55b15babe4010a74e715629bf1e9fd9179","title":"ChatVQG: A VQG Dataset Containing Diversified Conversational Questions and Guiding Information for Proactive Chatting with the Elderly"},{"paperId":"67ca824a96088a67d80ec3c7c2910f9923c78f42","title":"Enhancing Object Coherence in Layout-to-Image Synthesis"},{"paperId":"d7026c7e1c166d52923f24d9fa6d1d6b0a53d8f9","title":"RED-DOT: Multimodal Fact-checking via Relevant Evidence Detection"},{"paperId":"6dc8f09f3f39d41c21ec167b8a0ed9f12552f487","title":"Trustworthy Large Models in Vision: A Survey"},{"paperId":"391eaeb1092c2b145ff0e5a2fa61637a42921fce","title":"DRESS: Instructing Large Vision-Language Models to Align and Interact with Humans via Natural Language Feedback"},{"paperId":"107fb6eec2febbae12db29bf3e311aaf5680027c","title":"Video-LLaVA: Learning United Visual Representation by Alignment Before Projection"},{"paperId":"f1c40c5be6240b48eb9ed8e09be1da2661e6b670","title":"Correlation-guided Query-Dependency Calibration in Video Representation Learning for Temporal Grounding"},{"paperId":"87089dd53f6279b0c348b78d7cc19989349b48e7","title":"GRASP: A novel benchmark for evaluating language GRounding And Situated Physics understanding in multimodal language models"},{"paperId":"16513bc0dc13902334a9cb3657056763efdcec6f","title":"Towards Open-Ended Visual Recognition with Large Language Model"},{"paperId":"aad3d2e690f6c73f04a14622ceff51464bbc560e","title":"Chat-UniVi: Unified Visual Representation Empowers Large Language Models with Image and Video Understanding"},{"paperId":"f90595f99a0c66d2bb6d0f230f17c7cd8c58f44d","title":"Qwen-Audio: Advancing Universal Audio Understanding via Unified Large-Scale Audio-Language Models"},{"paperId":"3cc6c4e88b6135d53938315a262285a501803c48","title":"Vision-Language Instruction Tuning: A Review and Analysis"},{"paperId":"00d16f1232bfc0b0fee0fb6e56c90aa96f5a188f","title":"Language Grounded QFormer for Efficient Vision Language Understanding"},{"paperId":"76a3f4a79ae9a00db2f2b5f6877021d8deb96ada","title":"SPHINX: The Joint Mixing of Weights, Tasks, and Visual Embeddings for Multi-modal Large Language Models"},{"paperId":"8ac7df2d11170b0777b2d913d4a4b4887e127731","title":"A Comprehensive Evaluation of GPT-4V on Knowledge-Intensive Visual Question Answering"},{"paperId":"30adb0d4d89ddf53b14a15473efe4d5e9723c197","title":"What Large Language Models Bring to Text-rich VQA?"},{"paperId":"619184447595337a9fe3dca72c4e951e7ab7467c","title":"To See is to Believe: Prompting GPT-4V for Better Visual Instruction Tuning"},{"paperId":"4da938af4eb8e40857f30a3ef612ef4f2eeea300","title":"ViLMA: A Zero-Shot Benchmark for Linguistic and Temporal Grounding in Video-Language Models"},{"paperId":"e066347a8896058f50d0259b91b6ca3c40f52c2d","title":"InfMLLM: A Unified Framework for Visual-Language Tasks"},{"paperId":"bf14244669d5505f63343d4365d99d24aa6c5e82","title":"Monkey: Image Resolution and Text Label Are Important Things for Large Multi-modal Models"},{"paperId":"ef321c6f174ac59916ac54ec40ad18bca5b58e5c","title":"PerceptionGPT: Effectively Fusing Visual Perception into LLM"},{"paperId":"5698d1f43d2ee9fafbfab839706fd51585dd0fed","title":"Report of the 1st Workshop on Generative AI and Law"},{"paperId":"3af134a559a618b3185390646d49d1d4e7ffab45","title":"Instant3D: Fast Text-to-3D with Sparse-View Generation and Large Reconstruction Model"},{"paperId":"441bada9aa6dfd1f94d45d20e0f7eb060d59dd30","title":"Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks"},{"paperId":"07f9e1d3288b22cda2c980cfb969dcf410e5bd9e","title":"u-LLaVA: Unifying Multi-Modal Tasks via Large Language Model"},{"paperId":"b78b5ce5f21f46d8149824463f8eebd6103d49aa","title":"FigStep: Jailbreaking Large Vision-language Models via Typographic Visual Prompts"},{"paperId":"f7b841060aac8b9f1c39597686325103a2266921","title":"ConRad: Image Constrained Radiance Fields for 3D Generation from a Single Image"},{"paperId":"ecfff30e570498b6c87c5d1319a0cbcdf7a8b86d","title":"LLaVA-Plus: Learning to Use Tools for Creating Multimodal Agents"},{"paperId":"7059afa8c275391fedfeefceca258f3f6a6b063e","title":"Chain of Images for Intuitively Reasoning"},{"paperId":"adff39f3972d349afec4bc7bcbc580b670a99097","title":"GENOME: GenerativE Neuro-symbOlic visual reasoning by growing and reusing ModulEs"},{"paperId":"d6a8e685b46f79056076a6b65803d49493a99dca","title":"Beyond Size: How Gradients Shape Pruning Decisions in Large Language Models"},{"paperId":"59d716b442ab760a78f58de6748c0fa1d507bfc1","title":"TEAL: Tokenize and Embed ALL for Multi-modal Large Language Models"},{"paperId":"eb2cbd12f749f14716296f7f415e921562c9079b","title":"LRM: Large Reconstruction Model for Single Image to 3D"},{"paperId":"abb312b7cf508b91ae7a88b1890c13eb8b96ad1a","title":"NExT-Chat: An LMM for Chat, Detection and Segmentation"},{"paperId":"2f566575a246752d59438e2bde22f88680927af9","title":"OtterHD: A High-Resolution Multi-modality Model"},{"paperId":"bc6d13540a567629767c5b2102df4f789752c9e2","title":"A Data Perspective on Enhanced Identity Preservation for Diffusion Personalization"},{"paperId":"ad13b213681b6f634bc83a264df246e83dd9a9d9","title":"mPLUG-Owl2: Revolutionizing Multi-modal Large Language Model with Modality Collaboration"},{"paperId":"6ae4705139494fcb6b790b6dd6c4225b40ee40f8","title":"GLaMM: Pixel Grounding Large Multimodal Model"},{"paperId":"b7b7a549629bbe7fa325572339938980e77d155c","title":"AnyText: Multilingual Visual Text Generation And Editing"},{"paperId":"8dcc5f91e89f5cccec99386ccfbd32d1f7ef4629","title":"LDM3D-VR: Latent Diffusion Model for 3D VR"},{"paperId":"cf874e0c25f34cde830718724c3bd57f4c2badce","title":"Octavius: Mitigating Task Interference in MLLMs via MoE"},{"paperId":"4d5ae83108bbddc51e517921c69a8318ad6313ca","title":"MixCon3D: Synergizing Multi-View and Cross-Modal Contrastive Learning for Enhancing 3D Representation"},{"paperId":"a884d6284f330a38fbdb3ecc147d48482da28867","title":"COSMIC: Data Efficient Instruction-tuning For Speech In-Context Learning"},{"paperId":"dcd8c75c0d6741bcda77b5df0c75138f28674504","title":"Sam-Guided Enhanced Fine-Grained Encoding with Mixed Semantic Learning for Medical Image Captioning"},{"paperId":"0b3e7b5cbef627b1ceedceadc5f58787f432163b","title":"What Makes for Good Visual Instructions? Synthesizing Complex Visual Reasoning Instructions for Visual Instruction Tuning"},{"paperId":"01d9bdf230442f6460127def244fb0ef06b47b8b","title":"Vision-Language Interpreter for Robot Task Planning"},{"paperId":"8eab7b5ae5cacca0d8e14a02221d2b219e207825","title":"Detecting Deepfakes Without Seeing Any"},{"paperId":"d45805349623c1389d5861d8263fb06df922e05b","title":"GPT-4V(ision) as a Generalist Evaluator for Vision-Language Tasks"},{"paperId":"c62711f6b5d8620ba36bc2c378ec6ab53f6e197c","title":"RoboGen: Towards Unleashing Infinite Data for Automated Robot Learning via Generative Simulation"},{"paperId":"fbae34c21a6a0cbf3f9e2710b7fce0e011aec72c","title":"FAITHSCORE: Evaluating Hallucinations in Large Vision-Language Models"},{"paperId":"84d99893ee24fc825e359598d44d602c45c4865e","title":"LLM4Drive: A Survey of Large Language Models for Autonomous Driving"},{"paperId":"0c474b03e8fd385b08a40df22934c9d9b180ffb7","title":"De-Diffusion Makes Text a Strong Cross-Modal Interface"},{"paperId":"88bddfb7d1e0462be8fe99fdbd71c658140cb17b","title":"From Image to Language: A Critical Analysis of Visual Question Answering (VQA) Approaches, Challenges, and Opportunities"},{"paperId":"69630953cd28eed4ebf7c441766b34e562b7ad03","title":"NeoDescriber: An image-to-text model for automatic style description of neoclassical architecture"},{"paperId":"39a6520508ff88ef3a29304ad49af4f6725ba06b","title":"LLM Multimodal Traffic Accident Forecasting"},{"paperId":"3031eedb7cf3ef20f1911c2902f3a8e5aeeb2c3f","title":"VisPercep: A Vision-Language Approach to Enhance Visual Perception for People with Blindness and Low Vision"},{"paperId":"56cb3506586c540d4bd2c573fad66d22dff69826","title":"CapsFusion: Rethinking Image-Text Data at Scale"},{"paperId":"a221f7fd6b40168123e6577d983cdd0d51c54297","title":"The Generative AI Paradox: \"What It Can Create, It May Not Understand\""},{"paperId":"1f5e1a036b24b9dd34c006ba3bb61119624f4fdb","title":"A Comprehensive Study of GPT-4V's Multimodal Capabilities in Medical Imaging"},{"paperId":"337a5ff27423915ca1c223e6c5c1ffdf12a56dd5","title":"UAV Immersive Video Streaming: A Comprehensive Survey, Benchmarking, and Open Challenges"},{"paperId":"610b34302ab2f6ae6c954c14f8020c2aa82d6b66","title":"Exploring a CLIP-Enhanced Automated Approach for Video Description Generation"},{"paperId":"2010e5fb3a804ac376412b4fa65ee83f34d5e1d9","title":"A Systematic Evaluation of GPT-4V's Multimodal Capability for Medical Image Analysis"},{"paperId":"696ec1f8d25de41a082ff6c059e296c8fc6d9af2","title":"Challenges and Opportunities in Neuro-Symbolic Composition of Foundation Models"},{"paperId":"e68ab63d505c3ee8d0518c734c3d2b13071cc18d","title":"MM-VID: Advancing Video Understanding with GPT-4V(ision)"},{"paperId":"8ac96b1a5196011381a7ee52738ba7eaebcb44a0","title":"CustomNet: Zero-shot Object Customization with Variable-Viewpoints in Text-to-Image Diffusion Models"},{"paperId":"bd1818d345acb805ba7f0b9643852a52ccd765f9","title":"Myriad: Large Multimodal Model by Applying Vision Experts for Industrial Anomaly Detection"},{"paperId":"2b6a3cad4e4cbc2f2ae2a9f2d5b9e349071f24c2","title":"Open Visual Knowledge Extraction via Relation-Oriented Multimodality Model Prompting"},{"paperId":"8e5d42f5b98146d0784fe85e29c768a4989e1478","title":"Foundational Models in Medical Imaging: A Comprehensive Survey and Future Vision"},{"paperId":"7d0f5eb997fd963a152c731a7e3fa64f0fe50a01","title":"Text-Guided 3D Object Generation via Disentangled Shape and Appearance Score Distillation Sampling"},{"paperId":"e6f93ae09064de1cdba070ad71de69655cb44a01","title":"Image Clustering Conditioned on Text Criteria"},{"paperId":"84a2719338a1f1db73aaa7b5bd61ca507c63da8e","title":"Drive Anywhere: Generalizable End-to-end Autonomous Driving with Multi-modal Foundation Models"},{"paperId":"56128cecc92acb881f339bd5d85e1bba5b4d960c","title":"Evaluating Bias and Fairness in Gender-Neutral Pretrained Vision-and-Language Models"},{"paperId":"24d9542bffc4ce0f9928757895f93f2b7ceeeebc","title":"Defect Spectrum: A Granular Look of Large-Scale Defect Datasets with Rich Semantics"},{"paperId":"ac78b019c0cc883c79da0872cb4e1485a72928b6","title":"AntifakePrompt: Prompt-Tuned Vision-Language Models are Fake Image Detectors"},{"paperId":"288e7224d53d68669eb67f2496e068dc965c639e","title":"ControlLLM: Augment Language Models with Tools by Searching on Graphs"},{"paperId":"0f82929fcfc9958d442009a7d50e6794f024b7f1","title":"Large Language Models as Generalizable Policies for Embodied Tasks"},{"paperId":"01843a69f5c8a66da06ca883aabaa86ffa2b3fd8","title":"Semantic Generative Augmentations for Few-Shot Counting"},{"paperId":"1b672e2ea961ef45a9f3322430ca5df9ff8ba165","title":"CommonCanvas: An Open Diffusion Model Trained with Creative-Commons Images"},{"paperId":"f8b8f926bbfa327c86c40796131fe2695db81126","title":"DDCoT: Duty-Distinct Chain-of-Thought Prompting for Multimodal Reasoning in Language Models"},{"paperId":"da9134f694959b68027c33c8e998ffb3d41305da","title":"Exploring Question Decomposition for Zero-Shot VQA"},{"paperId":"43a3d51cc60c638e7523c63f203ac0c9026ce2d8","title":"Exploring OCR Capabilities of GPT-4V(ision) : A Quantitative and In-depth Evaluation"},{"paperId":"7ac58235e17a547229f5be89010f8546dd40e91f","title":"On the Powerfulness of Textual Outlier Exposure for Visual OoD Detection"},{"paperId":"10fce9e1718e968846c8429366c597380cce213d","title":"A Picture is Worth a Thousand Words: Principled Recaptioning Improves Image Generation"},{"paperId":"807f336176070bd3f95b82a16f125ee99b7d2c80","title":"Woodpecker: Hallucination Correction for Multimodal Large Language Models"},{"paperId":"0307c481f3b125fd87bd7f93fb3c460adf8c7187","title":"Recent Advances in Multi-modal 3D Scene Understanding: A Comprehensive Survey and Evaluation"},{"paperId":"0da1a668b08dc327182c77579e6a94e1f7ac7112","title":"TiC-CLIP: Continual Training of CLIP Models"},{"paperId":"2b724168c8f10bd095dc648b6d36fc01a325d107","title":"Leveraging Image-Text Similarity and Caption Modification for the DataComp Challenge: Filtering Track and BYOD Track"},{"paperId":"b1d2a29860e69c6ce9987ddefbe112feb1efa16a","title":"Large Language Models can Share Images, Too!"},{"paperId":"3bdf4c079e916b923646bff0776db7ca27f55005","title":"SAM-CLIP: Merging Vision Foundation Models towards Semantic and Spatial Understanding"},{"paperId":"0b395ed1c8b284e551172b728e83cf257e33729a","title":"HallusionBench: An Advanced Diagnostic Suite for Entangled Language Hallucination&Visual Illusion in Large Vision-Language Models"},{"paperId":"96d104dfe727f78a35faaafe81481f3672b485ee","title":"Large Language Models are Visual Reasoning Coordinators"},{"paperId":"3eef93ebf031e749c580c4ba37283fc5c90a8929","title":"Cultural and Linguistic Diversity Improves Visual Representations"},{"paperId":"938d1028b3c3cd4e3d34eed20b622bdc33453f6e","title":"MarineGPT: Unlocking Secrets of Ocean to the Public"},{"paperId":"f72be31de9f9a09d4410fd38bc717efe43444827","title":"SALMONN: Towards Generic Hearing Abilities for Large Language Models"},{"paperId":"0a1e0c479e13e7553ac79ddd6780fbf8e8e82165","title":"Benchmarking Sequential Visual Input Reasoning and Prediction in Multimodal Large Language Models"},{"paperId":"02a00ce9e7bce14937f46af0423eea40b7b63303","title":"Steve-Eye: Equipping LLM-based Embodied Agents with Visual Perception in Open Worlds"},{"paperId":"beb3e8acd816bac1a5b7fccfd073f79048877e33","title":"Frozen Transformers in Language Models Are Effective Visual Encoder Layers"},{"paperId":"ba7f09d76f465c7d5fefc87de859a491d6c5e145","title":"DiagrammerGPT: Generating Open-Domain, Open-Platform Diagrams via LLM Planning"},{"paperId":"dbde02978c8ed3f5fe586f63bab200f617225e0f","title":"LoHoRavens: A Long-Horizon Language-Conditioned Benchmark for Robotic Tabletop Manipulation"},{"paperId":"b844ae48660313c5354b274073336faf9c2fcc34","title":"Non-Intrusive Adaptation: Input-Centric Parameter-efficient Fine-Tuning for Versatile Multimodal Modeling"},{"paperId":"66d927fdb6c2774131960c75275546fd5ee3dd72","title":"EvalCrafter: Benchmarking and Evaluating Large Video Generation Models"},{"paperId":"3c7b23b343eb24f0662c8b9033ec1f2d15cc4c27","title":"EXMODD: An EXplanatory Multimodal Open-Domain Dialogue dataset"},{"paperId":"fe6e9576ad159fd6236227c6f0d062e14467bf2f","title":"UNK-VQA: A Dataset and A Probe into Multi-modal Large Models' Abstention Ability"},{"paperId":"ebb77213f63cda5612cc1bc48eb55c2b71a5cfd8","title":"Towards image compression with perfect realism at ultra-low bitrates"},{"paperId":"36b923d97d7cfaf73d11c55c15ea46605ba974a5","title":"BiLL-VTG: Bridging Large Language Models and Lightweight Visual Tools for Video-based Texts Generation"},{"paperId":"00c19d9818bf093a2eed323d1bd5c763c4f512b9","title":"Reading Books is Great, But Not if You Are Driving! Visually Grounded Reasoning about Defeasible Commonsense Norms"},{"paperId":"36819c779070076e9364786223bd341dd9a89a77","title":"RoboLLM: Robotic Vision Tasks Grounded on Multimodal Large Language Models"},{"paperId":"2b11dc5fb3cf109b05c041c549eebbe22946a8b8","title":"TOSS: High-quality Text-guided Novel View Synthesis from a Single Image"},{"paperId":"671ee2b83b3489ce9b3b3b41162ec3c4a2bf9c59","title":"A Survey on Video Diffusion Models"},{"paperId":"5c6ef918e0d16002bc1ee896911cdada4eda89ef","title":"Joint Music and Language Attention Models for Zero-shot Music Tagging"},{"paperId":"34a4229372313f3741c579c9f48c8687e40f1f1b","title":"Interpreting and Controlling Vision Foundation Models via Text Explanations"},{"paperId":"9c4ae24cb3d3230bd27e42d98124c22b1e0f9d48","title":"Bongard-OpenWorld: Few-Shot Reasoning for Free-form Visual Concepts in the Real World"},{"paperId":"5b038c1a93967072cc76689fd805e756f804cc42","title":"Large Models for Time Series and Spatio-Temporal Data: A Survey and Outlook"},{"paperId":"cac79e2a65f41f63c99bb5aa6ed9317a71b21874","title":"Real-Fake: Effective Training Data Synthesis Through Distribution Matching"},{"paperId":"9e6e6e3b9680e4d4ee15fa6ed5a3a178371f4cf4","title":"LLM4SGG: Large Language Model for Weakly Supervised Scene Graph Generation"},{"paperId":"ff4f3ac67c6d7d7da3d0f27ddafc0f9552a29b00","title":"VLIS: Unimodal Language Models Guide Multimodal Language Generation"},{"paperId":"ac2e5bf716aed246ca8914a6816ef73e00286099","title":"Beyond Segmentation: Road Network Generation with Multi-Modal LLMs"},{"paperId":"1ddbd08ad8cf22a5c66c4242194c4286328533bf","title":"MiniGPT-v2: large language model as a unified interface for vision-language multi-task learning"},{"paperId":"2179a13e78195fa78be5ccca2b5dcf9fad783ffc","title":"An Expression Tree Decoding Strategy for Mathematical Equation Generation"},{"paperId":"e9d7fb9b2c41fc76e55cbe48b0d30a9a57a8e023","title":"From CLIP to DINO: Visual Encoders Shout in Multi-modal Large Language Models"},{"paperId":"a09aec45d4eff67fd244b0f4035895cdd3fe72e9","title":"Vision-by-Language for Training-Free Compositional Image Retrieval"},{"paperId":"54f60adbec5af199907c07ed281ad893ccc00088","title":"MM-BigBench: Evaluating Multimodal Models on Multimodal Content Comprehension Tasks"},{"paperId":"68e0e789b5147b1e7d028c7a825650075f4e26bf","title":"PaLI-3 Vision Language Models: Smaller, Faster, Stronger"},{"paperId":"6cfd08bd3cd2e01b2cd276e3dd05217d8546684f","title":"Semantic Traffic Element Removal by Learning to Follow Linguistic Instructions"},{"paperId":"e4d41f2b0c5dfbf0c7078da5c984cea0b46411fd","title":"Ziya-Visual: Bilingual Large Vision-Language Model via Multi-Task Instruction Tuning"},{"paperId":"b3e9f249dd2e09ec111496f6b533101e8217a5b0","title":"Multimodal Large Language Model for Visual Navigation"},{"paperId":"825e965b41e033e85661ccf9ec22640a22cdb7b4","title":"Visual Data-Type Understanding does not emerge from Scaling Vision-Language Models"},{"paperId":"458111ac5a0f73bb35a2acf55298268be25ccfa2","title":"Ferret: Refer and Ground Anything Anywhere at Any Granularity"},{"paperId":"a710efa9247207a72f06e0c9db302fd3ecab5fbb","title":"Towards Robust Multi-Modal Reasoning via Model Selection"},{"paperId":"b6c8c1745a18d6e59c7a8a99f0df7aa4c18a1e73","title":"Octopus: Embodied Vision-Language Programmer from Environmental Feedback"},{"paperId":"e8d513bc7554a83161f2fb26c8299b471581cdb6","title":"Can We Edit Multimodal Large Language Models?"},{"paperId":"973ea24ba12d2ca16cda1ddbaa8d1a66ce7909e4","title":"Mitigating stereotypical biases in text to image generative systems"},{"paperId":"f27bcd2a7f68e47bb1f941b92617f9311b8a1bf0","title":"Multimodal Graph Learning for Generative Tasks"},{"paperId":"57a37c124ce272c20aacfcd595830d8fb8588fca","title":"IMITATE: Clinical Prior Guided Hierarchical Vision-Language Pre-training"},{"paperId":"db72807cb4d1439e299a0222e8d0306603d20edc","title":"Solution for SMART-101 Challenge of ICCV Multi-modal Algorithmic Reasoning Task 2023"},{"paperId":"449e130b89564b60f183190da77b8261b4af9142","title":"Improving Compositional Text-to-image Generation with Large Vision-Language Models"},{"paperId":"8085a65171e4fecbd0a5e5bf95850c5acada0665","title":"The Solution for the CVPR2023 NICE Image Captioning Challenge"},{"paperId":"804791cde0410d9725043a890f67fd8c621ddb32","title":"Sentence-level Prompts Benefit Composed Image Retrieval"},{"paperId":"28fbbf98bac1bb941162df553ca034d600cb59a6","title":"Rephrase, Augment, Reason: Visual Grounding of Questions for Vision-Language Models"},{"paperId":"33095b1334bed852e3652bd9d7da3f4df0cdf485","title":"ViCor: Bridging Visual Understanding and Commonsense Reasoning with Large Language Models"},{"paperId":"2020e6ecaddd06d991421a827207e8992474e701","title":"Fine-grained Audio-Visual Joint Representations for Multimodal Large Language Models"},{"paperId":"6fcfa4aea98c84fad86966e92372789ec793554b","title":"Building an Open-Vocabulary Video CLIP Model with Better Architectures, Optimization and Data"},{"paperId":"b1e097be0e7fe37a90507b66abfbb6f22fef38b5","title":"Lightweight In-Context Tuning for Multimodal Unified Models"},{"paperId":"471bb71c8797c253fe668af5899dfc7fc2ddd8ac","title":"Video-Teller: Enhancing Cross-Modal Generation with Fusion and Decoupling"},{"paperId":"69b90bd79bb0fc87d39180161926964ae9dd7cbc","title":"UReader: Universal OCR-free Visually-situated Language Understanding with Multimodal Large Language Model"},{"paperId":"f55bb5f2466ceec62a2e348e85059f7b99e2c105","title":"Video-CSR: Complex Video Digest Creation for Visual-Language Models"},{"paperId":"45afc0765fee381a279b4f3c8879b1b68aa7d6c8","title":"Compositional Semantics for Open Vocabulary Spatio-semantic Representations"},{"paperId":"84f9bc5f89dac53662fb467b6af8ff26415ca3e7","title":"InstructDET: Diversifying Referring Object Detection with Generalized Instructions"},{"paperId":"4001751f81dc71b79e48080da694720bbf01cb56","title":"Improving Discriminative Multi-Modal Learning with Large-Scale Pre-Trained Models"},{"paperId":"6d73d0547057134982acdaa190e741b1d037bd40","title":"Visual Abductive Reasoning Meets Driving Hazard Prediction: Problem Formulation and Dataset"},{"paperId":"e75bf8b34df50d84691c304c1c29e2d31f7caef9","title":"Analyzing Zero-Shot Abilities of Vision-Language Models on Video Understanding Tasks"},{"paperId":"24dd96da6f700f57132713aeb5e9b06905abab5d","title":"HowToCaption: Prompting LLMs to Transform Video Annotations at Scale"},{"paperId":"518c7ccb27ad4ca727311279e8c1e14acaf1f2c4","title":"(Re)framing Built Heritage through the Machinic Gaze"},{"paperId":"471da041762a44753251e2eeb16157a84da9fc25","title":"BrainSCUBA: Fine-Grained Natural Language Captions of Visual Cortex Selectivity"},{"paperId":"21cee445546991bcfe482bf90537e1db0c3c3d7f","title":"Measuring Information in Text Explanations"},{"paperId":"98260a8de3233f83b8992bc5beec72743a126bd1","title":"Stylist: Style-Driven Feature Ranking for Robust Novelty Detection"},{"paperId":"124d4d374fbef2016fa9880489871a58a7450644","title":"Improved Baselines with Visual Instruction Tuning"},{"paperId":"6983675062f186d487062b50e3c168ec7e60468e","title":"Leveraging Unpaired Data for Vision-Language Generative Models via Cycle Consistency"},{"paperId":"23cde1c9c8f4fe79e1ba5fc8f9c17bd1967f0a79","title":"Open-Fusion: Real-time Open-Vocabulary 3D Mapping and Queryable Scene Representation"},{"paperId":"d813e7edec4836dbf9d35363a6ed02782df3d18c","title":"ReForm-Eval: Evaluating Large Vision Language Models via Unified Re-Formulation of Task-Oriented Benchmarks"},{"paperId":"65a5a5cde93b65738d790b2f107b33010c605481","title":"On the Performance of Multimodal Language Models"},{"paperId":"477da0209e093448a8370862540182a1a77602fe","title":"ECoFLaP: Efficient Coarse-to-Fine Layer-Wise Pruning for Vision-Language Models"},{"paperId":"62e633f4b5cf8bc573e496602d3aa6e5919bbe61","title":"Improving Automatic VQA Evaluation Using Large Language Models"},{"paperId":"409961b88a3c7495afd21a9a4183c0bfaa858da0","title":"Kosmos-G: Generating Images in Context with Multimodal Large Language Models"},{"paperId":"7b8a85e0da9ac4701640f9cd77b744a4dda80794","title":"Multimodal Question Answering for Unified Information Extraction"},{"paperId":"c811b7e98b755ab7d34baa466796d00a93f662e7","title":"Navigating Cultural Chasms: Exploring and Unlocking the Cultural POV of Text-To-Image Models"},{"paperId":"f01ff5acf9e086030c01beda6f433f99013ebbd4","title":"Driving with LLMs: Fusing Object-Level Vector Modality for Explainable Autonomous Driving"},{"paperId":"bee68767debbdc96d6f75947e544a8be98b869e3","title":"Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model: Explorations with GPT4-Vision and Beyond"},{"paperId":"4f346e0e1cd1cfe8463af498e1e661a0b1f79126","title":"MarineDet: Towards Open-Marine Object Detection"},{"paperId":"d081501a74ef2934a2c30755b17fb5c339399b88","title":"SIEVE: Multimodal Dataset Pruning Using Image Captioning Models"},{"paperId":"e7d09b6f2bc878cf2c993acf675f409d0b55f35a","title":"MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens"},{"paperId":"77693ca00a8ef775af96b5c450aa0afdb0e10a51","title":"Talk2BEV: Language-enhanced Bird's-eye View Maps for Autonomous Driving"},{"paperId":"d540e23138ced475b53ec4908aa3bdd253c2976b","title":"HallE-Switch: Controlling Object Hallucination in Large Vision Language Models"},{"paperId":"388a5301b7020e1afb84a511ab57131d26e5e4ca","title":"Application of frozen large-scale models to multimodal task-oriented dialogue"},{"paperId":"ff543923421694ded755eb26db7f4a8196331f9a","title":"Direct Inversion: Boosting Diffusion-based Editing with 3 Lines of Code"},{"paperId":"ccd6f8b6544f112de632e49bfbe592a0a654537d","title":"DriveGPT4: Interpretable End-to-end Autonomous Driving via Large Language Model"},{"paperId":"e7b67fe319f812db6ba6789eb2d9bf8b445c1d64","title":"GRID: A Platform for General Robot Intelligence Development"},{"paperId":"0439bbee19b4fba596d1e2498b3f45ca265432ba","title":"Ground-A-Video: Zero-shot Grounded Video Editing using Text-to-image Diffusion Models"},{"paperId":"5ba1525dc6d382ee0a4a1ca3c64fc5907ca64c67","title":"Making LLaMA SEE and Draw with SEED Tokenizer"},{"paperId":"732ba2a13526fc828076a2e57277aed9ddcff198","title":"Towards reporting bias in visual-language datasets: bimodal augmentation by decoupling object-attribute association"},{"paperId":"d25abd12a3453180900bd6194a6f30f6ed893f94","title":"Fool Your (Vision and) Language Model With Embarrassingly Simple Permutations"},{"paperId":"20ae101289965d36dbd93e9b8c47ec9deab03ed0","title":"What If the TV Was Off? Examining Counterfactual Reasoning Abilities of Multi-modal Language Models"},{"paperId":"0296d5c4597c7aa44bf50fabd9bd2f1bb1051db0","title":"Towards an Exhaustive Evaluation of Vision-Language Foundation Models"},{"paperId":"8892b0937b1e6f3892647a812841e9dccacd7a34","title":"Dynamic Texts From UAV Perspective Natural Images"},{"paperId":"07d639b011f48615c1154cb6cdbc067bfe331348","title":"Reformulating Vision-Language Foundation Models and Datasets Towards Universal Multimodal Assistants"},{"paperId":"00c1ff63468305ea3fa430c2b3aef156d580c4ff","title":"PromptCap: Prompt-Guided Image Captioning for VQA with GPT-3"},{"paperId":"696d6b667926a559e63989d45eec53c3a15986be","title":"Pink: Unveiling the Power of Referential Comprehension for Multi-modal LLMs"},{"paperId":"a5d27bf7a2155d4ca016565a78b52ee90f81624c","title":"Beyond Task Performance: Evaluating and Reducing the Flaws of Large Multimodal Models with In-Context Learning"},{"paperId":"93c525267e93c78309a5b28a3eb0780704125744","title":"Analyzing and Mitigating Object Hallucination in Large Vision-Language Models"},{"paperId":"8d1f2e1beaf6905641740c6fee995f0b3f3e0938","title":"ProbVLM: Probabilistic Adapter for Frozen Vison-Language Models"},{"paperId":"2008356d334fb27ba7dbc638119f9d567d4adbd8","title":"Exploring Open-Vocabulary Semantic Segmentation from CLIP Vision Encoder Distillation Only"},{"paperId":"01e8f7ade6f9a625e30d79d62b389c9f6fed8bcc","title":"Text-image Alignment for Diffusion-based Perception"},{"paperId":"f349e5e8f0d18c948c1ffd92d3791db2b0ba2e55","title":"Data Filtering Networks"},{"paperId":"4d08d652a80050d3682a626ca0fa388534a160b4","title":"Building Privacy-Preserving and Secure Geospatial Artificial Intelligence Foundation Models (Vision Paper)"},{"paperId":"e448a0750f5ee1e54fd4fac374500f6e59270bcc","title":"TextField3D: Towards Enhancing Open-Vocabulary 3D Generation with Noisy Text Fields"},{"paperId":"93183f050e0ce0aff8ba0aa850c8353e24fb169d","title":"Fine-grained Late-interaction Multi-modal Retrieval for Retrieval Augmented Visual Question Answering"},{"paperId":"092245d86b77181c36f972b1b7a17a59cd989c4a","title":"Guiding Instruction-based Image Editing via Multimodal Large Language Models"},{"paperId":"332d6cfac230119a53b28fd9131ffb79b562b027","title":"PRIME: Prioritizing Interpretability in Failure Mode Extraction"},{"paperId":"e80b1b2484f3a020bc69cc5243b3bae49b0dfc3e","title":"FORB: A Flat Object Retrieval Benchmark for Universal Image Embedding"},{"paperId":"09565c0f4311f06c264086f2036a744dd2c92b62","title":"Toloka Visual Question Answering Benchmark"},{"paperId":"869acd5d303474fdd1dbf98c6aa6c8f8244a66b4","title":"Context-I2W: Mapping Images to Context-dependent Words for Accurate Zero-Shot Composed Image Retrieval"},{"paperId":"cc1a674bb164d09a060cf5b26fe518c02fae0ddc","title":"DreamGaussian: Generative Gaussian Splatting for Efficient 3D Content Creation"},{"paperId":"f2f9c02a7eb484dd7b7ac46892856e3f278eed77","title":"AnyMAL: An Efficient and Scalable Any-Modality Augmented Language Model"},{"paperId":"11a4284e335ba39330b59d9f42ca3272a6166991","title":"A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future"},{"paperId":"4f5c4ae2026b2bd97e26c6969e54cc634895e477","title":"Jointly Training Large Autoregressive Multimodal Models"},{"paperId":"a8f05b5ef3d60fb310f8366aa775118df5b700c3","title":"Tackling VQA with Pretrained Foundation Models without Further Training"},{"paperId":"b753162a5f807a7508741c798e1779d10eacedae","title":"One For All: Video Conversation is Feasible Without Video Instruction Tuning"},{"paperId":"c1e450284e7d6cac1855330a1197df8537df653f","title":"InternLM-XComposer: A Vision-Language Large Model for Advanced Text-image Comprehension and Composition"},{"paperId":"f11cd32e76cd17b561c088213dd5ee2930653c21","title":"BLIP-Adapter: Parameter-Efficient Transfer Learning for Mobile Screenshot Captioning"},{"paperId":"b0ef9c2007f0c0d59179e18868e3dce49d92db79","title":"Directional Texture Editing for 3D Models"},{"paperId":"20f3ca16e2b54aa817c0d9ef3f4dc096de924113","title":"VidChapters-7M: Video Chapters at Scale"},{"paperId":"5596bd3e26ec2207666ec1ff3db4415d212f14b9","title":"Connecting Speech Encoder and Large Language Model for ASR"},{"paperId":"96c43227831c4c3b12b7c64809e78674cea3a8a1","title":"DeepSpeed-VisualChat: Multi-Round Multi-Image Interleave Chat via Multi-Modal Causal Attention"},{"paperId":"844bb298d49ef4a07b5d4929dfdfd170f6a1d5f5","title":"Aligning Large Multimodal Models with Factually Augmented RLHF"},{"paperId":"1498ed76251d53d5a2fd2c56f3ca877f7eb32ac6","title":"Species196: A One-Million Semi-supervised Dataset for Fine-grained Species Recognition"},{"paperId":"772724892819d7e6f15ce536753fdc32d022c0e0","title":"A Survey on Image-text Multimodal Models"},{"paperId":"48ce837f6b41b49551db58f459e57cbd37566eca","title":"Identifying Systematic Errors in Object Detectors with the SCROD Pipeline"},{"paperId":"427939d4e1b9e12c5737244b1d9315b90497aa29","title":"Resolving References in Visually-Grounded Dialogue via Text Generation"},{"paperId":"9f4c17aebbb181756fab86ade02deadd90d5d4f9","title":"How Robust is Google's Bard to Adversarial Image Attacks?"},{"paperId":"1b588a1f24442ef275101ea9c53d7f1476ccf37b","title":"BELT: Bootstrapping Electroencephalography-to-Language Decoding and Zero-Shot Sentiment Classification by Natural Language Supervision"},{"paperId":"68240714989bd3b2f2a742a4fa71774021fa7184","title":"ContextRef: Evaluating Referenceless Metrics For Image Description Generation"},{"paperId":"6ab33b17cd45e7cbd2cb9b0c5a2d56e5eac1c814","title":"You Only Look at Screens: Multimodal Chain-of-Action Agents"},{"paperId":"e88bd2fa2329b25ebddabd67bf4878c4644cfecc","title":"A Large-scale Dataset for Audio-Language Representation Learning"},{"paperId":"7b689adb8c156d6158660f90d1c86888ee281f63","title":"DreamLLM: Synergistic Multimodal Comprehension and Creation"},{"paperId":"f34b6b4f75fb4e6f2c5654ee13fb2479c6170b3a","title":"Kosmos-2.5: A Multimodal Literate Model"},{"paperId":"e555100affa3131bc0e413ce8260870394772739","title":"Pointing out Human Answer Mistakes in a Goal-Oriented Visual Dialogue"},{"paperId":"72012855e57058c8adc04f71d6e5453819f715d1","title":"Language as the Medium: Multimodal Video Classification through text only"},{"paperId":"a281094d05e96b7cca044fdd87ff7c3c65649e20","title":"Investigating the Catastrophic Forgetting in Multimodal Large Language Models"},{"paperId":"5360550039b246d7d7388643708f731d5b57f6d9","title":"Does Video Summarization Require Videos? Quantifying the Effectiveness of Language in Video Summarization"},{"paperId":"c87f1f7ec6fd6b93d6f7626bebf6de2bf0b8c8e8","title":"Instruction-Following Speech Recognition"},{"paperId":"7a7128e9696dfedc24bf432c4b4c3aafa5e95a1a","title":"An Empirical Study of Scaling Instruct-Tuned Large Multimodal Models"},{"paperId":"679dc88f230072f5b41ad0c85398203be1134a6c","title":"A Continual Learning Paradigm for Non-differentiable Visual Programming Frameworks on Visual Reasoning Tasks"},{"paperId":"3ec464696db25acc2c39a6d967ec3df09e06f633","title":"Multimodal Multi-Hop Question Answering Through a Conversation Between Tools and Efficiently Finetuned Large Language Models"},{"paperId":"9d35d57ceba52ed4cea15355878f2e35495868e2","title":"Viewpoint Integration and Registration with Vision Language Foundation Model for Image Change Understanding"},{"paperId":"ceeb63c64cc30edc21a92c454ea905770196b43f","title":"MusiLingo: Bridging Music and Text with Pre-trained Language Models for Music Captioning and Query Response"},{"paperId":"872cb74ebbef880bc682b5b32ec22170ae5d2545","title":"PatFig: Generating Short and Long Captions for Patent Figures"},{"paperId":"3803d1f291e162bdaa4678a2c5a2bbcf63c050f4","title":"MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning"},{"paperId":"366564d210768814bc880e391b909cfbd95f8964","title":"SwitchGPT: Adapting Large Language Models for Non-Text Outputs"},{"paperId":"4eb87eaa193929dbef93fa2db9419245a8e8916f","title":"TextBind: Multi-turn Interleaved Multimodal Instruction-following in the Wild"},{"paperId":"8c68e6ed656b1a6712dcbafc972bdb18678b9d16","title":"Measuring the Quality of Text-to-Video Model Outputs: Metrics and Dataset"},{"paperId":"396305230ddcf915b19a19683a89e34d76321a33","title":"Cognitive Mirage: A Review of Hallucinations in Large Language Models"},{"paperId":"eae9ae43c5d4712e775912683268cf1ad02ff38f","title":"Sight Beyond Text: Multi-Modal Training Enhances LLMs in Truthfulness and Ethics"},{"paperId":"68708ea853006387537df85f4b811f7aeab6c4f5","title":"DreamStyler: Paint by Style Inversion with Text-to-Image Diffusion Models"},{"paperId":"2a085cec3833e50f4b222d33e43f0412335fb60a","title":"SoccerNet 2023 Challenges Results"},{"paperId":"d39182113cd4176ead48027b4fc05fe06ec6aaca","title":"Language Models as Black-Box Optimizers for Vision-Language Models"},{"paperId":"0b778079946764292de3771a489d5ce9e1868a8b","title":"The first step is the hardest: Pitfalls of Representing and Tokenizing Temporal Data for Large Language Models"},{"paperId":"16ed5f612b66cb7d91e534dd7126b69756f45c34","title":"HiLM-D: Towards High-Resolution Understanding in Multimodal Large Language Models for Autonomous Driving"},{"paperId":"fa75a55760e6ea49b39b83cb85c99a22e1088254","title":"NExT-GPT: Any-to-Any Multimodal LLM"},{"paperId":"7679dc8534cb1dd65c63c50b38f56386228d32d1","title":"Can you text what is happening? Integrating pre-trained language encoders into trajectory prediction models for autonomous driving"},{"paperId":"f8b0251b5ec8dc3fb6e4483df690149806e3aa13","title":"Incorporating Pre-trained Model Prompting in Multimodal Stock Volume Movement Prediction"},{"paperId":"f715b4e80ddd4515984c7a7591874e3a5b214fe1","title":"Editing 3D Scenes via Text Prompts without Retraining"},{"paperId":"392a07516142997b3542b0dfa516c49e32f506d6","title":"Neural Semantic Surface Maps"},{"paperId":"bcac614f9774488447221ebb4f16f05e3975ec1e","title":"Unified Language-Vision Pretraining in LLM with Dynamic Discrete Visual Tokenization"},{"paperId":"280353fd7a7a3e49c415c443e1b7ccf7de9c2b4e","title":"Measuring and Improving Chain-of-Thought Reasoning in Vision-Language Models"},{"paperId":"75ca0ce6493c94bb48766b042243023a5439beeb","title":"InstructDiffusion: A Generalist Modeling Interface for Vision Tasks"},{"paperId":"54c68b8623505dc6bf7a0b08aaa77ca9165f2d7f","title":"ImageBind-LLM: Multi-modality Instruction Tuning"},{"paperId":"fe672f07a0d30689d69da234d107901e881f314e","title":"DetermiNet: A Large-Scale Diagnostic Dataset for Complex Visually-Grounded Referencing using Determiners"},{"paperId":"aadd141d5853dabdc631b73d2bc8700e8ac7b8c0","title":"Reuse and Diffuse: Iterative Denoising for Text-to-Video Generation"},{"paperId":"1e291a7c189f4cce66cf647fdae9546465f73341","title":"Autoregressive Omni-Aware Outpainting for Open-Vocabulary 360-Degree Image Generation"},{"paperId":"01a5efd56d67efc4364dc496a99487efa6462fda","title":"Image Aesthetics Assessment via Learnable Queries"},{"paperId":"60872321f5fd58c454a25e8ad89ce880bca1ed7c","title":"Vote2Cap-DETR++: Decoupling Localization and Describing for End-to-End 3D Dense Captioning"},{"paperId":"c8712e4631241c7e00fb405b16f23f7a35b8fa36","title":"Distribution-Aware Prompt Tuning for Vision-Language Models"},{"paperId":"d7e0ca05d9aade28559d25db59ea54afd1e1ac0e","title":"NICE: CVPR 2023 Challenge on Zero-shot Image Captioning"},{"paperId":"316f980cfd2e217234386166a46eb080bf027cdd","title":"Physically Grounded Vision-Language Models for Robotic Manipulation"},{"paperId":"73814a52609a9ee4c8f1b115e376b6a300ab6a57","title":"CIEM: Contrastive Instruction Evaluation Method for Better Instruction Tuning"},{"paperId":"63c90ccf87293ecca791c374dd50635d335bd5a6","title":"DiverseMotion: Towards Diverse Human Motion Generation via Discrete Diffusion"},{"paperId":"5ab8337d6c594c4cec1ae67428a21dbfca1aff15","title":"MultiWay-Adapater: Adapting large-scale multi-modal models for scalable image-text retrieval"},{"paperId":"9db1ae5f17624020598a52abe961d894dc365c7a","title":"StyleAdapter: A Single-Pass LoRA-Free Model for Stylized Image Generation"},{"paperId":"f36c96827e629c86762bb96db70fab8f2660fc76","title":"Can I Trust Your Answer? Visually Grounded Video Question Answering"},{"paperId":"204fd6c5e247c477d607f507ee01d94a8dbd408f","title":"BLSP: Bootstrapping Language-Speech Pre-training via Behavior Alignment of Continuation Writing"},{"paperId":"cb2dea3be40b9c26397e03c273e744adeac33d74","title":"Zero-Shot Recommendations with Pre-Trained Large Language Models for Multimodal Nudging"},{"paperId":"355a3402d1a9f0c0199e1b8488ebeb5bc952b23a","title":"Large Content And Behavior Models To Understand, Simulate, And Optimize Content And Behavior"},{"paperId":"09602838d1fdb1d56c053e93c00fd9603b9abf29","title":"OpenIns3D: Snap and Lookup for 3D Open-vocabulary Instance Segmentation"},{"paperId":"156fc6ed50e9873814dd8554526eedf9841a1be0","title":"VideoGen: A Reference-Guided Latent Diffusion Approach for High Definition Text-to-Video Generation"},{"paperId":"2ffe53e55df2a8c32e53a9c4ecbbbc29307876fd","title":"Concept Parser With Multimodal Graph Learning for Video Captioning"},{"paperId":"f0614b98790329c096eed511d05e6c97af4e6d89","title":"TExplain: Explaining Learned Visual Features via Pre-trained (Frozen) Language Models"},{"paperId":"6f8d5b9383a85ed6aa481968d0f8451bfbaca848","title":"Distraction-free Embeddings for Robust VQA"},{"paperId":"b083a81fcf36e72bc3918bf8efa3de0e6541fc1e","title":"Socratis: Are large multimodal models emotionally aware?"},{"paperId":"f1dc0b8b844332f08b7503d9728e4831e8bd3607","title":"Expanding Frozen Vision-Language Models without Retraining: Towards Improved Robot Perception"},{"paperId":"6bcc6ab9c28805d4067e99b2cdc7524550fe80e1","title":"PointLLM: Empowering Large Language Models to Understand Point Clouds"},{"paperId":"ee0c9e8a935e047f2c030ecbfad93c2d80d642d7","title":"Sparkles: Unlocking Chats Across Multiple Images for Multimodal Instruction-Following Models"},{"paperId":"2928e5a5ee488104c5d7b636f577baef2d470310","title":"TouchStone: Evaluating Vision-Language Models by Language Models"},{"paperId":"6660a20e26e9e8c9916ffdc488e925e313605d8d","title":"Prompting Vision Language Model with Knowledge from Large Language Model for Knowledge-Based VQA"},{"paperId":"7b22ecd9f1ced58c1704ac6191e029b98054e330","title":"LLaSM: Large Language and Speech Model"},{"paperId":"f2ec0182c6646d3128afa5100f37d9de7b533463","title":"AnomalyGPT: Detecting Industrial Anomalies using Large Vision-Language Models"},{"paperId":"e1992a5b2fc5139b28fd6886e4106b63f0d03548","title":"A method for Selecting Scenes and Emotion-based Descriptions for a Robot’s Diary"},{"paperId":"0b0ba1e858573e25245c06e2ec54b72882e1d076","title":"Rethinking Mobile AI Ecosystem in the LLM Era"},{"paperId":"ccf43c35160954616b8f1c3c00e939883b666c2f","title":"Pixel-Aware Stable Diffusion for Realistic Image Super-resolution and Personalized Stylization"},{"paperId":"1b6e40c46f2e680620cf70218ae4edbc895d305f","title":"CoVR: Learning Composed Video Retrieval from Web Video Captions"},{"paperId":"894ed1aba8e42a4ec27ba53ecde383b14c5128ca","title":"Examining User-Friendly and Open-Sourced Large GPT Models: A Survey on Language, Multimodal, and Scientific GPT Models"},{"paperId":"95be6a38f9b3ca3b7a9d81215e52cdcf545d554a","title":"ORES: Open-vocabulary Responsible Visual Synthesis"},{"paperId":"fec17239569efd6914f0df9e25b66b310969d3c5","title":"EfficientDreamer: High-Fidelity and Robust 3D Creation via Orthogonal-view Diffusion Prior"},{"paperId":"ef1ebdb24ce45fb57e271783a0c9a877fe0e79c3","title":"Position-Enhanced Visual Instruction Tuning for Multimodal Large Language Models"},{"paperId":"49faa5c9bf6459a256f68872fb3b51df6b0a2dd8","title":"A Survey of Diffusion Based Image Generation Models: Issues and Their Solutions"},{"paperId":"c399a78321c2b0900d903eead65c683e0e451ad5","title":"Can Linguistic Knowledge Improve Multimodal Alignment in Vision-Language Pretraining?"},{"paperId":"e47d276bad18f441950c8136672ae6864e95323f","title":"VIGC: Visual Instruction Generation and Correction"},{"paperId":"e8ff485cdb398750a90b3e36214b85595e1d7ac1","title":"HuBo-VLM: Unified Vision-Language Model designed for HUman roBOt interaction tasks"},{"paperId":"e5daadcac08ebfc30759260cbd43812d4e3bc775","title":"DLIP: Distilling Language-Image Pre-training"},{"paperId":"437cfee2a7f7beadf09ad712f71b3265740e44a0","title":"Towards Realistic Zero-Shot Classification via Self Structural Semantic Alignment"},{"paperId":"717f8fded2be3d7343409be80f6fec045f8e9228","title":"APLA: Additional Perturbation for Latent Noise with Adversarial Training Enables Consistency"},{"paperId":"fc6a2f7478f68adefd69e2071f27e38aa1647f2f","title":"Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond"},{"paperId":"3e27dc211b1e1845d415b1fb9116130fd057f539","title":"Audio Generation with Multiple Conditional Diffusion Model"},{"paperId":"d7e92d03dfa5427c0c5ef2b59de54733e0589606","title":"InstructionGPT-4: A 200-Instruction Paradigm for Fine-Tuning MiniGPT-4"},{"paperId":"1245ef1926416d649b62323975c6fa22dfb885ee","title":"Large Multilingual Models Pivot Zero-Shot Multimodal Learning across Languages"},{"paperId":"e3abb313326ab2937d1a73dc4a45877d21af0075","title":"Overcoming Generic Knowledge Loss with Selective Parameter Update"},{"paperId":"645031d41240449567f072c6dd52e179f4a8e2d6","title":"ViCo: Engaging Video Comment Generation with Human Preference Rewards"},{"paperId":"7ae26eb895b2acd4949a33d0879cae15f1b0d6e3","title":"EVE: Efficient zero-shot text-based Video Editing with Depth Map Guidance and Temporal Consistency Constraints"},{"paperId":"afb39ed837db8750dd1c3b2a54ad442372c106b2","title":"WanJuan: A Comprehensive Multimodal Dataset for Advancing English and Chinese Large Models"},{"paperId":"66d3b7a6561148fd21c364315e67bf9373f50ef7","title":"An Examination of the Compositionality of Large Generative Vision-Language Models"},{"paperId":"f0950a3f27c0fefffba60ae1c9a8ee360d5eb55f","title":"Instruction Tuning for Large Language Models: A Survey"},{"paperId":"27e45a8aecc1fec246fd70c80d8f5104807cf0dd","title":"ViT-Lens: Towards Omni-modal Representations"},{"paperId":"da96ec9c32d63292e506ba8f8ea8e838df998c02","title":"StableLLaVA: Enhanced Visual Instruction Tuning with Synthesized Image-Dialogue Data"},{"paperId":"3ac3c10e1317fe8419f794cf30ce3227e95e1f54","title":"Causal Intersectionality and Dual Form of Gradient Descent for Multimodal Analysis: a Case Study on Hateful Memes"},{"paperId":"e58b0ee9a1fdb15a72ee721053df3569127cde42","title":"Tackling Vision Language Tasks Through Learning Inner Monologues"},{"paperId":"a32cfc2e12430cfe109a12087c9e0d8177c94397","title":"DUAW: Data-free Universal Adversarial Watermark against Stable Diffusion Customization"},{"paperId":"3647e7c48fa4888ba584d8b8a5c5813e6ee48366","title":"UniDoc: A Universal Large Multimodal Model for Simultaneous Text Detection, Recognition, Spotting and Understanding"},{"paperId":"eb5cf10406a8ad31e0ebe56b36571d5db4758a62","title":"PUMGPT: A Large Vision-Language Model for Product Understanding"},{"paperId":"362c78f5cc3fac37c3ca6d93f86019a45aad2324","title":"RLIPv2: Fast Scaling of Relational Language-Image Pre-training"},{"paperId":"de365213f0083c8388c54a0d063b9b11755abd35","title":"Text-Only Training for Visual Storytelling"},{"paperId":"d53945d4afb4528590d79e20de52883d29037e86","title":"FashionLOGO: Prompting Multimodal Large Language Models for Fashion Logo Embeddings"},{"paperId":"30cfc4e7174211aa48c965826d51db773f0d37c7","title":"Chat-3D: Data-efficiently Tuning Large Language Model for Universal Dialogue of 3D Scenes"},{"paperId":"9db8034f51981ce588caa9f44633a0b46e16d5cc","title":"Pro-Cap: Leveraging a Frozen Vision-Language Model for Hateful Meme Detection"},{"paperId":"394aaa4b6343ec1a25fbf4693ac0caf479984a03","title":"DragNUWA: Fine-grained Control in Video Generation by Integrating Text, Image, and Trajectory"},{"paperId":"07315e24f43308db08dcd0b61c16b40e5c0dcc90","title":"Boosting Commit Classification with Contrastive Learning"},{"paperId":"1fd31b74f5e1eeb67341982fd35a613c6fad10e0","title":"Link-Context Learning for Multimodal LLMs"},{"paperId":"10603f24428220cb3b78b0c23fb7e24cbee71f95","title":"Dancing Avatar: Pose and Text-Guided Human Motion Videos Synthesis with Image Diffusion Model"},{"paperId":"2e3dcf5a5d58ac210d0d87e9f918540a8373211a","title":"GIT-Mol: A Multi-modal Large Language Model for Molecular Science with Graph, Image, and Text"},{"paperId":"9144b4010332136da0584f202db624ce81d1bcba","title":"Foundation Models in Smart Agriculture: Basics, Opportunities, and Challenges"},{"paperId":"2854e5bab8e6f36e54c64456628a9559bf67019e","title":"IP-Adapter: Text Compatible Image Prompt Adapter for Text-to-Image Diffusion Models"},{"paperId":"d6c2523ab97416c2692cbbeab082ed1790e8e55e","title":"VisIT-Bench: A Benchmark for Vision-Language Instruction Following Inspired by Real-World Use"},{"paperId":"d25f8c388677d287d00ca67d44ef02da2b45f2d9","title":"Large Language Models and Knowledge Graphs: Opportunities and Challenges"},{"paperId":"d898963243ad4b177b799f197a7732395e165cf6","title":"Evaluating Picture Description Speech for Dementia Detection using Image-text Alignment"},{"paperId":"74d245de70e9a9f11d6eaa72439004e5cc2fabaf","title":"Prompting In-Context Operator Learning with Sensor Data, Equations, and Natural Language"},{"paperId":"7d78238a9bad60433d616abdd93c735087d99670","title":"LayoutLLM-T2I: Eliciting Layout Guidance from LLM for Text-to-Image Generation"},{"paperId":"8055af0f9e069af82ba22b637cf9d9c13f5a5974","title":"OmniDataComposer: A Unified Data Structure for Multimodal Data Fusion and Infinite Data Generation"},{"paperId":"cb712ab2b3bbaef6bff02efa7295ea420b58f654","title":"Fine-tuning Multimodal LLMs to Follow Zero-shot Demonstrative Instructions"},{"paperId":"1b656e3cdc49afab9ab9c1ec264850ef70153ecb","title":"Tiny LVLM-eHub: Early Multimodal Experiments with Bard"},{"paperId":"d3fd513594cd2e4cce10b50eb7ea16760b63a2b8","title":"AvatarVerse: High-quality & Stable 3D Avatar Creation from Text and Pose"},{"paperId":"4f2be887e991efa85f7b874e7ab871080a745c39","title":"CAESURA: Language Models as Multi-Modal Query Planners"},{"paperId":"2bd1b8990db73b6495c11082bea2d5f925c5226f","title":"SciGraphQA: A Large-Scale Synthetic Multi-Turn Question-Answering Dataset for Scientific Graphs"},{"paperId":"17099204d9c04841ce9c4ea6e3b4ba75affe5779","title":"E-CLIP: Towards Label-efficient Event-based Open-world Understanding by CLIP"},{"paperId":"e1266706b4f83130a170e2b066bf65a1e6d72387","title":"Improving Generalization of Image Captioning with Unsupervised Prompt Learning"},{"paperId":"5e26dddd695b09ee8ffddc458370f43b1aea746e","title":"A Comprehensive Analysis of Real-World Image Captioning and Scene Identification"},{"paperId":"94972e30504017156ef5b5debc419bf6edc67384","title":"MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities"},{"paperId":"df0ddb588a200d095743e9d26fc4a9318619766e","title":"Towards Generalist Foundation Model for Radiology"},{"paperId":"659a12d71d8709c132ccd9ccd235f0024cae0239","title":"The All-Seeing Project: Towards Panoptic Visual Recognition and Understanding of the Open World"},{"paperId":"7fbc502441d66daf1f53765d5d86a8dfba9ab0ce","title":"OpenFlamingo: An Open-Source Framework for Training Large Autoregressive Vision-Language Models"},{"paperId":"f660250f9fcd465acdf2e727d309acf1cc64c780","title":"ELIXR: Towards a general purpose X-ray artificial intelligence system through alignment of large language models and radiology vision encoders"},{"paperId":"475a4adb2fb0b5b3d8c37f600ad8c53c1689c0c2","title":"More Context, Less Distraction: Zero-shot Visual Classification by Inferring and Conditioning on Contextual Attributes"},{"paperId":"ad113d8b6f8f6dace7abd12dc88af520aaaf3fc7","title":"LISA: Reasoning Segmentation via Large Language Model"},{"paperId":"6f9b7c8cde1be2e62a503c31cac883c6d44c9d0d","title":"MovieChat: From Dense Token to Sparse Memory for Long Video Understanding"},{"paperId":"a5cddee937d7d2f005e781e453833cd64d3cf343","title":"Learning to Model the World with Language"},{"paperId":"4309d572a37d655779f9dce6a2c98c66334132de","title":"SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension"},{"paperId":"0fe88452660cb8a0e37f54bcd44f3cd6504354b5","title":"Unified Model for Image, Video, Audio and Language Tasks"},{"paperId":"872c111c4bed5aba086cc023ce6279edb469220a","title":"RSGPT: A Remote Sensing Vision Language Model and Benchmark"},{"paperId":"38939304bb760473141c2aca0305e44fbe04e6e8","title":"RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control"},{"paperId":"94f83b05752834b16dcbd93205aaf5d55c53db01","title":"Context-VQA: Towards Context-Aware and Purposeful Visual Question Answering"},{"paperId":"03d10eb9f1e03a059b41167a672af203419b0b7d","title":"Empowering MultiModal Models’ In-Context Learning Ability through Large Language Models"},{"paperId":"45cc3ccc88b07bb7c25889c014806cc04ad73d9a","title":"Exploring Annotation-free Image Captioning with Retrieval-augmented Pseudo Sentence Generation"},{"paperId":"ecb71f43d31416a320aba4e8845a9bc4cb37fde1","title":"Sample Less, Learn More: Efficient Action Recognition via Frame Feature Restoration"},{"paperId":"6eb3dd2b64db74f9743802acbb9875bdffb9d246","title":"Text-guided Foundation Model Adaptation for Pathological Image Classification"},{"paperId":"ead2d3cdbf5fffeed104f4c6b48902a87a37fecf","title":"Seeing through the Brain: Image Reconstruction of Visual Perception from Human Brain Signals"},{"paperId":"4814a0b3afb8b45749d7408aa798e2f389dd8aef","title":"Causal reasoning in typical computer vision tasks"},{"paperId":"95848bb02251ce2f107ec8bf567df22c8c575b6c","title":"A Survey on Generative Modeling with Limited Data, Few Shots, and Zero Shot"},{"paperId":"584ca135b61482fd89247113da87d784f738dbfa","title":"Foundational Models Defining a New Era in Vision: A Survey and Outlook"},{"paperId":"4be86923202f4a480067c54bb56e010239950e3d","title":"Benchmarking and Analyzing Generative Data for Visual Recognition"},{"paperId":"4712efe5f455c4983929692b0dceca0dee061297","title":"Learning Object Spatial Relationship from Demonstration"},{"paperId":"813ba033b8f593c98f9af44c5b4901408ba6f70a","title":"Towards a Visual-Language Foundation Model for Computational Pathology"},{"paperId":"8fd72a8bc1ab8fa08259656fa617d1c861f27239","title":"Interpreting Art by Leveraging Pre-Trained Models"},{"paperId":"838422b5ddaaa5637ba86056d1e964409bb2f016","title":"Robust Visual Question Answering: Datasets, Methods, and Future Challenges"},{"paperId":"2857576049b9b758416b4021a3fe6a7e13b14f3b","title":"Improving Viewpoint Robustness for Visual Recognition via Adversarial Training"},{"paperId":"97f4601d9ae203e8d14c354da722ed562e637473","title":"Identifying Interpretable Subspaces in Image Representations"},{"paperId":"a679c736d26dbcab41b483f2dbbc417da62c7a16","title":"UP-DP: Unsupervised Prompt Learning for Data Pre-Selection with Vision-Language Models"},{"paperId":"8b1ac02ae662c06de72daeac171a0a4461f1e692","title":"Improving Multimodal Datasets with Image Captioning"},{"paperId":"67188a50e1d8a601896f1217451b99f646af4ac8","title":"Towards A Unified Agent with Foundation Models"},{"paperId":"2e52178416b93a0a7dc1cb7d21d04c7e72c2ce0f","title":"ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring Instruction Tuning"},{"paperId":"8ab6174791a0299e779804300142237d1669c743","title":"Multimodal LLMs for health grounded in individual-specific data"},{"paperId":"eb3f3e42332a2aec2798752b30f93f93b647be8b","title":"Let's ViCE! Mimicking Human Cognitive Behavior in Image Generation Evaluation"},{"paperId":"87b8dfa1e68f56b3af825ae60c9ac073bc073113","title":"RewardTLG: Learning to Temporally Language Grounding from Flexible Reward"},{"paperId":"1c0dc6d5b4466f79686baa5e2bb2875520cd3332","title":"Unleashing the Imagination of Text: A Novel Framework for Text-to-image Person Retrieval via Exploring the Power of Words"},{"paperId":"9b4e61dda9db6317afae1bd4a12356d00769d9f3","title":"AnyDoor: Zero-shot Object-level Image Customization"},{"paperId":"962ccf1fc49c83817fb031e5b24b81b19cdfb89d","title":"BuboGPT: Enabling Visual Grounding in Multi-Modal LLMs"},{"paperId":"4d3b07859ee1fec686d82ba1975028e01fb5dd83","title":"BUS : Efficient and Effective Vision-language Pre-training with Bottom-Up Patch Summarization"},{"paperId":"631f4558f685cea7c2ce0a4285e0bbb5abe7d2df","title":"Image Captions are Natural Prompts for Text-to-Image Models"},{"paperId":"40298b8d50109c52fc10763eddc64a07cf8acb31","title":"Planting a SEED of Vision in Large Language Model"},{"paperId":"d364aa81e874ebab60d4ab8e32eee672cf63dc36","title":"PanopticBlue: Transformer-based Camera Sky-Cloud-Weather Recognition and Classification"},{"paperId":"7fc133b3a61e88338ae15a2bf72f08fdc2beb504","title":"SINC: Self-Supervised In-Context Learning for Vision-Language Tasks"},{"paperId":"c77ba737dc7f844a9ad16b66569bbf2fd19e029e","title":"Open Scene Understanding: Grounded Situation Recognition Meets Segment Anything for Helping People with Visual Impairments"},{"paperId":"b46bb1003b4c288028619201ede5afed4b29d8ec","title":"GenAssist: Making Image Generation Accessible"},{"paperId":"177bf0086a714ff305e45b720cda82e992c9fc7c","title":"PiTL: Cross-modal Retrieval with Weakly-supervised Vision-language Pre-training via Prompting"},{"paperId":"06f7066e76e5acf7b03d4fda2c0d12d71a209185","title":"Fine-grained Text-Video Retrieval with Frozen Image Encoders"},{"paperId":"98f8793a18eaced0ce93f5202065496cc5a84943","title":"Bootstrapping Vision-Language Learning with Decoupled Language Pre-training"},{"paperId":"369b449415d50387fba048bbd4d26ee890df84b5","title":"InternVid: A Large-scale Video-Text Dataset for Multimodal Understanding and Generation"},{"paperId":"41c6028c620debae00ca5b30e2db5977225fec57","title":"mBLIP: Efficient Bootstrapping of Multilingual Vision-LLMs"},{"paperId":"b37b1dc72b1882858f5120f2cd6883134089a6ed","title":"MMBench: Is Your Multi-modal Model an All-around Player?"},{"paperId":"a865f897eacb220c85dbc16977e5192e44556e31","title":"Can Vision-Language Models be a Good Guesser? Exploring VLMs for Times and Location Reasoning"},{"paperId":"1cd8373490efc2d74c2796f4b2aa27c7d4415ec9","title":"VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models"},{"paperId":"43dcf4e7f672b58981b885435ce083e6b069bfae","title":"T2I-CompBench: A Comprehensive Benchmark for Open-world Compositional Text-to-image Generation"},{"paperId":"ca31b8584b6c022ef15ddfe994fe361e002b7729","title":"A Comprehensive Overview of Large Language Models"},{"paperId":"94053805cd59f2e9a47fe3f080c7e7afefb337cc","title":"Generative Pretraining in Multimodality"},{"paperId":"73134ef3ac17961b4947c20aa5198c5b4affcc56","title":"EgoVLPv2: Egocentric Video-Language Pre-training with Fusion in the Backbone"},{"paperId":"650d7cf842b536dc6a3f46c189b9d8e3c8cc07a1","title":"Divide, Evaluate, and Refine: Evaluating and Improving Text-to-Image Alignment with Iterative VQA Feedback"},{"paperId":"451a3f03aca4aa87b93981364842137417549e58","title":"SVIT: Scaling up Visual Instruction Tuning"},{"paperId":"8a91067d34220e8542bc0cad529c214a5b1b75c0","title":"SAS Video-QA: Self-Adaptive Sampling for Efficient Video Question-Answering"},{"paperId":"ef4b604fca0c62dcd0d5caf7ca24ad74e285632d","title":"MultiQG-TI: Towards Question Generation from Multi-modal Sources"},{"paperId":"420f03d44a3bee8c1c51216e1045d0ff92e8fefc","title":"Vision Language Transformers: A Survey"},{"paperId":"97ffad903208c7bea48e4c8be0a68e27fc33a478","title":"Distilling Large Vision-Language Model with Out-of-Distribution Generalizability"},{"paperId":"ebddfdc5d845a788e8062eddbbf7a335737cb99b","title":"What Matters in Training a GPT4-Style Language Model with Multimodal Inputs?"},{"paperId":"df710c46594c04fb59ef9a93d3b4e1cb387a1b2b","title":"Embodied Task Planning with Large Language Models"},{"paperId":"c68d3127888912ead55096d30165de783fe24216","title":"JourneyDB: A Benchmark for Generative Image Understanding"},{"paperId":"8e2118de1b2723e0bf7afe60dc916e15567af516","title":"Image Background Serves as Good Proxy for Out-of-distribution Data"},{"paperId":"42b920abd44e76d73708859bfe13034555f1f8cb","title":"DoReMi: Grounding Language Model by Detecting and Recovering from Plan-Execution Misalignment"},{"paperId":"583a306fd169641152a147c3101bd1deef23d63d","title":"ProbVLM: Probabilistic Adapter for Frozen Vision-Language Models"},{"paperId":"42aab468882c1efc4ea33198c2eaffd0daadf184","title":"Look, Remember and Reason: Grounded reasoning in videos with language models"},{"paperId":"291d92da53d182c0fdf7eea465c9b519ef1fc1f3","title":"CLIPAG: Towards Generator-Free Text-to-Image Generation"},{"paperId":"a9d5d97733ccb15002ff3cfb95b0a7d8ba5236e3","title":"LLaVAR: Enhanced Visual Instruction Tuning for Text-Rich Image Understanding"},{"paperId":"f52293edcaccf33cc16c372175a69ffa63cf3460","title":"Towards Open Vocabulary Learning: A Survey"},{"paperId":"efc694164312006c543ef745611348ef64e68dda","title":"Towards Language Models That Can See: Computer Vision Through the LENS of Natural Language"},{"paperId":"a8f2c10466d356542b63dd0659dd113f8f0a3b34","title":"Federated Generative Learning with Foundation Models"},{"paperId":"2aa22c271390653cab888e5f8b22fadcdf68c7a9","title":"Palm: Predicting Actions through Language Models @ Ego4D Long-Term Action Anticipation Challenge 2023"},{"paperId":"e2a58fd18961c3941102989e3a3d0d27c615e015","title":"Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic"},{"paperId":"d9823ffa34f865fb1d0adef95d64a0c352ae125f","title":"REFLECT: Summarizing Robot Experiences for Failure Explanation and Correction"},{"paperId":"899316d60fde583dea135b82dc8506024b48bb3b","title":"Explainable Multimodal Emotion Reasoning"},{"paperId":"cde934546bbdb19094d8a53cc047d002c827f884","title":"Large Multimodal Models: Notes on CVPR 2023 Tutorial"},{"paperId":"dedbac319177d04ce63fe00d2fec24bdaab90d6d","title":"SugarCrepe: Fixing Hackable Benchmarks for Vision-Language Compositionality"},{"paperId":"3b6179c293df29e31d31cea46476f104ab6950f2","title":"Kosmos-2: Grounding Multimodal Large Language Models to the World"},{"paperId":"8724579d3f126e753a0451d98ff57b165f722e72","title":"Are aligned neural networks adversarially aligned?"},{"paperId":"d212fa27f5868f0fd106e1a7bba908fd47da0816","title":"MotionGPT: Human Motion as a Foreign Language"},{"paperId":"c7a7104df3db13737a865ede2be8146990fa4026","title":"Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning"},{"paperId":"b03118cadc0eb92cf4c4d75f2db3f24c8b15c460","title":"Chain-of-Thought Prompt Distillation for Multimodal Named Entity Recognition and Multimodal Relation Extraction"},{"paperId":"ebedc4d7a2356090904baba4104ef0832bc236df","title":"A Survey on Multimodal Large Language Models"},{"paperId":"697e0add95e880bd42e00bef838181e105f91981","title":"MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models"},{"paperId":"f3a8d7ae2ea571ca879368167ab51b71cfa423bf","title":"TaCA: Upgrading Your Visual Foundation Model with Task-agnostic Compatible Adapter"},{"paperId":"3a5d4352d3dd53148a9544233bb59f88d2504910","title":"Blended-NeRF: Zero-Shot Object Generation and Blending in Existing Neural Radiance Fields"},{"paperId":"f2f5c0d00e6a4ccaf099c11a9790aa0afefe611f","title":"Generative Multimodal Entity Linking"},{"paperId":"948e8cfae92c2004f2dd5c9316f5972f8baaea21","title":"OBELISC: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents"},{"paperId":"b01dd81fc103c7864a61f80085cb33bf69d90eb1","title":"VisoGender: A dataset for benchmarking gender bias in image-text pronoun resolution"},{"paperId":"e5d27e79d10a056cdeb86ca25853da8797413afb","title":"Improving Image Captioning Descriptiveness by Ranking and LLM-based Fusion"},{"paperId":"8ca84be6788622ed6700d2799bb6460b15707fca","title":"Dense Video Object Captioning from Disjoint Supervision"},{"paperId":"b7c081edd67f011fc45879c01a5166f889d77491","title":"RS5M and GeoRSCLIP: A Large Scale Vision-Language Dataset and A Large Vision-Language Model for Remote Sensing"},{"paperId":"d92c797f587ce7f1b001920ab9e6b7d31960bd77","title":"RemoteCLIP: A Vision Language Foundation Model for Remote Sensing"},{"paperId":"7839d037bb0e41f8a9898f177d2710cfe23633fc","title":"Path to Medical AGI: Unify Domain-specific Medical LLMs with the Lowest Cost"},{"paperId":"bc2333c9a667af90ee7ce52b911d2e04aed01526","title":"MotionGPT: Finetuned LLMs are General-Purpose Motion Generators"},{"paperId":"385ea5492cf26911bdba610c8edf0b3240e6e925","title":"Unsupervised Text Embedding Space Generation Using Generative Adversarial Networks for Text Synthesis"},{"paperId":"454850fcb311faf1de3f4028a312cfeb781857b4","title":"LLMVA-GEBC: Large Language Model with Video Adapter for Generic Event Boundary Captioning"},{"paperId":"8efc20988021ce3b4b05dd44b13e27260ee9b99b","title":"Investigating Prompting Techniques for Zero- and Few-Shot Visual Question Answering"},{"paperId":"c50348d3491567b2cdad5ea981620c31f876dad9","title":"Semantic HELM: A Human-Readable Memory for Reinforcement Learning"},{"paperId":"0983883619a0ca597d055d0e58da2f514052913d","title":"Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration"},{"paperId":"b634f9ba35123d40f0af8d96a9c154025cf2cf2a","title":"Contrasting Intra-Modal and Ranking Cross-Modal Hard Negatives to Enhance Visio-Linguistic Compositional Understanding"},{"paperId":"1474d4248e1cbcc91183456cdf1e7272e8a931de","title":"COSA: Concatenated Sample Pretrained Vision-Language Foundation Model"},{"paperId":"f30331e65ca2d5245b1a69db6aca4cb3c5e28b9a","title":"Training Multimedia Event Extraction With Generated Images and Captions"},{"paperId":"a8d02ff6d075c3cc48f0b97801cc52765c8f9ac9","title":"LVLM-eHub: A Comprehensive Evaluation Benchmark for Large Vision-Language Models"},{"paperId":"b937b5ad3c1ebe6007e744fa7864ec095e0070ab","title":"Tell Me Where to Go: A Composable Framework for Context-Aware Embodied Robot Navigation"},{"paperId":"966852963a88a28786b798c91b6662d6e501e590","title":"AssistGPT: A General Multi-modal Assistant that can Plan, Execute, Inspect, and Learn"},{"paperId":"051549d8ef56937b2f4d113afdcf8c7586d3770b","title":"Towards AGI in Computer Vision: Lessons Learned from GPT and Large Language Models"},{"paperId":"d4d4ca3f4e41c8121b07fb5d3f1f9514fc6b4c6e","title":"Babel-ImageNet: Massively Multilingual Evaluation of Vision-and-Language Representations"},{"paperId":"0b4fd518368afd639912a3425004d5ccc348c4d4","title":"Paste, Inpaint and Harmonize via Denoising: Subject-Driven Image Editing with Pre-Trained Diffusion Model"},{"paperId":"f7bcd0279f7f65112f1bcd13e539e26803b86bfc","title":"I See Dead People: Gray-Box Adversarial Attack on Image-To-Text Models"},{"paperId":"29cd4e8504df8c762b0b6eef8299584118feeb88","title":"Image Captioners Are Scalable Vision Learners Too"},{"paperId":"d98536f24272e258b1d399074b64284d64786099","title":"AVIS: Autonomous Visual Information Seeking with Large Language Models"},{"paperId":"bcb5416dc1bd7ad9e7e1ecde176ad8f21721d0c3","title":"Instruct-ReID: A Multi-purpose Person Re-identification Task with Instructions"},{"paperId":"9afa0c3227fd0ec3a76928784e59c4205cbace24","title":"AutoML in the Age of Large Language Models: Current Challenges, Future Opportunities and Risks"},{"paperId":"b36725dfe5e1b2aac1a5eead3d5e585dc4b96489","title":"MemoriEase: An Interactive Lifelog Retrieval System for LSC’23"},{"paperId":"6b138a2288a0d72ec3031f14dc103343fa6d3912","title":"Fill-Up: Balancing Long-Tailed Data with Generative Models"},{"paperId":"4279a38a098d1d359881b73c6a88a112fe93443a","title":"Scalable 3D Captioning with Pretrained Models"},{"paperId":"c581d2ad3b092a2cc152d0c6f55fd6320f78eb3a","title":"A Survey of Vision-Language Pre-training from the Lens of Multimodal Machine Translation"},{"paperId":"79150cb420d15830c8d36f0e91eea1b02e177f0f","title":"Sticker820K: Empowering Interactive Retrieval with Stickers"},{"paperId":"669a95efbb9a1388ef465c7d6eb891705842fa1c","title":"Controlling Text-to-Image Diffusion by Orthogonal Finetuning"},{"paperId":"4c4d176c6e28f48041f215d563f6ee8633534cff","title":"Valley: Video Assistant with Large Language model Enhanced abilitY"},{"paperId":"fd755dc7b5b206c17fd953db04e1c888d45b6e4e","title":"LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark"},{"paperId":"fed150a219f9c31bdb4920e615c7c9264c634736","title":"On the Challenges and Perspectives of Foundation Models for Medical Image Analysis"},{"paperId":"2933acb28b7369c7ea5b8728f6d8cb55e1beef98","title":"Customizing General-Purpose Foundation Models for Medical Report Generation"},{"paperId":"64c980e6b9bba5e854f9fe4bfdc86b9c2487b909","title":"DocumentCLIP: Linking Figures and Main Body Text in Reflowed Documents"},{"paperId":"bf7025a2e5dbb3c09deae02a1aa98a256ca559e2","title":"Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models"},{"paperId":"d47524cd5c3c4b57af2e5a29f6f91c420310f236","title":"MIMIC-IT: Multi-Modal In-Context Instruction Tuning"},{"paperId":"89689059d0cdcb52d7fbb6007ab953db22936a90","title":"M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models"},{"paperId":"da061a6e0016d6b625a8e86d64a797ca8ddb92a5","title":"Modular Visual Question Answering via Code Generation"},{"paperId":"d818f40ea693a335e02f32dab520351d271c58bf","title":"Artificial General Intelligence for Medical Imaging"},{"paperId":"d59c3daedc192ed40a5fe0fe83a594acbc04ed1d","title":"AircraftVerse: A Large-Scale Multimodal Dataset of Aerial Vehicle Designs"},{"paperId":"6a2a756c60dbc99f666ae6e32b0dd1a58e1e2de8","title":"M3IT: A Large-Scale Dataset towards Multi-Modal Multilingual Instruction Tuning"},{"paperId":"a01a9c4a114fbf201540268f928ccf77bc3f9357","title":"Fine-Grained Visual Prompting"},{"paperId":"d7a4b09a0e2c2d7b118144cf09895c640896da7b","title":"Youku-mPLUG: A 10 Million Large-scale Chinese Video-Language Dataset for Pre-training and Benchmarks"},{"paperId":"42ea55edb46395469aee1b760829657e65ab6577","title":"Zero-Shot 3D Shape Correspondence"},{"paperId":"3aa59581448441b7a05478c6922179ffba086afd","title":"Weakly-Supervised Conditional Embedding for Referred Visual Search"},{"paperId":"ae7ae7b7e123b8d5ec86cc1c53548943e88f386f","title":"Learning Probabilistic Symmetrization for Architecture Agnostic Equivariance"},{"paperId":"16877baf3874038233279e07e330f891455fd880","title":"ChatGPT as a mapping assistant: A novel method to enrich maps with generative AI and content derived from street-level photographs"},{"paperId":"5d321194696f1f75cf9da045e6022b2f20ba5b9c","title":"Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding"},{"paperId":"1edbde8f2cf41e74ab2c5fdb4e8aafd6599899d6","title":"MoviePuzzle: Visual Narrative Reasoning through Multimodal Order Learning"},{"paperId":"8213492345c67d2b0e692b6bb5c814d4f1aef8d2","title":"Benchmarking Robustness of Adaptation Methods on Pre-trained Vision-Language Models"},{"paperId":"8e9b649f6223caec508d55b0e6574a454c637a26","title":"Revisiting the Role of Language Priors in Vision-Language Models"},{"paperId":"77424562deba33e94ea5ca3c662ccfdc2b95fb5c","title":"Vocabulary-free Image Classification"},{"paperId":"31a68755ca6899e6c360ec8568704ae74f223a25","title":"GPT4Image: Can Large Pre-trained Models Help Vision Models on Perception Tasks?"},{"paperId":"3bdfc51afcfb62426b73814067f7ac90cfe42888","title":"Evaluating the Capabilities of Multi-modal Reasoning Models with Synthetic Task Data"},{"paperId":"99368d6fc86e2eb181d9d36165cfed578bfe938d","title":"Q: How to Specialize Large Vision-Language Models to Data-Scarce VQA Tasks? A: Self-Train on Unlabeled Images!"},{"paperId":"f7c6553d47bbe5682377e975058592d5d495ad39","title":"“Let’s not Quote out of Context”: Unified Vision-Language Pretraining for Context Assisted Image Captioning"},{"paperId":"0867f7029b3726740fb41ca8171833bf6f82e483","title":"Exploring Open-Vocabulary Semantic Segmentation without Human Labels"},{"paperId":"9ecda80a94213c4d8322ccfb34ff6e1bfc4a9390","title":"The Hidden Language of Diffusion Models"},{"paperId":"2c5ab7d87e3342d2dba7d1d113ca1b16c545e344","title":"AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration"},{"paperId":"43ec80eeb6f22431ae741796996b25ca3b6bf3e2","title":"Adapting Pre-trained Language Models to Vision-Language Tasks via Dynamic Visual Prompting"},{"paperId":"5fb7afae5fcacae1d40f109a348b43e00aa5d486","title":"Dense and Aligned Captions (DAC) Promote Compositional Reasoning in VL Models"},{"paperId":"eafce53443e9e6800c3850807dff74a5bb8c7c2b","title":"Using Visual Cropping to Enhance Fine-Detail Question Answering of BLIP-Family Models"},{"paperId":"461437e165fa8236373bdcc4db5f09667863bc29","title":"Chatting Makes Perfect: Chat-based Image Retrieval"},{"paperId":"d4af12327385260116dfd68ed1ec6d0602d26d1f","title":"Improving CLIP Training with Language Rewrites"},{"paperId":"d07b45085a2c26932a5af53d2690e9652c9a1bd0","title":"RealignDiff: Boosting Text-to-Image Diffusion Model with Coarse-to-fine Semantic Re-alignment"},{"paperId":"80be1426825288ff876acb8cc0babcc6629fa644","title":"AlphaBlock: Embodied Finetuning for Vision-Language Reasoning in Robot Manipulation"},{"paperId":"b458fc5261595f44b36325e5eaea1f874d65138f","title":"GPT4Tools: Teaching Large Language Model to Use Tools via Self-instruction"},{"paperId":"d7f1a876b7df0e10627bccb0c5b63faf2a1005e4","title":"PanoGen: Text-Conditioned Panoramic Environment Generation for Vision-and-Language Navigation"},{"paperId":"fd928577d67dd01048d13f284a6256164bbcf2f0","title":"Learning without Forgetting for Vision-Language Models"},{"paperId":"5d36d8817fb0ab69ee396250aed204dea70da9f8","title":"LANCE: Stress-testing Visual Models by Generating Language-guided Counterfactual Images"},{"paperId":"83ce9f983bf0d3691cdea9bb5beffa7e4f970a4d","title":"Real-World Image Variation by Aligning Diffusion Inversion Chain"},{"paperId":"ca3a71df6d2ec2abe3ec8a0049ac6fd93b307d96","title":"Generate then Select: Open-ended Visual Question Answering Guided by World Knowledge"},{"paperId":"a08e36ca94f702a6916010dabb09d8e8235ba6b7","title":"InstructEdit: Improving Automatic Masks for Diffusion-based Image Editing With User Instructions"},{"paperId":"50c1414fe41d0cb9db6f0933c9319aa124beac5d","title":"Contextual Object Detection with Multimodal Large Language Models"},{"paperId":"4e33c5756aa18d248cf50fef9382acda1e0f65da","title":"VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset"},{"paperId":"5fbe4c92791fbecb179c1ab79bba9a59b2e155ba","title":"GlyphControl: Glyph Conditional Control for Visual Text Generation"},{"paperId":"28870aedc2f1653e6d69f11fb792bc87537414cb","title":"FuseCap: Leveraging Large Language Models for Enriched Fused Image Captions"},{"paperId":"119a3ed0898499fce0ce6af6958d566d82390ba5","title":"GIMLET: A Unified Graph-Text Model for Instruction-Based Molecule Zero-Shot Learning"},{"paperId":"52ff22fd0cc294d09717027e8b2077e5ac979d8d","title":"Distill Gold from Massive Ores: Efficient Dataset Distillation via Critical Samples Selection"},{"paperId":"1fed19184785b2b50163b3d8ccb7bfaa0321d1aa","title":"CrossGET: Cross-Guided Ensemble of Tokens for Accelerating Vision-Language Transformers"},{"paperId":"5712960ca1d637ba6e57de43fad3daac04bff4e2","title":"Towards Consistent Video Editing with Text-to-Image Diffusion Models"},{"paperId":"8ecdbfe011b7189fa0ee49ffc4e42a93d728a371","title":"On Evaluating Adversarial Robustness of Large Vision-Language Models"},{"paperId":"6fb5c0eff3696ef252aca9638e10176ecce7cecb","title":"Generating Images with Multimodal Language Models"},{"paperId":"8199c9d55dd998f69f703e0ad250ca0697e3ad27","title":"NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models"},{"paperId":"41640c2acbdf158f8a538a5441caab80514c7bfe","title":"A Reminder of its Brittleness: Language Reward Shaping May Hinder Learning for Instruction Following Agents"},{"paperId":"d0410aae3f6648f5ee4176dc3308e98645e1d4fa","title":"S4M: Generating Radiology Reports by A Single Model for Multiple Body Parts"},{"paperId":"7657e124622858a970815a96991727884088d648","title":"Custom-Edit: Text-Guided Image Editing with Customized Diffusion Models"},{"paperId":"c6ac708b65b24c20f80831d518c1795ce8133ad5","title":"ChatBridge: Bridging Modalities with Large Language Model as a Language Catalyst"},{"paperId":"06091944b864d6dc473cab63321a95fb9c4067cc","title":"ChatCAD+: Towards a Universal and Reliable Interactive CAD using LLMs"},{"paperId":"5eceaeac5d45d49e1d5698947ed8292ff3fccd81","title":"Candidate Set Re-ranking for Composed Image Retrieval with Dual Multi-modal Encoder"},{"paperId":"d3f79210b54e168c76b8c311488f42d7d1048b81","title":"PandaGPT: One Model To Instruction-Follow Them All"},{"paperId":"9c3a9b4821daa03cb5369041d59d2714329a3811","title":"Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models"},{"paperId":"08b562aa8066c2342f0d03824221dea18f0a18d2","title":"Cream: Visually-Situated Natural Language Understanding with Contrastive Reading Model and Frozen Large Language Models"},{"paperId":"7cf64070fd3d7e53d80f260c10e6bd7018d580e1","title":"IdealGPT: Iteratively Decomposing Vision and Language Reasoning via Large Language Models"},{"paperId":"81dc25d636c022496dc930cb3c5ba2175592bc6e","title":"Interpretable by Design Visual Question Answering"},{"paperId":"9837349417e36ef5be06da0fd6c74042148bdaa2","title":"Visual Programming for Text-to-Image Generation and Evaluation"},{"paperId":"dc0c132b273456b288a785414db2fa72edf87b1a","title":"BLIP-Diffusion: Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing"},{"paperId":"d57caa06a42baa90eb741a9afb10fe4fff8be82a","title":"SmartTrim: Adaptive Tokens and Parameters Pruning for Efficient Vision-Language Models"},{"paperId":"0744783bbefc12b2b1383bed137e8a80061274b7","title":"Exploring Diverse In-Context Configurations for Image Captioning"},{"paperId":"00cb69a9f280317d1c59ac5827551ee9b10642b8","title":"EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought"},{"paperId":"2f4d3ab8ce8a37b035bcf9d007b78cd88bf1453d","title":"Alt-Text with Context: Improving Accessibility for Images on Twitter"},{"paperId":"ea62c8802554a203f39d790959b7b75475413034","title":"Learning UI-to-Code Reverse Generator Using Visual Critic Without Rendering"},{"paperId":"f4f88f2679ab7c4a6ac01eed77f52f9b5e01f2eb","title":"Weakly-Supervised Learning of Visual Relations in Multimodal Pretraining"},{"paperId":"e9ae0c76a71b8f302eb17b1c4462b9cc97d87cd0","title":"LLM-grounded Diffusion: Enhancing Prompt Understanding of Text-to-Image Diffusion Models with Large Language Models"},{"paperId":"b82c1b0512d25307e3c81bb8d9df1607267a7a52","title":"MemeCap: A Dataset for Captioning and Interpreting Memes"},{"paperId":"c22a8e36b7ffa69da0d70c9db58c78252567400a","title":"ReSee: Responding through Seeing Fine-grained Visual Knowledge in Open-domain Dialogue"},{"paperId":"0a61802b71aa044cf1fe0e81befec148e0d5001b","title":"VisorGPT: Learning Visual Prior via Generative Pre-Training"},{"paperId":"2ad8183c72a90511383a32ccaeea313eb85f4085","title":"DetGPT: Detect What You Need via Reasoning"},{"paperId":"b09533b1c2f4ee3261855147b01ebe481ea8dc1b","title":"R2H: Building Multimodal Navigation Helpers that Respond to Help Requests"},{"paperId":"c5b7f9e2af19db0c2bc9d57a113c17aa9d4d6fda","title":"S-CLIP: Semi-supervised Vision-Language Learning using Few Specialist Captions"},{"paperId":"43a55dbd95c9d5cd82de8db276f41adeec4a937d","title":"Interactive Data Synthesis for Systematic Vision Adaptation via LLMs-AIGCs Collaboration"},{"paperId":"0cc49320f77e384a9acde7fa9c1b7c776a4f04a4","title":"If at First You Don't Succeed, Try, Try Again: Faithful Diffusion-based Text-to-Image Generation by Selection"},{"paperId":"ca055cfb9d4d47124cc035c346f38577825fcacf","title":"Enhance Reasoning Ability of Visual-Language Models via Large Language Models"},{"paperId":"684c8dccfe7afc1b05057ebd5df0c90379443796","title":"AudioToken: Adaptation of Text-Conditioned Diffusion Models for Audio-to-Image Generation"},{"paperId":"f9bfc6d9ba1665b73af3323d46c7642b852759ef","title":"VideoLLM: Modeling Video Sequence with Large Language Models"},{"paperId":"251445d8b22b1c25ccad96f284c085dca49b57f3","title":"UVOSAM: A Mask-free Paradigm for Unsupervised Video Object Segmentation via Segment Anything Model"},{"paperId":"3c2fcef50b952097a31cfe1b9e1b1b89d5599744","title":"ViT-TTS: Visual Text-to-Speech with Scalable Diffusion Transformer"},{"paperId":"6a5525c316b9be7909c433a79e090ed731425083","title":"What Makes for Good Visual Tokenizers for Large Language Models?"},{"paperId":"ffcbc17638beb1e1aff3eba1dd48735ed72a02d4","title":"LLM-CXR: Instruction-Finetuned LLM for CXR Image Understanding and Generation"},{"paperId":"9f411fda2ad5b141a3115f707bcf5ee865b3fb94","title":"Any-to-Any Generation via Composable Diffusion"},{"paperId":"1b0841c955ffc296348ab2bb1e98fdb0995e928b","title":"Enhancing Vision-Language Pre-Training with Jointly Learned Questioner and Dense Captioner"},{"paperId":"daf34122a0c38531aeeb55069ba98e564c263d53","title":"MedBLIP: Bootstrapping Language-Image Pre-training from 3D Medical Images and Texts"},{"paperId":"50bbf2c11984d18aa14f964a4909ac25f07e50ea","title":"VideoFactory: Swap Attention in Spatiotemporal Diffusions for Text-to-Video Generation"},{"paperId":"972501b057e2b84d6ce6506f70bcac697bab7872","title":"LLMScore: Unveiling the Power of Large Language Models in Text-to-Image Synthesis Evaluation"},{"paperId":"3130643a5d02f0e849d83bb1f85577a924081f36","title":"Paxion: Patching Action Knowledge in Video-Language Foundation Models"},{"paperId":"86ea4aa29241149c3999301f0285d8cbb8542b11","title":"Vision-Language Pre-training with Object Contrastive Learning for 3D Scene Understanding"},{"paperId":"bfa9df38bd8597d8cdf0c3497fe5e5a5e31f646f","title":"ONE-PEACE: Exploring One General Representation Model Toward Unlimited Modalities"},{"paperId":"1bdd5fc17cc580efe998304692639c57c857cc84","title":"Going Denser with Open-Vocabulary Part Segmentation"},{"paperId":"42a30dc5470f54ec249f25d3c31e05d7c376c8e3","title":"VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks"},{"paperId":"c3eee48481b3b8f4be18026e389fadf9a53ad192","title":"Content-based Unrestricted Adversarial Attack"},{"paperId":"8ce6ad6d8a73757309d3b9f525cf15cb68e32397","title":"Efficient Prompting via Dynamic In-Context Learning"},{"paperId":"4bb0b12803791764d641a4cef1e0ce39cf049542","title":"Listen, Think, and Understand"},{"paperId":"f6fdac9b5e771394d22bfd5fbaf8147a52b6e792","title":"UniControl: A Unified Diffusion Model for Controllable Visual Generation In the Wild"},{"paperId":"a979975d1a0aea0e01423f092249cc3de575b6cd","title":"X-IQE: eXplainable Image Quality Evaluation for Text-to-Image Generation with Visual Large Language Models"},{"paperId":"32ef37692ace804e041ac28a219211e80f150c3b","title":"Vision-language models boost food composition compilation"},{"paperId":"206400aba5f12f734cdd2e4ab48ef6014ea60773","title":"Evaluating Object Hallucination in Large Vision-Language Models"},{"paperId":"b1ef91c5541f88297c7551e1adf15fcee7987197","title":"What You See is What You Read? Improving Text-Image Alignment Evaluation"},{"paperId":"1a3d6119d9513ad27fa4fc3262e517ec6a6d2261","title":"FastComposer: Tuning-Free Multi-Subject Image Generation with Localized Attention"},{"paperId":"f4793adffd6f67ffcb93ccfc5672ab301b8a2b96","title":"PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering"},{"paperId":"9b3ff4c05be2ee57021099ab07fadfb77440be45","title":"IMAD: IMage-Augmented multi-modal Dialogue"},{"paperId":"5f51eda9f7abddca027941d50fb0b6bf6f508eff","title":"Make-A-Protagonist: Generic Video Editing with An Ensemble of Experts"},{"paperId":"adf4655e6bb531ec1a03d8ec9e8c5c63ae771fb6","title":"Edit As You Wish: Video Description Editing with Multi-grained Commands"},{"paperId":"2bbd77ac3aeea12b1d7ceeaf3f4d55bce922c9de","title":"Bridging the Domain Gap: Self-Supervised 3D Scene Understanding with Foundation Models"},{"paperId":"ebe9fecee36516832b980e98dd156201fbfbd639","title":"Semantic Composition in Visually Grounded Language Models"},{"paperId":"4b203ee52e27cbf27d210dd671951150729a8259","title":"ULIP-2: Towards Scalable Multimodal Pre-training for 3D Understanding"},{"paperId":"848e690a62c327e1210532d58a6b914097cac763","title":"On the Hidden Mystery of OCR in Large Multimodal Models"},{"paperId":"65051f6836a4a618586c01deff43b46ab5e3f887","title":"Measuring Progress in Fine-grained Vision-and-Language Understanding"},{"paperId":"8bd6a2a89503be083176f2cc26fabedb79238cbd","title":"InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning"},{"paperId":"8badb0587fef2ffc078b0cec549eb8ec96ed3ad4","title":"Self-Chained Image-Language Model for Video Localization and Question Answering"},{"paperId":"240bc60c98c9b860c27c6f962992618a6775cab1","title":"Musketeer (All for One, and One for All): A Generalist Vision-Language Model with Task Explanation Prompts"},{"paperId":"38be7643bcad936739550a1802220eb53ca9b1df","title":"Simple Token-Level Confidence Improves Caption Correctness"}],"references":[{"paperId":"70feb009bc1e8b1cb8dff64bf9fd67789636438b","title":"From Images to Textual Prompts: Zero-shot Visual Question Answering with Frozen Large Language Models"},{"paperId":"78281482c1fdad8e167bab39cc9955c73d58ae8f","title":"EVA: Exploring the Limits of Masked Visual Representation Learning at Scale"},{"paperId":"cdbd4f9b6ab2e2fd1ddf5400d5ed2c18960635d1","title":"Scaling Instruction-Finetuned Language Models"},{"paperId":"26fd105d0b5a458979c012cddb3ba2de943388c4","title":"Plug-and-Play VQA: Zero-shot VQA by Conjoining Large Pretrained Models with Zero Training"},{"paperId":"1f86bf1e334200ec0481349255559fbfe7a33caa","title":"MAPL: Parameter-Efficient Adaptation of Unimodal Pre-Trained Models for Vision-Language Few-Shot Prompting"},{"paperId":"28630034bb29760df01ab033b743e30b37f336ae","title":"PaLI: A Jointly-Scaled Multilingual Language-Image Model"},{"paperId":"02251886950770e82b3d68564d60cdfe15e73199","title":"Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks"},{"paperId":"a26a7a74f1e5fd562be95c3611a0680759fbdf84","title":"CoCa: Contrastive Captioners are Image-Text Foundation Models"},{"paperId":"13a0d8bb38f739990c8cd65a44061c6534f17221","title":"OPT: Open Pre-trained Transformer Language Models"},{"paperId":"26218bdcc3945c7edae7aa2adbfba4cd820a2df3","title":"Flamingo: a Visual Language Model for Few-Shot Learning"},{"paperId":"8342b592fe238f3d230e4959b06fd10153c45db1","title":"Training Compute-Optimal Large Language Models"},{"paperId":"7f71875f8214dffa4f3276da123c4990a6d437cc","title":"Enabling Multimodal Generation on CLIP via Vision-Language Knowledge Distillation"},{"paperId":"1bfa62ddfa3f6691e0e40c06f8ead594b6449cfa","title":"OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework"},{"paperId":"a3b42a83669998f65df60d7c065a70d07ca95e99","title":"BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation"},{"paperId":"21ec90872abd986c12afe39bebe807732ffa70c9","title":"Florence: A New Foundation Model for Computer Vision"},{"paperId":"197d5867a45a2988f4dd159063cdfbfe90164962","title":"LiT: Zero-Shot Transfer with Locked-image text Tuning"},{"paperId":"f675c62abfa788ea0be85d3124eba15a14d5e9d6","title":"FILIP: Fine-grained Interactive Language-Image Pre-Training"},{"paperId":"cf7c2e0e4fb2af689aaf4b7a7cddf7b1f4d5e3f0","title":"VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts"},{"paperId":"b668ce936cff0b0ca8b635cd5f25a62eaf4eb3df","title":"LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs"},{"paperId":"32d59ab951be74be351f9777da2cbc71bb68c3c1","title":"A Good Prompt Is Worth Millions of Parameters: Low-resource Prompt-based Learning for Vision-Language Models"},{"paperId":"5e00596fa946670d894b1bdaeff5a98e3867ef13","title":"SimVLM: Simple Visual Language Model Pretraining with Weak Supervision"},{"paperId":"b82c5f9efdb2ae56baa084ca41aeddd8a665c1d1","title":"Align before Fuse: Vision and Language Representation Learning with Momentum Distillation"},{"paperId":"01b5412f3d17e90e09226d7c40ad4d4468a1414d","title":"Multimodal Few-Shot Learning with Frozen Language Models"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"616e0ed02ca024a8c1d4b86167f7486ea92a13d9","title":"VisualGPT: Data-efficient Adaptation of Pretrained Language Models for Image Captioning"},{"paperId":"394be105b87e9bfe72c20efe6338de10604e1a11","title":"Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts"},{"paperId":"141a5033d9994242b18bb3b217e79582f1ee9306","title":"Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision"},{"paperId":"cb596bffc5c5042c254058b62317a57fa156fea4","title":"Unifying Vision-and-Language Tasks via Text Generation"},{"paperId":"be0014c1fbc3e664686610d2c85f75038a4f6e4f","title":"VinVL: Making Visual Representations Matter in Vision-Language Models"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"818e5cbc337e4e1b98e65a2d7c2d6d2a0318cd57","title":"Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks"},{"paperId":"43f2ad297941db230c089ba353efc3f281ab678c","title":"5分で分かる!? 有名論文ナナメ読み：Jacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"dfc7b58b67c31932b48586b3e23a43cc94695290","title":"UNITER: UNiversal Image-TExt Representation Learning"},{"paperId":"79c93274429d6355959f1e4374c2147bb81ea649","title":"LXMERT: Learning Cross-Modality Encoder Representations from Transformers"},{"paperId":"28ad018c39d1578bea84e7cedf94459e3dbe1e70","title":"OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge"},{"paperId":"1c71771c701aadfd72c5866170a9f5d71464bb88","title":"Unified Language Model Pre-training for Natural Language Understanding and Generation"},{"paperId":"1ab7f7c1d328589f25c79515b9a5d824d7ffbbd1","title":"GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering"},{"paperId":"b4df354db88a70183a64dbc9e56cf14e7669a6c0","title":"Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning"},{"paperId":"d07284a6811f1b2745d91bdb06b040b57f226882","title":"Decoupled Weight Decay Regularization"},{"paperId":"7e232313a59d735ef7c8a9f4cc7bc980a29deb5e","title":"Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering"},{"paperId":"afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d","title":"Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations"},{"paperId":"11c9c31dff70de92ada9160c78ff8bb46b2912d6","title":"Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models"},{"paperId":"71b7178df5d2b112d07e45038cb5637208659ff7","title":"Microsoft COCO: Common Objects in Context"},{"paperId":"8e080b98efbe65c02a116439205ca2344b9f7cd4","title":"Im2Text: Describing Images Using 1 Million Captioned Photographs"},{"paperId":"ecce44df1956db4ec486539c6543345344809958","title":"Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"8b55402ffee2734bfc7d5d7595500916e1ef04e8","title":"nocaps: novel object captioning at scale"}],"id":"336ce63b472a65f053f854d45851d6f0e896f05e","summary":"BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods, and is demonstrated's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions."},{"url":"https://www.semanticscholar.org/paper/af997821231898a5f8d0fd78dad4eec526acabe5","title":"Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models","venue":"arXiv.org","year":2023,"referenceCount":61,"citationCount":320,"influentialCitationCount":6,"publicationDate":"08/03/2023","authors":"Chenfei Wu,Sheng-Kai Yin,Weizhen Qi,Xiaodong Wang,Zecheng Tang,Nan Duan","citations":[{"paperId":"c44393114047e0f9c32e45b381970d50ed503260","title":"LLMs Meet Long Video: Advancing Long Video Comprehension with An Interactive Visual Adapter in LLMs"},{"paperId":"b6b5ffb9a1faee7b488eb46f2d68f62beaecb8c7","title":"Modality-Aware Integration with Large Language Models for Knowledge-based Visual Question Answering"},{"paperId":"83d201d503b863fec7d1225f00a141e722e03f17","title":"Using Left and Right Brains Together: Towards Vision and Language Planning"},{"paperId":"710b1e23b09e0b826f9d47e7cc23b5f4c0808c7e","title":"Multi-modal preference alignment remedies regression of visual instruction tuning on language model"},{"paperId":"5439a10e45bf1af937b1009348fb9582d9c3d640","title":"L3GO: Language Agents with Chain-of-3D-Thoughts for Generating Unconventional Objects"},{"paperId":"f4e4f2090b639372daf32d4b466752b0fb32b261","title":"Tell Me More! Towards Implicit User Intention Understanding of Language Model Driven Agents"},{"paperId":"d6fc25e3ccb0ea314eefe7c49f02985e4ddd48de","title":"Towards a Foundation Model for Brain Age Prediction using coVariance Neural Networks"},{"paperId":"b90ea57bf10f7ed1d50dd051604d68fb892c5633","title":"GraphTranslator: Aligning Graph Model to Large Language Model for Open-ended Tasks"},{"paperId":"930b5331056d6e543191866df530ce3d5061c2ed","title":"The Future of Cognitive Strategy-enhanced Persuasive Dialogue Agents: New Perspectives and Trends"},{"paperId":"eaad6e351ab7ddb5a31bce3c5fe8bf38cd08c7f2","title":"Multimodal Query Suggestion with Multi-Agent Reinforcement Learning from Human Feedback"},{"paperId":"4c98e18cf16395b95ffaaeeac3eceffa608dcf8d","title":"\"Task Success\" is not Enough: Investigating the Use of Video-Language Models as Behavior Critics for Catching Undesirable Agent Behaviors"},{"paperId":"7e281e8ab380affd3c5724feae038274df378511","title":"Understanding the planning of LLM agents: A survey"},{"paperId":"83ee82e62f2eae18cc3472120eb9004109895a31","title":"Delving into Multi-modal Multi-task Foundation Models for Road Scene Understanding: From Learning Paradigm Perspectives"},{"paperId":"33b5dd24169ff876420f56c19c0a65b6194cb49f","title":"User Intent Recognition and Satisfaction with Large Language Models: A User Study with ChatGPT"},{"paperId":"f1057758c7ca7fe2babeea4908c281be9d1075c4","title":"IMUGPT 2.0: Language-Based Cross Modality Transfer for Sensor-Based Human Activity Recognition"},{"paperId":"6b3d58d367b049532c0dd7a203eb0c17c94e734e","title":"Image Anything: Towards Reasoning-coherent and Training-free Multi-modal Image Generation"},{"paperId":"b5710193dc676e0b9043f00759d776d10d3f6ea5","title":"PlantoGraphy: Incorporating Iterative Design Process into Generative Artificial Intelligence for Landscape Rendering"},{"paperId":"c5db6c2726911b72d534f97bd4d1ed63f6431340","title":"Mobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual Perception"},{"paperId":"9cd6c6d85de6180dd92ba43e685663067cf3ab7f","title":"Grounded SAM: Assembling Open-World Models for Diverse Visual Tasks"},{"paperId":"a050c9b0c321839e4427ab9defa3463be7825ac4","title":"MM-LLMs: Recent Advances in MultiModal Large Language Models"},{"paperId":"35c2f6f40d54d578bcdcf9797c7f153088554572","title":"GraphiMind: LLM-centric Interface for Information Graphics Design"},{"paperId":"140cfda71bfff852c3e205b7ad61854b78c76982","title":"Mastering Text-to-Image Diffusion: Recaptioning, Planning, and Generating with Multimodal LLMs"},{"paperId":"23957040943f883542f47850c709b9e7f9d6fa55","title":"Prompting Large Vision-Language Models for Compositional Reasoning"},{"paperId":"4f2a56102bcbf0fe79379c4c27daecbccfb35a26","title":"MLLM-Tool: A Multimodal Large Language Model For Tool Agent Learning"},{"paperId":"e80252dabd94148ad959c42e0121795939a78198","title":"Remote Sensing ChatGPT: Solving Remote Sensing Tasks with ChatGPT and Visual Models"},{"paperId":"7f6b1bdebb0f772230412a4451e98a5ba77ba043","title":"Vlogger: Make Your Dream A Vlog"},{"paperId":"4a48d628e53f554eb6ef09a457ca855188b96171","title":"DoraemonGPT: Toward Understanding Dynamic Scenes with Large Language Models"},{"paperId":"ff61aef2fef3a235bfaa123158a990c4f5f27d1a","title":"Small LLMs Are Weak Tool Learners: A Multi-LLM Agent"},{"paperId":"79e729b0a05547486f2e450387022e9c9fe6d729","title":"PersianMind: A Cross-Lingual Persian-English Large Language Model"},{"paperId":"002d2c4569d070a55fe69c25ebccad8e9ddae572","title":"Incorporating Visual Experts to Resolve the Information Loss in Multimodal Large Language Models"},{"paperId":"4bebe389dfa85423e5cc089edf20b2c3f572f38c","title":"Self-Contrast: Better Reflection Through Inconsistent Solving Perspectives"},{"paperId":"9eab4104973f5de650544729a4a69d84c594da92","title":"A Vision Check-up for Language Models"},{"paperId":"7bfc47f93c5b48222cc553883d24a4f0be291328","title":"Self-supervision advances morphological profiling by unlocking powerful image representations"},{"paperId":"b33b8f7508f8a802e2e669ec16348bb789ae0470","title":"Taking the Next Step with Generative Artificial Intelligence: The Transformative Role of Multimodal Large Language Models in Science Education"},{"paperId":"a06d3e9e90008c64c45a0029d580541d5f646771","title":"If LLM Is the Wizard, Then Code Is the Wand: A Survey on How Code Empowers Large Language Models to Serve as Intelligent Agents"},{"paperId":"575f403261d5f99526f0b4dfc8644352d6c4467a","title":"DocLLM: A layout-aware generative language model for multimodal document understanding"},{"paperId":"5f58863dd6474d6f127be995b5871e7c60f2792f","title":"Video Understanding with Large Language Models: A Survey"},{"paperId":"b6ca4b1bee98f4317dd25c764a8057e6ef11c89e","title":"GitAgent: Facilitating Autonomous Agent with GitHub by Tool Extension"},{"paperId":"46f64681d7a0c7f80303bebb0d62a3b7acbcdcd9","title":"An Improved Baseline for Reasoning Segmentation with Large Language Model"},{"paperId":"3e525c72d29c0f6a86dc4ebab673f5b3db069a0c","title":"Gemini Pro Defeated by GPT-4V: Evidence from Education"},{"paperId":"82dc8edc49f9edf4a53056aedcdfb339be070166","title":"IQAGPT: Image Quality Assessment with Vision-language and ChatGPT Models"},{"paperId":"4599d5af850da482f591a02a3b17d56e0d358771","title":"Plan, Posture and Go: Towards Open-World Text-to-Motion Generation"},{"paperId":"c672ec79f55cef8f7a32cd8dddfa981b893f1567","title":"V*: Guided Visual Search as a Core Mechanism in Multimodal LLMs"},{"paperId":"6a33e58ef961a3a0a5657518b2be86395eb7c8d0","title":"InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks"},{"paperId":"24fc9ad715372358bd0108eeb7c944b915963293","title":"ASSISTGUI: Task-Oriented Desktop Graphical User Interface Automation"},{"paperId":"17a32c825bd746a2625eddc2728092171a9ef72a","title":"Jack of All Tasks, Master of Many: Designing General-purpose Coarse-to-Fine Vision-Language Model"},{"paperId":"35a17f896847614a71df772bbe2b66ae231cabc7","title":"CLOVA: A Closed-Loop Visual Assistant with Tool Usage and Update"},{"paperId":"95d791ad14db2c779daa67ca7fdc3a75214c42eb","title":"3DAxiesPrompts: Unleashing the 3D Spatial Task Capabilities of GPT-4V"},{"paperId":"ea6982a936a2b263bbf46ff6eb27fc0b63fddaf7","title":"VL-GPT: A Generative Pre-trained Transformer for Vision and Language Understanding and Generation"},{"paperId":"6d2ab31aa75468f5458b9d96192c3f4a28f55d73","title":"DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving"},{"paperId":"5a97f677ab02602f5f319ad4b3dcd3ed8c53238f","title":"Spatial Interpretation and LLMs"},{"paperId":"55c6d16b550c606d62dd85084f0d373d8f087966","title":"VLAP: Efficient Video-Language Alignment via Frame Prompting and Distilling for Video Question Answering"},{"paperId":"475a396daeb1972b078ef175f51f44231e7f6b21","title":"LDM²: A Large Decision Model Imitating Human Cognition with Dynamic Memory Enhancement"},{"paperId":"33c3dbf86dd6e338e2a49bba9c2e2193d1eca08a","title":"Vista-LLaMA: Reliable Video Narrator via Equal Distance to Visual Tokens"},{"paperId":"b240a1d8ec2860bdd7370daa3144268ce46ac018","title":"Vary: Scaling up the Vision Vocabulary for Large Vision-Language Models"},{"paperId":"6e8a92479981afec0610414d0501d8ef56c555dc","title":"NLLG Quarterly arXiv Report 09/23: What are the most influential current AI Papers?"},{"paperId":"e2a4369c0febdc3676179ea7a4a68491282c67ae","title":"Image and Data Mining in Reticular Chemistry Using GPT-4V"},{"paperId":"96a7b0fe722e6d2d5167ef25a6aff714a20233a0","title":"Exploring the Limits of ChatGPT in Software Security Applications"},{"paperId":"10578bc0bdb3ebf9232931dd4961f55ba470caad","title":"LaMPilot: An Open Benchmark Dataset for Autonomous Driving with Language Model Programs"},{"paperId":"b92289123a94f6076505487adfb4513bd3495c1d","title":"LEGO: Learning EGOcentric Action Frame Generation via Visual Instruction Tuning"},{"paperId":"06b2ac5153e3d8d05c13c82f93d7f4e13eee6d0f","title":"Mitigating Fine-Grained Hallucination by Fine-Tuning Large Vision-Language Models with Caption Rewrites"},{"paperId":"ef4e4e4b52d4379ab5387d8dc53da87e561e78db","title":"Good Questions Help Zero-Shot Image Reasoning"},{"paperId":"246017780386eba39d6cda760a1c2c70356baa50","title":"VIoTGPT: Learning to Schedule Vision Tools towards Intelligent Video Internet of Things"},{"paperId":"9e2bac2777eebe603a39f69221689493609d4149","title":"MLLMs-Augmented Visual-Language Representation Learning"},{"paperId":"7492ef863cd376420de462dd1f041abb91172530","title":"Exposing Limitations of Language Model Agents in Sequential-Task Compositions on the Web"},{"paperId":"8441c30ad4abdca9ee380aa6f22ffd731b10231b","title":"COLE: A Hierarchical Generation Framework for Graphic Design"},{"paperId":"486c2df78cbb770a90a55f7fa3fe19102fba2c24","title":"LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models"},{"paperId":"b6ddf3f76eef409b9b105be68511dcfb9ed993d0","title":"The Influence of ChatGPT on Student Learning and Academic Performance"},{"paperId":"8073f493f33842979d71c5b05bbbb276493d3c54","title":"Function-constrained Program Synthesis"},{"paperId":"769a924d0af014acec326f50c15c5d70d258a969","title":"LLMGA: Multimodal Large Language Model based Generation Assistant"},{"paperId":"7c261866e9d8ddc42f3c1f0b1c2c882182d47fc9","title":"Mitigating Hallucination in Visual Language Models with Visual Supervision"},{"paperId":"5eea245cc12c55905d4df827d0c9776c5ddfa743","title":"Compositional Chain-of-Thought Prompting for Large Multimodal Models"},{"paperId":"a756b584f8f8b4307e52895ae2120bc339580ad8","title":"See and Think: Embodied Agent in Virtual Environment"},{"paperId":"7b0a186b0140ee91fb13991c9c7187f3dc3b0670","title":"Visual Programming for Zero-shot Open-Vocabulary 3D Visual Grounding"},{"paperId":"437c7836d32c7f221aad466047130075c7cb5336","title":"Point Cloud Pre-training with Diffusion Models"},{"paperId":"8708914c567ecfc9c1ef4b2a921bbd3abf687737","title":"Benchmarking Robustness of Text-Image Composed Retrieval"},{"paperId":"ee2c769943f9e46c3bbee117d1ecf14566b7bf1f","title":"Boosting the Power of Small Multimodal Reasoning Models to Match Larger Models with Self-Consistency Training"},{"paperId":"52941cadbd340344f3e0a6f50719fe55b3de5088","title":"Multimodal Large Language Models: A Survey"},{"paperId":"da9e284c75334c660029c79f09ed371aaf2f1139","title":"AcademicGPT: Empowering Academic Research"},{"paperId":"c3b75d5bc1401eaeecb365049dcf9781c6f359af","title":"NERIF: GPT-4V for Automatic Scoring of Drawn Models"},{"paperId":"451539c0d0f5f5785ff58d09ca5e67a5f129f9de","title":"A Survey on Multimodal Large Language Models for Autonomous Driving"},{"paperId":"ab8169d6e4dfabfe7c30ebec1bb871bf3e1551cd","title":"GAIA: a benchmark for General AI Assistants"},{"paperId":"5f370f52e24d185ae44bb0ea18cbd4be2aab0d15","title":"VLM-Eval: A General Evaluation on Video Large Language Models"},{"paperId":"107fb6eec2febbae12db29bf3e311aaf5680027c","title":"Video-LLaVA: Learning United Visual Representation by Alignment Before Projection"},{"paperId":"e72f612f29d48010501f043cc2d4c1d0762c2d98","title":"Knowledge Plugins: Enhancing Large Language Models for Domain-Specific Recommendations"},{"paperId":"0f993809c1fe00403ecea66d8f572832f075cfe4","title":"MMC: Advancing Multimodal Chart Understanding with Large-scale Instruction Tuning"},{"paperId":"9574261b66fc006bb7cd9091504b519ce0ce27cf","title":"Defining the boundaries: challenges and advances in identifying cells in microscopy images"},{"paperId":"aad3d2e690f6c73f04a14622ceff51464bbc560e","title":"Chat-UniVi: Unified Visual Representation Empowers Large Language Models with Image and Video Understanding"},{"paperId":"2fb605f67fee79cad94952ddfe0f686e926f49f5","title":"GPT-4V in Wonderland: Large Multimodal Models for Zero-Shot Smartphone GUI Navigation"},{"paperId":"76a3f4a79ae9a00db2f2b5f6877021d8deb96ada","title":"SPHINX: The Joint Mixing of Weights, Tasks, and Visual Embeddings for Multi-modal Large Language Models"},{"paperId":"69bbd2dbb866a6d646b46046851bde4e913f43b7","title":"ChatAnything: Facetime Chat with LLM-Enhanced Personas"},{"paperId":"ecfff30e570498b6c87c5d1319a0cbcdf7a8b86d","title":"LLaVA-Plus: Learning to Use Tools for Creating Multimodal Agents"},{"paperId":"242600cd00d959a0a11096d24fa44ecbdefcac0c","title":"Foundation Models for Mining 5.0: Challenges, Frameworks, and Opportunities"},{"paperId":"33af85c1b2b9325e096a5b8ee36af8d957a45914","title":"PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task Completion"},{"paperId":"990501f48725bf6f8e0a90f215e7426ced576317","title":"DECENTRALISED AUTONOMOUS SOCIETY THROUGH LARGE LANGUAGE MODELS’ BASED AGENTS: A PATHWAY TO EMPOWER SMALL COMMUNITIES"},{"paperId":"c020f15be1dee20f9e2e0c5a6f05f272b5508325","title":"LLaVA-Interactive: An All-in-One Demo for Image Chat, Segmentation, Generation and Editing"},{"paperId":"debe978e02b664fb7254b6c7b58a74c09ce897e3","title":"Plug-and-Play Policy Planner for Large Language Model Powered Dialogue Agents"},{"paperId":"88bddfb7d1e0462be8fe99fdbd71c658140cb17b","title":"From Image to Language: A Critical Analysis of Visual Question Answering (VQA) Approaches, Challenges, and Opportunities"},{"paperId":"e5bded12f9f9ae17ed1d29937967efe48684c47d","title":"Breaking the Trilemma of Privacy, Utility, Efficiency via Controllable Machine Unlearning"},{"paperId":"288e7224d53d68669eb67f2496e068dc965c639e","title":"ControlLLM: Augment Language Models with Tools by Searching on Graphs"},{"paperId":"d30f5e8cd863e89a0a84a9fa1bf84fddd428df7e","title":"MaTCR: Modality-Aligned Thought Chain Reasoning for Multimodal Task-Oriented Dialogue Generation"},{"paperId":"0212dca18cd0765deed0b6ba80a796f0ad46e066","title":"mPLUG-Octopus: The Versatile Assistant Empowered by A Modularized End-to-End Multimodal LLM"},{"paperId":"61a15fcd8c2c356994f56091e1c0af36d0f09d50","title":"Prompt Me Up: Unleashing the Power of Alignments for Multimodal Entity and Relation Extraction"},{"paperId":"f8b8f926bbfa327c86c40796131fe2695db81126","title":"DDCoT: Duty-Distinct Chain-of-Thought Prompting for Multimodal Reasoning in Language Models"},{"paperId":"807f336176070bd3f95b82a16f125ee99b7d2c80","title":"Woodpecker: Hallucination Correction for Multimodal Large Language Models"},{"paperId":"8e3e7deb95d2a984cba615ec847e64f354626cdf","title":"WebWISE: Web Interface Control and Sequential Exploration with Large Language Models"},{"paperId":"b1d2a29860e69c6ce9987ddefbe112feb1efa16a","title":"Large Language Models can Share Images, Too!"},{"paperId":"013719cdcc1cbf9861a661210d981a4f8bf24268","title":"Vision Language Models in Autonomous Driving and Intelligent Transportation Systems"},{"paperId":"f90c522b284a6c065fa5126216a26a7415a2b9fa","title":"MoqaGPT : Zero-Shot Multi-modal Open-domain Question Answering with Large Language Model"},{"paperId":"938d1028b3c3cd4e3d34eed20b622bdc33453f6e","title":"MarineGPT: Unlocking Secrets of Ocean to the Public"},{"paperId":"79e7ead8f59b17431de2b86af10dc0c30a1f5a2b","title":"ToolChain*: Efficient Action Space Navigation in Large Language Models with A* Search"},{"paperId":"58b77dc0603eb52559d98a383bf9649fd31d0bc5","title":"WordArt Designer: User-Driven Artistic Typography Synthesis using Large Language Models"},{"paperId":"cca4218dd7c10c1614bbd84aa7cd7e00027bdc7c","title":"Loop Copilot: Conducting AI Ensembles for Music Generation and Iterative Editing"},{"paperId":"beaf64df85f8204b8cd89a7f46827608e6d16922","title":"MusicAgent: An AI Agent for Music Understanding and Generation with Large Language Models"},{"paperId":"7451d756118628474dc022813eb952a21d34c5f6","title":"Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V"},{"paperId":"36b923d97d7cfaf73d11c55c15ea46605ba974a5","title":"BiLL-VTG: Bridging Large Language Models and Lightweight Visual Tools for Video-based Texts Generation"},{"paperId":"00c19d9818bf093a2eed323d1bd5c763c4f512b9","title":"Reading Books is Great, But Not if You Are Driving! Visually Grounded Reasoning about Defeasible Commonsense Norms"},{"paperId":"b217b6bc340af9a10bebbf8acc36ea30871769bd","title":"In-Context Learning with Iterative Demonstration Selection"},{"paperId":"a3c1f6809dd1455da73eec407bcb3be92e680112","title":"Penetrative AI: Making LLMs Comprehend the Physical World"},{"paperId":"a80546c9847710af1ba8d5f8dca9386e7a520d0a","title":"Precedent-Enhanced Legal Judgment Prediction with LLM and Domain-Model Collaboration"},{"paperId":"03bf1da1caa5f63203d43ed78c12c35a78fc6ed9","title":"EasyGen: Easing Multimodal Generation with a Bidirectional Conditional Diffusion Model and LLMs"},{"paperId":"1d14a708622917da4b9820ada6d32af24fc1651a","title":"Idea2Img: Iterative Self-Refinement with GPT-4V(ision) for Automatic Image Design and Generation"},{"paperId":"e5f8776db5801c61430cf18d3545806f822ef1cb","title":"Jigsaw: Supporting Designers in Prototyping Multimodal Applications by Assembling AI Foundation Models"},{"paperId":"a710efa9247207a72f06e0c9db302fd3ecab5fbb","title":"Towards Robust Multi-Modal Reasoning via Model Selection"},{"paperId":"e690af2f6aaef0ad3b534dd1323fa6f4db8fed5d","title":"NetDiffusion: Network Data Augmentation Through Protocol-Constrained Traffic Generation"},{"paperId":"b3e9f249dd2e09ec111496f6b533101e8217a5b0","title":"Multimodal Large Language Model for Visual Navigation"},{"paperId":"7f1ba5630c3baa09b11cc665b3f71cdb117e5ffb","title":"OpenLEAF: Open-Domain Interleaved Image-Text Generation and Evaluation"},{"paperId":"711e64934bc0bab2f82aee489cf0f8fdb837addc","title":"Pushing the Envelope: Investigating the Potential and Limitations of ChatGPT and Artificial Intelligence in Advancing Computer Science Research"},{"paperId":"33095b1334bed852e3652bd9d7da3f4df0cdf485","title":"ViCor: Bridging Visual Understanding and Commonsense Reasoning with Large Language Models"},{"paperId":"b9e8b62bcc019f47a0a015568f70039b3b7c1196","title":"Toolink: Linking Toolkit Creation and Using through Chain-of-Solving on Open-Source Model"},{"paperId":"10828be2eaa52ba7fd78356980afd0669e2f2879","title":"Large Language Model (LLM) as a System of Multiple Expert Agents: An Approach to solve the Abstraction and Reasoning Corpus (ARC) Challenge"},{"paperId":"84f9bc5f89dac53662fb467b6af8ff26415ca3e7","title":"InstructDET: Diversifying Referring Object Detection with Generalized Instructions"},{"paperId":"8918e3cc21ecaf81532e452d3b9518360d14860e","title":"Reinforced UI Instruction Grounding: Towards a Generic UI Task Automation API"},{"paperId":"97afc428341eca1011a142daf269c9b01230f410","title":"Reverse Chain: A Generic-Rule for LLMs to Master Multi-API Planning"},{"paperId":"bee68767debbdc96d6f75947e544a8be98b869e3","title":"Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model: Explorations with GPT4-Vision and Beyond"},{"paperId":"e7d09b6f2bc878cf2c993acf675f409d0b55f35a","title":"MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens"},{"paperId":"65bceaab98ed937854e5177fe4ff9c63d40b4685","title":"Semantically Enhanced Scene Captions with Physical and Weather Condition Changes"},{"paperId":"d67cff4079e5799dde2480575eafdeb50c525adb","title":"Unseen And Adverse Outdoor Scenes Recognition Through Event-based Captions"},{"paperId":"8892b0937b1e6f3892647a812841e9dccacd7a34","title":"Dynamic Texts From UAV Perspective Natural Images"},{"paperId":"bd074cc87fe3567b1f8d70825cc5249b167d1674","title":"ChatGPT's Epoch in Rheumatological Diagnostics: A Critical Assessment in the Context of Sjögren's Syndrome"},{"paperId":"a1426b13b74dbad17b34606d25aabe1d61f6e11a","title":"CRAFT: Customizing LLMs by Creating and Retrieving from Specialized Toolsets"},{"paperId":"092245d86b77181c36f972b1b7a17a59cd989c4a","title":"Guiding Instruction-based Image Editing via Multimodal Large Language Models"},{"paperId":"7b689adb8c156d6158660f90d1c86888ee281f63","title":"DreamLLM: Synergistic Multimodal Comprehension and Creation"},{"paperId":"f34b6b4f75fb4e6f2c5654ee13fb2479c6170b3a","title":"Kosmos-2.5: A Multimodal Literate Model"},{"paperId":"f39630035497c76e5b3ea3f96f5cd54b4911ecb3","title":"Integrating Visual Foundation Models for Enhanced Robot Manipulation and Motion Planning: A Layered Approach"},{"paperId":"46c435dbf3ae54d8ba1bacef9e424a46bfc2c643","title":"Large Language Models (LLMs): Representation Matters, Low-Resource Languages and Multi-Modal Architecture"},{"paperId":"bc9f29881c1d93d225f0a74fa700531202c7043a","title":"OpenBA: An Open-sourced 15B Bilingual Asymmetric seq2seq Model Pre-trained from Scratch"},{"paperId":"3ec464696db25acc2c39a6d967ec3df09e06f633","title":"Multimodal Multi-Hop Question Answering Through a Conversation Between Tools and Efficiently Finetuned Large Language Models"},{"paperId":"4eb87eaa193929dbef93fa2db9419245a8e8916f","title":"TextBind: Multi-turn Interleaved Multimodal Instruction-following in the Wild"},{"paperId":"8aab972b0c3a3d581536b0d74339794809dc1a64","title":"TrafficGPT: Viewing, Processing and Interacting with Traffic Foundation Models"},{"paperId":"d39182113cd4176ead48027b4fc05fe06ec6aaca","title":"Language Models as Black-Box Optimizers for Vision-Language Models"},{"paperId":"fa75a55760e6ea49b39b83cb85c99a22e1088254","title":"NExT-GPT: Any-to-Any Multimodal LLM"},{"paperId":"24d52678c887331b9da0368e8a2f58bec07f7203","title":"Exploring Large Language Models for Communication Games: An Empirical Study on Werewolf"},{"paperId":"e2f1f04f648a8863d11439aa4c80ee65d6caccda","title":"ModelScope-Agent: Building Your Customizable Agent System with Open-source Large Language Models"},{"paperId":"529ff7d6441d244212cf2becafd12a7e67ac56d9","title":"FederatedScope-LLM: A Comprehensive Package for Fine-tuning Large Language Models in Federated Learning"},{"paperId":"3cc896b9a9c4951510b6b9f28be930d65b24d29f","title":"A Comprehensive Survey of ChatGPT: Advancements, Applications, Prospects, and Challenges."},{"paperId":"a404da40ae0d188095b5b0a3a3385c1cc7cffe62","title":"VistaGPT: Generative Parallel Transformers for Vehicles With Intelligent Systems for Transport Automation"},{"paperId":"1e7a2f9f9441462e92ee349f00414aff49617caa","title":"Enhancing Subtask Performance of Multi-modal Large Language Model"},{"paperId":"c237a22698223e4060d83027f399f4fb2aa24291","title":"Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"},{"paperId":"6bcc6ab9c28805d4067e99b2cdc7524550fe80e1","title":"PointLLM: Empowering Large Language Models to Understand Point Clouds"},{"paperId":"3b36d16985286b03e06e8404a7be49a9713d37b9","title":"Confucius: Iterative Tool Learning from Introspection Feedback by Easy-to-Difficult Curriculum"},{"paperId":"894ed1aba8e42a4ec27ba53ecde383b14c5128ca","title":"Examining User-Friendly and Open-Sourced Large GPT Models: A Survey on Language, Multimodal, and Scientific GPT Models"},{"paperId":"d0a7f7fe31e0e0c42b471b4c47a313bd8c8e5206","title":"Empowering Dynamics-aware Text-to-Video Diffusion with Large Language Models"},{"paperId":"ef1ebdb24ce45fb57e271783a0c9a877fe0e79c3","title":"Position-Enhanced Visual Instruction Tuning for Multimodal Large Language Models"},{"paperId":"98a508f5a4d49490bf97f195238b4fa5c41b8088","title":"Rational Decision-Making Agent with Internalized Utility Judgment"},{"paperId":"da96ec9c32d63292e506ba8f8ea8e838df998c02","title":"StableLLaVA: Enhanced Visual Instruction Tuning with Synthesized Image-Dialogue Data"},{"paperId":"ecfe2becc1040d3810c10f006d4be43b4eef3c41","title":"Tree-of-Mixed-Thought: Combining Fast and Slow Thinking for Multi-hop Visual Reasoning"},{"paperId":"eb5cf10406a8ad31e0ebe56b36571d5db4758a62","title":"PUMGPT: A Large Vision-Language Model for Product Understanding"},{"paperId":"d53945d4afb4528590d79e20de52883d29037e86","title":"FashionLOGO: Prompting Multimodal Large Language Models for Fashion Logo Embeddings"},{"paperId":"2e3dcf5a5d58ac210d0d87e9f918540a8373211a","title":"GIT-Mol: A Multi-modal Large Language Model for Molecular Science with Graph, Image, and Text"},{"paperId":"d6c2523ab97416c2692cbbeab082ed1790e8e55e","title":"VisIT-Bench: A Benchmark for Vision-Language Instruction Following Inspired by Real-World Use"},{"paperId":"52bef3d7ed6d079772d5a6ddfc68304cf40122cf","title":"Progressive Spatio-temporal Perception for Audio-Visual Question Answering"},{"paperId":"4f2be887e991efa85f7b874e7ab871080a745c39","title":"CAESURA: Language Models as Multi-Modal Query Planners"},{"paperId":"dd0612ce863f64b0f69d0d9f708c52e829f6f859","title":"TPTU: Large Language Model-based AI Agents for Task Planning and Tool Usage"},{"paperId":"446fb5dead075a1a08862662738f462e9a0e91c8","title":"Tool Documentation Enables Zero-Shot Tool-Usage with Large Language Models"},{"paperId":"ad113d8b6f8f6dace7abd12dc88af520aaaf3fc7","title":"LISA: Reasoning Segmentation via Large Language Model"},{"paperId":"7d46a13a1edd02dd6ae2b9f713e6f91ea001dfb4","title":"When Large Language Models Meet Personalization: Perspectives of Challenges and Opportunities"},{"paperId":"0bfc804e31eecfd77f45e4ee7f4d629fffdcd628","title":"ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs"},{"paperId":"bbcd5cc4bf6c77282e88cae07f7f2adb1da818ca","title":"Testing the Depth of ChatGPT's Comprehension via Cross-Modal Tasks Based on ASCII-Art: GPT3.5's Abilities in Regard to Recognizing and Generating ASCII-Art Are Not Totally Lacking"},{"paperId":"584ca135b61482fd89247113da87d784f738dbfa","title":"Foundational Models Defining a New Era in Vision: A Survey and Outlook"},{"paperId":"ece8cc6de361afc27cddcb2a4113936a877d931c","title":"Fashion Matrix: Editing Photos by Just Talking"},{"paperId":"8fd72a8bc1ab8fa08259656fa617d1c861f27239","title":"Interpreting Art by Leveraging Pre-Trained Models"},{"paperId":"1b4012f38daa8f09299e16771973c91ce9464ee2","title":"DVPT: Dynamic Visual Prompt Tuning of Large Pre-trained Models for Medical Image Analysis"},{"paperId":"2e52178416b93a0a7dc1cb7d21d04c7e72c2ce0f","title":"ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring Instruction Tuning"},{"paperId":"962ccf1fc49c83817fb031e5b24b81b19cdfb89d","title":"BuboGPT: Enabling Visual Grounding in Multi-Modal LLMs"},{"paperId":"a68dc9208aae7578e8ee384caa8ccbcf34e539e8","title":"Mini-Giants: \"Small\" Language Models and Open Source Win-Win"},{"paperId":"65d7663b60d95f98e6281ecc4da9c7a975119b91","title":"GeoGPT: Understanding and Processing Geospatial Tasks through An Autonomous GPT"},{"paperId":"ca31b8584b6c022ef15ddfe994fe361e002b7729","title":"A Comprehensive Overview of Large Language Models"},{"paperId":"451a3f03aca4aa87b93981364842137417549e58","title":"SVIT: Scaling up Visual Instruction Tuning"},{"paperId":"094883e42bb9a41f602c0715c1059bc431e33fb2","title":"GPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest"},{"paperId":"ebddfdc5d845a788e8062eddbbf7a335737cb99b","title":"What Matters in Training a GPT4-Style Language Model with Multimodal Inputs?"},{"paperId":"ea566f87f6253bb2d32cf7b61cd3e2535a0c3f42","title":"mPLUG-DocOwl: Modularized Multimodal Large Language Model for Document Understanding"},{"paperId":"376f494126d1ea4f571ea0263c43ac2b6331800a","title":"SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs"},{"paperId":"fb4ed7d3f24c69d33dc0654cf91ed8409f8dafb3","title":"The Segment Anything Model (SAM) for Remote Sensing Applications: From Zero to One Shot"},{"paperId":"145be18b3e2e1baf3fed09e919da68da6e14a839","title":"Stone Needle: A General Multimodal Large-scale Model Framework towards Healthcare"},{"paperId":"15dfb06dab3162f4bb7939b0e54c3b68c2b34cc4","title":"Paradigm Shift in Sustainability Disclosure Analysis: Empowering Stakeholders with Chatreport, a Language Model-Based Tool"},{"paperId":"ebedc4d7a2356090904baba4104ef0832bc236df","title":"A Survey on Multimodal Large Language Models"},{"paperId":"697e0add95e880bd42e00bef838181e105f91981","title":"MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models"},{"paperId":"966852963a88a28786b798c91b6662d6e501e590","title":"AssistGPT: A General Multi-modal Assistant that can Plan, Execute, Inspect, and Learn"},{"paperId":"051549d8ef56937b2f4d113afdcf8c7586d3770b","title":"Towards AGI in Computer Vision: Lessons Learned from GPT and Large Language Models"},{"paperId":"d98536f24272e258b1d399074b64284d64786099","title":"AVIS: Autonomous Visual Information Seeking with Large Language Models"},{"paperId":"eaa7853facb9b49444b48a96192cb4be66b62671","title":"Synapse: Trajectory-as-Exemplar Prompting with Memory for Computer Control"},{"paperId":"473eb062612a17c965eaa62136322f0dec6b1f8e","title":"Data-Copilot: Bridging Billions of Data and Humans with Autonomous Workflow"},{"paperId":"79150cb420d15830c8d36f0e91eea1b02e177f0f","title":"Sticker820K: Empowering Interactive Retrieval with Stickers"},{"paperId":"4c4d176c6e28f48041f215d563f6ee8633534cff","title":"Valley: Video Assistant with Large Language model Enhanced abilitY"},{"paperId":"6294f078e79828cac21e717813e8f3d02b18a97c","title":"The importance of resource awareness in artificial intelligence for healthcare"},{"paperId":"fd755dc7b5b206c17fd953db04e1c888d45b6e4e","title":"LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark"},{"paperId":"ed30969f0e4811473144ffe83c1baa6d54f02202","title":"RestGPT: Connecting Large Language Models with Real-World RESTful APIs"},{"paperId":"fed150a219f9c31bdb4920e615c7c9264c634736","title":"On the Challenges and Perspectives of Foundation Models for Medical Image Analysis"},{"paperId":"2933acb28b7369c7ea5b8728f6d8cb55e1beef98","title":"Customizing General-Purpose Foundation Models for Medical Report Generation"},{"paperId":"dc0f9a4988fccd8cdd6d64e475306f8a639ac1af","title":"Decision Stacks: Flexible Reinforcement Learning via Modular Generative Models"},{"paperId":"d47524cd5c3c4b57af2e5a29f6f91c420310f236","title":"MIMIC-IT: Multi-Modal In-Context Instruction Tuning"},{"paperId":"2d338cdd12091814dec11155d3f6f848d7bab4d8","title":"Prompt Space Optimizing Few-shot Reasoning Success with Large Language Models"},{"paperId":"5bbc3b014f7c2dd151dc6b3cfb183889c44e772d","title":"Natural Language Commanding via Program Synthesis"},{"paperId":"42ea55edb46395469aee1b760829657e65ab6577","title":"Zero-Shot 3D Shape Correspondence"},{"paperId":"5d321194696f1f75cf9da045e6022b2f20ba5b9c","title":"Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding"},{"paperId":"486a8c8655b81c7f87ff257141466ec1186d4aea","title":"Prompt Sapper: LLM-Empowered Software Engineering Infrastructure for AI-Native Services"},{"paperId":"615962d8969c8e0ffe43319689dce6c50cbf1f29","title":"Responsible Task Automation: Empowering Large Language Models as Responsible Task Automators"},{"paperId":"c85c90ef9e9a71efe031c3f7d6e34561f91168fe","title":"Deliberate then Generate: Enhanced Prompting Framework for Text Generation"},{"paperId":"461437e165fa8236373bdcc4db5f09667863bc29","title":"Chatting Makes Perfect: Chat-based Image Retrieval"},{"paperId":"b458fc5261595f44b36325e5eaea1f874d65138f","title":"GPT4Tools: Teaching Large Language Model to Use Tools via Self-instruction"},{"paperId":"af705d648b5b16daa3dcc593bc593f2574d76c07","title":"Grammar Prompting for Domain-Specific Language Generation with Large Language Models"},{"paperId":"50c1414fe41d0cb9db6f0933c9319aa124beac5d","title":"Contextual Object Detection with Multimodal Large Language Models"},{"paperId":"828e27fd4fcd5e8982032b903950947b12afb6bb","title":"Diffusion Model is an Effective Planner and Data Synthesizer for Multi-Task Reinforcement Learning"},{"paperId":"5ff2f5212713ec424662ac3c9e4aa5a8790d40cf","title":"ANPL: Towards Natural Programming with Interactive Decomposition"},{"paperId":"7e72eb196b7c90b3a5d6385af536fe8e5934fb82","title":"ConvGenVisMo: Evaluation of Conversational Generative Vision Models"},{"paperId":"8ecdbfe011b7189fa0ee49ffc4e42a93d728a371","title":"On Evaluating Adversarial Robustness of Large Vision-Language Models"},{"paperId":"8199c9d55dd998f69f703e0ad250ca0697e3ad27","title":"NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models"},{"paperId":"51b169701290cd129e0781fc9f3a9918604c89b5","title":"Improving accuracy of GPT-3/4 results on biomedical data using a retrieval-augmented language model"},{"paperId":"c6ac708b65b24c20f80831d518c1795ce8133ad5","title":"ChatBridge: Bridging Modalities with Large Language Model as a Language Catalyst"},{"paperId":"06091944b864d6dc473cab63321a95fb9c4067cc","title":"ChatCAD+: Towards a Universal and Reliable Interactive CAD using LLMs"},{"paperId":"ef8c21e1f574495f0c80b8c1037dbdb886f0808d","title":"Towards Language-guided Interactive 3D Generation: LLMs as Layout Interpreter with Generative Feedback"},{"paperId":"f0888b9c0ef63e68c7758e6aec2370961c0eede9","title":"On the Tool Manipulation Capability of Open-source Large Language Models"},{"paperId":"9c3a9b4821daa03cb5369041d59d2714329a3811","title":"Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models"},{"paperId":"7cf64070fd3d7e53d80f260c10e6bd7018d580e1","title":"IdealGPT: Iteratively Decomposing Vision and Language Reasoning via Large Language Models"},{"paperId":"7c4f6fd4c7eadcc7189a6797db215895340f93c7","title":"ChatFace: Chat-Guided Real Face Editing via Diffusion Latent Space Manipulation"},{"paperId":"9837349417e36ef5be06da0fd6c74042148bdaa2","title":"Visual Programming for Text-to-Image Generation and Evaluation"},{"paperId":"abab79d1135684d039cfbebd0097e48ef4c1940c","title":"Vision + Language Applications: A Survey"},{"paperId":"66d755730f5d08a6f4fcc5e81f24982ba389dca9","title":"LayoutGPT: Compositional Visual Planning and Generation with Large Language Models"},{"paperId":"00cb69a9f280317d1c59ac5827551ee9b10642b8","title":"EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought"},{"paperId":"69335077fcacbff7a7cf25697da1949e6bdfa968","title":"The Art of SOCRATIC QUESTIONING: Recursive Thinking with Large Language Models"},{"paperId":"bd0488e0c1bb3ab375f28b3157058101350d3766","title":"ChipGPT: How far are we from natural language hardware design"},{"paperId":"90027ca7802645671a69b00b65e1fa94e6b63544","title":"ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models"},{"paperId":"e9ae0c76a71b8f302eb17b1c4462b9cc97d87cd0","title":"LLM-grounded Diffusion: Enhancing Prompt Understanding of Text-to-Image Diffusion Models with Large Language Models"},{"paperId":"8da9b1436212b233fc49c7daf1ba15c22874ff5a","title":"CREATOR: Tool Creation for Disentangling Abstract and Concrete Reasoning of Large Language Models"},{"paperId":"13a5140fc0b269c408ecfc666cb297410bc753c5","title":"Matcher: Segment Anything with One Shot Using All-Purpose Feature Matching"},{"paperId":"43a55dbd95c9d5cd82de8db276f41adeec4a937d","title":"Interactive Data Synthesis for Systematic Vision Adaptation via LLMs-AIGCs Collaboration"},{"paperId":"6fe2a4f7594abadafc9feb05e96ab7e9e634ce8f","title":"Album Storytelling with Iterative Story-aware Captioning and Large Language Models"},{"paperId":"ca055cfb9d4d47124cc035c346f38577825fcacf","title":"Enhance Reasoning Ability of Visual-Language Models via Large Language Models"},{"paperId":"7919cb1a1dcf70ed7803c43a71d43dba696ef149","title":"Making Language Models Better Tool Learners with Execution Feedback"},{"paperId":"205d2ed0906440f07a0275d7d6a63bced60951fc","title":"InstructVid2Vid: Controllable Video Editing with Natural Language Instructions"},{"paperId":"98d05533678a9a09a2053b7d974738c60c4894ea","title":"Empower Large Language Model to Perform Better on Industrial Domain-Specific Question Answering"},{"paperId":"f8cbcb106a48524edc39df23e2a95f1e6d4c739a","title":"Examining Inter-Consistency of Large Language Models Collaboration: An In-depth Analysis via Debate"},{"paperId":"2195676f111ad492c50f4d4c96abb2bd3d72f7fc","title":"Instruct2Act: Mapping Multi-modality Instructions to Robotic Actions with Large Language Model"},{"paperId":"42a30dc5470f54ec249f25d3c31e05d7c376c8e3","title":"VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks"},{"paperId":"1bdd5fc17cc580efe998304692639c57c857cc84","title":"Going Denser with Open-Vocabulary Part Segmentation"},{"paperId":"cb5f53cd7d43ac6f48abf4dde6fc58e53a083b0e","title":"Accelerating the integration of ChatGPT and other large‐scale AI models into biomedical research and healthcare"},{"paperId":"7787efaf502421eac9b6b0fd946a82e1ecf4c8c9","title":"Generating coherent comic with rich story using ChatGPT and Stable Diffusion"},{"paperId":"8dbb29f93292d8b1b861c322d232fe087b2ef7b1","title":"Small Models are Valuable Plug-ins for Large Language Models"},{"paperId":"0340c850e033abbf71c7214e403c8fe2be5ef91f","title":"Visual Tuning"},{"paperId":"d48cb91b9e555194f7494c4d4bb9815021d3ee45","title":"VideoChat: Chat-Centric Video Understanding"},{"paperId":"7a6dc7071891cb3d658c93418801942a4c6ed373","title":"Autonomous GIS: the next-generation AI-powered GIS"},{"paperId":"54a8b153ed04a872da878d695239bdc413dc782c","title":"InternGPT: Solving Vision-Centric Tasks by Interacting with Chatbots Beyond Language"},{"paperId":"e0dc8e113dbdd2896fb6420ac93e0b976c47f2a2","title":"Augmented Large Language Models with Parametric Knowledge Guiding"},{"paperId":"43e6e8d6663d83f1b74cf5a2be7b040b0928f867","title":"X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages"},{"paperId":"20fcc01d12a50f1da2af71d85f0a269b3ba48b77","title":"LMEye: An Interactive Perception Network for Large Language Models"},{"paperId":"d6d3604f369bb0415cbe814e43ca3131323b03e2","title":"Otter: A Multi-Modal Model with In-Context Instruction Tuning"},{"paperId":"6f8b9192b1f215254ee7625d752710182c05d2f9","title":"Caption Anything: Interactive Image Description with Diverse Multimodal Controls"},{"paperId":"c77d908ba29567445a9a4ad1bd4461d441cce174","title":"AutoML-GPT: Automatic Machine Learning with GPT"},{"paperId":"d473847dff63e3f5d238251cb23597f8205f72f2","title":"Generating Virtual On-body Accelerometer Data from Virtual Textual Descriptions for Human Activity Recognition"},{"paperId":"37ba1833e844f5178f91f50d82bfff616551e6ad","title":"The Role of Summarization in Generative Agents: A Preliminary Perspective"},{"paperId":"570079bbdd8758dfe865097e05719313c9c1301a","title":"LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model"},{"paperId":"c56a51728678e5b2e3ff95e51caf21d267439c36","title":"ChatVideo: A Tracklet-centric Multimodal and Versatile Video Understanding System"},{"paperId":"7e32aac43e9f1df49e116add03327ee6f365dbf3","title":"mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality"},{"paperId":"8bc617c9139648d7a92991d70c671230bac7b2e2","title":"AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head"},{"paperId":"f6654702479eddb01d6f1bd72d6d56102ebefbfc","title":"The Potential of Visual ChatGPT For Remote Sensing"},{"paperId":"4c8ef2db0c77aba453783f5211ebafc6695d3835","title":"ChatABL: Abductive Learning via Natural Language Interaction with ChatGPT"},{"paperId":"ca6a2bc279be5a3349a22bfd6866ed633d18734b","title":"MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models"},{"paperId":"5c0f3b0e46e6125bf2f454bcd8565a6f3430a54c","title":"Learning to Plan with Natural Language"},{"paperId":"170c97c7215f42edfb20c2248f954879e91ef86e","title":"Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models"},{"paperId":"352420ee61a8da783ca7750170793613b18b8d9c","title":"Tool Learning with Foundation Models"},{"paperId":"be2b0396de9431bae931642516a1d3e4906329f5","title":"Low-code LLM: Visual Programming over LLMs"},{"paperId":"a5036f31f0e629dc661f120b8c3b1f374d479ab8","title":"Visual Instruction Tuning"},{"paperId":"ba2f935d2578fbf77ec1aa79e26e3db396771e38","title":"Self-collaboration Code Generation via ChatGPT"},{"paperId":"0819c1e60c13b9797f937282d06b54d252d9d6ec","title":"Segment Everything Everywhere All at Once"},{"paperId":"6316cbb4f1e7dba5806a3310ec7f89f3571bc3db","title":"Boosting Cross-task Transferability of Adversarial Patches with Visual Relations"},{"paperId":"38179848e2d6a3ad373b1793848816111428ac36","title":"OpenAGI: When LLM Meets Domain Experts"},{"paperId":"0ebc861f5478561f12941e6b48aad30574e996d8","title":"Video ChatCaptioner: Towards Enriched Spatiotemporal Descriptions"},{"paperId":"16d83e930a4dab2d49f5d276838ddce79df3f787","title":"Should ChatGPT be Biased? Challenges and Risks of Bias in Large Language Models"},{"paperId":"51a0bba0c5fb4257e843040615bb23f712fed4e6","title":"Summary of ChatGPT-Related Research and Perspective Towards the Future of Large Language Models"},{"paperId":"c61d54644e9aedcfc756e5d6fe4cc8b78c87755d","title":"A Survey of Large Language Models"},{"paperId":"ac7771c332da42b29a913b116bd6ef622cbf89cf","title":"TaskMatrix.AI: Completing Tasks by Connecting Foundation Models with Millions of APIs"},{"paperId":"f8e77bd3d573d0daee0744443c65c40e3b5dc10f","title":"Unleashing the Power of Edge-Cloud Generative AI in Mobile Networks: A Survey of AIGC Services"},{"paperId":"23684a07517870cffd1f97fafbaae16ba22bd2b7","title":"Large AI Models in Health Informatics: Applications, Challenges, and the Future"},{"paperId":"c7a9c7302a72301ed79a7c0696d5af2e03ad3ac4","title":"MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action"},{"paperId":"9fe9af7cf3d54b707a7be3c53ce94b77dcc3bae5","title":"A Short Survey of Viewing Large Language Models in Legal Aspect"},{"paperId":"6e754273d54a91371efbc928cd6b156364d517da","title":"ViperGPT: Visual Inference via Python Execution for Reasoning"},{"paperId":"53df959bcf6499c45e316086a96a624389a39a52","title":"Accountable Textual-Visual Chat Learns to Reject Human Instructions in Image Re-creation"},{"paperId":"6845bea94b2fb17d4377b3bb2bd10f73a959f9cc","title":"Reasoning with Language Model Prompting: A Survey"},{"paperId":"419eb47fea3931c4098232f44ccbc216275d3f56","title":"Decoding Visual Neural Representations by Multimodal Learning of Brain-Visual-Linguistic Features"},{"paperId":"e342165a614588878ad0f4bc9bacf3905df34d08","title":"Diffusion Models: A Comprehensive Survey of Methods and Applications"},{"paperId":"5ce5eb5c4f4dc9e17e3548be931d48c96f5d14c9","title":"Priors in Deep Image Restoration and Enhancement: A Survey"},{"paperId":"cc074c98f3fa83e5367f3e593fc07bde22d32a7a","title":"Intelligent Practices of Large Language Models in Digital Government Services"},{"paperId":"aa0ed1d8a3d128b6159b543b57ed7cfec035f23d","title":"Efficient Classification of Malicious URLs: M-BERT—A Modified BERT Variant for Enhanced Semantic Understanding"},{"paperId":"13b5b69355555e0c8b702261c5de3b4172ba653c","title":"The Art of SOCRATIC QUESTIONING: Zero-shot Multimodal Reasoning with Recursive Thinking and Self-Questioning"},{"paperId":"06d8562831c32844285a691c5250d04726df3c61","title":"A Systematic Survey of Prompt Engineering on Vision-Language Foundation Models"},{"paperId":"d1120d67b700e4dfe8b39eb1e48fbdea4e1a0c43","title":"HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face"},{"paperId":"c2408a3a8da4f12d3eb156fe359a96b428e5aff1","title":"Chain-of-Symbol Prompting Elicits Planning in Large Langauge Models"},{"paperId":"eb291a2e237774b162d9c51c21c4868795589e94","title":"Diving into the Inter-Consistency of Large Language Models: An Insightful Analysis through Debate"},{"paperId":"ed9943d73eb42116fe33564b5065c78b5ca0b16e","title":"RestGPT: Connecting Large Language Models with Real-World Applications via RESTful APIs"},{"paperId":"8cbc8c83b9f612f298e89b65500f4f362a6b0439","title":"Frontier Review of Multimodal AI"},{"paperId":"990eefe3b7f65736c8cf6a7dcaf4cdbc21cc71b3","title":"Towards AI-based Accessible Digital Media: Image Analysis Pipelines and Blind User Studies*"},{"paperId":"7e3bbd7be60bb50a8093152795f269a69a4a0fd9","title":"Chatting Makes Perfect - Chat-based Image Retrieval"},{"paperId":"6c5a1079d9705c0ee022cef77207daa20ce2cde5","title":"Summary of ChatGPT/GPT-4 Research and Perspective Towards the Future of Large Language Models"},{"paperId":"96a6df2b4aa50cfbd8984933e9c66b0763fc08a6","title":"Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V"},{"paperId":"f02241105c2a72943e24c37ae58a22c46db88720","title":"PCLmed at ImageCLEFmedical 2023: Customizing General-Purpose Foundation Models for Medical Report Generation"},{"paperId":"09ca5072a76796c65e5936b6fb4968afead61944","title":"Semantics-Empowered Communications: A Tutorial-cum-Survey"},{"paperId":"5ce94181ea702f69c3651dce721d6bd8026b8106","title":"TPTU: Task Planning and Tool Usage of Large Language Model-based AI Agents"},{"paperId":"e611f556e8c871bdad67f06f336ca05dda600035","title":"Diverse Hyperspectral Remote Sensing Image Synthesis With Diffusion Models"},{"paperId":"44ccf252018f71898d52d89539f17d77a4f8d548","title":"Chart Understanding with Large Language Model"}],"references":[{"paperId":"efbe97d20c4ffe356e8826c01dc550bacc405add","title":"Adding Conditional Control to Text-to-Image Diffusion Models"},{"paperId":"780a7f5e8ba9b4b451e3dfee1bcfb0f68aba5050","title":"Multimodal Chain-of-Thought Reasoning in Language Models"},{"paperId":"3f5b31c4f7350dc88002c121aecbdc82f86eb5bb","title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models"},{"paperId":"a2d2bbe4c542173662a444b33b76c66992697830","title":"InstructPix2Pix: Learning to Follow Image Editing Instructions"},{"paperId":"964bd39b546f0f6625ff3b9ef1083f797807ef2e","title":"BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"},{"paperId":"90350aa626bed47b02d0c162462e5b0ca82be6b2","title":"Automatic Chain of Thought Prompting in Large Language Models"},{"paperId":"d3135733aa39dec20ce72aa138589dda27c8406d","title":"Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering"},{"paperId":"e7ad08848d5d7c5c47673ffe0da06af443643bda","title":"Large Language Models are Zero-Shot Reasoners"},{"paperId":"5437e8adab596d7294124c0e798708e050e25321","title":"Least-to-Most Prompting Enables Complex Reasoning in Large Language Models"},{"paperId":"26218bdcc3945c7edae7aa2adbfba4cd820a2df3","title":"Flamingo: a Visual Language Model for Few-Shot Learning"},{"paperId":"ada81a4de88a6ce474df2e2446ad11fea480616e","title":"Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language"},{"paperId":"23dd78e424d32f6a48660dcd67ce994b8a7db8be","title":"STaR: Bootstrapping Reasoning With Reasoning"},{"paperId":"5f19ae1135a9500940978104ec15a5b8751bc7d2","title":"Self-Consistency Improves Chain of Thought Reasoning in Language Models"},{"paperId":"d766bffc357127e0dc86dd69561d5aeb520d6f4c","title":"Training language models to follow instructions with human feedback"},{"paperId":"1b6e810ce0afd0dd093f789d2b2742d047e316d5","title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"a3b42a83669998f65df60d7c065a70d07ca95e99","title":"BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation"},{"paperId":"f4b11a696aa5a03fed1bfc47e65fdb7eb0e529c1","title":"UniFormer: Unifying Convolution and Self-Attention for Visual Recognition"},{"paperId":"400d619cbabeb669115bb7281a889ab869829ef5","title":"MERLOT RESERVE: Neural Script Knowledge through Vision and Language and Sound"},{"paperId":"c10075b3746a9f3dd5811970e93c8ca3ad39b39d","title":"High-Resolution Image Synthesis with Latent Diffusion Models"},{"paperId":"bdea16e93fc70f316002e5f6aac8ce17388c6ee9","title":"MAGMA - Multimodal Augmentation of Generative Models through Adapter-based Finetuning"},{"paperId":"ba9d736006b897d06f75586ad46e28e00a5e566e","title":"VIOLET : End-to-End Video-Language Transformers with Masked Visual-token Modeling"},{"paperId":"197d5867a45a2988f4dd159063cdfbfe90164962","title":"LiT: Zero-Shot Transfer with Locked-image text Tuning"},{"paperId":"cf7c2e0e4fb2af689aaf4b7a7cddf7b1f4d5e3f0","title":"VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts"},{"paperId":"767923635f2fd4467d848dba9655866e4f9b55c8","title":"Supervision Exists Everywhere: A Data Efficient Contrastive Language-Image Pre-training Paradigm"},{"paperId":"2672777d25562c9df6fc13b653181db62d39bece","title":"An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA"},{"paperId":"260ad39a1dac4b451019e2bf17925f4df8e3b69a","title":"Per-Pixel Classification is Not All You Need for Semantic Segmentation"},{"paperId":"01b5412f3d17e90e09226d7c40ad4d4468a1414d","title":"Multimodal Few-Shot Learning with Frozen Language Models"},{"paperId":"2a805d0e1b067444a554c5169d189fa1f649f411","title":"Scaling Vision Transformers"},{"paperId":"0cceb0393b87d3ff65a1f0beea696ce40e889597","title":"Towards Light-Weight and Real-Time Line Segment Detection"},{"paperId":"8e33914d6051dd031a5e096962b9398fc1d16067","title":"Vision Transformers for Dense Prediction"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"616e0ed02ca024a8c1d4b86167f7486ea92a13d9","title":"VisualGPT: Data-efficient Adaptation of Pretrained Language Models for Image Captioning"},{"paperId":"be0014c1fbc3e664686610d2c85f75038a4f6e4f","title":"VinVL: Making Visual Representations Matter in Vision-Language Models"},{"paperId":"053b1d7b97eb2c91fc3921d589c160b0923c70b1","title":"Learning to summarize from human feedback"},{"paperId":"4a657ceb97829a4ab7502f6d01235d1f0140eb0f","title":"Text as Neural Operator:Image Manipulation by Text Instruction"},{"paperId":"2f5f81bc516a6d085d39479378af1fc27104f91e","title":"Large-Scale Adversarial Training for Vision-and-Language Representation Learning"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"818e5cbc337e4e1b98e65a2d7c2d6d2a0318cd57","title":"Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks"},{"paperId":"953667588e089ae99f049e8574d013bb70aa8517","title":"ManiGAN: Text-Guided Image Manipulation"},{"paperId":"6c4b76232bb72897685d19b3d264c6ee3005bc2b","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"dfc7b58b67c31932b48586b3e23a43cc94695290","title":"UNITER: UNiversal Image-TExt Representation Learning"},{"paperId":"7bd83b055702bc178aa26def5b6df463f8eab7b9","title":"Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-Shot Cross-Dataset Transfer"},{"paperId":"29ddc1f43f28af7c846515e32cc167bc66886d0c","title":"Parameter-Efficient Transfer Learning for NLP"},{"paperId":"6dfc2ff03534a4325d06c6f88c3144831996629b","title":"From Recognition to Cognition: Visual Commonsense Reasoning"},{"paperId":"19c12e12946eb3bb9aa7fdeb511eef79fc53b6b3","title":"Canny edge detection based on Open CV"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"dfad8f616bd2a05c8cae5f61060f743f966ece85","title":"Realtime Multi-person 2D Pose Estimation Using Part Affinity Fields"},{"paperId":"778ce81457383bd5e3fdb11b145ded202ebb4970","title":"Semantic Compositional Networks for Visual Captioning"},{"paperId":"8acbe90d5b852dadea7810345451a99608ee54c7","title":"Image-to-Image Translation with Conditional Adversarial Networks"},{"paperId":"9a522bdd86531839ff292d096e0c4050b787de02","title":"Commonsense reasoning and commonsense knowledge in artificial intelligence"},{"paperId":"97ad70a9fa3f99adf18030e5e38ebe3d90daa2db","title":"VQA: Visual Question Answering"},{"paperId":"8da55e685a7bef9c897788ab519a8710c695c419","title":"Holistically-Nested Edge Detection"},{"paperId":"d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0","title":"Show and tell: A neural image caption generator"},{"paperId":"71b7178df5d2b112d07e45038cb5637208659ff7","title":"Microsoft COCO: Common Objects in Context"},{"paperId":"356770d13ad2e7b60961e5bc7368ffd3b9a2bcd9","title":"Learning to summarize with human feedback"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":null,"title":": ”timbrooks/instruct-pix2pix” from Hugging-Face, StableDiffusionInstructPix2PixPipeline model"},{"paperId":null,"title":"image path, textual how to modify → image path"},{"paperId":null,"title":"Harrison Chase"},{"paperId":null,"title":"Prompt: Generate Image Condition On Pose Image: useful for when you want to generate a new real image from both the user desciption and a human pose image"}],"id":"af997821231898a5f8d0fd78dad4eec526acabe5","summary":"A system to enable the user to interact with ChatGPT by sending and receiving not only languages but also images and providing complex visual questions or visual editing instructions that require the collaboration of multiple AI models with multi-steps, and opens the door to investigating the visual roles ofChatGPT with the help of Visual Foundation Models."},{"url":"https://www.semanticscholar.org/paper/994e08ac813028601907516aee9c4699234a6b4d","title":"Large AI Models in Health Informatics: Applications, Challenges, and the Future","venue":"arXiv.org","year":2023,"referenceCount":347,"citationCount":32,"influentialCitationCount":0,"publicationDate":"21/03/2023","authors":"Jianing Qiu,Lin Li,Jiankai Sun,Jiachuan Peng,Peilun Shi,Rui Zhang,Yinzhao Dong,Kyle Lam,F. P. Lo,Bo Xiao,Wu Yuan,Dong Xu,Benny P. L. Lo","citations":[{"paperId":"a624041a028404e7d1fdb40987b1780ce4f1c842","title":"BitDelta: Your Fine-Tune May Only Be Worth One Bit"},{"paperId":"8f018d29d9fbd7ff2c37df676081cd47f321b59b","title":"Evaluation of General Large Language Models in Contextually Assessing Semantic Concepts Extracted from Adult Critical Care Electronic Health Record Notes"},{"paperId":"12ed45473dee6d0917f8577157cb86952cb162ce","title":"Detecting Multimedia Generated by Large AI Models: A Survey"},{"paperId":"a1c04034df40cbe6f0d0004d456fdab807ae63ec","title":"Generalist embedding models are better at short-context clinical semantic search than specialized embedding models"},{"paperId":"ee1390a871c7269b40b173666d805f228f21b0f6","title":"Rational Design of Flexible Mechanical Force Sensors for Healthcare and Diagnosis"},{"paperId":"28236a007a9aeaf63f2f4685b5705c0f2755d1dc","title":"Dietary Assessment with Multimodal ChatGPT: A Systematic Analysis"},{"paperId":"5f0b826ffe17faa2cdec21752e7d1863bd909f2c","title":"Foundation Models in Robotics: Applications, Challenges, and the Future"},{"paperId":"782a5600bf2c67ea95812fc6b51bb56ed05f2b94","title":"AI-SAM: Automatic and Interactive Segment Anything Model"},{"paperId":"7bf0c5dd006174a161a67d75eacedb0ec0d88d35","title":"GlanceSeg: Real-time microaneurysm lesion segmentation with gaze-map-guided foundation model for early detection of diabetic retinopathy"},{"paperId":"2033c8d1a160814022501032137adac2ccb83495","title":"Aria-NeRF: Multimodal Egocentric View Synthesis"},{"paperId":"8d2709ed1788a67e64425fb410bb49f3ee49e088","title":"Large Language Models Illuminate a Progressive Pathway to Artificial Healthcare Assistant: A Review"},{"paperId":"5b038c1a93967072cc76689fd805e756f804cc42","title":"Large Models for Time Series and Spatio-Temporal Data: A Survey and Outlook"},{"paperId":"8e6c4425e48b09d64827c64d8de0008f41f9be54","title":"Qilin-Med: Multi-stage Knowledge Injection Advanced Medical Large Language Model"},{"paperId":"72b06aef94f798ad9035b5775c186fd2fd6a8f38","title":"Multi-domain improves out-of-distribution and data-limited scenarios for medical image analysis"},{"paperId":"bba297cb1e35cb357273df56b495a60933157af4","title":"VisionFM: a Multi-Modal Multi-Task Vision Foundation Model for Generalist Ophthalmic Artificial Intelligence"},{"paperId":"16e0b8c878c75bb57ffb62c08ebf23b51ac10b99","title":"BioBridge: Bridging Biomedical Foundation Models via Knowledge Graphs"},{"paperId":"f8b5ee53c3410f20049e7def47bd52403fa388e3","title":"LEGO-Prover: Neural Theorem Proving with Growing Libraries"},{"paperId":"45abdf6604eec7e4b2fe8116f6fd9c18d01fe962","title":"CauDR: A Causality-inspired Domain Generalization Framework for Fundus-based Diabetic Retinopathy Grading"},{"paperId":"1ddd474f8a7d596b7609d705f960d2d924ef8e00","title":"Lyra: Orchestrating Dual Correction in Automated Theorem Proving"},{"paperId":"10fa85cdb6c64f2e9081c3422de628686ae87e6f","title":"Data Decentralisation of LLM-Based Chatbot Systems in Chronic Disease Self-Management"},{"paperId":"374ebdc8240a35820cb7ab8bfca37e180e21b605","title":"Sparks of Large Audio Models: A Survey and Outlook"},{"paperId":"121a7077db9314d63cc9ef5718e196ff0d75ab92","title":"BioSignal Copilot: Leveraging the power of LLMs in drafting reports for biomedical signals"},{"paperId":"6e715cd0ab4036f757f2e30a7a36108f067fa247","title":"EHRSHOT: An EHR Benchmark for Few-Shot Evaluation of Foundation Models"},{"paperId":"586bdad62d35342ec4aae0dd539379fff1ea4547","title":"PULSAR at MEDIQA-Sum 2023: Large Language Models Augmented by Synthetic Dialogue Convert Patient Dialogues to Medical Records"},{"paperId":"362d4e00506f9bb39d42185a0b128f8602e139a8","title":"Utilizing ChatGPT to Enhance Clinical Trial Enrollment"},{"paperId":"06091944b864d6dc473cab63321a95fb9c4067cc","title":"ChatCAD+: Towards a Universal and Reliable Interactive CAD using LLMs"},{"paperId":"d0ee000f30420953f10dfcfd608a7f9ad40f1635","title":"Humans are still better than ChatGPT: Case of the IEEEXtreme competition"},{"paperId":"80785017029cab501fcdb90b98985cd2b36e1fb8","title":"Evaluation of GPT-3.5 and GPT-4 for supporting real-world information needs in healthcare delivery"},{"paperId":"78b03df885cbb57361e8efc5ce5ad5eea211dda6","title":"Generalist Vision Foundation Models for Medical Imaging: A Case Study of Segment Anything Model on Zero-Shot Medical Segmentation"},{"paperId":"5f0dcd30110d064d775e1b16e5550a207a90eb11","title":"Label-Efficient Deep Learning in Medical Image Analysis: Challenges and Future Directions"},{"paperId":"bca0bbd01ea917b7a9fe369288ea3ba03d3b1ff3","title":"A Survey of Large Language Models in Medicine: Progress, Application, and Challenge"},{"paperId":"a10acbe4657c8764a1119b605ca8afbd26922a7a","title":"Assessing ChatGPT's Potential in Endodontics: Preliminary Findings from A Diagnostic Accuracy Study"}],"references":[{"paperId":"c064c79e3026f81e5043cd5b0f4264b4d43336e6","title":"xTrimoPGLM: Unified 100B-Scale Pre-trained Transformer for Deciphering the Language of Protein"},{"paperId":"0f0024bfef037b97b324b97150ee022c178d6282","title":"A visual–language foundation model for pathology image analysis using medical Twitter"},{"paperId":"c51192d7440807dc98cc4374fb5d919390d70b0b","title":"OpenFold: Retraining AlphaFold2 yields new insights into its learning mechanisms and capacity for generalization"},{"paperId":"83c48aa341850af478247e3b34ba1ee1db9f1236","title":"Meta-Transformer: A Unified Framework for Multimodal Learning"},{"paperId":"104b0bb1da562d53cbda87aec79ef6a2827d191a","title":"Llama 2: Open Foundation and Fine-Tuned Chat Models"},{"paperId":"e548942b23a54f3492d2ff566a6729d8f37278bd","title":"Large language model AI chatbots require approval as medical devices"},{"paperId":"25b67873c4bc9afb35224bd984554430fe91d5a7","title":"Foundation Model for Endoscopy Video Analysis via Large-scale Self-supervised Pre-train"},{"paperId":"66d7d8dc54ea3dff10a11df2f29dc2104df86a57","title":"XrayGPT: Chest Radiographs Summarization using Medical Vision-Language Models"},{"paperId":"37e48270c592dbda0437ef8d6f0fdfd8268cf705","title":"A foundational vision transformer improves diagnostic performance for electrocardiograms"},{"paperId":"06091944b864d6dc473cab63321a95fb9c4067cc","title":"ChatCAD+: Towards a Universal and Reliable Interactive CAD using LLMs"},{"paperId":"4f0c7f4df04f07609bdb67944af2a529d5a4517b","title":"A Survey of Safety and Trustworthiness of Large Language Models through the Lens of Verification and Validation"},{"paperId":"7ed0faa6720cd176d57badbc0455af31a03f080c","title":"Towards Expert-Level Medical Question Answering with Large Language Models"},{"paperId":"7dc6da87eaa6f830354feb2db14023cab8678c91","title":"ImageBind One Embedding Space to Bind Them All"},{"paperId":"77877e028e5dace94f1f042c2a4e5e71e840c6a5","title":"A Step Towards Conditional Autonomy - Robotic Appendectomy"},{"paperId":"78b03df885cbb57361e8efc5ce5ad5eea211dda6","title":"Generalist Vision Foundation Models for Medical Imaging: A Case Study of Segment Anything Model on Zero-Shot Medical Segmentation"},{"paperId":"49f9882d5fd442f02f9c9dff780336f6dce2da4f","title":"Medical SAM Adapter: Adapting Segment Anything Model for Medical Image Segmentation"},{"paperId":"95430a76264a9be7d64633e56831c60041fb2948","title":"SurgicalGPT: End-to-End Language-Vision GPT for Visual Question Answering in Surgery"},{"paperId":"ba82f8aa4526c326dc0e82e7443a017a409b865a","title":"In ChatGPT We Trust? Measuring and Characterizing the Reliability of ChatGPT"},{"paperId":"17606dbe67df42d973015fdd35f2807b0cafc15b","title":"The Self-Perception and Political Biases of ChatGPT"},{"paperId":"38a9609a5bd874534527df9b00f2897927e57be9","title":"MedAlpaca - An Open-Source Collection of Medical Conversational AI Models and Training Data"},{"paperId":"302ee27524a717ddc21f332ca634b9211c6ec6aa","title":"HuaTuo: Tuning LLaMA Model with Chinese Medical Knowledge"},{"paperId":"ebb1acf1eb3705510de5a19065092404427428a5","title":"STU-Net: Scalable and Transferable Medical Image Segmentation Models Empowered by Large-Scale Supervised Pre-training"},{"paperId":"025ca4c125d6ecabc816a56f160e5c992abc76d9","title":"Multi-step Jailbreaking Privacy Attacks on ChatGPT"},{"paperId":"7470a1702c8c86e6f28d32cfa315381150102f5b","title":"Segment Anything"},{"paperId":"72b74bcff8fd76eff6789111a7ce5d0d6c5ac4db","title":"Benefits, Limits, and Risks of GPT-4 as an AI Chatbot for Medicine."},{"paperId":"4a7f6c4e71e20311ade4e76e8d0945d499c31fcd","title":"ChatDoctor: A Medical Chat Model Fine-Tuned on a Large Language Model Meta-AI (LLaMA) Using Medical Domain Knowledge"},{"paperId":"348a1efa54376fa39053e5e25d52bd0eb6a0ba68","title":"Capabilities of GPT-4 on Medical Challenge Problems"},{"paperId":"ace2b6367a067898f66a33fca19581ebe71fccc5","title":"GPT-4 Technical Report"},{"paperId":"6470b561d3426714847fd9201c8ea4ab8585fb96","title":"GeneTuring tests GPT models in genomics"},{"paperId":"af997821231898a5f8d0fd78dad4eec526acabe5","title":"Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models"},{"paperId":"703a72ad206272d2022c9b1d7eb775e275f4b39c","title":"Empowering Beginners in Bioinformatics with ChatGPT"},{"paperId":"38fe8f324d2162e63a967a9ac6648974fc4c66f3","title":"PaLM-E: An Embodied Multimodal Language Model"},{"paperId":"777317e5af8742b30408e98778fa067750e69f78","title":"Google USM: Scaling Automatic Speech Recognition Beyond 100 Languages"},{"paperId":"76b19363b10d7ea783e4a6494eae40d73c8e9628","title":"Parameter-efficient fine-tuning of large-scale pre-trained language models"},{"paperId":"57e849d0de13ed5f91d086936296721d4ff75a75","title":"LLaMA: Open and Efficient Foundation Language Models"},{"paperId":"fbfef4723d8c8467d7bd523e1d0b703cce0e0f9c","title":"Language Is Not All You Need: Aligning Perception with Language Models"},{"paperId":"a68db57f08f6d72c0d3b22d451d2606dca880f94","title":"MimicPlay: Long-Horizon Imitation Learning by Watching Human Play"},{"paperId":"5c7353fac22a8fdc43fc2f5c006b5d6902c47e75","title":"On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective"},{"paperId":"0ba581718f294db1d7b3dbc159cc3d3380f74606","title":"ChatGPT for Robotics: Design Principles and Model Abilities"},{"paperId":"efa06fe7c6a4abbe465dbea4f7130f45720ac6f0","title":"Tuning computer vision models with task rewards"},{"paperId":"5ef821267fa68d3231ed8135ff8ec09f25bb1398","title":"ChatCAD: Interactive Computer-Aided Diagnosis on Medical Image using Large Language Models"},{"paperId":"61e721334296ebfbbf6443b5ed9eb8c83b708c95","title":"Scaling Vision Transformers to 22 Billion Parameters"},{"paperId":"38c14931cf5dc7781b4f24af15e8938dfb898317","title":"Are Diffusion Models Vulnerable to Membership Inference Attacks?"},{"paperId":"60f78afe2040f33988c71d585c3f42f06814d0de","title":"ChatGPT: the future of discharge summaries?"},{"paperId":"3f5b31c4f7350dc88002c121aecbdc82f86eb5bb","title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models"},{"paperId":"9a23e7bd9177ce404e0415bbb8b245c6458d0102","title":"Medical artificial intelligence is as much social as it is technological"},{"paperId":"9a7ac45eafe11ca003db3a300505f3b5c3f9009a","title":"DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature"},{"paperId":"428854d9e75f94f0e61f37c6887c77800437d516","title":"MusicLM: Generating Music From Text"},{"paperId":"68be99a37c23486fcdd016fdf7833bb9092146ae","title":"Data Augmentation Alone Can Improve Adversarial Training"},{"paperId":"35cdc00a4e2bc1f6c253a34a5e3a6f697050d1e9","title":"Evaluating the Performance of ChatGPT in Ophthalmology"},{"paperId":"874deb5f06f35e52ae13a921b23611eec4abd1da","title":"ClimaX: A foundation model for weather and climate"},{"paperId":"ee57e4d7a125f4ca8916284a857c3760d7d378d3","title":"Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture"},{"paperId":"1420b9ff9c7ecfc8c8b5fdce4517e6fc51cebf92","title":"Ankh ☥: Optimized Protein Language Model Unlocks General-Purpose Modelling"},{"paperId":"23cae400cfd1a7c455c721256b838e98a307d5e6","title":"ChatGPT Makes Medicine Easy to Swallow: An Exploratory Case Study on Simplified Radiology Reports"},{"paperId":"6052486bc9144dc1730c12bf35323af3792a1fd0","title":"Large language models encode clinical knowledge"},{"paperId":"c49a0912595a1cc70aab63524f64ed08c92194a8","title":"Evolutionary-scale prediction of atomic level protein structure with a language model"},{"paperId":"cf1f26e7cbed3958b3c2870656568c299fece6e3","title":"Performance of ChatGPT on USMLE: Potential for AI-assisted medical education using large language models"},{"paperId":"fd1cf28a2b8caf2fe29af5e7fa9191cecfedf84d","title":"RT-1: Robotics Transformer for Real-World Control at Scale"},{"paperId":"9b5744d9274fe69fb95ede5035acce49fc5ab27a","title":"ManyFold: an efficient and flexible library for training and validating protein folding models"},{"paperId":"750676b67abef11d102f0a5e7e221bbb56fca2c8","title":"UNETR++: Delving into Efficient and Accurate 3D Medical Image Segmentation"},{"paperId":"a02fbaf22237a1aedacb1320b6007cd70c1fe6ec","title":"Robust Speech Recognition via Large-Scale Weak Supervision"},{"paperId":"ee96ec926f06ff2f3ce3d131cffcbfe63af39f0c","title":"Towards All-in-One Pre-Training via Maximizing Multi-Modal Mutual Information"},{"paperId":"c6d24ca67b3592adc23503f7af0635f9f94c4116","title":"A Review on the Use of Mobile Service Robots in Elderly Care"},{"paperId":"26c80bd65baa90f5b18157de4951f4eb0b62ab69","title":"InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions"},{"paperId":"964bd39b546f0f6625ff3b9ef1083f797807ef2e","title":"BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"},{"paperId":"cdbd4f9b6ab2e2fd1ddf5400d5ed2c18960635d1","title":"Scaling Instruction-Finetuned Language Models"},{"paperId":"ad95ce28151b7b77e88b425cb936c84e043e94d6","title":"Modular robotic platform for precision neurosurgery with a bio-inspired needle: System overview and first in-vivo deployment"},{"paperId":"cdd9c1d23f9e89d5113f3e31821bb174c6a6afed","title":"MedCLIP: Contrastive Learning from Unpaired Medical Images and Text"},{"paperId":"e5c8960eb2ec034ffbd353ef39fd1cb541d3c7c9","title":"LAION-5B: An open large-scale dataset for training next generation image-text models"},{"paperId":"f5225015bcdad0a1daf7d205c67fc297fb9ec978","title":"Adapting Pretrained Vision-Language Foundational Models to Medical Imaging Domains"},{"paperId":"25425e299101b13ec2872417a14f961f4f8aa18e","title":"VIMA: General Robot Manipulation with Multimodal Prompts"},{"paperId":"7698498dcb14db063154f4c955fc041114d1960d","title":"Single-sequence protein structure prediction using a language model and deep learning"},{"paperId":"ebb85974e06c4879b451fdfcb4f472a09471935b","title":"AudioGen: Textually Guided Audio Generation"},{"paperId":"74eae12620bd1c1393e268bddcb6f129a5025166","title":"Improving alignment of dialogue agents via targeted human judgements"},{"paperId":"168db75f79cd2d39a7802451578662bb15572de4","title":"Improving Radiology Report Generation Systems by Removing Hallucinated References to Non-existent Priors"},{"paperId":"edb5495bc9081f2adfc8de51e0981510802e4090","title":"scBERT as a large-scale pretrained deep language model for cell type annotation of single-cell RNA-seq data"},{"paperId":"44279244407a64431810f982be6d0c7da4429dd7","title":"BioGPT: Generative Pre-trained Transformer for Biomedical Text Generation and Mining"},{"paperId":"33fd110d1e4ca5f91d1b7ca7ff24ce1e9335359e","title":"Metadata Archaeology: Unearthing Data Subsets by Leveraging Training Dynamics"},{"paperId":"76120de60a9e59c23a372457a056da3c220c64b6","title":"Expert-level detection of pathologies from unannotated chest X-ray images via self-supervised learning"},{"paperId":"81adb80e390f25d4a2d764b1063eeaa2c334d441","title":"Multi-modal Masked Autoencoders for Medical Vision-and-Language Pre-training"},{"paperId":"28630034bb29760df01ab033b743e30b37f336ae","title":"PaLI: A Jointly-Scaled Multilingual Language-Image Model"},{"paperId":"60c8d0619481eaafdd1189af610d0e636271fed5","title":"Perceiver-Actor: A Multi-Task Transformer for Robotic Manipulation"},{"paperId":"efa1647594b236361610a20d507127f0586a379b","title":"Diffusion Models in Vision: A Survey"},{"paperId":"8c870bef01a4fbb20f60722ffc2f6bee3870b18b","title":"AudioLM: A Language Modeling Approach to Audio Generation"},{"paperId":"79198b4009b375ae3746b7331a57f2aef54a456c","title":"Uni-Fold: An Open-Source Platform for Developing Protein Folding Models beyond AlphaFold"},{"paperId":"790f73b4019edb6a90bdc7e7063bb541786b17f3","title":"Clustering Egocentric Images in Passive Dietary Monitoring with Self-Supervised Learning"},{"paperId":"8ad0578c56a7ebe26a308592a1c4e4813dd66ccb","title":"BRAX, Brazilian labeled chest x-ray dataset"},{"paperId":"bc103c1f62e93ba31f012a1503a99cb481310b7b","title":"Interpretable bilinear attention network with domain adaptation improves drug–target prediction"},{"paperId":"867d80c8779e1d301a5fc6e267e263f7e4c4c5c7","title":"High-resolution de novo structure prediction from primary sequence"},{"paperId":"c036f75da24ba64a583e0b6d41c5b792347bffa6","title":"Diffsound: Discrete Diffusion Model for Text-to-Sound Generation"},{"paperId":"d697b440dd0e65a05fe027e4c0ea85f62dcba033","title":"Can large language models reason about medical questions?"},{"paperId":"bd1cd034470c1d3983e93302acab720f6af281d1","title":"HelixFold: An Efficient Implementation of AlphaFold2 using PaddlePaddle"},{"paperId":"84912ef04ee98e325d8335bc64d73ca3f3aa5328","title":"E2Efold-3D: End-to-End Deep Learning Method for accurate de novo RNA 3D Structure Prediction"},{"paperId":"22374e8aa39c3f39b7f578b3fd04d969df93854d","title":"AttentionSiteDTI: an interpretable graph-based model for drug-target interaction prediction using NLP sentence-level relation classification"},{"paperId":"1243e13254bb4ea1f71b4be8a3e4e54ffd02d2fe","title":"Scaling Autoregressive Models for Content-Rich Text-to-Image Generation"},{"paperId":"dac3a172b504f4e33c029655e9befb3386e5f63a","title":"Emergent Abilities of Large Language Models"},{"paperId":"d46fe4aabc55a2d0232a5729fc5710410e932edd","title":"Discovering the Hidden Vocabulary of DALLE-2"},{"paperId":"a20411effeaac9aa457e528090dc274cb46c3412","title":"ColabFold: making protein folding accessible to all"},{"paperId":"686d9ee744fa013cc21cdd86acd864c936e9e456","title":"Large language models are few-shot clinical information extractors"},{"paperId":"e7ad08848d5d7c5c47673ffe0da06af443643bda","title":"Large Language Models are Zero-Shot Reasoners"},{"paperId":"9695824d7a01fad57ba9c01d7d76a519d78d65e7","title":"Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding"},{"paperId":"5922f437512158970c417f4413bface021df5f78","title":"A Generalist Agent"},{"paperId":"13a0d8bb38f739990c8cd65a44061c6534f17221","title":"OPT: Open Pre-trained Transformer Language Models"},{"paperId":"c57293882b2561e1ba03017902df9fc2f289dea2","title":"Hierarchical Text-Conditional Image Generation with CLIP Latents"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"cb5e3f085caefd1f3d5e08637ab55d39e61234fc","title":"Do As I Can, Not As I Say: Grounding Language in Robotic Affordances"},{"paperId":"00df82d233825b6d9a1fe5ae482e27f981a91024","title":"PanGu Drug Model: learn a molecule like a human"},{"paperId":"9068d48f34b70e96cb4f9fd35ad87ce31e98de96","title":"Emotion Recognizing by a Robotic Solution Initiative (EMOTIVE Project)"},{"paperId":"07264347e959913a6ea37953d9c0e30ed4efb3ba","title":"Interpretable RNA Foundation Model from Unannotated Data for Highly Accurate RNA Structure and Function Predictions"},{"paperId":"a83cdcc0135c58fddf89fc72f1b92b7a9d1e170f","title":"LinkBERT: Pretraining Language Models with Document Links"},{"paperId":"8342b592fe238f3d230e4959b06fd10153c45db1","title":"Training Compute-Optimal Large Language Models"},{"paperId":"c9bdc9ad2c3cf3230ba9aac7b5783ab411f0d204","title":"R3M: A Universal Visual Representation for Robot Manipulation"},{"paperId":"9dac4cecd2099e8f73970fb589f884a4e460a0fe","title":"LocATe: End-to-end Localization of Actions in 3D with Transformers"},{"paperId":"d766bffc357127e0dc86dd69561d5aeb520d6f4c","title":"Training language models to follow instructions with human feedback"},{"paperId":"315ae320de0d6b31c94d70999b23028b688c272d","title":"FastFold: Reducing AlphaFold Training Time from 11 Days to 67 Hours"},{"paperId":"28c7e583d90ccfc5c3078dfc1d6b80a9ad90248d","title":"Quantifying Memorization Across Neural Language Models"},{"paperId":"76cb108e37d9d2a06f5a49df04e993f5fb123c26","title":"The Carbon Footprint of Machine Learning Training Will Plateau, Then Shrink"},{"paperId":"332dc8b2ca9d49fad607c7282f3360bb2a9aacf3","title":"A large language model for electronic health records"},{"paperId":"1b6e810ce0afd0dd093f789d2b2742d047e316d5","title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"a3b42a83669998f65df60d7c065a70d07ca95e99","title":"BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation"},{"paperId":"7cbc2a7843411a1768ab762930707af0a3c33a19","title":"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model"},{"paperId":"229ca1d46d63ccac4b82e5cca5c4dd676aaa870d","title":"Autonomous robotic laparoscopic surgery for intestinal anastomosis"},{"paperId":"dcb21a5a0bc8b6d1bcfff10659e192a95ea20773","title":"DrugOOD: Out-of-Distribution (OOD) Dataset Curator and Benchmark for AI-aided Drug Discovery - A Focus on Affinity Prediction Problems with Noise Annotations"},{"paperId":"b3848d32f7294ec708627897833c4097eb4d8778","title":"LaMDA: Language Models for Dialog Applications"},{"paperId":"b7dc007054cf17dea3b22a2d1e71ba4cc8606648","title":"Revisiting Weakly Supervised Pre-Training of Visual Perception Models"},{"paperId":"177e957f5cd93229c9794ea652c646d2557b4a69","title":"A ConvNet for the 2020s"},{"paperId":"3141e5cf27e69e5e8bd8b5c6cc97c23380e8a9c1","title":"Can AlphaFold2 predict the impact of missense mutations on structure?"},{"paperId":"c10075b3746a9f3dd5811970e93c8ca3ad39b39d","title":"High-Resolution Image Synthesis with Latent Diffusion Models"},{"paperId":"7002ae048e4b8c9133a55428441e8066070995cb","title":"GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models"},{"paperId":"2f3efe44083af91cef562c1a3451eee2f8601d22","title":"WebGPT: Browser-assisted question-answering with human feedback"},{"paperId":"3c6eefec0dfd60c33a1611275fa301a265d8b3a1","title":"Deciphering antibody affinity maturation with language models and weakly supervised learning"},{"paperId":"002c256d30d6be4b23d365a8de8ae0e67e4c9641","title":"Improving language models by retrieving from trillions of tokens"},{"paperId":"2fd6f77540c1cc8e70b96208ccf9971b4251fc02","title":"FLAVA: A Foundational Language And Vision Alignment Model"},{"paperId":"68f141724814839d556a989646194be88641b143","title":"Scaling Language Models: Methods, Analysis & Insights from Training Gopher"},{"paperId":"5341b412383c43f4a693ad63ec4489e3ec7688c8","title":"Grounded Language-Image Pre-training"},{"paperId":"dfe72973324814adba51fac772c9a52ca53024cd","title":"Transformer-Based Generative Model Accelerating the Development of Novel BRAF Inhibitors"},{"paperId":"076a8e778f2e9efb3c2fd45fed534ae9e6035f1b","title":"Self-Supervised Pre-Training of Swin Transformers for 3D Medical Image Analysis"},{"paperId":"21ec90872abd986c12afe39bebe807732ffa70c9","title":"Florence: A New Foundation Model for Computer Vision"},{"paperId":"da4261a957eaa96bf626e9641ef68ebed1d5333f","title":"RedCaps: web-curated image-text data created by the people, for the people"},{"paperId":"be0fbb810583930c071d0b9b2c5187fe260783f5","title":"Swin Transformer V2: Scaling Up Capacity and Resolution"},{"paperId":"19dbb57ad106137553bff4282149ac2800b5c176","title":"XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale"},{"paperId":"6351ebb4a3287f5f3e1273464b3b91e5df5a16d7","title":"Masked Autoencoders Are Scalable Vision Learners"},{"paperId":"4a02061f8623f68502991d8bdf7728ae50669091","title":"A decade retrospective of medical robotics research from 2010 to 2020"},{"paperId":"f675c62abfa788ea0be85d3124eba15a14d5e9d6","title":"FILIP: Fine-grained Interactive Language-Image Pre-Training"},{"paperId":"e3a8ea14fd7c7355ac4af43fcac99961dcfe2bfa","title":"Egocentric Human Trajectory Forecasting With a Wearable Camera and Multi-Modal Fusion"},{"paperId":"a66606a0bb11a1d5a7a14ec09df8a3481121ad6c","title":"MedMNIST v2 - A large-scale lightweight benchmark for 2D and 3D biomedical image classification"},{"paperId":"65a57c948755cffad38de4230937ffbff8a19783","title":"MedMNIST v2: A Large-Scale Lightweight Benchmark for 2D and 3D Biomedical Image Classification"},{"paperId":"77805d75199e7b9e580b4827f56a069ba0ddd13f","title":"MolGPT: Molecular Generation Using a Transformer-Decoder Model"},{"paperId":"17dd3555fd1ccf1141cf984347fa1b3fd6b009ca","title":"Multitask Prompted Training Enables Zero-Shot Task Generalization"},{"paperId":"847a153286d7f6f496f1ff61089831c267d68e30","title":"Ego4D: Around the World in 3,000 Hours of Egocentric Video"},{"paperId":"2556e820cba6bda75f6f31b76bc74d9e36d72cb3","title":"Protein complex prediction with AlphaFold-Multimer"},{"paperId":"0b500aa5fcc175f07aecf26c0e8ddc4f0c6a931d","title":"GLoRIA: A Multimodal Global-Local Representation Learning Framework for Label-efficient Medical Image Recognition"},{"paperId":"69ee9b3a915951cc84b74599a3a2699a66d4004f","title":"CLIPort: What and Where Pathways for Robotic Manipulation"},{"paperId":"d6a1e9699d4e3571ab1eb74ab9eaba75095b809c","title":"PlaTe: Visually-Grounded Planning With Transformers in Procedural Tasks"},{"paperId":"5e00596fa946670d894b1bdaeff5a98e3867ef13","title":"SimVLM: Simple Visual Language Model Pretraining with Weak Supervision"},{"paperId":"4f68e07c6c3173480053fd52391851d6f80d651b","title":"On the Opportunities and Risks of Foundation Models"},{"paperId":"ae73f99c5de069c0b41c7472397b64631fee281b","title":"Extracting Predictive Representations from Hundreds of Millions of Molecules."},{"paperId":"ebe259796870ebccf26577044d0087884209b884","title":"w2v-BERT: Combining Contrastive Learning and Masked Language Modeling for Self-Supervised Speech Pre-Training"},{"paperId":"5c84ffe8bb0da3e359cc71a9a08d5379ff5eac12","title":"Faculty Opinions recommendation of Accurate prediction of protein structures and interactions using a three-track neural network."},{"paperId":"28692beece311a90f5fa1ca2ec9d0c2ce293d069","title":"Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"},{"paperId":"f4d845c40176c349ef9701cb379f7959ca2a47f2","title":"Efficient Medical Image Segmentation Based on Knowledge Distillation"},{"paperId":"dc32a984b651256a8ec282be52310e6bd33d9815","title":"Highly accurate protein structure prediction with AlphaFold"},{"paperId":"86f01b405b0bc6269b409fb88f7d9473c1fd9010","title":"Egocentric Image Captioning for Privacy-Preserved Passive Dietary Intake Monitoring"},{"paperId":"a8ca46b171467ceb2d7652fbfb67fe701ad86092","title":"LoRA: Low-Rank Adaptation of Large Language Models"},{"paperId":"51362860fb8ad08255660303550ed4bc345535fc","title":"Accurate prediction of protein structures and interactions using a 3-track neural network"},{"paperId":"4fffa5245d3972077c83614c2a08a47cb578631e","title":"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units"},{"paperId":"979a9f247700d00ff2c3f0612d5eb001379f93c8","title":"The Medical Segmentation Decathlon"},{"paperId":"1f47d68fe87d0b1317b34a71f98548df3f5da5ff","title":"Algebraic graph-assisted bidirectional transformers for molecular property prediction"},{"paperId":"9f4b69762ffb1ba42b573fd4ced996f3153e21c0","title":"CoAtNet: Marrying Convolution and Attention for All Data Sizes"},{"paperId":"2a805d0e1b067444a554c5169d189fa1f649f411","title":"Scaling Vision Transformers"},{"paperId":"d4f6ef636e16b001986b541aa2afc76eed42ae34","title":"Considering the possibilities and pitfalls of Generative Pre-trained Transformer 3 (GPT-3) in healthcare delivery"},{"paperId":"c1ad5f9b32d80f1c65d67894e5b8c2fdf0ae4500","title":"Decision Transformer: Reinforcement Learning via Sequence Modeling"},{"paperId":"ea7cfe7f2340584cbe653da6077ee7c213e49b92","title":"Swin-Unet: Unet-like Pure Transformer for Medical Image Segmentation"},{"paperId":"3d5b29a3e6f05c6030366e5395918469543d0a7e","title":"MG-BERT: leveraging unsupervised atomic representation learning for molecular property prediction"},{"paperId":"e87c4248e031a846e6a4486b818a37a1b2233b36","title":"An effective self-supervised framework for learning expressive molecular global representations to drug discovery"},{"paperId":"27ade5114d61e26efdc4d35acc83a234ade3bd94","title":"ADMETlab 2.0: an integrated online platform for accurate and comprehensive predictions of ADMET properties"},{"paperId":"0d5406775fab3e71848908327fb5504df5f60f92","title":"ImageNet-21K Pretraining for the Masses"},{"paperId":"7a16d9b4e04300d034502dc7dd58428714594e2c","title":"Carbon Emissions and Large Neural Network Training"},{"paperId":"8f8f73f0f208302546c825ed474432389ed63be4","title":"EfficientNetV2: Smaller Models and Faster Training"},{"paperId":"fb37f2259f63bb0bffd6a001cdf0168b928dc488","title":"Federated deep learning for detecting COVID-19 lung abnormalities in CT: a privacy-preserving multinational validation study"},{"paperId":"610b302950a19acef1c45456111dcd495f638c18","title":"ConViT: improving vision transformers with soft convolutional inductive biases"},{"paperId":"7519a1e9e7371df79bd8a21cee871feb0ec597a5","title":"UNETR: Transformers for 3D Medical Image Segmentation"},{"paperId":"8356d155d730e374f4db6dfd03d19a7b66c348a8","title":"CoTr: Efficiently Bridging CNN and Transformer for 3D Medical Image Segmentation"},{"paperId":"0f8aa47ff8c6c49a347e192debe20ce4e5a4caea","title":"Self-supervised Pretraining of Visual Features in the Wild"},{"paperId":"98e565fa06f6c7bf7c46833b5106b26dc45130c4","title":"WIT: Wikipedia-based Image Text Dataset for Multimodal Multilingual Machine Learning"},{"paperId":"0ae67202f0584afccefa770865d14a46655d2975","title":"Transformer in Transformer"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"2cd605106b88c85d7d8b865b1ef0f8c8293debf1","title":"Zero-Shot Text-to-Image Generation"},{"paperId":"1e5e8106700c8dbdfa036a5a9be5e61e06c0ed02","title":"Medical Transformer: Gated Axial-Attention for Medical Image Segmentation"},{"paperId":"394be105b87e9bfe72c20efe6338de10604e1a11","title":"Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts"},{"paperId":"20db2a2fadcf563a2d522aabc440b6b4f3ee46f4","title":"TransFuse: Fusing Transformers and CNNs for Medical Image Segmentation"},{"paperId":"deee48c5e0ac0407a1e002905caaf2b174bdb0e6","title":"MSA Transformer"},{"paperId":"141a5033d9994242b18bb3b217e79582f1ee9306","title":"Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision"},{"paperId":"24b8a0b02bcb7934967757fc59d273a71ba67e30","title":"TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation"},{"paperId":"197ec03481b5e845fb4d34dd99a4b8e844fdabcc","title":"Big Self-Supervised Models Advance Medical Image Classification"},{"paperId":"e52bb04d8ecad05d0cde8a57af94478c0b904712","title":"Learn molecular representations from large-scale unlabeled molecules for drug discovery"},{"paperId":"df7d26339adf4eb0c07160947b9d2973c24911ba","title":"Extracting Training Data from Large Language Models"},{"paperId":"3dfd924ad26e737db805ed29af61cc827e876bd9","title":"IPN-V2 and OCTA-500: Methodology and Dataset for Retinal Image Segmentation"},{"paperId":"6f6f73e69ee0d9d5d7d088bb882db1851d98175a","title":"Pre-Trained Image Processing Transformer"},{"paperId":"e4c5e81e6e337bb94af3eb719df5f029b40434fa","title":"Molecular representation learning with language models and domain-relevant auxiliary tasks"},{"paperId":"9f7626c7af925b7b69f1ba86ceb916d21bc03dbe","title":"Pfam: The protein families database in 2021"},{"paperId":"8c2b647a4ddd5d490ada2ed21e28aed460640926","title":"ZINC20 - A Free Ultralarge-Scale Chemical Database for Ligand Discovery"},{"paperId":"06a1958dbd5c94cdb1cfb164d57bf89e4a5dd788","title":"RNAcentral 2021: secondary structure integration, improved sequence search and new member databases"},{"paperId":"8c56b85571933e7520bdd07ed3946126005a4f10","title":"DrugSpaceX: a large screenable and synthetically tractable database extending drug space"},{"paperId":"268d347e8a55b5eb82fb5e7d2f800e33c75ab18a","title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"},{"paperId":"6d6595766a35f12a6ad671d05634b5e2159d4f3e","title":"Bio-Megatron: Larger Biomedical Domain Language Model"},{"paperId":"223225d2828dcebf6b300622b6734f2397eaa0ed","title":"MoCo Pretraining Improves Representation and Transferability of Chest X-ray Models"},{"paperId":"6dd9f99cecd38504b667d320eb2a6267a9fee35d","title":"Contrastive Learning of Medical Visual Representations from Paired Images and Text"},{"paperId":"df70f6a46e6cf21a1c83db9fc9a4e16129393fee","title":"CMNPD: a comprehensive marine natural products database towards facilitating drug discovery from the ocean"},{"paperId":"399e7d8129c60818ee208f236c8dda17e876d21f","title":"RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models"},{"paperId":"b8fb507b780e6fd62ab9a5983155fc5ed791cda9","title":"UFold: fast and accurate RNA secondary structure prediction with deep learning"},{"paperId":"65906e6027246ae9e4ecd18d6e019a24505c842e","title":"Aligning AI With Shared Human Values"},{"paperId":"a2f38d03fd363e920494ad65a5f0ad8bd18cd60b","title":"Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing"},{"paperId":"1ce7901245879ef2bf371c4339507d2ea666eb2a","title":"Comparing to Learn: Surpassing ImageNet Pretraining on Radiographs By Comparing Image Representations"},{"paperId":"bc022dbb37b1bbf3905a7404d19c03ccbf6b81a8","title":"Generative Pretraining From Pixels"},{"paperId":"ca9b4fc03ad3ea4680ab2204ecf215f333c616a4","title":"ProtTrans: Towards Cracking the Language of Life’s Code Through Self-Supervised Deep Learning and High Performance Computing"},{"paperId":"a9a4e8e631890a14257539948e1813b5214c60dd","title":"Self-Supervised Graph Transformer on Large-Scale Molecular Data"},{"paperId":"aa64d955454464ef5d921cc9df6682ff4921b2e3","title":"Sex and gender differences and biases in artificial intelligence for biomedicine and healthcare"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"5d4de0fa45aeddc31142e6a24666d06ed7923f1e","title":"Med-BERT: pretrained contextualized embeddings on large-scale structured electronic health records for disease prediction"},{"paperId":"39262e9ef11c77e0a7ebaffe8bc7afd2945b75bb","title":"A new paradigm for drug development"},{"paperId":"3eb85dbadeb5074325a404304313bed536fb6157","title":"MolTrans: Molecular Interaction Transformer for drug–target interaction prediction"},{"paperId":"e816f788767eec6a8ef0ea9eddd0e902435d4271","title":"Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks"},{"paperId":"8d4b4d9ecb25b84528d73b1495928f4295902cdf","title":"Autonomous task planning and situation awareness in robotic surgery*"},{"paperId":"2709167f1c3a03fa5b970a665ea48ed243aab582","title":"Designing Network Design Spaces"},{"paperId":"5603fcb32aaae2f8d1597ccfe6009c14f54a7548","title":"COVID-Net: a tailored deep convolutional neural network design for detection of COVID-19 cases from chest X-ray images"},{"paperId":"e2bda1a9c0c5263b0812a9227460db6b710c9fac","title":"Tracking Social Media Discourse About the COVID-19 Pandemic: Development of a Public Coronavirus Twitter Data Set"},{"paperId":"c5f7074a264356c9a022a8dff24df79d1db8c3d3","title":"ProGen: Language Modeling for Protein Generation"},{"paperId":"34733eaf66007516347a40ad5d9bbe1cc9dacb6b","title":"A Simple Framework for Contrastive Learning of Visual Representations"},{"paperId":"bc51622358d8eea83248ef29402fe10640d07ba6","title":"Big Transfer (BiT): General Visual Representation Learning"},{"paperId":"110930cd8e47f6cc801c38523ea100e276795882","title":"Dynamics of the double burden of malnutrition and the changing nutrition reality"},{"paperId":"4925d50efc261cd9f251a1106f109f98ed2eb85a","title":"Transformer neural network for protein-specific de novo drug generation as a machine translation problem"},{"paperId":"d1f407b16fb8d99487baee37ed0805676c58e7ac","title":"MIMIC-CXR, a de-identified publicly available database of chest radiographs with free-text reports"},{"paperId":"add2f205338d70e10ce5e686df4a690e2851bdfc","title":"Momentum Contrast for Unsupervised Visual Representation Learning"},{"paperId":"6289471f2eca01dbde71e4832f93891f54b91cfe","title":"SMILES Transformer: Pre-trained Molecular Fingerprint for Low Data Drug Discovery"},{"paperId":"4e40cdf1e44c820096865da63180ddaf583e1ed5","title":"Dissecting racial bias in an algorithm used to manage the health of populations"},{"paperId":"6c4b76232bb72897685d19b3d264c6ee3005bc2b","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"3d99747cc3e13d22f21e02c35e82b57d2e351e2a","title":"SMILES-BERT: Large Scale Unsupervised Pre-Training for Molecular Property Prediction"},{"paperId":"222baa4e9e7ce691fdfddbc826a70e027daed70d","title":"Reinforcement Learning in Healthcare: A Survey"},{"paperId":"75dd6e09e2a44c363bb50e463c812fd3393cbf1c","title":"Models Genesis: Generic Autodidactic Models for 3D Medical Image Analysis"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"57633ff5c6f0708be25e651f51eef29d2fbfe48b","title":"BEHRT: Transformer for Electronic Health Records"},{"paperId":"97b446e2ec3b402ea104421dab5eb4b99a21e42a","title":"Understanding artificial intelligence ethics and safety"},{"paperId":"6447deb18d6d2510c147afcf1b04408250f0d7c6","title":"A Model to Search for Synthesizable Molecules"},{"paperId":"4f2eda8077dc7a69bb2b4e0a1a086cf054adb3f9","title":"EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks"},{"paperId":"18a93dc1558bf9d7534d0b416633cebaf75c1145","title":"Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences"},{"paperId":"2a567ebd78939d0861d788f0fedff8d40ae62bf2","title":"Publicly Available Clinical BERT Embeddings"},{"paperId":"5bcda431e0b615e094562bf038f1ef4df1865088","title":"Med3D: Transfer Learning for 3D Medical Image Analysis"},{"paperId":"1ab7f7c1d328589f25c79515b9a5d824d7ffbbd1","title":"GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering"},{"paperId":"d9ad92812d61709a9bf35b09078361d8bffd3f7a","title":"Autonomous Tissue Manipulation via Surgical Robot Using Learning Based Model Predictive Control"},{"paperId":"1e43c7084bdcb6b3102afaf301cce10faead2702","title":"BioBERT: a pre-trained biomedical language representation model for biomedical text mining"},{"paperId":"f1a17b7c4cae4513731f6d81b433e338cf4114eb","title":"PadChest: A large chest x-ray image dataset with multi-label annotated reports"},{"paperId":"89a816719613e220a64ab2590c938c23bbfe187e","title":"CheXpert: A Large Chest Radiograph Dataset with Uncertainty Labels and Expert Comparison"},{"paperId":"fa795716fb68ccbe52a8479c4d28da340d932222","title":"PanglaoDB: a web server for exploration of mouse and human single-cell RNA sequencing data"},{"paperId":"d79a26226393f687ddbc375e32055b40b8ad8d38","title":"GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism"},{"paperId":"64d025132b34770bfa43d16c1e36662af687607d","title":"DeepConv-DTI: Prediction of drug-target interactions via deep learning with convolution on protein sequences"},{"paperId":"92a79ba7bb783b64069ca006206b05458a696b36","title":"PubChem 2019 update: improved access to chemical data"},{"paperId":"a364a2b6bb97fcb4207468e6500cf3042bbf8d07","title":"Speech Recognition"},{"paperId":"b4df354db88a70183a64dbc9e56cf14e7669a6c0","title":"Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning"},{"paperId":"98a68e8148d380120924d5c2810e3e0f513a75b2","title":"DeepLesion: automated mining of large-scale lesion annotations and universal lesion detection with deep learning"},{"paperId":"7a68063b403e804f015550a6763a0cdc656a87da","title":"Observed Antibody Space: A Resource for Data Mining Next-Generation Sequencing of Antibody Repertoires"},{"paperId":"0f885fd46064d271d4404cf9bb3d758e1a6f8d55","title":"Exploring the Limits of Weakly Supervised Pretraining"},{"paperId":"1c0285923920d64c2744f0577f627019bf426fd2","title":"The HAM10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions"},{"paperId":"adfc508b9b3d4fc3903aa383a290dc68fb8bbe5a","title":"Implementing Machine Learning in Health Care - Addressing Ethical Challenges."},{"paperId":"d667b71d056ed1c407d456a31982b45f63dbf264","title":"Where is crystallography going?"},{"paperId":"784ed2daacbc7d721fe599e71ef48ab733700716","title":"MURA: Large Dataset for Abnormality Detection in Musculoskeletal Radiographs."},{"paperId":"c3e4b8dcdd4821bcf484ffc29930b6016e863196","title":"Humans forget, machines remember: Artificial intelligence and the Right to Be Forgotten"},{"paperId":"d0611891b9e8a7c5731146097b6f201578f47b2f","title":"Learning Transferable Architectures for Scalable Image Recognition"},{"paperId":"dce6f9d4017b1785979e7520fd0834ef8cf02f4b","title":"Proximal Policy Optimization Algorithms"},{"paperId":"35f7b928a5ed86b3a480a71846c3dfb19f3104fd","title":"Robot Autonomy for Surgery"},{"paperId":"8760bc7631c0cb04e7138254e9fd6451b7def8ca","title":"Revisiting Unreasonable Effectiveness of Data in Deep Learning Era"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd","title":"Deep Reinforcement Learning from Human Preferences"},{"paperId":"2ca9bd07ed18aadf5c532d0bf3cce25149260a89","title":"Medical robotics—Regulatory, ethical, and legal considerations for increasing levels of autonomy"},{"paperId":"3658413fe0008d7e4a4decaabcac0a3033f56d43","title":"Clustering huge protein sequence sets in linear time"},{"paperId":"5c45a5d05ac564adb67811eeb9d41d6460c70135","title":"Development and Validation of a Deep Learning Algorithm for Detection of Diabetic Retinopathy in Retinal Fundus Photographs."},{"paperId":"7e232313a59d735ef7c8a9f4cc7bc980a29deb5e","title":"Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering"},{"paperId":"f6e0856b4a9199fa968ac00da612a9407b5cb85c","title":"Aggregated Residual Transformations for Deep Neural Networks"},{"paperId":"f0dcc9aa31dc9b31b836bcac1b140c8c94a2982d","title":"Membership Inference Attacks Against Machine Learning Models"},{"paperId":"95cd83603a0d2b6918a8e34a5637a8f382da96f5","title":"MIMIC-III, a freely accessible critical care database"},{"paperId":"77f0a39b8e02686fd85b01971f8feb7f60971f80","title":"Identity Mappings in Deep Residual Networks"},{"paperId":"afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d","title":"Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations"},{"paperId":"2c03df8b48bf3fa39054345bafabfeff15bfd11d","title":"Deep Residual Learning for Image Recognition"},{"paperId":"def584565d05d6a8ba94de6621adab9e301d375d","title":"Visual7W: Grounded Question Answering in Images"},{"paperId":"3b26ba39bcc122f2a7c60d41f8cd40e2024d9bb2","title":"Implicit Racial/Ethnic Bias Among Health Care Professionals and Its Influence on Health Care Outcomes: A Systematic Review."},{"paperId":"c657ab339517fb8def7ce7f83bb81e746d558218","title":"Data Resource Profile: Clinical Practice Research Datalink (CPRD)"},{"paperId":"6364fdaa0a0eccd823a779fcdd489173f938e91a","title":"U-Net: Convolutional Networks for Biomedical Image Segmentation"},{"paperId":"696ca58d93f6404fea0fc75c62d1d7b378f47628","title":"Microsoft COCO Captions: Data Collection and Evaluation Server"},{"paperId":"2dcef55a07f8607a819c21fe84131ea269cc2e3c","title":"Deep Unsupervised Learning using Nonequilibrium Thermodynamics"},{"paperId":"449532187c94af3dd3aa55e16d2c50f7854d2199","title":"Trust Region Policy Optimization"},{"paperId":"d6f2f611da110b5b5061731be3fc4c7f45d8ee23","title":"Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification"},{"paperId":"795dc87b4727b303d3672539e4578d41dfd3aeb3","title":"UniRef clusters: a comprehensive and scalable alternative for improving sequence similarity searches"},{"paperId":"081651b38ff7533550a3adfc1c00da333a8fe86c","title":"How transferable are features in deep neural networks?"},{"paperId":"e15cf50aa89fee8535703b9f9512fca5bfc43327","title":"Going deeper with convolutions"},{"paperId":"eb42cf88027de515750f230b23b1a057dc782108","title":"Very Deep Convolutional Networks for Large-Scale Image Recognition"},{"paperId":"44040913380206991b1991daf1192942e038fe31","title":"From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions"},{"paperId":"251c39c1b9372f3055cd53d0001fc122bfb2e418","title":"The Cancer Imaging Archive (TCIA): Maintaining and Operating a Public Information Repository"},{"paperId":"2df9a391c1a8124be63cd2bc71624b80d4e6af1f","title":"UniChem: a unified chemical structure cross-referencing and identifier tracking system"},{"paperId":"abd1c342495432171beb7ca8fd9551ef13cbd0ff","title":"ImageNet classification with deep convolutional neural networks"},{"paperId":"8e080b98efbe65c02a116439205ca2344b9f7cd4","title":"Im2Text: Describing Images Using 1 Million Captioned Photographs"},{"paperId":"a8db50edfe26a6ae33a6787e2049de5bacd18666","title":"ChEMBL: a large-scale bioactivity database for drug discovery"},{"paperId":"d2c733e34d48784a37d717fe43d9e93277a8c53e","title":"ImageNet: A large-scale hierarchical image database"},{"paperId":"3d9fbcf35f53bd84c75fd99daa6b2c69397b0a01","title":"The Universal Protein Resource (UniProt)"},{"paperId":"0784e90fd935a5e81135f80b35daa8d0d0a562c9","title":"The way to NMR structures of proteins"},{"paperId":"04c08cc2f621cdc812769f15366a494c493b8fe6","title":"Frank"},{"paperId":"df239785e6d26a45e9c8e06551cfecba92d1ecad","title":"Exploring AI Ethics of ChatGPT: A Diagnostic Analysis"},{"paperId":"ae7f67c705c12391f7198527a0b962340ac8d39c","title":"ChatAug: Leveraging ChatGPT for Text Data Augmentation"},{"paperId":"11f42721f56f36a64638677ccde7784040829656","title":"Uni-Mol: A Universal 3D Molecular Representation Learning Framework"},{"paperId":"8fdd34153d1035d09dd4a6efa9cb0c91d23d0045","title":"More than you've asked for: A Comprehensive Analysis of Novel Prompt Injection Threats to Application-Integrated Large Language Models"},{"paperId":"440d08df0bcfe1e67fd68208d9203e3120f4bc8e","title":"Investigating the Existence of \"Secret Language\" in Language Models"},{"paperId":"9904557ddf52402170da288cf3a88a4f28847f0d","title":"Peer Review File Manuscript Title: Accurate medium-range global weather forecasting with 3D neural networks Reviewer Comments & Author Rebuttals"},{"paperId":null,"title":"Be my eyes"},{"paperId":"08602d1fd5b11a82910ec781f8bfef4f7daba55b","title":"Improve the Protein Complex Prediction with Protein 1 Language Models"},{"paperId":"da2bb505ba1fcfa8877c008309bbed49be1332a5","title":"Combined Scaling for Open-Vocabulary Image Classiﬁcation"},{"paperId":"775f42ed458b8c5b0f2094ea4ff5b64c557b1a34","title":"A Path Towards Autonomous Machine Intelligence Version 0.9.2, 2022-06-27"},{"paperId":null,"title":"Focus on Afﬁnity Prediction Problems with Noise Annotations"},{"paperId":null,"title":"Language models of protein sequences at the scale of evolution enable accurate structure prediction"},{"paperId":null,"title":"2 2 A pr 2 02 1 ImageNet-21 K Pretraining for the Masses"},{"paperId":"c8b25fab5608c3e033d34b4483ec47e68ba109b7","title":"Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"},{"paperId":"ee6564e2e0f47c6ac131b093f057ca75907958c9","title":"BioELECTRA:Pretrained Biomedical text Encoder using Discriminators"},{"paperId":null,"title":"“A mathematical framework for transformer circuits,”"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","title":"Improving Language Understanding by Generative Pre-Training"},{"paperId":null,"title":"Nih chest x-rays"},{"paperId":null,"title":"YFCC100M: the new data in multimedia research"},{"paperId":"18567e254d078ba4d03b1ba81a741f1b1405d369","title":"How cryo-EM is revolutionizing structural biology."},{"paperId":"955fe5b0eb5e9fefab7bf21dede20e1285dca410","title":"UniProt: the Universal Protein knowledgebase"},{"paperId":"5eb0fdb55eab1efd5ea9a548f2e9f5cd9642c698","title":"PubMed Central."},{"paperId":"a0b37ef9a6212ced7a5bfd7d31bb531323e5a7aa","title":"GraphDTA: Predicting drug–target binding aﬃnity with graph neural networks"},{"paperId":null,"title":"“Fairway health - process prior authorization faster,”"},{"paperId":null,"title":"“How your data is used to improve model performance,”"},{"paperId":null,"title":"education; 6)"},{"paperId":null,"title":"“March 20 chatGPT outage: Here’s what happened,”"},{"paperId":null,"title":"T HE introduction of ChatGPT [1] has triggered a new wave of development and deployment of Large Models (LAMs) recently"},{"paperId":null,"title":"2023, accessed on Month Day, Year"},{"paperId":null,"title":"Large Vision Model (LVM): LVMs are pre-trained on vision data and applied to vision downstream tasks"},{"paperId":null,"title":"The AI Revolution in Medicine GPT-4 and Beyond"},{"paperId":null,"title":"with the Department of Engineering Science"},{"paperId":null,"title":"Ruiyang"},{"paperId":null,"title":"Lin Li is with the Department of Informatics, King’s College London, U.K."},{"paperId":null,"title":"“Bionemo,” 2023"},{"paperId":null,"title":"Large Audio Model (LAudiM)"},{"paperId":null,"title":"“ChatGPT: Optimizing language models for dialogue,”"},{"paperId":null,"title":"are with the Department of Biomedical Engineering, The Chinese University of Hong Kong, Hong Kong SAR"},{"paperId":null,"title":"Improving antibody structure prediction without multiple sequence alignments"},{"paperId":null,"title":"The national library of medicine presents medpix"}],"id":"994e08ac813028601907516aee9c4699234a6b4d","summary":"An up-to-date comprehensive review of large AI models, from background to their applications, is presented, including seven key sectors that largeAI models are applicable and might have substantial influence, including 1) molecular biology and drug discovery; 2) medical diagnosis and decision-making; 3) medical imaging and vision; 4) medical informatics; 5) medical education; 6) public health; and 7) medical robotics."},{"url":"https://www.semanticscholar.org/paper/6e754273d54a91371efbc928cd6b156364d517da","title":"ViperGPT: Visual Inference via Python Execution for Reasoning","venue":"arXiv.org","year":2023,"referenceCount":79,"citationCount":161,"influentialCitationCount":1,"publicationDate":"14/03/2023","authors":"D'idac Sur'is,Sachit Menon,Carl Vondrick","citations":[{"paperId":"5e7274bcda47b704b6797bb14be8b7a61c047a61","title":"Uncertainty-Aware Evaluation for Vision-Language Models"},{"paperId":"593908583968815003049398de77585a99038fb7","title":"DeiSAM: Segment Anything with Deictic Prompting"},{"paperId":"4315094bec44f310d662b349aff6305b3dde9a07","title":"EVEDIT: Event-based Knowledge Editing with Deductive Editing Boundaries"},{"paperId":"3871fe3b83090bebf3eba40a3afa4e0b66a3f165","title":"Question-Instructed Visual Descriptions for Zero-Shot Video Question Answering"},{"paperId":"83d201d503b863fec7d1225f00a141e722e03f17","title":"Using Left and Right Brains Together: Towards Vision and Language Planning"},{"paperId":"339f775915160c1293e02579f8226a227f270cbb","title":"AgentLens: Visual Analysis for Agent Behaviors in LLM-based Autonomous Systems"},{"paperId":"f86f71cfd2e9682a56d7334736a7b8a0b1c70b45","title":"Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models"},{"paperId":"637b28b5c8283b2118b7bc0aabae6f4795bbee67","title":"ContPhy: Continuum Physical Concept Learning and Reasoning from Videos"},{"paperId":"da68013fd293594ad21eb1c3d838d06e691a556b","title":"The Role of Foundation Models in Neuro-Symbolic Learning and Reasoning"},{"paperId":"78fbb6e7a1c568a04e8c935aa9909d0c942ea5f6","title":"Executable Code Actions Elicit Better LLM Agents"},{"paperId":"3a1910dc9212cc78e62d29a44e011d14e2516edc","title":"A Survey on Visual Anomaly Detection: Challenge, Approach, and Prospect"},{"paperId":"94701d9c6cfc1aecc174ff62ccda939f790c1710","title":"ReGAL: Refactoring Programs to Discover Generalizable Abstractions"},{"paperId":"1bdbb7b4d657a2fd3e9f64fc689a14a8c94ebfaf","title":"ChatterBox: Multi-round Multimodal Referring and Grounding"},{"paperId":"a050c9b0c321839e4427ab9defa3463be7825ac4","title":"MM-LLMs: Recent Advances in MultiModal Large Language Models"},{"paperId":"7e6c1bb54bb2e36cc1092b080e9928942f7f8a68","title":"TroVE: Inducing Verifiable and Efficient Toolboxes for Solving Programmatic Tasks"},{"paperId":"c00a86523bcacf4db91d9970e1957e78152cb103","title":"Zero Shot Open-ended Video Inference"},{"paperId":"140cfda71bfff852c3e205b7ad61854b78c76982","title":"Mastering Text-to-Image Diffusion: Recaptioning, Planning, and Generating with Multimodal LLMs"},{"paperId":"23957040943f883542f47850c709b9e7f9d6fa55","title":"Prompting Large Vision-Language Models for Compositional Reasoning"},{"paperId":"c2b0291ffedf70f5cc66425e49837cba49aaf8e9","title":"PhotoScout: Synthesis-Powered Multi-Modal Image Search"},{"paperId":"4f2a56102bcbf0fe79379c4c27daecbccfb35a26","title":"MLLM-Tool: A Multimodal Large Language Model For Tool Agent Learning"},{"paperId":"d4b1a1c62a03ccffcf24983eb4fe22335cbb89b6","title":"DiffusionGPT: LLM-Driven Text-to-Image Generation System"},{"paperId":"0a8a776054a087118f4f9523994ef084b2b2469a","title":"Advancing Large Multi-modal Models with Explicit Chain-of-Reasoning and Visual Question Generation"},{"paperId":"bf08ad906baad71283aa538532cf6e7ff12db352","title":"LangProp: A code optimization framework using Language Models applied to driving"},{"paperId":"4a48d628e53f554eb6ef09a457ca855188b96171","title":"DoraemonGPT: Toward Understanding Dynamic Scenes with Large Language Models"},{"paperId":"5502d769595981009e43344f8914e287acca2359","title":"ModaVerse: Efficiently Transforming Modalities with LLMs"},{"paperId":"fc7feeaddc5a38c0d6f0d793737584e5f0bb7519","title":"Towards Truly Zero-shot Compositional Visual Reasoning with LLMs as Programmers"},{"paperId":"a06d3e9e90008c64c45a0029d580541d5f646771","title":"If LLM Is the Wizard, Then Code Is the Wand: A Survey on How Code Empowers Large Language Models to Serve as Intelligent Agents"},{"paperId":"4641fe56cd44144b6cabea583233ed952f97f4c0","title":"A Simple LLM Framework for Long-Range Video Question-Answering"},{"paperId":"86e3e63e83011aecd4a59999c632d44f117efbc5","title":"WildCLIP: Scene and animal attribute retrieval from camera trap data with domain-adapted vision-language models"},{"paperId":"6a33e58ef961a3a0a5657518b2be86395eb7c8d0","title":"InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks"},{"paperId":"24fc9ad715372358bd0108eeb7c944b915963293","title":"ASSISTGUI: Task-Oriented Desktop Graphical User Interface Automation"},{"paperId":"35a17f896847614a71df772bbe2b66ae231cabc7","title":"CLOVA: A Closed-Loop Visual Assistant with Tool Usage and Update"},{"paperId":"dc1fd73926a3ea0751019ca6b027fa1042d4dcda","title":"Designing LLM Chains by Adapting Techniques from Crowdsourcing Workflows"},{"paperId":"7d24fd1248fac1ff022b436667b78e42919c3ba6","title":"InstructPipe: Building Visual Programming Pipelines with Human Instructions"},{"paperId":"6d2ab31aa75468f5458b9d96192c3f4a28f55d73","title":"DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving"},{"paperId":"3f622f71276ffdeca771f0f3758f3974d3a18f28","title":"Remote Sensing Vision-Language Foundation Models without Annotations via Ground Remote Alignment"},{"paperId":"33c3dbf86dd6e338e2a49bba9c2e2193d1eca08a","title":"Vista-LLaMA: Reliable Video Narrator via Equal Distance to Visual Tokens"},{"paperId":"3a56bc074b8f3f985599627404b70e16fc5bce1b","title":"Chain of Code: Reasoning with a Language Model-Augmented Code Emulator"},{"paperId":"1894b5729cb837cf1d9e17158c9dd2a5541512ff","title":"AVA: Towards Autonomous Visualization Agents through Visual Perception-Driven Decision-Making"},{"paperId":"10578bc0bdb3ebf9232931dd4961f55ba470caad","title":"LaMPilot: An Open Benchmark Dataset for Autonomous Driving with Language Model Programs"},{"paperId":"f32ea390686b1eee3ba5b53c7a85e9e9385d4b94","title":"Visual Program Distillation: Distilling Tools and Programmatic Reasoning into Vision-Language Models"},{"paperId":"0d8da8431bee9b2c2d40b605a754c5c840833323","title":"Recursive Visual Programming"},{"paperId":"cc9627c4475b9ad1c7e4105f3bceb1e7b59b7cab","title":"Zero-Shot Video Question Answering with Procedural Programs"},{"paperId":"246017780386eba39d6cda760a1c2c70356baa50","title":"VIoTGPT: Learning to Schedule Vision Tools towards Intelligent Video Internet of Things"},{"paperId":"70ea292c2312afdbe474e3c095dfa620b2cd46cc","title":"Video Summarization: Towards Entity-Aware Captions"},{"paperId":"d4ba81bda42b408cd6d48205a593826060efc1ed","title":"Evaluating VLMs for Score-Based, Multi-Probe Annotation of 3D Objects"},{"paperId":"7450c20d844ca05e643ece2461ff7aa2f381e22a","title":"Towards Top-Down Reasoning: An Explainable Multi-Agent Approach for Visual Question Answering"},{"paperId":"5eea245cc12c55905d4df827d0c9776c5ddfa743","title":"Compositional Chain-of-Thought Prompting for Large Multimodal Models"},{"paperId":"7b0a186b0140ee91fb13991c9c7187f3dc3b0670","title":"Visual Programming for Zero-shot Open-Vocabulary 3D Visual Grounding"},{"paperId":"033607afc2e08a58383ad78deb98c844017109c1","title":"ViStruct: Visual Structural Knowledge Extraction via Curriculum Guided Code-Vision Representation"},{"paperId":"1e838bd5fa2f5bca805493d0f672d03514b36869","title":"Vamos: Versatile Action Models for Video Understanding"},{"paperId":"7f807249c0ef0fe07d5e9c810684cd5daba0edc5","title":"De-fine: Decomposing and Refining Visual Programs with Auto-Feedback"},{"paperId":"ab8169d6e4dfabfe7c30ebec1bb871bf3e1551cd","title":"GAIA: a benchmark for General AI Assistants"},{"paperId":"6fa0677731184444df0e1fc8070938419cd6da47","title":"Igniting Language Intelligence: The Hitchhiker's Guide From Chain-of-Thought Reasoning to Language Agents"},{"paperId":"107fb6eec2febbae12db29bf3e311aaf5680027c","title":"Video-LLaVA: Learning United Visual Representation by Alignment Before Projection"},{"paperId":"aad3d2e690f6c73f04a14622ceff51464bbc560e","title":"Chat-UniVi: Unified Visual Representation Empowers Large Language Models with Image and Video Understanding"},{"paperId":"2fb605f67fee79cad94952ddfe0f686e926f49f5","title":"GPT-4V in Wonderland: Large Multimodal Models for Zero-Shot Smartphone GUI Navigation"},{"paperId":"6a28a2c2282c23e173cb3feefea27566eb2c6377","title":"Past as a Guide: Leveraging Retrospective Learning for Python Code Completion"},{"paperId":"ef321c6f174ac59916ac54ec40ad18bca5b58e5c","title":"PerceptionGPT: Effectively Fusing Visual Perception into LLM"},{"paperId":"cf7d69709bdeddd561c183178bbc1f0c2e156a08","title":"Analyzing Modular Approaches for Visual Question Decomposition"},{"paperId":"8ec7d50250203543a0098d99f04957b22bbe2c77","title":"How to Bridge the Gap between Modalities: A Comprehensive Survey on Multimodal Large Language Model"},{"paperId":"de27e0add04612f85b96180dc6fac9c713397d9f","title":"Follow-Up Differential Descriptions: Language Models Resolve Ambiguities for Image Classification"},{"paperId":"ecfff30e570498b6c87c5d1319a0cbcdf7a8b86d","title":"LLaVA-Plus: Learning to Use Tools for Creating Multimodal Agents"},{"paperId":"adff39f3972d349afec4bc7bcbc580b670a99097","title":"GENOME: GenerativE Neuro-symbOlic visual reasoning by growing and reusing ModulEs"},{"paperId":"c62711f6b5d8620ba36bc2c378ec6ab53f6e197c","title":"RoboGen: Towards Unleashing Infinite Data for Automated Robot Learning via Generative Simulation"},{"paperId":"c020f15be1dee20f9e2e0c5a6f05f272b5508325","title":"LLaVA-Interactive: An All-in-One Demo for Image Chat, Segmentation, Generation and Editing"},{"paperId":"5b0458d5e839117884739d4b9dead0285beb4a76","title":"Symbolic Planning and Code Generation for Grounded Dialogue"},{"paperId":"3dd8a397abc6fcd7e945cea2d02a1ea3266e0bc9","title":"What's Left? Concept Grounding with Logic-Enhanced Foundation Models"},{"paperId":"8e3e7deb95d2a984cba615ec847e64f354626cdf","title":"WebWISE: Web Interface Control and Sequential Exploration with Large Language Models"},{"paperId":"29b3ce4de9dd9d784ca1d876957950f4b2d3796a","title":"Towards Perceiving Small Visual Details in Zero-shot Visual Question Answering with Multimodal LLMs"},{"paperId":"133777180e326dfa53523bf53b0a969bbdccb0ee","title":"API-Assisted Code Generation for Question Answering on Varied Table Structures"},{"paperId":"729fc01274cc26798654a318d1a95e73c61f99a3","title":"Open-Ended Instructable Embodied Agents with Memory-Augmented Large Language Models"},{"paperId":"96d104dfe727f78a35faaafe81481f3672b485ee","title":"Large Language Models are Visual Reasoning Coordinators"},{"paperId":"02a00ce9e7bce14937f46af0423eea40b7b63303","title":"Steve-Eye: Equipping LLM-based Embodied Agents with Visual Perception in Open Worlds"},{"paperId":"f90c522b284a6c065fa5126216a26a7415a2b9fa","title":"MoqaGPT : Zero-Shot Multi-modal Open-domain Question Answering with Large Language Model"},{"paperId":"ad636bf86e81f976af6cb4b95a83bd65b6443ed7","title":"Neurosymbolic Grounding for Compositional World Models"},{"paperId":"beb3e8acd816bac1a5b7fccfd073f79048877e33","title":"Frozen Transformers in Language Models Are Effective Visual Encoder Layers"},{"paperId":"4f63c5a89c7299a864c6c48aa1844fb0fe8c9437","title":"Survey of Vulnerabilities in Large Language Models Revealed by Adversarial Attacks"},{"paperId":"a710efa9247207a72f06e0c9db302fd3ecab5fbb","title":"Towards Robust Multi-Modal Reasoning via Model Selection"},{"paperId":"b6c8c1745a18d6e59c7a8a99f0df7aa4c18a1e73","title":"Octopus: Embodied Vision-Language Programmer from Environmental Feedback"},{"paperId":"1d14a708622917da4b9820ada6d32af24fc1651a","title":"Idea2Img: Iterative Self-Refinement with GPT-4V(ision) for Automatic Image Design and Generation"},{"paperId":"7f1ba5630c3baa09b11cc665b3f71cdb117e5ffb","title":"OpenLEAF: Open-Domain Interleaved Image-Text Generation and Evaluation"},{"paperId":"8147cec9245d34d13732a08e915c920a1a499bb5","title":"Lemur: Harmonizing Natural Language and Code for Language Agents"},{"paperId":"33095b1334bed852e3652bd9d7da3f4df0cdf485","title":"ViCor: Bridging Visual Understanding and Commonsense Reasoning with Large Language Models"},{"paperId":"84f9bc5f89dac53662fb467b6af8ff26415ca3e7","title":"InstructDET: Diversifying Referring Object Detection with Generalized Instructions"},{"paperId":"700bd9681f1b9e9e2212e10415d27b11c7e6836b","title":"Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models"},{"paperId":"e7b67fe319f812db6ba6789eb2d9bf8b445c1d64","title":"GRID: A Platform for General Robot Intelligence Development"},{"paperId":"20ae101289965d36dbd93e9b8c47ec9deab03ed0","title":"What If the TV Was Off? Examining Counterfactual Reasoning Abilities of Multi-modal Language Models"},{"paperId":"092245d86b77181c36f972b1b7a17a59cd989c4a","title":"Guiding Instruction-based Image Editing via Multimodal Large Language Models"},{"paperId":"a1426b13b74dbad17b34606d25aabe1d61f6e11a","title":"CRAFT: Customizing LLMs by Creating and Retrieving from Specialized Toolsets"},{"paperId":"b7e0ea06f096d4963d96739ce154ba6bfe5af7ee","title":"Compositional Sculpting of Iterative Generative Processes"},{"paperId":"16753e0317730e8c1b297338300a8c6163dd06f2","title":"VideoDirectorGPT: Consistent Multi-scene Video Generation via LLM-Guided Planning"},{"paperId":"1ef9048e0465e1855b4b87c0630e85688e903b57","title":"Man vs the machine in the struggle for effective text anonymisation in the age of large language models"},{"paperId":"7b689adb8c156d6158660f90d1c86888ee281f63","title":"DreamLLM: Synergistic Multimodal Comprehension and Creation"},{"paperId":"f34b6b4f75fb4e6f2c5654ee13fb2479c6170b3a","title":"Kosmos-2.5: A Multimodal Literate Model"},{"paperId":"3ec464696db25acc2c39a6d967ec3df09e06f633","title":"Multimodal Multi-Hop Question Answering Through a Conversation Between Tools and Efficiently Finetuned Large Language Models"},{"paperId":"4eb87eaa193929dbef93fa2db9419245a8e8916f","title":"TextBind: Multi-turn Interleaved Multimodal Instruction-following in the Wild"},{"paperId":"4cf527e9e0d68e3fc16d39fbcdb3869cd3ccf60f","title":"Hypothesis Search: Inductive Reasoning with Language Models"},{"paperId":"67c09c9f93aa91f2419f2b348e1545bc2c115e71","title":"Gesture-Informed Robot Assistance via Foundation Models"},{"paperId":"6bcc6ab9c28805d4067e99b2cdc7524550fe80e1","title":"PointLLM: Empowering Large Language Models to Understand Point Clouds"},{"paperId":"894ed1aba8e42a4ec27ba53ecde383b14c5128ca","title":"Examining User-Friendly and Open-Sourced Large GPT Models: A Survey on Language, Multimodal, and Scientific GPT Models"},{"paperId":"28c6ac721f54544162865f41c5692e70d61bccab","title":"A Survey on Large Language Model based Autonomous Agents"},{"paperId":"451a657dabf80ebc43f6a3be518250b2cd5dfe1a","title":"Through the Lens of Core Competency: Survey on Evaluation of Large Language Models"},{"paperId":"d6c2523ab97416c2692cbbeab082ed1790e8e55e","title":"VisIT-Bench: A Benchmark for Vision-Language Instruction Following Inspired by Real-World Use"},{"paperId":"a6c57acfbcec39af366d33db18cff1cb8803b27a","title":"Dynamic Planning with a LLM"},{"paperId":"dd0612ce863f64b0f69d0d9f708c52e829f6f859","title":"TPTU: Large Language Model-based AI Agents for Task Planning and Tool Usage"},{"paperId":"446fb5dead075a1a08862662738f462e9a0e91c8","title":"Tool Documentation Enables Zero-Shot Tool-Usage with Large Language Models"},{"paperId":"6024f320e0a5b9b8fc29b86903aa9a96956b26dd","title":"AntGPT: Can Large Language Models Help Long-term Action Anticipation from Videos?"},{"paperId":"aa7bcd1f9453c9096ec78900a7b94e816ed0e1c5","title":"WavJourney: Compositional Audio Creation with Large Language Models"},{"paperId":"584ca135b61482fd89247113da87d784f738dbfa","title":"Foundational Models Defining a New Era in Vision: A Survey and Outlook"},{"paperId":"1cd8373490efc2d74c2796f4b2aa27c7d4415ec9","title":"VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models"},{"paperId":"ca31b8584b6c022ef15ddfe994fe361e002b7729","title":"A Comprehensive Overview of Large Language Models"},{"paperId":"ec592e12f45e20819afe203164bbbd0de8990510","title":"AmadeusGPT: a natural language interface for interactive animal behavioral analysis"},{"paperId":"094883e42bb9a41f602c0715c1059bc431e33fb2","title":"GPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest"},{"paperId":"03251361c1d67c6b5badffc7059fdd7fbfea1fed","title":"Statler: State-Maintaining Language Models for Embodied Reasoning"},{"paperId":"42aab468882c1efc4ea33198c2eaffd0daadf184","title":"Look, Remember and Reason: Grounded reasoning in videos with language models"},{"paperId":"efc694164312006c543ef745611348ef64e68dda","title":"Towards Language Models That Can See: Computer Vision Through the LENS of Natural Language"},{"paperId":"8efc20988021ce3b4b05dd44b13e27260ee9b99b","title":"Investigating Prompting Techniques for Zero- and Few-Shot Visual Question Answering"},{"paperId":"b937b5ad3c1ebe6007e744fa7864ec095e0070ab","title":"Tell Me Where to Go: A Composable Framework for Context-Aware Embodied Robot Navigation"},{"paperId":"966852963a88a28786b798c91b6662d6e501e590","title":"AssistGPT: A General Multi-modal Assistant that can Plan, Execute, Inspect, and Learn"},{"paperId":"051549d8ef56937b2f4d113afdcf8c7586d3770b","title":"Towards AGI in Computer Vision: Lessons Learned from GPT and Large Language Models"},{"paperId":"c03313fe8a59a2b8a9871b5226fc971fccbc2ba9","title":"Toward Grounded Commonsense Reasoning"},{"paperId":"d98536f24272e258b1d399074b64284d64786099","title":"AVIS: Autonomous Visual Information Seeking with Large Language Models"},{"paperId":"473eb062612a17c965eaa62136322f0dec6b1f8e","title":"Data-Copilot: Bridging Billions of Data and Humans with Autonomous Workflow"},{"paperId":"caae5e44957dea66fc309a55925e132de0fdb456","title":"Looking Around Corners: Generative Methods in Terrain Extension"},{"paperId":"fd755dc7b5b206c17fd953db04e1c888d45b6e4e","title":"LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark"},{"paperId":"ed30969f0e4811473144ffe83c1baa6d54f02202","title":"RestGPT: Connecting Large Language Models with Real-World RESTful APIs"},{"paperId":"da061a6e0016d6b625a8e86d64a797ca8ddb92a5","title":"Modular Visual Question Answering via Code Generation"},{"paperId":"50f44ef10335d59cec145b15effae20ff22c1fdb","title":"ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory"},{"paperId":"6847b9658f287f430098199cd81bf26308da13f9","title":"Domain Specialization as the Key to Make Large Language Models Disruptive: A Comprehensive Survey"},{"paperId":"af705d648b5b16daa3dcc593bc593f2574d76c07","title":"Grammar Prompting for Domain-Specific Language Generation with Large Language Models"},{"paperId":"e45036dddb5f27d3e87d2f14a2d9e6a402e7b5b7","title":"SheetCopilot: Bringing Software Productivity to the Next Level through Large Language Models"},{"paperId":"5ff2f5212713ec424662ac3c9e4aa5a8790d40cf","title":"ANPL: Towards Natural Programming with Interactive Decomposition"},{"paperId":"7cf64070fd3d7e53d80f260c10e6bd7018d580e1","title":"IdealGPT: Iteratively Decomposing Vision and Language Reasoning via Large Language Models"},{"paperId":"9837349417e36ef5be06da0fd6c74042148bdaa2","title":"Visual Programming for Text-to-Image Generation and Evaluation"},{"paperId":"66d755730f5d08a6f4fcc5e81f24982ba389dca9","title":"LayoutGPT: Compositional Visual Planning and Generation with Large Language Models"},{"paperId":"69335077fcacbff7a7cf25697da1949e6bdfa968","title":"The Art of SOCRATIC QUESTIONING: Recursive Thinking with Large Language Models"},{"paperId":"8da9b1436212b233fc49c7daf1ba15c22874ff5a","title":"CREATOR: Tool Creation for Disentangling Abstract and Concrete Reasoning of Large Language Models"},{"paperId":"3130643a5d02f0e849d83bb1f85577a924081f36","title":"Paxion: Patching Action Knowledge in Video-Language Foundation Models"},{"paperId":"2195676f111ad492c50f4d4c96abb2bd3d72f7fc","title":"Instruct2Act: Mapping Multi-modality Instructions to Robotic Actions with Large Language Model"},{"paperId":"692bc40edf4785d88c39e0c0fe9f270541fecf8a","title":"Towards Generalist Robots: A Promising Paradigm via Generative Simulation"},{"paperId":"8badb0587fef2ffc078b0cec549eb8ec96ed3ad4","title":"Self-Chained Image-Language Model for Video Localization and Question Answering"},{"paperId":"d6d3604f369bb0415cbe814e43ca3131323b03e2","title":"Otter: A Multi-Modal Model with In-Context Instruction Tuning"},{"paperId":"570079bbdd8758dfe865097e05719313c9c1301a","title":"LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model"},{"paperId":"ca6a2bc279be5a3349a22bfd6866ed633d18734b","title":"MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models"},{"paperId":"170c97c7215f42edfb20c2248f954879e91ef86e","title":"Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models"},{"paperId":"a5036f31f0e629dc661f120b8c3b1f374d479ab8","title":"Visual Instruction Tuning"},{"paperId":"0ebc861f5478561f12941e6b48aad30574e996d8","title":"Video ChatCaptioner: Towards Enriched Spatiotemporal Descriptions"},{"paperId":"2d3905c1a92c28c056dff1225d89e4ca72ac4d8e","title":"Man vs the machine: The Struggle for Effective Text Anonymisation in the Age of Large Language Models"},{"paperId":"c7a9c7302a72301ed79a7c0696d5af2e03ad3ac4","title":"MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action"},{"paperId":"f02d56e630986997e0aea3d92bf53e0f363ce401","title":"Prismer: A Vision-Language Model with Multi-Task Experts"},{"paperId":"6845bea94b2fb17d4377b3bb2bd10f73a959f9cc","title":"Reasoning with Language Model Prompting: A Survey"},{"paperId":"3d9703a974184ca8a1f3c97e1fb8acff1a50a18d","title":"Towards Data-and Knowledge-Driven Artificial Intelligence: A Survey on Neuro-Symbolic Computing"},{"paperId":"75c08892179fc478f87d7020b5daff9fca4f3389","title":"Beyond One-Model-Fits-All: A Survey of Domain Specialization for Large Language Models"},{"paperId":"ed9943d73eb42116fe33564b5065c78b5ca0b16e","title":"RestGPT: Connecting Large Language Models with Real-World Applications via RESTful APIs"},{"paperId":"a3711dbf296b5ddd97ba93826660cd3995611625","title":"Towards A Foundation Model for Generalist Robots: Diverse Skill Learning at Scale via Automated Task and Scene Generation"},{"paperId":"13b5b69355555e0c8b702261c5de3b4172ba653c","title":"The Art of SOCRATIC QUESTIONING: Zero-shot Multimodal Reasoning with Recursive Thinking and Self-Questioning"},{"paperId":"d1120d67b700e4dfe8b39eb1e48fbdea4e1a0c43","title":"HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face"},{"paperId":"72160b3c0f73c968fcb903db71817d1bed695f4d","title":"Look, Remember and Reason: Visual Reasoning with Grounded Rationales"},{"paperId":"5ce94181ea702f69c3651dce721d6bd8026b8106","title":"TPTU: Task Planning and Tool Usage of Large Language Model-based AI Agents"},{"paperId":"d7cdcedebd7013f102fc4f1ca5890ac352c7cfa0","title":"TOA: Task-oriented Active VQA"}],"references":[{"paperId":"da061a6e0016d6b625a8e86d64a797ca8ddb92a5","title":"Modular Visual Question Answering via Code Generation"},{"paperId":"af997821231898a5f8d0fd78dad4eec526acabe5","title":"Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models"},{"paperId":"fbfef4723d8c8467d7bd523e1d0b703cce0e0f9c","title":"Language Is Not All You Need: Aligning Perception with Language Models"},{"paperId":"53d128ea815bcc0526856eb5a9c42cc977cb36a7","title":"Toolformer: Language Models Can Teach Themselves to Use Tools"},{"paperId":"3f5b31c4f7350dc88002c121aecbdc82f86eb5bb","title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models"},{"paperId":"b909c1905063fe247a7c9359842e8437448f929d","title":"HiTeA: Hierarchical Temporal-Aware Video-Language Pre-training"},{"paperId":"4eb5198062f78ecf844ff48bcaefe4c1c0f395cc","title":"Doubly Right Object Recognition: A Why Prompt for Visual Rationales"},{"paperId":"3e8251f259dc529b3aa2366fc68c1516b202cfb9","title":"Reveal: Retrieval-Augmented Visual-Language Pre-Training with Multi-Source Multimodal Knowledge Memory"},{"paperId":"e1c2a926df37107358ac51e460361e2a249c8b26","title":"Open-vocabulary Attribute Detection"},{"paperId":"6c943670dca38bfc7c8b477ae7c2d1fba1ad3691","title":"Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks"},{"paperId":"6c1e1cc1e0e1f8fd026fe517607b2d4535565fa7","title":"PAL: Program-aided Language Models"},{"paperId":"af1c871282ec122869d03f5420ef5d9143358a91","title":"Visual Programming: Compositional visual reasoning without training"},{"paperId":"a5cb8f26acb71edd77ff9a143d3ddaab2367eb40","title":"PromptCap: Prompt-Guided Task-Aware Image Captioning"},{"paperId":"26fd105d0b5a458979c012cddb3ba2de943388c4","title":"Plug-and-Play VQA: Zero-shot VQA by Conjoining Large Pretrained Models with Zero Training"},{"paperId":"11f86abe3d6b1de0678390fed442fdcb62667768","title":"COFAR: Commonsense and Factual Reasoning in Image Search"},{"paperId":"a42b091adaf29b06a092b67192ac07cb93312f2a","title":"Visual Classification via Description from Large Language Models"},{"paperId":"39e40821b7207125e54e6ed7112e55cd38c6f0c3","title":"Language Models of Code are Few-Shot Commonsense Learners"},{"paperId":"6f85ec89d9c07a8db4545e64888ced820370a21b","title":"Retrieval Augmented Visual Question Answering with Outside Knowledge"},{"paperId":"f58ca7ba4a08b7082e86b7a5989b4b0fda2107ab","title":"Binding Language Models in Symbolic Languages"},{"paperId":"009e40cdc9d98b9e5f6279d38b46936ceffcc124","title":"Video Graph Transformer for Video Question Answering"},{"paperId":"57c64f233a0db4d17e0e750c12516364ca009fb2","title":"REVIVE: Regional Visual Representation Matters in Knowledge-Based Visual Question Answering"},{"paperId":"02720ba7a4c0c70506ef63e039387c10b227d8e3","title":"Transform-Retrieve-Generate: Natural Language-Centric Outside-Knowledge Visual Question Answering"},{"paperId":"809822d59203a462bc9f2e0f0e9a8314d6d469d4","title":"Revisiting the “Video” in Video-Language Understanding"},{"paperId":"354bf043179e3e9f05df73e3f04517e53c326d1f","title":"TALM: Tool Augmented Language Models"},{"paperId":"c1ace33daf974d3d16752c7a8565f32a63b09c49","title":"Language Models with Image Descriptors are Strong Few-Shot Video-Language Learners"},{"paperId":"9dae204dad41633188022002a04c8aa67c79a4e1","title":"Simple Open-Vocabulary Object Detection with Vision Transformers"},{"paperId":"26218bdcc3945c7edae7aa2adbfba4cd820a2df3","title":"Flamingo: a Visual Language Model for Few-Shot Learning"},{"paperId":"408efdd599b2b27ecb95a4d799869c9ff568fb31","title":"ReCLIP: A Strong Zero-Shot Baseline for Referring Expression Comprehension"},{"paperId":"ada81a4de88a6ce474df2e2446ad11fea480616e","title":"Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language"},{"paperId":"1bfa62ddfa3f6691e0e40c06f8ead594b6449cfa","title":"OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework"},{"paperId":"1b6e810ce0afd0dd093f789d2b2742d047e316d5","title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"bba57c53ab9b600f71d888601ed0aa03812c8199","title":"MuMuQA: Multimedia Multi-Hop News Question Answering via Cross-Media Knowledge Extraction and Grounding"},{"paperId":"ab8ee8d06ed581c876da3a2e7a5fdb1cbec21a45","title":"KAT: A Knowledge Augmented Transformer for Vision-and-Language"},{"paperId":"5341b412383c43f4a693ad63ec4489e3ec7688c8","title":"Grounded Language-Image Pre-training"},{"paperId":"ec8afc75ec219f2a5f9ed9d7c9dde0720f69b5a2","title":"Multi-Grained Vision Language Pre-Training: Aligning Texts with Visual Concepts"},{"paperId":"09f2b1f1bd313cf9183c138fca8f17bb228b4435","title":"Coarse-to-Fine Reasoning for Visual Question Answering"},{"paperId":"2672777d25562c9df6fc13b653181db62d39bece","title":"An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"f46f77630b35a43e8c247916da5d809d6e5b4210","title":"Interpretable visual reasoning: A survey"},{"paperId":"6be64445935dcdf4053a6e78b623b80a314d9bbc","title":"Separating Skills and Concepts for Novel Visual Question Answering"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"0416fda32c39fc9531e87bab6a8a1a552bf9ada0","title":"Obtaining Faithful Interpretations from Compositional Neural Networks"},{"paperId":"3c8a456509e6c0805354bd40a35e3f2dbf8069b1","title":"PyTorch: An Imperative Style, High-Performance Deep Learning Library"},{"paperId":"007ca8ca7a68451c32da034c72a06238434843c1","title":"Learning to Learn Words from Visual Scenes"},{"paperId":"79c93274429d6355959f1e4374c2147bb81ea649","title":"LXMERT: Learning Cross-Modality Encoder Representations from Transformers"},{"paperId":"136c05cb8dd359fb8e0dc7947172a9ecb74ccbec","title":"Learning by Abstraction: The Neural State Machine"},{"paperId":"7bd83b055702bc178aa26def5b6df463f8eab7b9","title":"Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-Shot Cross-Dataset Transfer"},{"paperId":"28ad018c39d1578bea84e7cedf94459e3dbe1e70","title":"OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge"},{"paperId":"2dc698077cb178286c737484dcf67c5ab19314d0","title":"Language-Conditioned Graph Networks for Relational Reasoning"},{"paperId":"1ab7f7c1d328589f25c79515b9a5d824d7ffbbd1","title":"GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering"},{"paperId":"9d15ebe3f5aaf32a9f835f88703241461324c35b","title":"Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding"},{"paperId":"97b93509f6c3c33dd3665d05b1878e36d58a1efb","title":"Interpretable Visual Question Answering by Reasoning on Dependency Trees"},{"paperId":"b1b852d4bf934863397e7b965a5dd0124ad8670c","title":"Interpretable Visual Question Answering by Visual Grounding From Attention Supervision Mining"},{"paperId":"7af4a37e6e63b5f06e7bfb6e7c8910322774efb9","title":"Visual Reasoning by Progressive Module Networks"},{"paperId":"1fe32a88a2e4162f4b3fe73ffa1fcb120bd5b1bf","title":"Visual Grounding Via Accumulated Attention"},{"paperId":"289fb3709475f5c87df8d97f129af54029d27fee","title":"Compositional Attention Networks for Machine Reasoning"},{"paperId":"ef153ece43ee50f8208f6197f0eaf3d324e4475b","title":"Multimodal Explanations: Justifying Decisions and Pointing to the Evidence"},{"paperId":"0fff5c49c05c27c22ac7685130197146491f0b36","title":"The Consciousness Prior"},{"paperId":"2e17cf6a339fd071ad222062f868e882ef4120a4","title":"Inferring and Executing Programs for Visual Reasoning"},{"paperId":"a396a6febdacb84340d139096455e67049ac1e22","title":"Learning to Reason: End-to-End Module Networks for Visual Question Answering"},{"paperId":"5582bebed97947a41e3ddd9bd1f284b73f1648c2","title":"Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization"},{"paperId":"3c1bbd2672c11a796f1e6e6aa787257498ec8bec","title":"Revisiting Visual Question Answering Baselines"},{"paperId":"21c99706bb26e9012bfb4d8d48009a3d45af59b2","title":"Neural Module Networks"},{"paperId":"4d8f2d14af5991d4f0d050d22216825cac3157bd","title":"Show, Attend and Tell: Neural Image Caption Generation with Visual Attention"},{"paperId":"2f2961362355e45fa014ca0bb8ce4495aedf8824","title":"Thinking fast and slow."},{"paperId":"c3a24b0b38922c4f3a825edb97cc470a4ca7af75","title":"Vision"},{"paperId":"f9bdd27c48c57426179b1b09ffc517e94cbfca56","title":"Information Streams Sharing a Finite Buffer"},{"paperId":"450b8dff662a5d41388d04d994e5117020777ce5","title":"Code4Struct: Code Generation for Few-Shot Structured Prediction from Natural Language"},{"paperId":null,"title":"Plug-and-play VQA: Zeroshot VQA by conjoining large pretrained models with zero training. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 951–967"},{"paperId":null,"title":"the 2022 Conference on Empirical Methods in Natural Language Processing"},{"paperId":null,"title":"arXiv:2201.11903 [cs"},{"paperId":null,"title":"Systematic Generalization: What Is Required and Can It Be Learned"},{"paperId":"da0068c430b96347682bcc5b590b15ed8b1a41c4","title":"Visual Programming"},{"paperId":"c9ffae72c63dc28142d0828ceace60a8ace539eb","title":"2009 IEEE 12th International Conference on Computer Vision (ICCV)"},{"paperId":"48ffc7197f27ab7e96e1eba1f929513f1848e522","title":"Programs"},{"paperId":null,"title":"Christopher"},{"paperId":null,"title":"Felipe Petroski Such"},{"paperId":null,"title":"11862 applicable license agreement with IEEE. Restrictions apply"}],"id":"6e754273d54a91371efbc928cd6b156364d517da","summary":"ViperGPT is introduced, a framework that leverages code-generation models to compose vision-and-language models into subroutines to produce a result for any query and achieves state-of-the-art results across various complex visual tasks."},{"url":"https://www.semanticscholar.org/paper/d8da72e7857cc1a0d3505e6c8a746eac815901b2","title":"Large-Scale Domain-Specific Pretraining for Biomedical Vision-Language Processing","venue":"arXiv.org","year":2023,"referenceCount":58,"citationCount":41,"influentialCitationCount":0,"publicationDate":"02/03/2023","authors":"Shenmin Zhang,Yanbo Xu,Naoto Usuyama,J. Bagga,Robert Tinn,Sam Preston,Rajesh N. Rao,Mu-Hsin Wei,Naveen Valluri,Cliff Wong,M. Lungren,Tristan Naumann,Hoifung Poon","citations":[{"paperId":"93886752191db25efd096a65af7b09df5c0a64e0","title":"Data-Centric Foundation Models in Computational Healthcare: A Survey"},{"paperId":"b1721374889899950994f67029fe899de257c140","title":"A Foundational Multimodal Vision Language AI Assistant for Human Pathology"},{"paperId":"6bdfffbf92d01c8b543088d40d46233610e469a8","title":"CLIP in Medical Imaging: A Comprehensive Survey"},{"paperId":"0bbd619ad6dfb69114735d6d8ca166c20301188b","title":"Learned representation-guided diffusion models for large-image generation"},{"paperId":"2c7e346aa311fec4dda04bdf3a214ce2026d8807","title":"Medical Vision Language Pretraining: A survey"},{"paperId":"21a2a2e96e5ba50140507cbf4d42b69756ae109b","title":"Improving Medical Report Generation with Adapter Tuning and Knowledge Enhancement in Vision-Language Foundation Models"},{"paperId":"1df49b7449a7697a0370253f0e0c2fd50a0093c3","title":"3D-MIR: A Benchmark and Empirical Study on 3D Medical Image Retrieval in Radiology"},{"paperId":"da9e284c75334c660029c79f09ed371aaf2f1139","title":"AcademicGPT: Empowering Academic Research"},{"paperId":"9af80b834e2c4fbd4f2a700ed9e3b4ec8c2c178f","title":"Rotation-Agnostic Image Representation Learning for Digital Pathology"},{"paperId":"59ba440bdce4b9b963124a46ee87b63b67c4c58c","title":"Training CLIP models on Data from Scientific Papers"},{"paperId":"8d2709ed1788a67e64425fb410bb49f3ee49e088","title":"Large Language Models Illuminate a Progressive Pathway to Artificial Healthcare Assistant: A Review"},{"paperId":"8e5d42f5b98146d0784fe85e29c768a4989e1478","title":"Foundational Models in Medical Imaging: A Comprehensive Survey and Future Vision"},{"paperId":"2d9310132cfe9046f4b61f6e90a5ef92c6e0ba71","title":"BiomedJourney: Counterfactual Biomedical Image Generation by Instruction-Learning from Multimodal Patient Journeys"},{"paperId":"0e1a7f453976d7aa74eed46686c943bc6e630b56","title":"Estimating Uncertainty in Multimodal Foundation Models using Public Internet Data"},{"paperId":"3cd68281e1a1b945abb4a3941c5dfed08ca2d57e","title":"Improving mitosis detection on histopathology images using large vision-language models"},{"paperId":"c7492913370b5726eaa6ced163a60de6c9d4bb7f","title":"A Survey of Large Language Models for Healthcare: from Data, Technology, and Applications to Accountability and Ethics"},{"paperId":"8b4d09436fbc533fb136eb48624997215e7c5054","title":"I-AI: A Controllable&Interpretable AI System for Decoding Radiologists' Intense Focus for Accurate CXR Diagnoses"},{"paperId":"5ca34d383136623bf8d851b63bc43a5c9674feac","title":"SDA-CLIP: surgical visual domain adaptation using video and text labels"},{"paperId":"fc2b6a9f77469784966d3ee50863ee78cce4a993","title":"When is a Foundation Model a Foundation Model"},{"paperId":"823b4b5403500e5020ba38f345d48803f33f8a35","title":"An Empirical Analysis for Zero-Shot Multi-Label Classification on COVID-19 CT Scans and Uncurated Reports"},{"paperId":"eb40c436d18f8e657cf2626fe94507c7d8f462ec","title":"PathLDM: Text conditioned Latent Diffusion Model for Histopathology"},{"paperId":"1399f72f14731038abb73613a3824ef981ec1fe1","title":"PECon: Contrastive Pretraining to Enhance Feature Alignment between CT and EHR Data for Improved Pulmonary Embolism Diagnosis"},{"paperId":"d0871bd71161d8ead536f63101e362fc0eda723d","title":"ViLLA: Fine-Grained Vision-Language Representation Learning from Real-World Data"},{"paperId":"e9f0223f8dce8b04d37d1f56e6c976b5d0cb5956","title":"A Foundation LAnguage-Image model of the Retina (FLAIR): Encoding expert knowledge in text supervision"},{"paperId":"b18daa14486920016c4664c3ed1759f2de1ba854","title":"Exploring Transfer Learning in Medical Image Segmentation using Vision-Language Models"},{"paperId":"db5dc8a44511654dc7e0bbebd44f7b502e5e90be","title":"Generative AI for Medical Imaging: extending the MONAI Framework"},{"paperId":"c9dbdae8146b9f97e254f5d26fd6efde96eaa703","title":"Med-Flamingo: a Multimodal Medical Few-shot Learner"},{"paperId":"813ba033b8f593c98f9af44c5b4901408ba6f70a","title":"Towards a Visual-Language Foundation Model for Computational Pathology"},{"paperId":"b0883ddf7fb07aabfeb2b2f593e7ac302aa42372","title":"PRIOR: Prototype Representation Joint Learning from Medical Images and Reports"},{"paperId":"f0dd5a849e17a6dbe9a9ca73688cd8e94c78b349","title":"To pretrain or not to pretrain? A case study of domain-specific pretraining for semantic segmentation in histopathology"},{"paperId":"0f8d12775a4685575f1489796b5dee9e11fbdfb5","title":"OphGLM: Training an Ophthalmology Large Language-and-Vision Assistant based on Instructions and Dialogue"},{"paperId":"e17e34fc53ce5e1d039d5dce056cb9c7691eb568","title":"Quilt-1M: One Million Image-Text Pairs for Histopathology"},{"paperId":"fed150a219f9c31bdb4920e615c7c9264c634736","title":"On the Challenges and Perspectives of Foundation Models for Medical Image Analysis"},{"paperId":"a42fc49a300136d60aaebb668369010ee7746150","title":"Visual Language Pretrained Multiple Instance Zero-Shot Transfer for Histopathology Images"},{"paperId":"ef6548e20c8f26ad0c8001d0ecf42afdd7420805","title":"Exploring the Versatility of Zero-Shot CLIP for Interstitial Lung Disease Classification"},{"paperId":"07d45ce7de598ef03b400f8ddba7d2e055e77a08","title":"BiomedGPT: A Unified and Generalist Biomedical Generative Pre-trained Transformer for Vision, Language, and Multimodal Tasks"},{"paperId":"c5b7f9e2af19db0c2bc9d57a113c17aa9d4d6fda","title":"S-CLIP: Semi-supervised Vision-Language Learning using Few Specialist Captions"},{"paperId":"62976d38e53d73fe89b7f7394ea8e89358a998dd","title":"Does GPT4 dream of counting electric nodules?"},{"paperId":"31a7d8c4a5ab6bab522494b57270249105c8748e","title":"BiomedGPT: A Unified and Generalist Biomedical Generative Pre-trained Transformer for Vision, Language, and Multimodal Tasks"},{"paperId":"bca0bbd01ea917b7a9fe369288ea3ba03d3b1ff3","title":"A Survey of Large Language Models in Medicine: Progress, Application, and Challenge"},{"paperId":"fcf880799607a2a9e5b24126b7ac2fae708a5b8d","title":"Decoding Radiologists’ Intense Focus for Accurate CXR Diagnoses: A Controllable & Interpretable AI System"}],"references":[{"paperId":"0f0024bfef037b97b324b97150ee022c178d6282","title":"A visual–language foundation model for pathology image analysis using medical Twitter"},{"paperId":"304f8b4edea01fdb5a2f7f8b998c83188deeccff","title":"Towards Generalist Biomedical AI"},{"paperId":"e17e34fc53ce5e1d039d5dce056cb9c7691eb568","title":"Quilt-1M: One Million Image-Text Pairs for Histopathology"},{"paperId":"9faa2b0e5cb93f20df0555c3c350fab0b2eccf3a","title":"Foundation models for generalist medical artificial intelligence"},{"paperId":"16de2006e2960ba410772c6b6d460b83c0a5cc4b","title":"Reproducible Scaling Laws for Contrastive Language-Image Learning"},{"paperId":"cfca7eedc6ede9d363d1662280a74d78dcdc9d4a","title":"Scaling Language-Image Pre-Training via Masking"},{"paperId":"282bed72d56115077bb6cc0004c991a99a2c216a","title":"Self-Supervised Pretraining Enables High-Performance Chest X-Ray Interpretation Across Clinical Distributions"},{"paperId":"cdd9c1d23f9e89d5113f3e31821bb174c6a6afed","title":"MedCLIP: Contrastive Learning from Unpaired Medical Images and Text"},{"paperId":"e5c8960eb2ec034ffbd353ef39fd1cb541d3c7c9","title":"LAION-5B: An open large-scale dataset for training next generation image-text models"},{"paperId":"44279244407a64431810f982be6d0c7da4429dd7","title":"BioGPT: Generative Pre-trained Transformer for Biomedical Text Generation and Mining"},{"paperId":"a1aeb5442e31276b197696f49b3243112a4049ce","title":"Making the Most of Text Semantics to Improve Biomedical Vision-Language Processing"},{"paperId":"7d3b912398c6132d506bebb070211f6b9c339482","title":"ELEVATER: A Benchmark and Toolkit for Evaluating Language-Augmented Visual Models"},{"paperId":"57e6a77d58b7f06ff8578e8b34f1b5072098f082","title":"Beyond Medical Imaging - A Review of Multimodal Deep Learning in Radiology"},{"paperId":"4ef3d9e492479e28fa57d107e52acc6a0c803de2","title":"Does CLIP Benefit Visual Question Answering in the Medical Domain as Much as it Does in the General Domain?"},{"paperId":"c10075b3746a9f3dd5811970e93c8ca3ad39b39d","title":"High-Resolution Image Synthesis with Latent Diffusion Models"},{"paperId":"2fca2821ac2beb60fa0e26866e8f063261713951","title":"Joint Learning of Localized Representations from Medical Images and Reports"},{"paperId":"94ff111c4d81bd03f159321728ceec8b4711c89d","title":"An Empirical Study of Training End-to-End Vision-and-Language Transformers"},{"paperId":"0b500aa5fcc175f07aecf26c0e8ddc4f0c6a931d","title":"GLoRIA: A Multimodal Global-Local Representation Learning Framework for Label-efficient Medical Image Recognition"},{"paperId":"c49d8a576ee4c1778eafd75f00565f75864054e4","title":"Self-supervised Image-text Pre-training With Mixed Data In Chest X-rays"},{"paperId":"25a493caee870b9950a9d972bdfae6c478d0816d","title":"Multimodal Representation Learning via Maximization of Local Mutual Information"},{"paperId":"2972bd6cb49883a5c75e26f8f7266dc91e1af25a","title":"Multiple Instance Captioning: Learning Representations from Histopathology Textbooks and Articles"},{"paperId":"98e565fa06f6c7bf7c46833b5106b26dc45130c4","title":"WIT: Wikipedia-based Image Text Dataset for Multimodal Multilingual Machine Learning"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"2cd605106b88c85d7d8b865b1ef0f8c8293debf1","title":"Zero-Shot Text-to-Image Generation"},{"paperId":"07559ad8ccaf1110232a4be78f825691e8416d8c","title":"MedAug: Contrastive learning leveraging patient metadata improves representations for chest X-ray interpretation"},{"paperId":"93b6b79b4ef6c345f31722ce7c829385c6dce0d6","title":"Slake: A Semantically-Labeled Knowledge-Enhanced Dataset For Medical Visual Question Answering"},{"paperId":"394be105b87e9bfe72c20efe6338de10604e1a11","title":"Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts"},{"paperId":"268d347e8a55b5eb82fb5e7d2f800e33c75ab18a","title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"},{"paperId":"fea4a2819f9146e5d1ad2fad3d89436523d530a4","title":"Fusion of medical imaging and electronic health records using deep learning: a systematic review and implementation guidelines"},{"paperId":"b8d5b853f2212cbb48a43f1edec9b96d76d388ec","title":"Medical Visual Question Answering via Conditional Reasoning"},{"paperId":"6dd9f99cecd38504b667d320eb2a6267a9fee35d","title":"Contrastive Learning of Medical Visual Representations from Paired Images and Text"},{"paperId":"a2f38d03fd363e920494ad65a5f0ad8bd18cd60b","title":"Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing"},{"paperId":"62cbbf69269dc25f5458d63aea9aa7f95ed72add","title":"Pubmed Parser: A Python Parser for PubMed Open-Access XML Subset and MEDLINE XML Dataset XML Dataset"},{"paperId":"3c8a456509e6c0805354bd40a35e3f2dbf8069b1","title":"PyTorch: An Imperative Style, High-Performance Deep Learning Library"},{"paperId":"d1f407b16fb8d99487baee37ed0805676c58e7ac","title":"MIMIC-CXR, a de-identified publicly available database of chest radiographs with free-text reports"},{"paperId":"89a816719613e220a64ab2590c938c23bbfe187e","title":"CheXpert: A Large Chest Radiograph Dataset with Uncertainty Labels and Expert Comparison"},{"paperId":"95b19e31af5385800855f245744aabfb0b0ee74e","title":"Augmenting the National Institutes of Health Chest Radiograph Dataset with Expert Annotations of Possible Pneumonia."},{"paperId":"a564fabf130ff6e2742cfba90c7a4018937d764d","title":"Radiology Objects in COntext (ROCO): A Multimodal Image Dataset"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"b227f3e4c0dc96e5ac5426b85485a70f2175a205","title":"Representation Learning with Contrastive Predictive Coding"},{"paperId":"b4df354db88a70183a64dbc9e56cf14e7669a6c0","title":"Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning"},{"paperId":"2a96afaf3261a87f0daa51699b4b3cf169e092c4","title":"Rotation Equivariant CNNs for Digital Pathology"},{"paperId":"0e8b061e08eb1ac8968e44edc0e54da658afad0e","title":"Spatial Organization and Molecular Correlation of Tumor-Infiltrating Lymphocytes Using Deep Learning on Pathology Images."},{"paperId":"d07284a6811f1b2745d91bdb06b040b57f226882","title":"Decoupled Weight Decay Regularization"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"2c03df8b48bf3fa39054345bafabfeff15bfd11d","title":"Deep Residual Learning for Image Recognition"},{"paperId":"1518039b5001f1836565215eb047526b3ac7f462","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"2f9916a5a7e19e720a3db98171484e0911b4b116","title":"Overview of the ImageCLEF 2015 Medical Classification Task"},{"paperId":"251c39c1b9372f3055cd53d0001fc122bfb2e418","title":"The Cancer Imaging Archive (TCIA): Maintaining and Operating a Public Information Repository"},{"paperId":"f7609414ae3a117cd2828c7dae19ad34ff7d72e6","title":"PubMed Central: The GenBank of the published literature."},{"paperId":"c8b25fab5608c3e033d34b4483ec47e68ba109b7","title":"Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"},{"paperId":null,"title":"Pytorch distributed: Experiences on accelerating data parallel training"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":null,"title":"Lung and colon cancer"},{"paperId":null,"title":"Tumor-infiltrating lymphocytes maps from tcga h&e whole slide pathology images"},{"paperId":"18f9a6045ba01cb079c4fa49a630d71bbd27cd92","title":"Descriptor : A dataset of clinically generated visual questions and answers about radiology images"},{"paperId":null,"title":"Pmc open access subset"},{"paperId":"c213af6582c0d518a6e8e14217611c733eeb1ef1","title":"Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem"}],"id":"d8da72e7857cc1a0d3505e6c8a746eac815901b2","summary":"This paper conducted by far the largest study on biomedical VLP, using 15 million figure-caption pairs extracted from biomedical research articles in PubMed Central, and established new state of the art in a wide range of standard datasets, substantially outperformed prior VLP approaches."},{"url":"https://www.semanticscholar.org/paper/f44ad7ad67ddd5fe74598fe491ca75c5221380df","title":"MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models","venue":"","year":2023,"referenceCount":49,"citationCount":584,"influentialCitationCount":1,"publicationDate":"20/04/2023","authors":"Deyao Zhu,Jun Chen,Xiaoqian Shen,Xiang Li,Mohamed Elhoseiny","citations":[{"paperId":"51c375a70ca4a7736585ce6d0fed8bb1888dafba","title":"DualFocus: Integrating Macro and Micro Perspectives in Multi-modal Large Language Models"},{"paperId":"5e7274bcda47b704b6797bb14be8b7a61c047a61","title":"Uncertainty-Aware Evaluation for Vision-Language Models"},{"paperId":"4d7c68ec1a86ef5d187e7edb2f0ad63adddc8ea2","title":"Visual Hallucinations of Multi-modal Large Language Models"},{"paperId":"d30e33051496383a66b2a2ab1b961bc866afa409","title":"VL-Trojan: Multimodal Instruction Backdoor Attacks against Autoregressive Visual Language Models"},{"paperId":"e9acf2a53c406849c717ef4bae6492cd1076d5db","title":"UniGraph: Learning a Cross-Domain Graph Foundation Model From Natural Language"},{"paperId":"c4a1d57011307c575bfa6f41d0afe9dc75fed10b","title":"A Touch, Vision, and Language Dataset for Multimodal Alignment"},{"paperId":"71e599dbfe90f6a4bdbdb7dee8464b54a0801379","title":"Slot-VLM: SlowFast Slots for Video-Language Modeling"},{"paperId":"d9f198541267870ae71087f120ea543ebc38d1c6","title":"Robust CLIP: Unsupervised Adversarial Fine-Tuning of Vision Embeddings for Robust Large Vision-Language Models"},{"paperId":"9be1b9b8b6e97e31c3e16c0263c54b337f46baa2","title":"SInViG: A Self-Evolving Interactive Visual Agent for Human-Robot Interaction"},{"paperId":"a8c422a624ad846ac4d2074a98645d9eb2f364a8","title":"Model Tailor: Mitigating Catastrophic Forgetting in Multi-modal Large Language Models"},{"paperId":"b3f3cfc5b726b1968dfbffdd668f0b53bf39445b","title":"Avoiding Feature Suppression in Contrastive Learning: Learning What Has Not Been Learned Before"},{"paperId":"a1714677252a39d1835824efb185beb0113ca189","title":"Efficient Multimodal Learning from Data-centric Perspective"},{"paperId":"2eb70bf96e170f34d72f0023fc0c4f6fb06b6009","title":"PaLM2-VAdapter: Progressively Aligned Language Model Makes a Strong Vision-language Adapter"},{"paperId":"7f254928af6718c80224199e9cd915e7372ac54d","title":"RAG-Driver: Generalisable Driving Explanations with Retrieval-Augmented In-Context Learning in Multi-Modal Large Language Model"},{"paperId":"83d201d503b863fec7d1225f00a141e722e03f17","title":"Using Left and Right Brains Together: Towards Vision and Language Planning"},{"paperId":"61f4b39325cf28026f3491ec78d938c78bb50dda","title":"ProtChatGPT: Towards Understanding Proteins with Large Language Models"},{"paperId":"7580327ffc9bd5daef83fe8285c0476ca074051d","title":"OmniMedVQA: A New Large-Scale Comprehensive Evaluation Benchmark for Medical LVLM"},{"paperId":"9f12a20f62238f5206520e52e83e2ccd1da17f03","title":"Test-Time Backdoor Attacks on Multimodal Large Language Models"},{"paperId":"2d95b78baf55983be482b5fcb8f7c7db156a6b65","title":"Intriguing Differences Between Zero-Shot and Systematic Evaluations of Vision-Language Transformer Models"},{"paperId":"80da74d006d9e9ac1252c4092bb72b9878d8bec7","title":"World Model on Million-Length Video And Language With RingAttention"},{"paperId":"a082c9c93b5cc0d38e7ac14c6c9dfe186bb5c824","title":"Mitigating Object Hallucination in Large Vision-Language Models via Classifier-Free Guidance"},{"paperId":"77ff5c9276bcf30b524a7cc030e0365a33b686bf","title":"PIN: Positional Insert Unlocks Object Localisation Abilities in VLMs"},{"paperId":"7dddcbb769ce827cc7154af50cb72658fa847d88","title":"Visually Dehallucinative Instruction Generation"},{"paperId":"a41d4a3b005c8ec4f821e6ee96672d930ca9596c","title":"G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering"},{"paperId":"bcc2a6593c7667d341a36e76a0d9d40b4efdf787","title":"TransGPT: Multi-modal Generative Pre-trained Transformer for Transportation"},{"paperId":"1cfb7fba7194860e8b8818eb5e87e0a8e14e518a","title":"ViGoR: Improving Visual Grounding of Large Vision Language Models with Fine-Grained Reward Modeling"},{"paperId":"86c26c7b14874d43de2eb40da6fec2e9182e2f22","title":"Reasoning Grasping via Multimodal Large Language Model"},{"paperId":"ec8e2b45c4601730015608a58e33409224a81228","title":"SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models"},{"paperId":"1254a2c223accad9e13dd639c51c1f81698bcaae","title":"CREMA: Multimodal Compositional Video Reasoning via Efficient Modular Adaptation and Fusion"},{"paperId":"4dd1b575a6ac74bfe9fab2e40c2cf1567c82fc6e","title":"WebLINX: Real-World Website Navigation with Multi-Turn Dialogue"},{"paperId":"793f8572a022866caa66e44c98ff2839c1b4587a","title":"Code as Reward: Empowering Reinforcement Learning with VLMs"},{"paperId":"c7a66961ee07b0e9e792d3625c1b20d510f29429","title":"CogCoM: Train Large Vision-Language Models Diving into Details through Chain of Manipulations"},{"paperId":"956c34e071a4b7ec348e37f1aeeeaf909d2cd6a9","title":"EVA-CLIP-18B: Scaling CLIP to 18 Billion Parameters"},{"paperId":"2a1168abf9e5bf1c4a9cfc5b4aab681c1479b110","title":"Scientific Language Modeling: A Quantitative Review of Large Language Models in Molecular Science"},{"paperId":"83ee82e62f2eae18cc3472120eb9004109895a31","title":"Delving into Multi-modal Multi-task Foundation Models for Road Scene Understanding: From Learning Paradigm Perspectives"},{"paperId":"4b7ad761c08a7ffb191577b3d4fd4ff2d66a8a38","title":"V-IRL: Grounding Virtual Intelligence in Real Life"},{"paperId":"9e2524448155a93473d6d4fa2f3bef34b0ed9622","title":"Shadowcast: Stealthy Data Poisoning Attacks Against Vision-Language Models"},{"paperId":"19e909f88b8b9b0635bd6e441094e1738c3bba9a","title":"Unified Hallucination Detection for Multimodal Large Language Models"},{"paperId":"1b05e2243134099efb6d6bde5f2d4944869596a3","title":"LHRS-Bot: Empowering Remote Sensing with VGI-Enhanced Large Multimodal Language Model"},{"paperId":"d57dea679fae7cd5bca45adb882f2d334b495cfb","title":"GeReA: Question-Aware Prompt Captions for Knowledge-based Visual Question Answering"},{"paperId":"d37e03761c70266b9276506d70faa14c3c199d6d","title":"Generalizable Entity Grounding via Assistance of Large Language Model"},{"paperId":"24253eb1c425de4c4e4c9946ed1b6eb5bdc8fba9","title":"Jailbreaking Attack against Multimodal Large Language Model"},{"paperId":"a3570e82001666955d319647ba832df4f60a2044","title":"A Survey on Robotics with Foundation Models: toward Embodied AI"},{"paperId":"c991dedacba67949a28640cd8755de4c8ae297b0","title":"A Survey on Data Selection for LLM Instruction Tuning"},{"paperId":"52f3338b8969517629097e745f66cb9eac5f99c5","title":"Advancing Graph Representation Learning with Large Language Models: A Comprehensive Survey of Techniques"},{"paperId":"0681c7ad009859c22e7638d4636d13e74cca08c6","title":"Image Fusion via Vision-Language Model"},{"paperId":"7c9fb2be8b091e89e7edefe6e3f999fc896e1bae","title":"Rendering Graphs for Graph Reasoning in Multimodal Large Language Models"},{"paperId":"a6c691b7f35188909946c9272a31dec947678380","title":"GPT-4V as Traffic Assistant: An In-depth Look at Vision Language Model on Complex Traffic Events"},{"paperId":"33b5dd24169ff876420f56c19c0a65b6194cb49f","title":"User Intent Recognition and Satisfaction with Large Language Models: A User Study with ChatGPT"},{"paperId":"3e6a9b9bb793458ca0e97eab78251ab38f8fd417","title":"Can MLLMs Perform Text-to-Image In-Context Learning?"},{"paperId":"844c50db2438cedff6ff9af987f3569dadd7e006","title":"Multimodal Embodied Interactive Agent for Cafe Scene"},{"paperId":"d1313748a791e7ac21e4310d9b0e0d2febb8ab56","title":"Instruction Makes a Difference"},{"paperId":"fc4c380102d6f72657d1ab54dffd6be536bb01c7","title":"A Survey on Hallucination in Large Vision-Language Models"},{"paperId":"b738b46b2deccd72f0dea5c93d33e9fbd69d51f7","title":"PVLR: Prompt-driven Visual-Linguistic Representation Learning for Multi-Label Image Recognition"},{"paperId":"6b3d58d367b049532c0dd7a203eb0c17c94e734e","title":"Image Anything: Towards Reasoning-coherent and Training-free Multi-modal Image Generation"},{"paperId":"2bbcf2aabb9d7c346b58b4051d7b5d235c6c7953","title":"Proximity QA: Unleashing the Power of Multi-Modal Large Language Models for Spatial Proximity Analysis"},{"paperId":"afa085ae73302f16965ae918bd9dc4042cde60cd","title":"Common Sense Reasoning for Deep Fake Detection"},{"paperId":"fb4bf63ca27f2b2931c5076f7ca85ddb5dad41b4","title":"Large Language Model Evaluation via Matrix Entropy"},{"paperId":"153403b7d775d7d5655de0f7d785a082c5f9c2ed","title":"Towards Unified Interactive Visual Grounding in The Wild"},{"paperId":"8c888fb89f1b2bbd66f95e39899a07cf2100fa38","title":"EarthGPT: A Universal Multi-modal Large Language Model for Multi-sensor Image Comprehension in Remote Sensing Domain"},{"paperId":"ab003ca4c9479d1b155f8da9505160e8c07e83ce","title":"MouSi: Poly-Visual-Expert Vision-Language Models"},{"paperId":"af9676f9beaeca214bfbf7f2897828820d48abdc","title":"LLaVA-MoLE: Sparse Mixture of LoRA Experts for Mitigating Data Conflicts in Instruction Finetuning MLLMs"},{"paperId":"c5db6c2726911b72d534f97bd4d1ed63f6431340","title":"Mobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual Perception"},{"paperId":"bce43cb9af37a0c8d90f8cadaebd6bb002685edd","title":"InternLM-XComposer2: Mastering Free-form Text-Image Composition and Comprehension in Vision-Language Large Model"},{"paperId":"cd1d7f5c4ce2d31ce9ee72db165a8272624da7d3","title":"MoE-LLaVA: Mixture of Experts for Large Vision-Language Models"},{"paperId":"a6a8896dea728310d1bfe829e027e10cccdf4974","title":"Fortifying Ethical Boundaries in AI: Advanced Strategies for Enhancing Security in Large Language Models"},{"paperId":"4d5262260022d8baff65242c6eb879d184447d5f","title":"CMMU: A Benchmark for Chinese Multi-modal Multi-type Question Understanding and Reasoning"},{"paperId":"8ab77a9eb414a6d1e9b7f81399483ff17d6ec877","title":"MLLMReID: Multimodal Large Language Model-based Person Re-identification"},{"paperId":"086168c2ad02930279fe481a52641a565b55c4a0","title":"Democratizing Fine-grained Visual Recognition with Large Language Models"},{"paperId":"41f12456780aecd204a210ce04b1a92d022b8c4c","title":"Small Language Model Meets with Reinforced Vision Vocabulary"},{"paperId":"9dad4d7c6da4f3bb56a24742998c76a2629ac9fd","title":"Benchmarking Large Multimodal Models against Common Corruptions"},{"paperId":"140cfda71bfff852c3e205b7ad61854b78c76982","title":"Mastering Text-to-Image Diffusion: Recaptioning, Planning, and Generating with Multimodal LLMs"},{"paperId":"d7c98899eb281d8f67899bf1959f288627780d3e","title":"A Framework to Implement 1+N Multi-task Fine-tuning Pattern in LLMs Using the CGC-LORA Algorithm"},{"paperId":"436b05a34c13dfaa357dd93661b474c151098506","title":"Using Large Language Model for End-to-End Chinese ASR and NER"},{"paperId":"fb60a8655b3b174aa34a5ffe0ee72ca732d287e4","title":"LLMRA: Multi-modal Large Language Model based Restoration Assistant"},{"paperId":"61ea0a87eab0029de9f4f6032108cb8d94cca3ac","title":"Inducing High Energy-Latency of Large Vision-Language Models with Verbose Images"},{"paperId":"0be1c71b1710f01fb5d321e9b1459a7d2a7cdaf2","title":"Q&A Prompts: Discovering Rich Visual Clues through Mining Question-Answer Prompts for VQA requiring Diverse World Knowledge"},{"paperId":"4f2a56102bcbf0fe79379c4c27daecbccfb35a26","title":"MLLM-Tool: A Multimodal Large Language Model For Tool Agent Learning"},{"paperId":"9c20d8d5cfc60f5b9aa058ff2968563f2af33398","title":"Temporal Insight Enhancement: Mitigating Temporal Hallucination in Multimodal Large Language Models"},{"paperId":"0a8a776054a087118f4f9523994ef084b2b2469a","title":"Advancing Large Multi-modal Models with Explicit Chain-of-Reasoning and Visual Question Generation"},{"paperId":"02d96eb0da4a282831f14923d1a65976952b7177","title":"Towards Language-Driven Video Inpainting via Multimodal Large Language Models"},{"paperId":"14ed0a54cb784d939a142beddad63724fa052f36","title":"SkyEyeGPT: Unifying Remote Sensing Vision-Language Tasks via Instruction Tuning with Large Language Model"},{"paperId":"616e98ba9e60f36c6ee226cc66c787610f0bbb62","title":"MM-Interleaved: Interleaved Image-Text Generative Modeling via Multi-modal Feature Synchronizer"},{"paperId":"7f6b1bdebb0f772230412a4451e98a5ba77ba043","title":"Vlogger: Make Your Dream A Vlog"},{"paperId":"4a48d628e53f554eb6ef09a457ca855188b96171","title":"DoraemonGPT: Toward Understanding Dynamic Scenes with Large Language Models"},{"paperId":"a07de765c59755fa9653276f0b5ccb68f77d7660","title":"AesBench: An Expert Benchmark for Multimodal Large Language Models on Image Aesthetics Perception"},{"paperId":"d291f736a53b48c95edfb213e8d2a5fdeed31dd0","title":"Seeing the Unseen: Visual Common Sense for Semantic Placement"},{"paperId":"c5a3b7f3d0203f001621e48a52e24c029963a2a3","title":"Towards A Better Metric for Text-to-Video Generation"},{"paperId":"5502d769595981009e43344f8914e287acca2359","title":"ModaVerse: Efficiently Transforming Modalities with LLMs"},{"paperId":"ca00f4056f9039d3c1a4c3a113f5ee0527149b66","title":"Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs"},{"paperId":"3791dad1e3c1eec45a3833e5264574a308cc0014","title":"An EcoSage Assistant: Towards Building A Multimodal Plant Care Dialogue Assistant"},{"paperId":"af5f256e9771bf9cd02451195e3a7ac693fde3ed","title":"Exploring the Reasoning Abilities of Multimodal Large Language Models (MLLMs): A Comprehensive Survey on Emerging Trends in Multimodal Reasoning"},{"paperId":"5714bd69ffc2e377bcf370c23aec7e6991891762","title":"Uni3D-LLM: Unifying Point Cloud Perception, Generation and Editing with Large Language Models"},{"paperId":"002d2c4569d070a55fe69c25ebccad8e9ddae572","title":"Incorporating Visual Experts to Resolve the Information Loss in Multimodal Large Language Models"},{"paperId":"c21781a9ebb3da9dfc318259633f6eaea8e8447f","title":"CaMML: Context-Aware Multimodal Learner for Large Models"},{"paperId":"d9263ba30a16067c44884ad559802505acf2f82d","title":"3DMIT: 3D Multi-modal Instruction Tuning for Scene Understanding"},{"paperId":"5e9985b784430c33d25458d291b989e011fc9634","title":"Reading Between the Frames: Multi-Modal Depression Detection in Videos from Non-Verbal Cues"},{"paperId":"4281d8d35d4c4dae3c0619c30abf084daf1b9385","title":"AccidentGPT: Large Multi-Modal Foundation Model for Traffic Accident Analysis"},{"paperId":"420087f314633a381e61e6c5cd73ccc2070a749e","title":"PeFoMed: Parameter Efficient Fine-tuning on Multimodal Large Language Models for Medical Visual Question Answering"},{"paperId":"e7e76e0af01c7f7cbf61c2544a3f0a58794bbf11","title":"ChartAssisstant: A Universal Chart Multimodal Language Model via Chart-to-Table Pre-training and Multitask Instruction Tuning"},{"paperId":"ece33ee67d74c29cd2a83c505e5bf0b818f9c2a1","title":"LLaVA-Phi: Efficient Multi-Modal Assistant with Small Language Model"},{"paperId":"efc5e94635a850ede9c1f8dbce65d5dc536f3bfb","title":"Understanding LLMs: A Comprehensive Overview from Training to Inference"},{"paperId":"0725debf3183589626823dbb64107bba8ed22448","title":"MedSumm: A Multimodal Approach to Summarizing Code-Mixed Hindi-English Clinical Queries"},{"paperId":"9eab4104973f5de650544729a4a69d84c594da92","title":"A Vision Check-up for Language Models"},{"paperId":"c844694387a89a477e7a8bbf918171cdc3b85672","title":"GPT-4V(ision) is a Generalist Web Agent, if Grounded"},{"paperId":"cd49101103f73d88a4a3b368898066f03984c339","title":"Holistic Autonomous Driving Understanding by Bird's-Eye-View Injected Multi-Modal Large Models"},{"paperId":"b33b8f7508f8a802e2e669ec16348bb789ae0470","title":"Taking the Next Step with Generative Artificial Intelligence: The Transformative Role of Multimodal Large Language Models in Science Education"},{"paperId":"575f403261d5f99526f0b4dfc8644352d6c4467a","title":"DocLLM: A layout-aware generative language model for multimodal document understanding"},{"paperId":"5f58863dd6474d6f127be995b5871e7c60f2792f","title":"Video Understanding with Large Language Models: A Survey"},{"paperId":"061f91f1740ba6ee860d38f8637cb6b1b98bfb10","title":"Tracking with Human-Intent Reasoning"},{"paperId":"e36f79b2bfbf50cf6362cc563a6e5c261e3f0615","title":"MobileVLM : A Fast, Strong and Open Vision Language Assistant for Mobile Devices"},{"paperId":"01ad38120fdd571c68184946f5c24aa0928aebc3","title":"Improving In-context Learning via Bidirectional Alignment"},{"paperId":"074ab652cf539d695aabd9d5fe07c69386deb8da","title":"MIVC: Multiple Instance Visual Component for Visual-Language Models"},{"paperId":"46f64681d7a0c7f80303bebb0d62a3b7acbcdcd9","title":"An Improved Baseline for Reasoning Segmentation with Large Language Model"},{"paperId":"1e1230ef1de1ba9c4f6cb4789184a295133afac0","title":"Visual Instruction Tuning towards General-Purpose Multimodal Model: A Survey"},{"paperId":"6dbb5a3f98625d7d090b37ae36dbf3e64d1f1d0e","title":"Understanding Naturalistic Facial Expressions with Deep Learning and Multimodal Large Language Models"},{"paperId":"f2b6323973955f9a1ebb9be76a616991de3d3a8f","title":"ChartBench: A Benchmark for Complex Visual Reasoning in Charts"},{"paperId":"0023adac4b41549eb163e4fd523e9f2976dd5694","title":"LLM-SAP: Large Language Model Situational Awareness Based Planning"},{"paperId":"1a6bd4f70bceb26ddb722ce98c0eae2a64147048","title":"Knowledge-Based and Generative-AI-Driven Pedagogical Conversational Agents: A Comparative Study of Grice’s Cooperative Principles and Trust"},{"paperId":"82dc8edc49f9edf4a53056aedcdfb339be070166","title":"IQAGPT: Image Quality Assessment with Vision-language and ChatGPT Models"},{"paperId":"b137709522bc70b42b026cae192de2a45000b22e","title":"MetaAID 2.5: A Secure Framework for Developing Metaverse Applications via Large Language Models"},{"paperId":"0ca80b1244ee5419ff9108fbf39aa59f24fdfc4c","title":"FoodLMM: A Versatile Food Assistant using Large Multi-modal Model"},{"paperId":"1c9bab7ab072c619133c936b5b85160e5373e638","title":"VCoder: Versatile Vision Encoders for Multimodal Large Language Models"},{"paperId":"c672ec79f55cef8f7a32cd8dddfa981b893f1567","title":"V*: Guided Visual Search as a Core Mechanism in Multimodal LLMs"},{"paperId":"6a33e58ef961a3a0a5657518b2be86395eb7c8d0","title":"InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks"},{"paperId":"5edf706467dc76cd09319592d18db0ad4e1fb64d","title":"LiDAR-LLM: Exploring the Potential of Large Language Models for 3D LiDAR Understanding"},{"paperId":"4b1b5e219fb41a7413599c3b2ca6a7fdf045d1a5","title":"Generative Multimodal Models are In-Context Learners"},{"paperId":"592ac35991e583fc37c26ee6659d2deb85142ad9","title":"Large Language Models Empowered Agent-based Modeling and Simulation: A Survey and Perspectives"},{"paperId":"17a32c825bd746a2625eddc2728092171a9ef72a","title":"Jack of All Tasks, Master of Many: Designing General-purpose Coarse-to-Fine Vision-Language Model"},{"paperId":"2d4a853affeb0b164fc1134df612aea658f36459","title":"Mixture of Cluster-conditional LoRA Experts for Vision-language Instruction Tuning"},{"paperId":"25fdd7247b042f6976baeae258b69f910ecaf9d9","title":"VQA4CIR: Boosting Composed Image Retrieval with Visual Question Answering"},{"paperId":"7a31971b0af439dec6fc484cca20df57f440b644","title":"One Shot Learning as Instruction Data Prospector for Large Language Models"},{"paperId":"493a931db01812489c04b04fec525e6bb44d83de","title":"M2ConceptBase: A Fine-grained Aligned Multi-modal Conceptual Knowledge Base"},{"paperId":"0098137fe15b446c5d629f9a6ad05b1ff96be30f","title":"DeepArt: A Benchmark to Advance Fidelity Research in AI-Generated Content"},{"paperId":"852ab98a42204ea905ba41b3b1354820be4f201c","title":"When Parameter-efficient Tuning Meets General-purpose Vision-language Models"},{"paperId":"b5503967d39557a77c70076c308183e92d6d775a","title":"Osprey: Pixel Understanding with Visual Instruction Tuning"},{"paperId":"95d791ad14db2c779daa67ca7fdc3a75214c42eb","title":"3DAxiesPrompts: Unleashing the 3D Spatial Task Capabilities of GPT-4V"},{"paperId":"3cc0f0b40589f4e0f971280cad48919048d22a41","title":"GSVA: Generalized Segmentation via Multimodal Large Language Models"},{"paperId":"381bf6f86718599c6649e89a39bcbb58ec2f7626","title":"ICD-LM: Configuring Vision-Language In-Context Demonstrations by Language Modeling"},{"paperId":"81c34355f585f40e827f79acaccbcc885ed133a0","title":"Depicting Beyond Scores: Advancing Image Quality Assessment through Multi-modal Language Models"},{"paperId":"ea6982a936a2b263bbf46ff6eb27fc0b63fddaf7","title":"VL-GPT: A Generative Pre-trained Transformer for Vision and Language Understanding and Generation"},{"paperId":"6d2ab31aa75468f5458b9d96192c3f4a28f55d73","title":"DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving"},{"paperId":"6768a6aeb61ad8522795d92bb0ca44f87a327a59","title":"Pixel Aligned Language Models"},{"paperId":"5a97f677ab02602f5f319ad4b3dcd3ed8c53238f","title":"Spatial Interpretation and LLMs"},{"paperId":"8380f2082dbe3effead38b7b32e9615d6c30b381","title":"Chat-3D v2: Bridging 3D Scene and Large Language Models with Object Identifiers"},{"paperId":"d48fa3ed73817563130ef217d85011ce1fbe7470","title":"BESTMVQA: A Benchmark Evaluation System for Medical Visual Question Answering"},{"paperId":"2a76a0dbb9bb9997536b2b3b42c28a01ad27e5cf","title":"Modality Plug-and-Play: Elastic Modality Adaptation in Multimodal LLMs for Embodied AI"},{"paperId":"2141ed804636a1cf339d606cd03fd3b3e9582133","title":"VILA: On Pre-training for Visual Language Models"},{"paperId":"ffa7e52470437d61110b5498fcfa6d4d6e18b78c","title":"Turbo: Informativity-Driven Acceleration Plug-In for Vision-Language Models"},{"paperId":"d1f925c65d56ff4de5d317a54d47d6df34b17d4e","title":"Hallucination Augmented Contrastive Learning for Multimodal Large Language Model"},{"paperId":"2d62cc03397ec7389ae42979420e0280073affa3","title":"MP5: A Multi-modal Open-ended Embodied System in Minecraft via Active Perception"},{"paperId":"e0b05e314372ed580d9612ef5f0ee672b17ad2e4","title":"LMDrive: Closed-Loop End-to-End Driving with Large Language Models"},{"paperId":"33c3dbf86dd6e338e2a49bba9c2e2193d1eca08a","title":"Vista-LLaMA: Reliable Video Narrator via Equal Distance to Visual Tokens"},{"paperId":"1ae9afce62c60fd0bfc9f5b57f8d8a1bbc3641eb","title":"Audio-Visual LLM for Video Understanding"},{"paperId":"b240a1d8ec2860bdd7370daa3144268ce46ac018","title":"Vary: Scaling up the Vision Vocabulary for Large Vision-Language Models"},{"paperId":"257eaf21e6e6926e9693e04e946ec2199b3afee1","title":"NuScenes-MQA: Integrated Evaluation of Captions and QA for Autonomous Driving Datasets using Markup Annotations"},{"paperId":"32198cdd33613a3f7fc2719acd600a98ae1ce31e","title":"Grounded Question-Answering in Long Egocentric Videos"},{"paperId":"369b34826e23cb43bea9a91395e9603eacfa7420","title":"EgoPlan-Bench: Benchmarking Egocentric Embodied Planning with Multimodal Large Language Models"},{"paperId":"cb2295766b2f8f35524f6a9f93ae39d948d50bd4","title":"Genixer: Empowering Multimodal Large Language Models as a Powerful Data Generator"},{"paperId":"4f5654ec1dfc04478be42d03eee8e6db6bd9ca14","title":"Honeybee: Locality-enhanced Projector for Multimodal LLM"},{"paperId":"3581d76349a569c80f331c92710839974308eb1c","title":"Interactive Planning Using Large Language Models for Partially Observable Robotics Tasks"},{"paperId":"4923e93f043091da51ac41b039be1698a1158d80","title":"Causal-CoG: A Causal-Effect Look at Context Generation for Boosting Multi-modal Language Models"},{"paperId":"d23f08611ea1e64f691ecddee1f7f48c8015eea6","title":"Lyrics: Boosting Fine-grained Language-Vision Alignment and Comprehension via Semantic-aware Visual Objects"},{"paperId":"0a36008613d67fb3aac8345f847fc4787a0d69f3","title":"PixLore: A Dataset-driven Approach to Rich Image Captioning"},{"paperId":"08cece40d3bfbb649cc4818db91c2d8242bd07f6","title":"VRPTEST: Evaluating Visual Referring Prompting in Large Multimodal Models"},{"paperId":"2192512a33548a34e47d7b709c9d4d0b09936055","title":"Digital Life Project: Autonomous 3D Characters with Social Intelligence"},{"paperId":"0b46b666ee076185bea1a453175e43e22706d8ae","title":"Text as Image: Learning Transferable Adapter for Multi-Label Classification"},{"paperId":"fb064f8376eba221245c551cc028e4dbcb9f043e","title":"Prompt Highlighter: Interactive Control for Multi-Modal LLMs"},{"paperId":"205ada2927972cb3156e94247e29d454bf620399","title":"Towards Knowledge-driven Autonomous Driving"},{"paperId":"e1a083f8a8faa6b86a381c4a76fc5f4d16308bb5","title":"Quilt-LLaVA: Visual Instruction Tuning by Extracting Localized Narratives from Open-Source Histopathology Videos"},{"paperId":"06d2de84ae766fb5483796fd2b1edd836ae46525","title":"Hijacking Context in Large Multi-modal Models"},{"paperId":"e775684d28e92be78704df7cb82fc64cf9b2538c","title":"GPT-4V with Emotion: A Zero-shot Benchmark for Generalized Emotion Recognition"},{"paperId":"b2c426ae8aff6fd32c850c5fb24266b47417d96e","title":"An Integration of Pre-Trained Speech and Language Models for End-to-End Speech Recognition"},{"paperId":"53c3c3984649ca82a2f85629dae01087e9e72991","title":"OneLLM: One Framework to Align All Modalities with Language"},{"paperId":"91159f6d3d52e6cfed1e4d1c6e50d1b17086a910","title":"On the Robustness of Large Multimodal Models Against Image Adversarial Attacks"},{"paperId":"b92289123a94f6076505487adfb4513bd3495c1d","title":"LEGO: Learning EGOcentric Action Frame Generation via Visual Instruction Tuning"},{"paperId":"d77bc1a237b67c57b0c1b99b4802e703747a9688","title":"BenchLMM: Benchmarking Cross-style Visual Capability of Large Multimodal Models"},{"paperId":"1eea88f53291103a6e999b2bf64484cd705300a4","title":"Lenna: Language Enhanced Reasoning Detection Assistant"},{"paperId":"4ada01c0cc79ed4abe081bd59300247626501808","title":"GPT4Point: A Unified Framework for Point-Language Understanding and Generation"},{"paperId":"adee8299464cbd308db9d6f2daf73c7dbf83977a","title":"UPOCR: Towards Unified Pixel-Level OCR Interface"},{"paperId":"769b794fe9f97268007676171f246d45e0631014","title":"Towards More Unified In-context Visual Understanding"},{"paperId":"1c5d887ec6c4d57bfee29fa632a96a75129a50ce","title":"LLaVA-Grounding: Grounded Visual Chat with Large Multimodal Models"},{"paperId":"f32ea390686b1eee3ba5b53c7a85e9e9385d4b94","title":"Visual Program Distillation: Distilling Tools and Programmatic Reasoning into Vision-Language Models"},{"paperId":"d839519477bd2f2055ec189f796bce578c578102","title":"Foundation Models for Weather and Climate Data Understanding: A Comprehensive Survey"},{"paperId":"41fcd41e01ce316172448646f2ee732602531d2a","title":"Breast Ultrasound Report Generation using LangChain"},{"paperId":"0e8dec431a62dea147139d7805ab3a0a97bf3857","title":"LLaRA: Aligning Large Language Models with Sequential Recommenders"},{"paperId":"54906ca77682c1cdef779e2121ef8919dd397dfe","title":"CLAMP: Contrastive LAnguage Model Prompt-tuning"},{"paperId":"b9401a81dca0bebfc1d602843ddc43ac1b4a3d1b","title":"InstructTA: Instruction-Tuned Targeted Attack for Large Vision-Language Models"},{"paperId":"ef4e4e4b52d4379ab5387d8dc53da87e561e78db","title":"Good Questions Help Zero-Shot Image Reasoning"},{"paperId":"eca8a3e6383e3618e0bc984382e08c09be3cca6c","title":"TimeChat: A Time-sensitive Multimodal Large Language Model for Long Video Understanding"},{"paperId":"24b489376adf63bfae2fd1852c7f9fe7227d154c","title":"Bootstrapping SparseFormers from Vision Foundation Models"},{"paperId":"d8aca097ddc0062828948655d3a97e4204881582","title":"Towards Learning a Generalist Model for Embodied Navigation"},{"paperId":"a6fb97da5ccf224eb6ecaff10b7518ddcbb86ed8","title":"Semantics-aware Motion Retargeting with Vision-Language Models"},{"paperId":"60b460c7939f828e156508dcf6b5f532589bc6cf","title":"MedXChat: Bridging CXR Modalities with a Unified Multimodal Large Model"},{"paperId":"adb969668f191839d273af5743948ed10b28c43c","title":"PixelLM: Pixel Reasoning with Large Multimodal Model"},{"paperId":"e49cb2ab3a7990e3d05042197ae8b3fd934453de","title":"StoryGPT-V: Large Language Models as Consistent Story Visualizers"},{"paperId":"57427ef02d111ba8dfe270f0db970f56eeb8f826","title":"Language-driven All-in-one Adverse Weather Removal"},{"paperId":"2566aee8cd777abbf9f27aaf8315bf6c3962a255","title":"QPoser: Quantized Explicit Pose Prior Modeling for Controllable Pose Generation"},{"paperId":"0f9a94ae9c99fa4205bab07b3930d8c8c041f058","title":"Making Large Multimodal Models Understand Arbitrary Visual Prompts"},{"paperId":"c95c4fb96868d6512c32988632a7b101a42c455d","title":"Dolphins: Multimodal Language Model for Driving"},{"paperId":"0f9a3c5c6a54fca6be2afa0fd5fd34eed96a31e8","title":"RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from Fine-grained Correctional Human Feedback"},{"paperId":"cc9627c4475b9ad1c7e4105f3bceb1e7b59b7cab","title":"Zero-Shot Video Question Answering with Procedural Programs"},{"paperId":"339ec34efdccdf2bf43bb817ef7cab5058bfa2e7","title":"Segment and Caption Anything"},{"paperId":"9e2bac2777eebe603a39f69221689493609d4149","title":"MLLMs-Augmented Visual-Language Representation Learning"},{"paperId":"4673c2ac4abb4b055da87171231acb60801ffe74","title":"PoseGPT: Chatting about 3D Human Pose"},{"paperId":"17ff6a0844afe74796022e7aaf372553e9303d72","title":"VTimeLLM: Empower LLM to Grasp Video Moments"},{"paperId":"fc53f8f3a84f1fc4993689d8f98cf6551d07a22d","title":"LL3DA: Visual Interactive Instruction Tuning for Omni-3D Understanding, Reasoning, and Planning"},{"paperId":"40de3296157b9d7a7882b61f967e37b3cc93f197","title":"Merlin: Empowering Multimodal LLMs with Foresight Minds"},{"paperId":"ed4e20bcd73b1138d3bb2ed4dbbf5e8b224ef5c7","title":"mPLUG-PaperOwl: Scientific Diagram Analysis with the Multimodal Large Language Model"},{"paperId":"2b41b3e23d6b8b84445abb77870aa3b9e71c75d8","title":"Contrastive Vision-Language Alignment Makes Efficient Instruction Learner"},{"paperId":"49b79d61ffc2db6dce8c2cd9cda06e1876ed8b4c","title":"OPERA: Alleviating Hallucination in Multi-Modal Large Language Models via Over-Trust Penalty and Retrospection-Allocation"},{"paperId":"09157a8c0e7d7263ac035690118ddcbe295cee5c","title":"ShapeGPT: 3D Shape Generation with A Unified Multi-modal Language Model"},{"paperId":"74423a9ee66085e74cd2b2e42303f28359c74eb6","title":"Query-Relevant Images Jailbreak Large Multi-Modal Models"},{"paperId":"ff9a7018f392d9e639a5fc8f59765967f1d28d7f","title":"VIM: Probing Multimodal Large Language Models for Visual Embedded Instruction Following"},{"paperId":"37118f07979c4fe7382c2cfff9c98654a128ac1d","title":"Non-Visible Light Data Synthesis and Application: A Case Study for Synthetic Aperture Radar Imagery"},{"paperId":"679105b4343f316cab0c2e1ce3be0ed498341b86","title":"VITATECS: A Diagnostic Dataset for Temporal Concept Understanding of Video-Language Models"},{"paperId":"22d55c52f43f59634586ab95fefbb7dba8c8b190","title":"ChatIllusion: Efficient-Aligning Interleaved Generation ability with Visual Instruction Model"},{"paperId":"4b08aaa03f6998236407d61612c7e55f01c308e8","title":"Understanding and Improving In-Context Learning on Vision-language Models"},{"paperId":"498decc50ccea9293f63a98c30d7c3439be074b7","title":"Look Before You Leap: Unveiling the Power of GPT-4V in Robotic Vision-Language Planning"},{"paperId":"ea3448eb86a233189631d914721e587d45931b64","title":"MVBench: A Comprehensive Multi-modal Video Understanding Benchmark"},{"paperId":"2baf63dede1a96cae314c4be99bd3cf9f49b148e","title":"Beyond Hallucinations: Enhancing LVLMs through Hallucination-Aware Direct Preference Optimization"},{"paperId":"486c2df78cbb770a90a55f7fa3fe19102fba2c24","title":"LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models"},{"paperId":"328eb183007bf4aefbf42437b42a15db375803e3","title":"Mitigating Object Hallucinations in Large Vision-Language Models through Visual Contrastive Decoding"},{"paperId":"425f1edd88fe3539c40ddd93c3e07c95de67ba00","title":"Embodied Multi-Modal Agent trained by an LLM from a Parallel TextWorld"},{"paperId":"154cc4e8a9e8ad24d4f9c9440b187d06b9ba57bd","title":"SEED-Bench-2: Benchmarking Multimodal Large Language Models"},{"paperId":"ef7097244ee0cc2d54649e7bec121abf7c628947","title":"A Survey of the Evolution of Language Model-Based Dialogue Systems"},{"paperId":"012e24f7c4dc92f26b41cacca501c0d4355b02fd","title":"Towards Vision Enhancing LLMs: Empowering Multimodal Knowledge Storage and Sharing in LLMs"},{"paperId":"6cc00fe6d7a78d166327c95e5ad4ebe4654b71d2","title":"Can Vision-Language Models Think from a First-Person Perspective?"},{"paperId":"1c9060b7246bae6f5000ddd3041b54619243e2bf","title":"EVCap: Retrieval-Augmented Image Captioning with External Visual-Name Memory for Open-World Comprehension"},{"paperId":"bee0116321a62fd7194ff3bbbd0dbf2a917d4403","title":"Beyond Pixels: Exploring Human-Readable SVG Generation for Simple Images with Vision Language Models"},{"paperId":"73f082fc7df9f2b9f3bf7dafb7c4422bb7aae968","title":"How Many Unicorns Are in This Image? A Safety Evaluation Benchmark for Vision LLMs"},{"paperId":"ed2a6cebe1d0ebfe4dfe3fe42fd6bb5d8206a2fa","title":"ViT-Lens-2: Gateway to Omni-modal Intelligence"},{"paperId":"fe92a35c51ebba91ed99f7da0e0124434229a469","title":"Continual Instruction Tuning for Large Multimodal Models"},{"paperId":"2b3554a8fea6f123fc04bd3e120f2293f227e1b2","title":"InstructMol: Multi-Modal Integration for Building a Versatile and Reliable Molecular Assistant in Drug Discovery"},{"paperId":"769a924d0af014acec326f50c15c5d70d258a969","title":"LLMGA: Multimodal Large Language Model based Generation Assistant"},{"paperId":"40cd34f260d5596263654caf9d911d4355bf4f4e","title":"ChartLlama: A Multimodal LLM for Chart Understanding and Generation"},{"paperId":"5eea245cc12c55905d4df827d0c9776c5ddfa743","title":"Compositional Chain-of-Thought Prompting for Large Multimodal Models"},{"paperId":"a756b584f8f8b4307e52895ae2120bc339580ad8","title":"See and Think: Embodied Agent in Virtual Environment"},{"paperId":"6c6ed197443a857edfa55029141b90a18c8cfac5","title":"Generating Human-Centric Visual Cues for Human-Object Interaction Detection via Large Vision-Language Models"},{"paperId":"4d51f4698725ad73be4d8e096ea755b5ed9eb45e","title":"Mug-STAN: Adapting Image-Language Pretrained Models for General Video Understanding"},{"paperId":"76803ea9ccc93a4f277aae5d4714ea79a99e55d6","title":"Griffon: Spelling out All Object Locations at Any Granularity with Large Language Models"},{"paperId":"ed4f1b0f6c09f59d07e817e532a25f4d25e94dbc","title":"Paragraph-to-Image Generation with Information-Enriched Diffusion Model"},{"paperId":"d0a5a6a5b5540967d6f12651aebd446e4c0dc807","title":"GeoChat: Grounded Large Vision-Language Model for Remote Sensing"},{"paperId":"6692fc513f8b649fef164704422647c55bd744eb","title":"GPT-4V Takes the Wheel: Evaluating Promise and Challenges for Pedestrian Behavior Prediction"},{"paperId":"3f5c903024a0f94a03191c6999fba0f2760e4d42","title":"A Systematic Review of Deep Learning-based Research on Radiology Report Generation"},{"paperId":"6985a3d0b4a3cc9e8facf19ca7a477a843f387ab","title":"Compositional Zero-shot Learning via Progressive Language-based Observations"},{"paperId":"cf193b5b34178a444cb9bd9f51beb4124b753935","title":"HalluciDoctor: Mitigating Hallucinatory Toxicity in Visual Instruction Data"},{"paperId":"4edbb942c2d20a6f5a4e3caa763a9761be953231","title":"PG-Video-LLaVA: Pixel Grounding Large Video-Language Models"},{"paperId":"52941cadbd340344f3e0a6f50719fe55b3de5088","title":"Multimodal Large Language Models: A Survey"},{"paperId":"fb01eb4411e9563ae90fde80d24d3118e54df5e4","title":"Towards Improving Document Understanding: An Exploration on Text-Grounding via MLLMs"},{"paperId":"3039e5c8bd6147b6ee08f0f50d52047cc3be2372","title":"ADriver-I: A General World Model for Autonomous Driving"},{"paperId":"c4b7f1ceef3f91619e80a040dbc5a9fdbd32ab22","title":"Physical Reasoning and Object Planning for Household Embodied Agents"},{"paperId":"f68f6f2a057c4e6e5a3c91fc8563533d9bf6e560","title":"ShareGPT4V: Improving Large Multi-Modal Models with Better Captions"},{"paperId":"f23dc380e870c47d5cc778cf88c3ac1f4dbbfa8e","title":"Towards Natural Language-Guided Drones: GeoText-1652 Benchmark with Spatially Relation Matching"},{"paperId":"451539c0d0f5f5785ff58d09ca5e67a5f129f9de","title":"A Survey on Multimodal Large Language Models for Autonomous Driving"},{"paperId":"7f807249c0ef0fe07d5e9c810684cd5daba0edc5","title":"De-fine: Decomposing and Refining Visual Programs with Auto-Feedback"},{"paperId":"12f50bf46bb64952bbba8ebf77ffef5da27f2971","title":"GeoLocator: a location-integrated large multimodal model for inferring geo-privacy"},{"paperId":"3dee0acc8728655b3458704ca90778ec0b28758b","title":"LLMs as Visual Explainers: Advancing Image Classification with Evolving Visual Descriptions"},{"paperId":"dfa7120276a0a5d36c40de13278c9884305b7c7d","title":"DocPedia: Unleashing the Power of Large Multimodal Model in the Frequency Domain for Versatile Document Understanding"},{"paperId":"5f370f52e24d185ae44bb0ea18cbd4be2aab0d15","title":"VLM-Eval: A General Evaluation on Video Large Language Models"},{"paperId":"0c7ca22dfbe1ce02cb0f0658292499457db8ec6e","title":"InfiMM-Eval: Complex Open-Ended Reasoning Evaluation For Multi-Modal Large Language Models"},{"paperId":"56025f5034f7aebe1b7292284d33d3d0e3317614","title":"Deep Tensor Network"},{"paperId":"8ece28f449ee8f1109ce0bae8f3375ef9e08b4b0","title":"Behavior Optimized Image Generation"},{"paperId":"391eaeb1092c2b145ff0e5a2fa61637a42921fce","title":"DRESS: Instructing Large Vision-Language Models to Align and Interact with Humans via Natural Language Feedback"},{"paperId":"6dc8f09f3f39d41c21ec167b8a0ed9f12552f487","title":"Trustworthy Large Models in Vision: A Survey"},{"paperId":"20294c321ffb5b61edcc2bc1c8ac66265a22c243","title":"UnifiedVisionGPT: Streamlining Vision-Oriented AI through Generalized Multimodal Framework"},{"paperId":"107fb6eec2febbae12db29bf3e311aaf5680027c","title":"Video-LLaVA: Learning United Visual Representation by Alignment Before Projection"},{"paperId":"87089dd53f6279b0c348b78d7cc19989349b48e7","title":"GRASP: A novel benchmark for evaluating language GRounding And Situated Physics understanding in multimodal language models"},{"paperId":"16513bc0dc13902334a9cb3657056763efdcec6f","title":"Towards Open-Ended Visual Recognition with Large Language Model"},{"paperId":"aad3d2e690f6c73f04a14622ceff51464bbc560e","title":"Chat-UniVi: Unified Visual Representation Empowers Large Language Models with Image and Video Understanding"},{"paperId":"3cc6c4e88b6135d53938315a262285a501803c48","title":"Vision-Language Instruction Tuning: A Review and Analysis"},{"paperId":"619184447595337a9fe3dca72c4e951e7ab7467c","title":"To See is to Believe: Prompting GPT-4V for Better Visual Instruction Tuning"},{"paperId":"d3367dc9a7a1d7ae70a06eadc02b2430f4529f7c","title":"Large Language Models for Robotics: A Survey"},{"paperId":"b835f2c5ed8fe9cf38ec020feb78c9797f2fd772","title":"Towards General-Purpose Speech Abilities for Large Language Models Using Unpaired Data"},{"paperId":"ef321c6f174ac59916ac54ec40ad18bca5b58e5c","title":"PerceptionGPT: Effectively Fusing Visual Perception into LLM"},{"paperId":"bf14244669d5505f63343d4365d99d24aa6c5e82","title":"Monkey: Image Resolution and Text Label Are Important Things for Large Multi-modal Models"},{"paperId":"8ec7d50250203543a0098d99f04957b22bbe2c77","title":"How to Bridge the Gap between Modalities: A Comprehensive Survey on Multimodal Large Language Model"},{"paperId":"b78b5ce5f21f46d8149824463f8eebd6103d49aa","title":"FigStep: Jailbreaking Large Vision-language Models via Typographic Visual Prompts"},{"paperId":"07f9e1d3288b22cda2c980cfb969dcf410e5bd9e","title":"u-LLaVA: Unifying Multi-Modal Tasks via Large Language Model"},{"paperId":"abb312b7cf508b91ae7a88b1890c13eb8b96ad1a","title":"NExT-Chat: An LMM for Chat, Detection and Segmentation"},{"paperId":"ad13b213681b6f634bc83a264df246e83dd9a9d9","title":"mPLUG-Owl2: Revolutionizing Multi-modal Large Language Model with Modality Collaboration"},{"paperId":"6ae4705139494fcb6b790b6dd6c4225b40ee40f8","title":"GLaMM: Pixel Grounding Large Multimodal Model"},{"paperId":"2313afae52d98e569da2dedbf14daf9efc74e7cf","title":"CogVLM: Visual Expert for Pretrained Language Models"},{"paperId":"8d2709ed1788a67e64425fb410bb49f3ee49e088","title":"Large Language Models Illuminate a Progressive Pathway to Artificial Healthcare Assistant: A Review"},{"paperId":"0b3e7b5cbef627b1ceedceadc5f58787f432163b","title":"What Makes for Good Visual Instructions? Synthesizing Complex Visual Reasoning Instructions for Visual Instruction Tuning"},{"paperId":"fbae34c21a6a0cbf3f9e2710b7fce0e011aec72c","title":"FAITHSCORE: Evaluating Hallucinations in Large Vision-Language Models"},{"paperId":"b08ea5d95445bf84003604472d312b6506bc5f56","title":"The Development of LLMs for Embodied Navigation"},{"paperId":"1f5e1a036b24b9dd34c006ba3bb61119624f4fdb","title":"A Comprehensive Study of GPT-4V's Multimodal Capabilities in Medical Imaging"},{"paperId":"56cb3506586c540d4bd2c573fad66d22dff69826","title":"CapsFusion: Rethinking Image-Text Data at Scale"},{"paperId":"bd1818d345acb805ba7f0b9643852a52ccd765f9","title":"Myriad: Large Multimodal Model by Applying Vision Experts for Industrial Anomaly Detection"},{"paperId":"e6f93ae09064de1cdba070ad71de69655cb44a01","title":"Image Clustering Conditioned on Text Criteria"},{"paperId":"b8d70fbf2d57a3cc6c858655bbb0f65a69e31eff","title":"Collaborative Multimodal Diagnostic: Fusion of Pathological Labels and Vision-Language Model"},{"paperId":"f8b8f926bbfa327c86c40796131fe2695db81126","title":"DDCoT: Duty-Distinct Chain-of-Thought Prompting for Multimodal Reasoning in Language Models"},{"paperId":"da9134f694959b68027c33c8e998ffb3d41305da","title":"Exploring Question Decomposition for Zero-Shot VQA"},{"paperId":"807f336176070bd3f95b82a16f125ee99b7d2c80","title":"Woodpecker: Hallucination Correction for Multimodal Large Language Models"},{"paperId":"0b395ed1c8b284e551172b728e83cf257e33729a","title":"HallusionBench: An Advanced Diagnostic Suite for Entangled Language Hallucination&Visual Illusion in Large Vision-Language Models"},{"paperId":"f72be31de9f9a09d4410fd38bc717efe43444827","title":"SALMONN: Towards Generic Hearing Abilities for Large Language Models"},{"paperId":"dacb85537b0a48285209c7c1737db908182f1cdb","title":"InViG: Benchmarking Interactive Visual Grounding with 500K Human-Robot Interactions"},{"paperId":"180c106603124cbee14af9a5ea95925c29caa575","title":"Towards Training-free Open-world Segmentation via Image Prompting Foundation Models"},{"paperId":"4f63c5a89c7299a864c6c48aa1844fb0fe8c9437","title":"Survey of Vulnerabilities in Large Language Models Revealed by Adversarial Attacks"},{"paperId":"34a4229372313f3741c579c9f48c8687e40f1f1b","title":"Interpreting and Controlling Vision Foundation Models via Text Explanations"},{"paperId":"36b923d97d7cfaf73d11c55c15ea46605ba974a5","title":"BiLL-VTG: Bridging Large Language Models and Lightweight Visual Tools for Video-based Texts Generation"},{"paperId":"208b93a39802466785169494caa7f2a8995ea39f","title":"Large Language Model-Empowered Agents for Simulating Macroeconomic Activities"},{"paperId":"671ee2b83b3489ce9b3b3b41162ec3c4a2bf9c59","title":"A Survey on Video Diffusion Models"},{"paperId":"4cb2c262ce34f41974f1b1623fc5a6e32956ded3","title":"LLM Blueprint: Enabling Text-to-Image Generation with Complex and Detailed Prompts"},{"paperId":"9e6e6e3b9680e4d4ee15fa6ed5a3a178371f4cf4","title":"LLM4SGG: Large Language Model for Weakly Supervised Scene Graph Generation"},{"paperId":"ac2e5bf716aed246ca8914a6816ef73e00286099","title":"Beyond Segmentation: Road Network Generation with Multi-Modal LLMs"},{"paperId":"b217b6bc340af9a10bebbf8acc36ea30871769bd","title":"In-Context Learning with Iterative Demonstration Selection"},{"paperId":"1ddbd08ad8cf22a5c66c4242194c4286328533bf","title":"MiniGPT-v2: large language model as a unified interface for vision-language multi-task learning"},{"paperId":"8ae6b41e4dfa7b27dcc34eb3ae39351a5d1b4698","title":"JM3D&JM3D-LLM: Elevating 3D Understanding with Joint Multi-modal Cues"},{"paperId":"e9d7fb9b2c41fc76e55cbe48b0d30a9a57a8e023","title":"From CLIP to DINO: Visual Encoders Shout in Multi-modal Large Language Models"},{"paperId":"54f60adbec5af199907c07ed281ad893ccc00088","title":"MM-BigBench: Evaluating Multimodal Models on Multimodal Content Comprehension Tasks"},{"paperId":"b3e9f249dd2e09ec111496f6b533101e8217a5b0","title":"Multimodal Large Language Model for Visual Navigation"},{"paperId":"458111ac5a0f73bb35a2acf55298268be25ccfa2","title":"Ferret: Refer and Ground Anything Anywhere at Any Granularity"},{"paperId":"a710efa9247207a72f06e0c9db302fd3ecab5fbb","title":"Towards Robust Multi-Modal Reasoning via Model Selection"},{"paperId":"e8d513bc7554a83161f2fb26c8299b471581cdb6","title":"Can We Edit Multimodal Large Language Models?"},{"paperId":"da14841dca2b0316a5885b98ee2f35701d23af14","title":"Principles, applications, and future of artificial intelligence in dermatology"},{"paperId":"b3d08827560e65c57675c54aa90d282d47ac5d49","title":"AutoRepo: A general framework for multi-modal LLM-based automated construction reporting"},{"paperId":"56c26605712b3073e36a646b710ae4eb3764f8f1","title":"Knowledge Generation Pipeline using LLM for Building 3D Object Knowledge Base"},{"paperId":"91eaf9752002f6999a3bd8d33303cdcdbd135317","title":"LLark: A Multimodal Instruction-Following Language Model for Music"},{"paperId":"eab54bef4b479e0b06fc65c8cab2c0400fa69b8a","title":"On the Evaluation and Refinement of Vision-Language Instruction Tuning Datasets"},{"paperId":"582c2f270a6c0ce89679eebaa78797711fa20293","title":"Making Large Language Models Perform Better in Knowledge Graph Completion"},{"paperId":"6f2f20c78d311c4ce8be0bb6855177c5169bb6cd","title":"MuseChat: A Conversational Music Recommendation System for Videos"},{"paperId":"449e130b89564b60f183190da77b8261b4af9142","title":"Improving Compositional Text-to-image Generation with Large Vision-Language Models"},{"paperId":"28fbbf98bac1bb941162df553ca034d600cb59a6","title":"Rephrase, Augment, Reason: Visual Grounding of Questions for Vision-Language Models"},{"paperId":"69b90bd79bb0fc87d39180161926964ae9dd7cbc","title":"UReader: Universal OCR-free Visually-situated Language Understanding with Multimodal Large Language Model"},{"paperId":"471bb71c8797c253fe668af5899dfc7fc2ddd8ac","title":"Video-Teller: Enhancing Cross-Modal Generation with Fusion and Decoupling"},{"paperId":"e55d12cb0e92aa5f4f8ca5dc24d57bd0dcc861ae","title":"Counter Turing Test CT^2: AI-Generated Text Detection is Not as Easy as You May Think - Introducing AI Detectability Index"},{"paperId":"24dd96da6f700f57132713aeb5e9b06905abab5d","title":"HowToCaption: Prompting LLMs to Transform Video Annotations at Scale"},{"paperId":"9c7af6cef69c3417c4f57627332b1256f53c231a","title":"Tree-GPT: Modular Large Language Model Expert System for Forest Remote Sensing Image Understanding and Interactive Analysis"},{"paperId":"0e0e706e13f160e74cac9556f28ab9a358c148d2","title":"Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!"},{"paperId":"124d4d374fbef2016fa9880489871a58a7450644","title":"Improved Baselines with Visual Instruction Tuning"},{"paperId":"d813e7edec4836dbf9d35363a6ed02782df3d18c","title":"ReForm-Eval: Evaluating Large Vision Language Models via Unified Re-Formulation of Task-Oriented Benchmarks"},{"paperId":"477da0209e093448a8370862540182a1a77602fe","title":"ECoFLaP: Efficient Coarse-to-Fine Layer-Wise Pruning for Vision-Language Models"},{"paperId":"7b8a85e0da9ac4701640f9cd77b744a4dda80794","title":"Multimodal Question Answering for Unified Information Extraction"},{"paperId":"65a5a5cde93b65738d790b2f107b33010c605481","title":"On the Performance of Multimodal Language Models"},{"paperId":"77693ca00a8ef775af96b5c450aa0afdb0e10a51","title":"Talk2BEV: Language-enhanced Bird's-eye View Maps for Autonomous Driving"},{"paperId":"11ea5758186f09ab68e2c783e8202c507cec1974","title":"Tuning Large language model for End-to-end Speech Translation"},{"paperId":"bee68767debbdc96d6f75947e544a8be98b869e3","title":"Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model: Explorations with GPT4-Vision and Beyond"},{"paperId":"e7d09b6f2bc878cf2c993acf675f409d0b55f35a","title":"MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens"},{"paperId":"8946891e94831adc8cddb0d32311cce2445c96d2","title":"MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts"},{"paperId":"20ae101289965d36dbd93e9b8c47ec9deab03ed0","title":"What If the TV Was Off? Examining Counterfactual Reasoning Abilities of Multi-modal Language Models"},{"paperId":"388a5301b7020e1afb84a511ab57131d26e5e4ca","title":"Application of frozen large-scale models to multimodal task-oriented dialogue"},{"paperId":"ccd6f8b6544f112de632e49bfbe592a0a654537d","title":"DriveGPT4: Interpretable End-to-end Autonomous Driving via Large Language Model"},{"paperId":"5ba1525dc6d382ee0a4a1ca3c64fc5907ca64c67","title":"Making LLaMA SEE and Draw with SEED Tokenizer"},{"paperId":"12d8dfe6df1829b0ebe065621ed71c5e8196aad1","title":"Alignment and Generation Adapter for Efficient Video-text Understanding"},{"paperId":"07d639b011f48615c1154cb6cdbc067bfe331348","title":"Reformulating Vision-Language Foundation Models and Datasets Towards Universal Multimodal Assistants"},{"paperId":"696d6b667926a559e63989d45eec53c3a15986be","title":"Pink: Unveiling the Power of Referential Comprehension for Multi-modal LLMs"},{"paperId":"a5d27bf7a2155d4ca016565a78b52ee90f81624c","title":"Beyond Task Performance: Evaluating and Reducing the Flaws of Large Multimodal Models with In-Context Learning"},{"paperId":"2008356d334fb27ba7dbc638119f9d567d4adbd8","title":"Exploring Open-Vocabulary Semantic Segmentation from CLIP Vision Encoder Distillation Only"},{"paperId":"e448a0750f5ee1e54fd4fac374500f6e59270bcc","title":"TextField3D: Towards Enhancing Open-Vocabulary 3D Generation with Noisy Text Fields"},{"paperId":"092245d86b77181c36f972b1b7a17a59cd989c4a","title":"Guiding Instruction-based Image Editing via Multimodal Large Language Models"},{"paperId":"3cbfe152220de84ecf8059fa50c47587a3134c86","title":"DiLu: A Knowledge-Driven Approach to Autonomous Driving with Large Language Models"},{"paperId":"f2f9c02a7eb484dd7b7ac46892856e3f278eed77","title":"AnyMAL: An Efficient and Scalable Any-Modality Augmented Language Model"},{"paperId":"c1e450284e7d6cac1855330a1197df8537df653f","title":"InternLM-XComposer: A Vision-Language Large Model for Advanced Text-image Comprehension and Composition"},{"paperId":"96c43227831c4c3b12b7c64809e78674cea3a8a1","title":"DeepSpeed-VisualChat: Multi-Round Multi-Image Interleave Chat via Multi-Modal Causal Attention"},{"paperId":"593b42c628b49937bcd7f7c4a7d54d5f97e6b414","title":"Q-Bench: A Benchmark for General-Purpose Foundation Models on Low-level Vision"},{"paperId":"5596bd3e26ec2207666ec1ff3db4415d212f14b9","title":"Connecting Speech Encoder and Large Language Model for ASR"},{"paperId":"1498ed76251d53d5a2fd2c56f3ca877f7eb32ac6","title":"Species196: A One-Million Semi-supervised Dataset for Fine-grained Species Recognition"},{"paperId":"772724892819d7e6f15ce536753fdc32d022c0e0","title":"A Survey on Image-text Multimodal Models"},{"paperId":"9f4c17aebbb181756fab86ade02deadd90d5d4f9","title":"How Robust is Google's Bard to Adversarial Image Attacks?"},{"paperId":"7b689adb8c156d6158660f90d1c86888ee281f63","title":"DreamLLM: Synergistic Multimodal Comprehension and Creation"},{"paperId":"f34b6b4f75fb4e6f2c5654ee13fb2479c6170b3a","title":"Kosmos-2.5: A Multimodal Literate Model"},{"paperId":"c96297261467b5daa2d01227496a70d444602434","title":"Baichuan 2: Open Large-scale Language Models"},{"paperId":"72012855e57058c8adc04f71d6e5453819f715d1","title":"Language as the Medium: Multimodal Video Classification through text only"},{"paperId":"b1e36c125d0b0ef48bc55ae7b8a92c8afc40e300","title":"Bridging Zero-shot Object Navigation and Foundation Models through Pixel-Guided Navigation Skill"},{"paperId":"a281094d05e96b7cca044fdd87ff7c3c65649e20","title":"Investigating the Catastrophic Forgetting in Multimodal Large Language Models"},{"paperId":"7a7128e9696dfedc24bf432c4b4c3aafa5e95a1a","title":"An Empirical Study of Scaling Instruct-Tuned Large Multimodal Models"},{"paperId":"754d98b1baf2973167a4bf578d4e13cad587d2dc","title":"Triple Regression for Camera Agnostic Sim2Real Robot Grasping and Manipulation Tasks"},{"paperId":"18e73620979ac99d775251da435733661e549041","title":"Find What You Want: Learning Demand-conditioned Object Attribute Space for Demand-driven Navigation"},{"paperId":"ceeb63c64cc30edc21a92c454ea905770196b43f","title":"MusiLingo: Bridging Music and Text with Pre-trained Language Models for Music Captioning and Query Response"},{"paperId":"872cb74ebbef880bc682b5b32ec22170ae5d2545","title":"PatFig: Generating Short and Long Captions for Patent Figures"},{"paperId":"9d35d57ceba52ed4cea15355878f2e35495868e2","title":"Viewpoint Integration and Registration with Vision Language Foundation Model for Image Change Understanding"},{"paperId":"3803d1f291e162bdaa4678a2c5a2bbcf63c050f4","title":"MMICL: Empowering Vision-language Model with Multi-Modal In-Context Learning"},{"paperId":"366564d210768814bc880e391b909cfbd95f8964","title":"SwitchGPT: Adapting Large Language Models for Non-Text Outputs"},{"paperId":"4eb87eaa193929dbef93fa2db9419245a8e8916f","title":"TextBind: Multi-turn Interleaved Multimodal Instruction-following in the Wild"},{"paperId":"eae9ae43c5d4712e775912683268cf1ad02ff38f","title":"Sight Beyond Text: Multi-Modal Training Enhances LLMs in Truthfulness and Ethics"},{"paperId":"396305230ddcf915b19a19683a89e34d76321a33","title":"Cognitive Mirage: A Review of Hallucinations in Large Language Models"},{"paperId":"16ed5f612b66cb7d91e534dd7126b69756f45c34","title":"HiLM-D: Towards High-Resolution Understanding in Multimodal Large Language Models for Autonomous Driving"},{"paperId":"fa75a55760e6ea49b39b83cb85c99a22e1088254","title":"NExT-GPT: Any-to-Any Multimodal LLM"},{"paperId":"31abed8ed8b4308556d1708163e519f694701495","title":"ACIGS: An automated large-scale crops image generation system based on large visual language multi-modal models"},{"paperId":"bcac614f9774488447221ebb4f16f05e3975ec1e","title":"Unified Language-Vision Pretraining in LLM with Dynamic Discrete Visual Tokenization"},{"paperId":"280353fd7a7a3e49c415c443e1b7ccf7de9c2b4e","title":"Measuring and Improving Chain-of-Thought Reasoning in Vision-Language Models"},{"paperId":"aadd141d5853dabdc631b73d2bc8700e8ac7b8c0","title":"Reuse and Diffuse: Iterative Denoising for Text-to-Video Generation"},{"paperId":"54c68b8623505dc6bf7a0b08aaa77ca9165f2d7f","title":"ImageBind-LLM: Multi-modality Instruction Tuning"},{"paperId":"6770061933096bc52b4e2f817923c285be68204f","title":"Artificial general intelligence for radiation oncology"},{"paperId":"73814a52609a9ee4c8f1b115e376b6a300ab6a57","title":"CIEM: Contrastive Instruction Evaluation Method for Better Instruction Tuning"},{"paperId":"204fd6c5e247c477d607f507ee01d94a8dbd408f","title":"BLSP: Bootstrapping Language-Speech Pre-training via Behavior Alignment of Continuation Writing"},{"paperId":"22ebfc211d184ed615729378a43fde175bf14478","title":"Point-Bind & Point-LLM: Aligning Point Cloud with Multi-modality for 3D Understanding, Generation, and Instruction Following"},{"paperId":"355a3402d1a9f0c0199e1b8488ebeb5bc952b23a","title":"Large Content And Behavior Models To Understand, Simulate, And Optimize Content And Behavior"},{"paperId":"2ccdfdbf3bb408a1c74dde2dbb2cf8492b3591db","title":"Large language models in medicine: the potentials and pitfalls"},{"paperId":"1e7a2f9f9441462e92ee349f00414aff49617caa","title":"Enhancing Subtask Performance of Multi-modal Large Language Model"},{"paperId":"ee0c9e8a935e047f2c030ecbfad93c2d80d642d7","title":"Sparkles: Unlocking Chats Across Multiple Images for Multimodal Instruction-Following Models"},{"paperId":"f1dc0b8b844332f08b7503d9728e4831e8bd3607","title":"Expanding Frozen Vision-Language Models without Retraining: Towards Improved Robot Perception"},{"paperId":"6bcc6ab9c28805d4067e99b2cdc7524550fe80e1","title":"PointLLM: Empowering Large Language Models to Understand Point Clouds"},{"paperId":"2928e5a5ee488104c5d7b636f577baef2d470310","title":"TouchStone: Evaluating Vision-Language Models by Language Models"},{"paperId":"6660a20e26e9e8c9916ffdc488e925e313605d8d","title":"Prompting Vision Language Model with Knowledge from Large Language Model for Knowledge-Based VQA"},{"paperId":"7b22ecd9f1ced58c1704ac6191e029b98054e330","title":"LLaSM: Large Language and Speech Model"},{"paperId":"a4c52075ed4646f2c0562ece5081c0984461b9e8","title":"Learning modular language-conditioned robot policies through attention"},{"paperId":"f2ec0182c6646d3128afa5100f37d9de7b533463","title":"AnomalyGPT: Detecting Industrial Anomalies using Large Vision-Language Models"},{"paperId":"bb1083425517bdac8d9a6438fcf5032543acb20e","title":"Evaluation and Analysis of Hallucination in Large Vision-Language Models"},{"paperId":"894ed1aba8e42a4ec27ba53ecde383b14c5128ca","title":"Examining User-Friendly and Open-Sourced Large GPT Models: A Survey on Language, Multimodal, and Scientific GPT Models"},{"paperId":"58a282c89864f35bff1741f5ab439222da6bb3ec","title":"MLLM-DataEngine: An Iterative Refinement Approach for MLLM"},{"paperId":"e47d276bad18f441950c8136672ae6864e95323f","title":"VIGC: Visual Instruction Generation and Correction"},{"paperId":"fc6a2f7478f68adefd69e2071f27e38aa1647f2f","title":"Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond"},{"paperId":"ac1788e9a168a6455beb6316f316950842297c11","title":"Use of LLMs for Illicit Purposes: Threats, Prevention Measures, and Vulnerabilities"},{"paperId":"1245ef1926416d649b62323975c6fa22dfb885ee","title":"Large Multilingual Models Pivot Zero-Shot Multimodal Learning across Languages"},{"paperId":"d7e92d03dfa5427c0c5ef2b59de54733e0589606","title":"InstructionGPT-4: A 200-Instruction Paradigm for Fine-Tuning MiniGPT-4"},{"paperId":"53e8d327e7ceda6f4efd321752da57edbaee6257","title":"ROSGPT_Vision: Commanding Robots Using Only Language Models' Prompts"},{"paperId":"d955956378b40b23fa4b34098662c54a3b1fd64d","title":"LLaMA-Reviewer: Advancing Code Review Automation with Large Language Models through Parameter-Efficient Fine-Tuning"},{"paperId":"66d3b7a6561148fd21c364315e67bf9373f50ef7","title":"An Examination of the Compositionality of Large Generative Vision-Language Models"},{"paperId":"5690e35b8beab92a80055fe2530c29c24e495379","title":"On the Adversarial Robustness of Multi-Modal Foundation Models"},{"paperId":"f0950a3f27c0fefffba60ae1c9a8ee360d5eb55f","title":"Instruction Tuning for Large Language Models: A Survey"},{"paperId":"da96ec9c32d63292e506ba8f8ea8e838df998c02","title":"StableLLaVA: Enhanced Visual Instruction Tuning with Synthesized Image-Dialogue Data"},{"paperId":"53adaabb301a91c606dfef8fdae97b8b7c031e7b","title":"Imaginations of WALL-E : Reconstructing Experiences with an Imagination-Inspired Module for Advanced AI Systems"},{"paperId":"27e45a8aecc1fec246fd70c80d8f5104807cf0dd","title":"ViT-Lens: Towards Omni-modal Representations"},{"paperId":"30cc95639cffca4ffa8c0eafbc502636c0c88fa5","title":"BLIVA: A Simple Multimodal LLM for Better Handling of Text-Rich Visual Questions"},{"paperId":"e58b0ee9a1fdb15a72ee721053df3569127cde42","title":"Tackling Vision Language Tasks Through Learning Inner Monologues"},{"paperId":"3647e7c48fa4888ba584d8b8a5c5813e6ee48366","title":"UniDoc: A Universal Large Multimodal Model for Simultaneous Text Detection, Recognition, Spotting and Understanding"},{"paperId":"0815c5a05f50fc3405299abdb97cf1343ad63ac9","title":"Synergistic Integration of Large Language Models and Cognitive Architectures for Robust AI: An Exploratory Analysis"},{"paperId":"eb5cf10406a8ad31e0ebe56b36571d5db4758a62","title":"PUMGPT: A Large Vision-Language Model for Product Understanding"},{"paperId":"30cfc4e7174211aa48c965826d51db773f0d37c7","title":"Chat-3D: Data-efficiently Tuning Large Language Model for Universal Dialogue of 3D Scenes"},{"paperId":"d53945d4afb4528590d79e20de52883d29037e86","title":"FashionLOGO: Prompting Multimodal Large Language Models for Fashion Logo Embeddings"},{"paperId":"aae1d88c70cf18ef6aa23693a4dce8204e22d087","title":"A Bi-Step Grounding Paradigm for Large Language Models in Recommendation Systems"},{"paperId":"e2cc232ad999164f1bf340996bb5db62b6602d31","title":"Robustness Over Time: Understanding Adversarial Examples' Effectiveness on Longitudinal Versions of Large Language Models"},{"paperId":"1fd31b74f5e1eeb67341982fd35a613c6fad10e0","title":"Link-Context Learning for Multimodal LLMs"},{"paperId":"2e3dcf5a5d58ac210d0d87e9f918540a8373211a","title":"GIT-Mol: A Multi-modal Large Language Model for Molecular Science with Graph, Image, and Text"},{"paperId":"2854e5bab8e6f36e54c64456628a9559bf67019e","title":"IP-Adapter: Text Compatible Image Prompt Adapter for Text-to-Image Diffusion Models"},{"paperId":"d6c2523ab97416c2692cbbeab082ed1790e8e55e","title":"VisIT-Bench: A Benchmark for Vision-Language Instruction Following Inspired by Real-World Use"},{"paperId":"ea3e32ac8e2bc37251255df90f274530067c3164","title":"Encode-Store-Retrieve: Enhancing Memory Augmentation through Language-Encoded Egocentric Perception"},{"paperId":"74d245de70e9a9f11d6eaa72439004e5cc2fabaf","title":"Prompting In-Context Operator Learning with Sensor Data, Equations, and Natural Language"},{"paperId":"64a1de81466c354ea7b7cde87aec41f994db34d4","title":"Fine-Tune Language Models as Multi-Modal Differential Equation Solvers"},{"paperId":"cb712ab2b3bbaef6bff02efa7295ea420b58f654","title":"Fine-tuning Multimodal LLMs to Follow Zero-shot Demonstrative Instructions"},{"paperId":"1b656e3cdc49afab9ab9c1ec264850ef70153ecb","title":"Tiny LVLM-eHub: Early Multimodal Experiments with Bard"},{"paperId":"e1266706b4f83130a170e2b066bf65a1e6d72387","title":"Improving Generalization of Image Captioning with Unsupervised Prompt Learning"},{"paperId":"94972e30504017156ef5b5debc419bf6edc67384","title":"MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities"},{"paperId":"659a12d71d8709c132ccd9ccd235f0024cae0239","title":"The All-Seeing Project: Towards Panoptic Visual Recognition and Understanding of the Open World"},{"paperId":"1ee8c8dd9d04247515b33775532b72df7b8ec0f3","title":"RegionBLIP: A Unified Multi-modal Pre-training Framework for Holistic and Regional Comprehension"},{"paperId":"475a4adb2fb0b5b3d8c37f600ad8c53c1689c0c2","title":"More Context, Less Distraction: Zero-shot Visual Classification by Inferring and Conditioning on Contextual Attributes"},{"paperId":"ad113d8b6f8f6dace7abd12dc88af520aaaf3fc7","title":"LISA: Reasoning Segmentation via Large Language Model"},{"paperId":"6f9b7c8cde1be2e62a503c31cac883c6d44c9d0d","title":"MovieChat: From Dense Token to Sparse Memory for Long Video Understanding"},{"paperId":"0edcd1ce1d44359e8bf255b7216b9b56fa2cea33","title":"FinVis-GPT: A Multimodal Large Language Model for Financial Chart Analysis"},{"paperId":"1c4777a086d61ee4fdb8cd9cf88eff79d3d863be","title":"Towards General Visual-Linguistic Face Forgery Detection"},{"paperId":"4309d572a37d655779f9dce6a2c98c66334132de","title":"SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension"},{"paperId":"872c111c4bed5aba086cc023ce6279edb469220a","title":"RSGPT: A Remote Sensing Vision Language Model and Benchmark"},{"paperId":"92b9d8b8c81c4c53ea62000c0924500b2dd11bce","title":"Jailbreak in pieces: Compositional Adversarial Attacks on Multi-Modal Language Models"},{"paperId":"584ca135b61482fd89247113da87d784f738dbfa","title":"Foundational Models Defining a New Era in Vision: A Survey and Outlook"},{"paperId":"58d1a002a0ff0aa40b6633f0a7073d48f1cdff53","title":"Empower Your Model with Longer and Better Context Comprehension"},{"paperId":"dcf941c2985d8b585f7023305002dbfdbb5c9fcc","title":"Enhancing Human-like Multi-Modal Reasoning: A New Challenging Dataset and Comprehensive Framework"},{"paperId":"f63f37d981e2c36b1ea35f2025e55adee7906f69","title":"Prompting Large Language Models with Speech Recognition Abilities"},{"paperId":"2e52178416b93a0a7dc1cb7d21d04c7e72c2ce0f","title":"ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring Instruction Tuning"},{"paperId":"962ccf1fc49c83817fb031e5b24b81b19cdfb89d","title":"BuboGPT: Enabling Visual Grounding in Multi-Modal LLMs"},{"paperId":"40298b8d50109c52fc10763eddc64a07cf8acb31","title":"Planting a SEED of Vision in Large Language Model"},{"paperId":"37c4820eb7607c8f8de2b9bdab3918035ef1c038","title":"Sim2Plan: Robot Motion Planning via Message Passing between Simulation and Reality"},{"paperId":"369b449415d50387fba048bbd4d26ee890df84b5","title":"InternVid: A Large-scale Video-Text Dataset for Multimodal Understanding and Generation"},{"paperId":"41c6028c620debae00ca5b30e2db5977225fec57","title":"mBLIP: Efficient Bootstrapping of Multilingual Vision-LLMs"},{"paperId":"b37b1dc72b1882858f5120f2cd6883134089a6ed","title":"MMBench: Is Your Multi-modal Model an All-around Player?"},{"paperId":"43dcf4e7f672b58981b885435ce083e6b069bfae","title":"T2I-CompBench: A Comprehensive Benchmark for Open-world Compositional Text-to-image Generation"},{"paperId":"ca31b8584b6c022ef15ddfe994fe361e002b7729","title":"A Comprehensive Overview of Large Language Models"},{"paperId":"94053805cd59f2e9a47fe3f080c7e7afefb337cc","title":"Generative Pretraining in Multimodality"},{"paperId":"085ce7865e7492487f2f367f39d22a44bc64d151","title":"Linear Alignment of Vision-language Models for Image Captioning"},{"paperId":"451a3f03aca4aa87b93981364842137417549e58","title":"SVIT: Scaling up Visual Instruction Tuning"},{"paperId":"8e1868f84091272544cb4209c4ccaad7cc88af27","title":"On Decoder-Only Architecture For Speech-to-Text and Large Language Model Integration"},{"paperId":"094883e42bb9a41f602c0715c1059bc431e33fb2","title":"GPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest"},{"paperId":"c43fde4e5edcc52d2612d3f96374190e8a5376e6","title":"AutoDecoding Latent 3D Diffusion Models"},{"paperId":"ebddfdc5d845a788e8062eddbbf7a335737cb99b","title":"What Matters in Training a GPT4-Style Language Model with Multimodal Inputs?"},{"paperId":"1e3ef48abeef882e12f9553a1baf8944f3782c88","title":"Several categories of Large Language Models (LLMs): A Short Survey"},{"paperId":"ea566f87f6253bb2d32cf7b61cd3e2535a0c3f42","title":"mPLUG-DocOwl: Modularized Multimodal Large Language Model for Document Understanding"},{"paperId":"df710c46594c04fb59ef9a93d3b4e1cb387a1b2b","title":"Embodied Task Planning with Large Language Models"},{"paperId":"da08e9f21ef361e0e1242f8849a18a4ea1a3d27e","title":"SCITUNE: Aligning Large Language Models with Scientific Multimodal Instructions"},{"paperId":"d8156b4043c2eeba96d52fd31d215c607c860fb1","title":"RefSAM: Efficiently Adapting Segmenting Anything Model for Referring Video Object Segmentation"},{"paperId":"c68d3127888912ead55096d30165de783fe24216","title":"JourneyDB: A Benchmark for Generative Image Understanding"},{"paperId":"82a9b8984e26fdf234431459bdb445fbcfc3cb76","title":"Visual Instruction Tuning with Polite Flamingo"},{"paperId":"44bdd340aa7d54c3afb1831ffe6b6a8035b41200","title":"UMASS_BioNLP at MEDIQA-Chat 2023: Can LLMs generate high-quality synthetic note-oriented doctor-patient conversations?"},{"paperId":"a9d5d97733ccb15002ff3cfb95b0a7d8ba5236e3","title":"LLaVAR: Enhanced Visual Instruction Tuning for Text-Rich Image Understanding"},{"paperId":"efc694164312006c543ef745611348ef64e68dda","title":"Towards Language Models That Can See: Computer Vision Through the LENS of Natural Language"},{"paperId":"c94f0acf00530dbf9f275dad8515e23dc30666d3","title":"Prompting Large Language Models for Zero-Shot Domain Adaptation in Speech Recognition"},{"paperId":"e2a58fd18961c3941102989e3a3d0d27c615e015","title":"Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic"},{"paperId":"899316d60fde583dea135b82dc8506024b48bb3b","title":"Explainable Multimodal Emotion Reasoning"},{"paperId":"8724579d3f126e753a0451d98ff57b165f722e72","title":"Are aligned neural networks adversarially aligned?"},{"paperId":"c7a7104df3db13737a865ede2be8146990fa4026","title":"Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning"},{"paperId":"cde934546bbdb19094d8a53cc047d002c827f884","title":"Large Multimodal Models: Notes on CVPR 2023 Tutorial"},{"paperId":"ebedc4d7a2356090904baba4104ef0832bc236df","title":"A Survey on Multimodal Large Language Models"},{"paperId":"697e0add95e880bd42e00bef838181e105f91981","title":"MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models"},{"paperId":"142e934dd5d6c53f877c30243d436255e3a0dde7","title":"Visual Adversarial Examples Jailbreak Aligned Large Language Models"},{"paperId":"7839d037bb0e41f8a9898f177d2710cfe23633fc","title":"Path to Medical AGI: Unify Domain-specific Medical LLMs with the Lowest Cost"},{"paperId":"bc2333c9a667af90ee7ce52b911d2e04aed01526","title":"MotionGPT: Finetuned LLMs are General-Purpose Motion Generators"},{"paperId":"8efc20988021ce3b4b05dd44b13e27260ee9b99b","title":"Investigating Prompting Techniques for Zero- and Few-Shot Visual Question Answering"},{"paperId":"859baf28d0c2530307f3242ae7662a4dee89acd1","title":"Sample-Efficient Learning of Novel Visual Concepts"},{"paperId":"a8d02ff6d075c3cc48f0b97801cc52765c8f9ac9","title":"LVLM-eHub: A Comprehensive Evaluation Benchmark for Large Vision-Language Models"},{"paperId":"0983883619a0ca597d055d0e58da2f514052913d","title":"Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration"},{"paperId":"6f4974c33e758ddd9524684744407b7de5525caf","title":"Stochastic Re-weighted Gradient Descent via Distributionally Robust Optimization"},{"paperId":"b634f9ba35123d40f0af8d96a9c154025cf2cf2a","title":"Contrasting Intra-Modal and Ranking Cross-Modal Hard Negatives to Enhance Visio-Linguistic Compositional Understanding"},{"paperId":"9e8b7b0d4c628c12b6a65ab56ac5f33a35eff2e6","title":"Unifying Large Language Models and Knowledge Graphs: A Roadmap"},{"paperId":"966852963a88a28786b798c91b6662d6e501e590","title":"AssistGPT: A General Multi-modal Assistant that can Plan, Execute, Inspect, and Learn"},{"paperId":"051549d8ef56937b2f4d113afdcf8c7586d3770b","title":"Towards AGI in Computer Vision: Lessons Learned from GPT and Large Language Models"},{"paperId":"66d7d8dc54ea3dff10a11df2f29dc2104df86a57","title":"XrayGPT: Chest Radiographs Summarization using Medical Vision-Language Models"},{"paperId":"79150cb420d15830c8d36f0e91eea1b02e177f0f","title":"Sticker820K: Empowering Interactive Retrieval with Stickers"},{"paperId":"4c4d176c6e28f48041f215d563f6ee8633534cff","title":"Valley: Video Assistant with Large Language model Enhanced abilitY"},{"paperId":"fd755dc7b5b206c17fd953db04e1c888d45b6e4e","title":"LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark"},{"paperId":"073e4f0c3a66b7557abd053301b5104cdc582636","title":"Empowering Molecule Discovery for Molecule-Caption Translation with Large Language Models: A ChatGPT Perspective"},{"paperId":"d198b0b155313afe350e91a77c3d73cffa39d2a9","title":"Leveraging Large Language Models for Scalable Vector Graphics-Driven Image Understanding"},{"paperId":"bf7025a2e5dbb3c09deae02a1aa98a256ca559e2","title":"Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models"},{"paperId":"d47524cd5c3c4b57af2e5a29f6f91c420310f236","title":"MIMIC-IT: Multi-Modal In-Context Instruction Tuning"},{"paperId":"d818f40ea693a335e02f32dab520351d271c58bf","title":"Artificial General Intelligence for Medical Imaging"},{"paperId":"6a2a756c60dbc99f666ae6e32b0dd1a58e1e2de8","title":"M3IT: A Large-Scale Dataset towards Multi-Modal Multilingual Instruction Tuning"},{"paperId":"d7a4b09a0e2c2d7b118144cf09895c640896da7b","title":"Youku-mPLUG: A 10 Million Large-scale Chinese Video-Language Dataset for Pre-training and Benchmarks"},{"paperId":"5d321194696f1f75cf9da045e6022b2f20ba5b9c","title":"Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding"},{"paperId":"0244aeb7c6927e2fb0c2e668687e160a00737dbe","title":"Orca: Progressive Learning from Complex Explanation Traces of GPT-4"},{"paperId":"42ea55edb46395469aee1b760829657e65ab6577","title":"Zero-Shot 3D Shape Correspondence"},{"paperId":"31a68755ca6899e6c360ec8568704ae74f223a25","title":"GPT4Image: Can Large Pre-trained Models Help Vision Models on Perception Tasks?"},{"paperId":"0867f7029b3726740fb41ca8171833bf6f82e483","title":"Exploring Open-Vocabulary Semantic Segmentation without Human Labels"},{"paperId":"aa828072e36be23887eeb3ac277901d8f893ef53","title":"Layout and Task Aware Instruction Prompt for Zero-shot Document Image Question Answering"},{"paperId":"e8753374e402cd756dcfec9bc74a34f8a5047f80","title":"Intelligent Grimm - Open-ended Visual Storytelling via Latent Diffusion Models"},{"paperId":"5fb7afae5fcacae1d40f109a348b43e00aa5d486","title":"Dense and Aligned Captions (DAC) Promote Compositional Reasoning in VL Models"},{"paperId":"b458fc5261595f44b36325e5eaea1f874d65138f","title":"GPT4Tools: Teaching Large Language Model to Use Tools via Self-instruction"},{"paperId":"1fed19184785b2b50163b3d8ccb7bfaa0321d1aa","title":"CrossGET: Cross-Guided Ensemble of Tokens for Accelerating Vision-Language Transformers"},{"paperId":"cd3783937ee72d2e3e9c6c8074b7576eedcb36bb","title":"Pre-trained transformer for adversarial purification"},{"paperId":"a79c7062809548fdfb593d68c106034012a401d9","title":"Integrating action knowledge and LLMs for task planning and situation handling in open worlds"},{"paperId":"8ecdbfe011b7189fa0ee49ffc4e42a93d728a371","title":"On Evaluating Adversarial Robustness of Large Vision-Language Models"},{"paperId":"d3f79210b54e168c76b8c311488f42d7d1048b81","title":"PandaGPT: One Model To Instruction-Follow Them All"},{"paperId":"5d44f16a36ba7ae6b3d9d7c98bbc1b877e598f35","title":"The False Promise of Imitating Proprietary LLMs"},{"paperId":"7cf64070fd3d7e53d80f260c10e6bd7018d580e1","title":"IdealGPT: Iteratively Decomposing Vision and Language Reasoning via Large Language Models"},{"paperId":"9c3a9b4821daa03cb5369041d59d2714329a3811","title":"Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models"},{"paperId":"08b562aa8066c2342f0d03824221dea18f0a18d2","title":"Cream: Visually-Situated Natural Language Understanding with Contrastive Reading Model and Frozen Large Language Models"},{"paperId":"00cb69a9f280317d1c59ac5827551ee9b10642b8","title":"EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought"},{"paperId":"2ad8183c72a90511383a32ccaeea313eb85f4085","title":"DetGPT: Detect What You Need via Reasoning"},{"paperId":"b82c1b0512d25307e3c81bb8d9df1607267a7a52","title":"MemeCap: A Dataset for Captioning and Interpreting Memes"},{"paperId":"7bf902fb94a577d15293ac4f90d8967163850fb1","title":"Let's Think Frame by Frame: Evaluating Video Chain of Thought with Video Infilling and Prediction"},{"paperId":"c22a8e36b7ffa69da0d70c9db58c78252567400a","title":"ReSee: Responding through Seeing Fine-grained Visual Knowledge in Open-domain Dialogue"},{"paperId":"13a5140fc0b269c408ecfc666cb297410bc753c5","title":"Matcher: Segment Anything with One Shot Using All-Purpose Feature Matching"},{"paperId":"6a5525c316b9be7909c433a79e090ed731425083","title":"What Makes for Good Visual Tokenizers for Large Language Models?"},{"paperId":"ffcbc17638beb1e1aff3eba1dd48735ed72a02d4","title":"LLM-CXR: Instruction-Finetuned LLM for CXR Image Understanding and Generation"},{"paperId":"bfa9df38bd8597d8cdf0c3497fe5e5a5e31f646f","title":"ONE-PEACE: Exploring One General Representation Model Toward Unlimited Modalities"},{"paperId":"42a30dc5470f54ec249f25d3c31e05d7c376c8e3","title":"VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks"},{"paperId":"a979975d1a0aea0e01423f092249cc3de575b6cd","title":"X-IQE: eXplainable Image Quality Evaluation for Text-to-Image Generation with Visual Large Language Models"},{"paperId":"c77c48fe9060aa83627fc2c7f331325de0c4fdac","title":"DrugChat: Towards Enabling ChatGPT-Like Capabilities on Drug Molecule Graphs"},{"paperId":"32ef37692ace804e041ac28a219211e80f150c3b","title":"Vision-language models boost food composition compilation"},{"paperId":"206400aba5f12f734cdd2e4ab48ef6014ea60773","title":"Evaluating Object Hallucination in Large Vision-Language Models"},{"paperId":"f4793adffd6f67ffcb93ccfc5672ab301b8a2b96","title":"PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering"},{"paperId":"692bc40edf4785d88c39e0c0fe9f270541fecf8a","title":"Towards Generalist Robots: A Promising Paradigm via Generative Simulation"},{"paperId":"848e690a62c327e1210532d58a6b914097cac763","title":"On the Hidden Mystery of OCR in Large Multimodal Models"},{"paperId":"8bd6a2a89503be083176f2cc26fabedb79238cbd","title":"InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning"},{"paperId":"8badb0587fef2ffc078b0cec549eb8ec96ed3ad4","title":"Self-Chained Image-Language Model for Video Localization and Question Answering"},{"paperId":"d48cb91b9e555194f7494c4d4bb9815021d3ee45","title":"VideoChat: Chat-Centric Video Understanding"},{"paperId":"54a8b153ed04a872da878d695239bdc413dc782c","title":"InternGPT: Solving Vision-Centric Tasks by Interacting with Chatbots Beyond Language"},{"paperId":"80c44fab16852ea9599411da14de7079c4514172","title":"Vision-Language Models in Remote Sensing: Current Progress and Future Trends"},{"paperId":"81e7e82245c2f230eeb8aaaa1a2b2604c143754a","title":"MultiModal-GPT: A Vision and Language Model for Dialogue with Humans"},{"paperId":"43e6e8d6663d83f1b74cf5a2be7b040b0928f867","title":"X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages"},{"paperId":"20fcc01d12a50f1da2af71d85f0a269b3ba48b77","title":"LMEye: An Interactive Perception Network for Large Language Models"},{"paperId":"d6d3604f369bb0415cbe814e43ca3131323b03e2","title":"Otter: A Multi-Modal Model with In-Context Instruction Tuning"},{"paperId":"0046306876ff2d5600699327e52bc29fa5e9ec91","title":"Transfer Visual Prompt Generator across LLMs"},{"paperId":"570079bbdd8758dfe865097e05719313c9c1301a","title":"LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model"},{"paperId":"7e32aac43e9f1df49e116add03327ee6f365dbf3","title":"mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality"},{"paperId":"131f499e4d3503da93022d07fcf804a18483bea9","title":"WizardLM: Empowering Large Language Models to Follow Complex Instructions"},{"paperId":"27d0d2923a42bd2bced1b100844e232ff87368e3","title":"SkinGPT-4: An Interactive Dermatology Diagnostic System with Visual Large Language Model"},{"paperId":"a11ab8708733359a05df7987e5097c501381f844","title":"From Isolated Islands to Pangea: Unifying Semantic Space for Human Action Understanding"},{"paperId":"fc8988585c6846fdeee33b34779a6a87b92c3e86","title":"Equivariant Similarity for Vision-Language Foundation Models"},{"paperId":"00a48c76e123ab77f301bf4dfd88b9b376b234c6","title":"Chat with the Environment: Interactive Multimodal Perception Using Large Language Models"},{"paperId":"53df959bcf6499c45e316086a96a624389a39a52","title":"Accountable Textual-Visual Chat Learns to Reject Human Instructions in Image Re-creation"},{"paperId":"f890b4dfe915174b23db909b07c515d465eaeff2","title":"Can Pre-trained Vision and Language Models Answer Visual Information-Seeking Questions?"},{"paperId":"e7e3d285379e8cb9cc151aa585fd89eb88aafdf1","title":"Multi-modal Large Language Model Enhanced Pseudo 3D Perception Framework for Visual Commonsense Reasoning"},{"paperId":"30c0cdc414f68211d5d0514df027cec22e005174","title":"A Survey on In-context Learning"},{"paperId":"d0c87ca688547f5e63fd4900300474980d900b57","title":"Towards Inadequately Pre-trained Models in Transfer Learning"},{"paperId":"cfda60d69d39674549d15a78e65fc5ec2f81de7f","title":"Visually Grounded Story Generation Challenge"},{"paperId":"5ddb51ae85deca14dc7fc8adc07305c22a1ebe0a","title":"Qwen-VL: A Frontier Large Vision-Language Model with Versatile Abilities"},{"paperId":"e3fe61ce7a3c013de8770f7a43f3666f9a8e2ce5","title":"Empowering Vision-Language Models to Follow Interleaved Vision-Language Instructions"},{"paperId":"738852940591ecf864abf402878ecf66e2945267","title":"Visual Adversarial Examples Jailbreak Large Language Models"},{"paperId":"84dc889beff9d51fe429cff8c92735e7410ee3c2","title":"Aligning Large Multi-Modal Model with Robust Instruction Tuning"},{"paperId":"7562e25b666cba841b1dd5cf6e700978922beb04","title":"SkinGPT: A Dermatology Diagnostic System with Vision Large Language Model"},{"paperId":"a3711dbf296b5ddd97ba93826660cd3995611625","title":"Towards A Foundation Model for Generalist Robots: Diverse Skill Learning at Scale via Automated Task and Scene Generation"},{"paperId":"6f425ba8c1fe3139fcb886d9dda30cd6520517ac","title":"More Context, Less Distraction: Visual Classification by Inferring and Conditioning on Contextual Attributes"},{"paperId":"07861fcf16e8308775bad81b479cd89edd77a24f","title":"ChatGPT as a Copilot for Investigating Digital Evidence"},{"paperId":"daa594d00df767f14242f483409181e8c17288ff","title":"L ARGE C ONTENT AND B EHAVIOR M ODELS TO U N - DERSTAND , S IMULATE , AND O PTIMIZE C ONTENT AND B EHAVIOR"},{"paperId":"1e7115da4db1622c01ec8e268e622fa8f254599c","title":"L ARGE C ONTENT AND B EHAVIOR M ODELS TO U N - DERSTAND , S IMULATE , AND O PTIMIZE C ONTENT AND B EHAVIOR"},{"paperId":"698d83e2ba10d94c2a0723e907eb297ff4a6249d","title":"HallE-Switch: Rethinking and Controlling Object Existence Hallucinations in Large Vision Language Models for Detailed Caption"},{"paperId":"03374fddfa23df906de7c5758df332543e4b7539","title":"JM3D & JM3D-LLM: Elevating 3D Representation with Joint Multi-modal Cues"},{"paperId":"65d5728ea17f016382870aa27aac1e78d590b50c","title":"HallusionBench: You See What You Think? Or You Think What You See? An Image-Context Reasoning Benchmark Challenging for GPT-4V(ision), LLaVA-1.5, and Other Multi-modality Models"},{"paperId":"96a6df2b4aa50cfbd8984933e9c66b0763fc08a6","title":"Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V"},{"paperId":"f45b836d1ff5045b653e77999ad8106fe27121aa","title":"CORE-MM: Complex Open-Ended Reasoning Evaluation For Multi-Modal Large Language Models"},{"paperId":"a6a2df6b37121a673a44ad3b06a55002d5acf192","title":"Evaluation and Mitigation of Agnosia in Multimodal Large Language Models"},{"paperId":"95cf0808135ee411204eac267909eb008b8cdb29","title":"Fine-Tune Language Models as Differential Equation Solvers"},{"paperId":"44ccf252018f71898d52d89539f17d77a4f8d548","title":"Chart Understanding with Large Language Model"},{"paperId":"9d0bec28fc70ff4b0a6656e162a5cbff5fb3ccc8","title":"VL-FAS: DOMAIN GENERALIZATION VIA VISION-LANGUAGE MODEL FOR FACE ANTI-SPOOFING"}],"references":[{"paperId":"0ebc861f5478561f12941e6b48aad30574e996d8","title":"Video ChatCaptioner: Towards Enriched Spatiotemporal Descriptions"},{"paperId":"c7a9c7302a72301ed79a7c0696d5af2e03ad3ac4","title":"MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action"},{"paperId":"6e754273d54a91371efbc928cd6b156364d517da","title":"ViperGPT: Visual Inference via Python Execution for Reasoning"},{"paperId":"b60dfaa68bd7d280c6bd2fe87595abab8e5a2a4e","title":"Breaking Common Sense: WHOOPS! A Vision-and-Language Benchmark of Synthetic and Compositional Images"},{"paperId":"69cfdc8df16ae63b7acba4ac6f727f78b86893c3","title":"ChatGPT Asks, BLIP-2 Answers: Automatic Questioning Towards Enriched Visual Descriptions"},{"paperId":"af997821231898a5f8d0fd78dad4eec526acabe5","title":"Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models"},{"paperId":"38fe8f324d2162e63a967a9ac6648974fc4c66f3","title":"PaLM-E: An Embodied Multimodal Language Model"},{"paperId":"fbfef4723d8c8467d7bd523e1d0b703cce0e0f9c","title":"Language Is Not All You Need: Aligning Perception with Language Models"},{"paperId":"57e849d0de13ed5f91d086936296721d4ff75a75","title":"LLaMA: Open and Efficient Foundation Language Models"},{"paperId":"3f5b31c4f7350dc88002c121aecbdc82f86eb5bb","title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models"},{"paperId":"78281482c1fdad8e167bab39cc9955c73d58ae8f","title":"EVA: Exploring the Limits of Masked Visual Representation Learning at Scale"},{"paperId":"964bd39b546f0f6625ff3b9ef1083f797807ef2e","title":"BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"},{"paperId":"cdbd4f9b6ab2e2fd1ddf5400d5ed2c18960635d1","title":"Scaling Instruction-Finetuned Language Models"},{"paperId":"26fd105d0b5a458979c012cddb3ba2de943388c4","title":"Plug-and-Play VQA: Zero-shot VQA by Conjoining Large Pretrained Models with Zero Training"},{"paperId":"a970c8fadef8497576660b288c52c0ec8eebdc12","title":"Zero-Shot Video Question Answering via Frozen Bidirectional Language Models"},{"paperId":"dac3a172b504f4e33c029655e9befb3386e5f63a","title":"Emergent Abilities of Large Language Models"},{"paperId":"47a67e76ed84260ff19f7a948d764005d1edf1c9","title":"A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge"},{"paperId":"13a0d8bb38f739990c8cd65a44061c6534f17221","title":"OPT: Open Pre-trained Transformer Language Models"},{"paperId":"26218bdcc3945c7edae7aa2adbfba4cd820a2df3","title":"Flamingo: a Visual Language Model for Few-Shot Learning"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"8342b592fe238f3d230e4959b06fd10153c45db1","title":"Training Compute-Optimal Large Language Models"},{"paperId":"d766bffc357127e0dc86dd69561d5aeb520d6f4c","title":"Training language models to follow instructions with human feedback"},{"paperId":"a3b42a83669998f65df60d7c065a70d07ca95e99","title":"BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation"},{"paperId":"7cbc2a7843411a1768ab762930707af0a3c33a19","title":"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model"},{"paperId":"b668ce936cff0b0ca8b635cd5f25a62eaf4eb3df","title":"LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs"},{"paperId":"01b5412f3d17e90e09226d7c40ad4d4468a1414d","title":"Multimodal Few-Shot Learning with Frozen Language Models"},{"paperId":"a8ca46b171467ceb2d7652fbfb67fe701ad86092","title":"LoRA: Low-Rank Adaptation of Large Language Models"},{"paperId":"616e0ed02ca024a8c1d4b86167f7486ea92a13d9","title":"VisualGPT: Data-efficient Adaptation of Pretrained Language Models for Image Captioning"},{"paperId":"394be105b87e9bfe72c20efe6338de10604e1a11","title":"Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"43f2ad297941db230c089ba353efc3f281ab678c","title":"5分で分かる!? 有名論文ナナメ読み：Jacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"439369de9514e41e0f03fed552d8f6e5aebf51b2","title":"Connecting Vision and Language with Localized Narratives"},{"paperId":"6c4b76232bb72897685d19b3d264c6ee3005bc2b","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"1ab7f7c1d328589f25c79515b9a5d824d7ffbbd1","title":"GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering"},{"paperId":"4921243268c81d0d6db99053a9d004852225a622","title":"Object Hallucination in Image Captioning"},{"paperId":"b4df354db88a70183a64dbc9e56cf14e7669a6c0","title":"Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning"},{"paperId":"afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d","title":"Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations"},{"paperId":"92c141447f51b6732242376164ff961e464731c8","title":"ReferItGame: Referring to Objects in Photographs of Natural Scenes"},{"paperId":"8e080b98efbe65c02a116439205ca2344b9f7cd4","title":"Im2Text: Describing Images Using 1 Million Captioned Photographs"},{"paperId":null,"title":"Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023"},{"paperId":null,"title":"Wenxuan Zhang, and Mohamed Elhoseiny"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":null,"title":"OpenAI"},{"paperId":null,"title":"Stanford alpaca: An instruction-following llama model"},{"paperId":null,"title":"with the olive oil, garlic, thyme, oregano, paprika, salt, and pepper. 3. Roast the lobsters in the preheated oven for 20-25 minutes"},{"paperId":null,"title":"Introducing chatgpt"},{"paperId":null,"title":"I hope this helps! Let me know if you have any other questions"},{"paperId":null,"title":"Preheat the oven to"}],"id":"f44ad7ad67ddd5fe74598fe491ca75c5221380df","summary":"MiniGPT-4 is presented, which aligns a frozen visual encoder with a frozen LLM, Vicuna, using just one projection layer and possesses many capabilities similar to those exhibited by G PT-4 like detailed image description generation and website creation from hand-written drafts."},{"url":"https://www.semanticscholar.org/paper/6ae700c89a9a9a3da7e55dd51c4710b5ed8c8d4e","title":"Caption-Aware Medical VQA via Semantic Focusing and Progressive Cross-Modality Comprehension","venue":"ACM Multimedia","year":2022,"referenceCount":45,"citationCount":6,"influentialCitationCount":0,"publicationDate":"10/10/2022","authors":"Fu'ze Cong,Shibiao Xu,Li Guo,Yinbing Tian","citations":[{"paperId":"1e0d21dc2caf7b58342ddc8609fb30cdc1e27cd5","title":"MISS: A Generative Pretraining and Finetuning Approach for Med-VQA"},{"paperId":"420087f314633a381e61e6c5cd73ccc2070a749e","title":"PeFoMed: Parameter Efficient Fine-tuning on Multimodal Large Language Models for Medical Visual Question Answering"},{"paperId":"85ff452c26cc502d095d4ee017e2678df1470bf1","title":"An Effective Med-VQA Method Using a Transformer with Weights Fusion of Multiple Fine-Tuned Models"},{"paperId":"bf40c9e7832e1b2887cbf5798455f91705ea11ba","title":"Masked Vision and Language Pre-training with Unimodal and Multimodal Contrastive Losses for Medical Visual Question Answering"},{"paperId":"785650a805851c7e945523e495c5a523c60f72a4","title":"Open-Ended Medical Visual Question Answering Through Prefix Tuning of Language Models"},{"paperId":"b88f6aa65a4e1faf963494a76d28cc12112c9543","title":"A Critical Analysis of Benchmarks, Techniques, and Models in Medical Visual Question Answering"}],"references":[{"paperId":"6e7763ec04906726377953cc85f31a1a0c889001","title":"Anomaly Matters: An Anomaly-Oriented Model for Medical Visual Question Answering"},{"paperId":"4ef3d9e492479e28fa57d107e52acc6a0c803de2","title":"Does CLIP Benefit Visual Question Answering in the Medical Domain as Much as it Does in the General Domain?"},{"paperId":"c2d5426ee019d3d894b9d4416dc866b65fe64312","title":"Image Retrieval on Real-life Images with Pre-trained Vision-and-Language Models"},{"paperId":"5bd42c29a5ba8a6c39547db89023d879e98a6b32","title":"Multiple Meta-model Quantifying for Medical Visual Question Answering"},{"paperId":"17c2bb358169541f2d0a769f80779f46d1cd3d37","title":"MMBERT: Multimodal BERT Pretraining for Improved Medical VQA"},{"paperId":"93b6b79b4ef6c345f31722ce7c829385c6dce0d6","title":"Slake: A Semantically-Labeled Knowledge-Enhanced Dataset For Medical Visual Question Answering"},{"paperId":"b8d5b853f2212cbb48a43f1edec9b96d76d388ec","title":"Medical Visual Question Answering via Conditional Reasoning"},{"paperId":"b1b8ffe938f706a9416c319a34793a2389866773","title":"Visual Question Answering as a Multi-Task Problem"},{"paperId":"818e5cbc337e4e1b98e65a2d7c2d6d2a0318cd57","title":"Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks"},{"paperId":"667b88984df0c5c11ac07899ffb5509185abdf57","title":"Visual question answering: a state-of-the-art review"},{"paperId":"ed6ce80789889c0fd56c8117f85079c1c31fe426","title":"CGMVQA: A New Classification and Generative Model for Medical Visual Question Answering"},{"paperId":"6adb61121ca4560915ade532910acde56440b88f","title":"A Question-Centric Model for Visual Question Answering in Medical Imaging"},{"paperId":"3c8a456509e6c0805354bd40a35e3f2dbf8069b1","title":"PyTorch: An Imperative Style, High-Performance Deep Learning Library"},{"paperId":"27cb0b42e0573c4891ae2ca444776dee57bfe2ac","title":"Compact Trilinear Interaction for Visual Question Answering"},{"paperId":"33301b25a297b701bdc287e985c006375cb7bb21","title":"Overcoming Data Limitation in Medical Visual Question Answering"},{"paperId":"6648b4db5f12c30941ea78c695e77aded19672bb","title":"Unified Vision-Language Pre-Training for Image Captioning and VQA"},{"paperId":"1e43c7084bdcb6b3102afaf301cce10faead2702","title":"BioBERT: a pre-trained biomedical language representation model for biomedical text mining"},{"paperId":"97add9744ae63c5e7af9d9861ecc18a2734d3f0c","title":"Examine before You Answer: Multi-task Learning with Adaptive-attentions for Multiple-choice VQA"},{"paperId":"a564fabf130ff6e2742cfba90c7a4018937d764d","title":"Radiology Objects in COntext (ROCO): A Multimodal Image Dataset"},{"paperId":"a5d10341717c0519cf63151b496a6d2ed67aa05f","title":"Bilinear Attention Networks"},{"paperId":"5d45cc9a3a2fc064eccc0c915dbdf73cce559ce7","title":"On the Automatic Generation of Medical Imaging Reports"},{"paperId":"8e9ad6f8b2bc97f0412fa0cc243ac6975864534a","title":"Multi-modal Factorized Bilinear Pooling with Co-attention Learning for Visual Question Answering"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"1a2118bed729579528deb51e745d58dd3629baf6","title":"Learning Important Features Through Propagating Activation Differences"},{"paperId":"2c03df8b48bf3fa39054345bafabfeff15bfd11d","title":"Deep Residual Learning for Image Recognition"},{"paperId":"2c1890864c1c2b750f48316dc8b650ba4772adc5","title":"Stacked Attention Networks for Image Question Answering"},{"paperId":"580062407427236ced45253a2ff7df2e147a81e2","title":"The Multimodal Brain Tumor Image Segmentation Benchmark (BRATS)"},{"paperId":"4d8f2d14af5991d4f0d050d22216825cac3157bd","title":"Show, Attend and Tell: Neural Image Caption Generation with Visual Attention"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"f37e1b62a767a307c046404ca96bc140b3e68cb5","title":"GloVe: Global Vectors for Word Representation"},{"paperId":"0b544dfe355a5070b60986319a3f51fb45d1348e","title":"Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation"},{"paperId":"d2c733e34d48784a37d717fe43d9e93277a8c53e","title":"ImageNet: A large-scale hierarchical image database"},{"paperId":"2e9d221c206e9503ceb452302d68d10e293f2a10","title":"Long Short-Term Memory"},{"paperId":"161ffb54a3fdf0715b198bb57bd22f910242eb49","title":"Multitask Learning"},{"paperId":"0d46f2f51836f6b455c60d08309d7c48ff354ef1","title":"Yunnan University at VQA-Med 2021: Pretrained BioBERT for Medical Domain Visual Question Answering"},{"paperId":"519799a9e2a72b808d81c6bcba5de263503de053","title":"Contrastive Pre-training and Representation Distillation for Medical Visual Question Answering Based on Radiology Images"},{"paperId":"931759c41d431b6a5421d75532416edc2e3fadc5","title":"kdevqa at VQA-Med 2020: Focusing on GLU-based Classification"},{"paperId":"39dbb2e49fb33351044a9b8c152a173b31f4c405","title":"Overview of the VQA-Med Task at ImageCLEF 2020: Visual Question Answering and Generation in the Medical Domain"},{"paperId":"737806db2a06048433d4976f10625f98dcba815c","title":"HARENDRAKV at VQA-Med 2020: Sequential VQA with Attention for Medical Visual Question Answering"},{"paperId":"e5d250b1d69d36131e4848969f1ff9dd69486c44","title":"Shengyan at VQA-Med 2020: An Encoder-Decoder Model for Medical Domain Visual Question Answering Task"},{"paperId":"ea357fc547e83bbd7cec964840327a5e266f552a","title":"NLM at VQA-Med 2020: Visual Question Answering and Generation in the Medical Domain"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"18f9a6045ba01cb079c4fa49a630d71bbd27cd92","title":"Descriptor : A dataset of clinically generated visual questions and answers about radiology images"},{"paperId":"441281c07b5e5949aeb56375e25623ddbdab94f4","title":"Intravascular Imaging and Computer Assisted Stenting, and Large-Scale Annotation of Biomedical Data and Expert Label Synthesis"},{"paperId":"fdbb252f29ee0b72fc5467c0ae11f7cb30149f46","title":"Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)"}],"id":"6ae700c89a9a9a3da7e55dd51c4710b5ed8c8d4e","summary":"A caption-aware VQA method that can read the summary information of image content and clinic diagnoses from plenty of medical images and answer the medical question with richer multimodality features is proposed."},{"url":"https://www.semanticscholar.org/paper/28ff0816f19a5e3e37eac5569de41872fd262f0a","title":"Align, Reason and Learn: Enhancing Medical Vision-and-Language Pre-training with Knowledge","venue":"ACM Multimedia","year":2022,"referenceCount":61,"citationCount":24,"influentialCitationCount":1,"publicationDate":"15/09/2022","authors":"Zhihong Chen,Guanbin Li,Xiang Wan","citations":[{"paperId":"ecddb1287f241e4abf2874a7b95172a68c21a2e5","title":"MLIP: Enhancing Medical Visual Representation with Divergence Encoder and Knowledge-guided Contrastive Learning"},{"paperId":"420087f314633a381e61e6c5cd73ccc2070a749e","title":"PeFoMed: Parameter Efficient Fine-tuning on Multimodal Large Language Models for Medical Visual Question Answering"},{"paperId":"6bdfffbf92d01c8b543088d40d46233610e469a8","title":"CLIP in Medical Imaging: A Comprehensive Survey"},{"paperId":"2c7e346aa311fec4dda04bdf3a214ce2026d8807","title":"Medical Vision Language Pretraining: A survey"},{"paperId":"749104d1a207f5bc192c7d95a12856b5e7f84d1f","title":"Mapping medical image-text to a joint space via masked modeling"},{"paperId":"c7492913370b5726eaa6ced163a60de6c9d4bb7f","title":"A Survey of Large Language Models for Healthcare: from Data, Technology, and Applications to Accountability and Ethics"},{"paperId":"e9f0223f8dce8b04d37d1f56e6c976b5d0cb5956","title":"A Foundation LAnguage-Image model of the Retina (FLAIR): Encoding expert knowledge in text supervision"},{"paperId":"92a63df9582c25418b5de50705a0f68760f6aa49","title":"Re-mine, Learn and Reason: Exploring the Cross-modal Semantic Correlations for Language-guided HOI detection"},{"paperId":"2bc6d41caf81e62eb60b0829a521cfee085715c0","title":"Bridging Vision and Language Encoders: Parameter-Efficient Tuning for Referring Image Segmentation"},{"paperId":"9b348715d0311056eee850dd1cce1cdd3c64eec8","title":"Knowledge Boosting: Rethinking Medical Contrastive Vision-Language Pre-Training"},{"paperId":"baa1dc079d98ca76b0173c8d653fed759fd0a371","title":"A scoping review on multimodal deep learning in biomedical images and texts"},{"paperId":"64fa56962dd0f4bbe206be6142fbe0315c4e7c2f","title":"Multi-modal Pre-training for Medical Vision-language Understanding and Generation: An Empirical Study with A New Benchmark"},{"paperId":"8700c5af25450bf8e84b94783344b054d268738b","title":"Bi-VLGM : Bi-Level Class-Severity-Aware Vision-Language Graph Matching for Text Guided Medical Image Segmentation"},{"paperId":"0fe1b1bfd634ee42846afbd64cef1c682e02e5e7","title":"Retrieval-based Knowledge Augmented Vision Language Pre-training"},{"paperId":"ac4d13b6a4f9fb67337099f4602135a0351f5c99","title":"Towards Medical Artificial General Intelligence via Knowledge-Enhanced Multimodal Pretraining"},{"paperId":"8f3138f7ee5127faab265793be8ae278bc49d9b1","title":"PMC-CLIP: Contrastive Language-Image Pre-training using Biomedical Documents"},{"paperId":"4d4a96708fc67403176bb2b891b564af7a20c148","title":"RAMM: Retrieval-augmented Biomedical Visual Question Answering with Multi-modal Pre-training"},{"paperId":"5d937b7811d8fd4208b2810971cb2e33f64bcfa2","title":"Knowledge-enhanced visual-language pre-training on chest radiology images"},{"paperId":"880a0029ddeba3e3f8b6bd50cc6843ea760cc3d3","title":"MDF-Net for abnormality detection by fusing X-rays with clinical data"},{"paperId":"da9579539385daedd33a0de0f814e2977ad0d1f5","title":"Towards Unifying Medical Vision-and-Language Pre-training via Soft Prompts"},{"paperId":"e3c70b0b71b51872bbdaa0f4bf2b56908f97abec","title":"MedKLIP: Medical Knowledge Enhanced Language-Image Pre-Training for X-ray Diagnosis"},{"paperId":"508d9b43832790b4d35f4ae1fa76e9712859d6aa","title":"Vision and Structured-Language Pretraining for Cross-Modal Food Retrieval"},{"paperId":"b15469d0ab3dc3a9dec037d761817b3fe546bed6","title":"Pre-trained Language Models in Biomedical Domain: A Systematic Survey"},{"paperId":"10a8e7a7e07256178665f90074c5c41b071e73d3","title":"MDF-Net: Multimodal Dual-Fusion Network for Abnormality Detection using CXR Images and Clinical Data"}],"references":[{"paperId":"4ef3d9e492479e28fa57d107e52acc6a0c803de2","title":"Does CLIP Benefit Visual Question Answering in the Medical Domain as Much as it Does in the General Domain?"},{"paperId":"6351ebb4a3287f5f3e1273464b3b91e5df5a16d7","title":"Masked Autoencoders Are Scalable Vision Learners"},{"paperId":"94ff111c4d81bd03f159321728ceec8b4711c89d","title":"An Empirical Study of Training End-to-End Vision-and-Language Transformers"},{"paperId":"5e00596fa946670d894b1bdaeff5a98e3867ef13","title":"SimVLM: Simple Visual Language Model Pretraining with Weak Supervision"},{"paperId":"d0b59b3e34a79c8c79a31bf3944ded8ab7a803ae","title":"ROSITA: Enhancing Vision-and-Language Semantic Alignments via Cross- and Intra-modal Knowledge Integration"},{"paperId":"b82c5f9efdb2ae56baa084ca41aeddd8a665c1d1","title":"Align before Fuse: Vision and Language Representation Learning with Momentum Distillation"},{"paperId":"63c74d15940af1af9b386b5762e4445e54c73719","title":"VinVL: Revisiting Visual Representations in Vision-Language Models"},{"paperId":"d9317660e2a538d9c018028956fd114d55330f82","title":"Multi-Modal Understanding and Generation for Medical Images and Text via Vision-Language Pre-Training"},{"paperId":"3c83f80f06633ff4598d33c2959f8e4cdcad3e93","title":"Cross-Modal Self-Attention with Multi-Task Pre-Training for Medical Visual Question Answering"},{"paperId":"17c2bb358169541f2d0a769f80779f46d1cd3d37","title":"MMBERT: Multimodal BERT Pretraining for Improved Medical VQA"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"93b6b79b4ef6c345f31722ce7c829385c6dce0d6","title":"Slake: A Semantically-Labeled Knowledge-Enhanced Dataset For Medical Visual Question Answering"},{"paperId":"0839722fb5369c0abaff8515bfc08299efc790a1","title":"ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision"},{"paperId":"c9074d9719c5ce0dd3a7369dd0749cd08d7f67ed","title":"MELINDA: A Multimodal Dataset for Biomedical Experiment Method Classification"},{"paperId":"268d347e8a55b5eb82fb5e7d2f800e33c75ab18a","title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"},{"paperId":"5ba77a5bdeffb62aa0902ae68997bbc38db8a722","title":"MedICaT: A Dataset of Medical Images, Captions, and Textual References"},{"paperId":"b8d5b853f2212cbb48a43f1edec9b96d76d388ec","title":"Medical Visual Question Answering via Conditional Reasoning"},{"paperId":"7eda139d737eea10fc1d95364327a41ec0cee4a4","title":"CoLAKE: Contextualized Language and Knowledge Embedding"},{"paperId":"ed2a06388dd14b052f33bac5e3bfc0fa26243b55","title":"A Comparison of Pre-trained Vision-and-Language Models for Multimodal Representation Learning across Medical Images and Reports"},{"paperId":"bc996a4dbf9d4234eacdd0b930a94de1d158e256","title":"ERNIE-ViL: Knowledge Enhanced Vision-Language Representations Through Scene Graph"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"818e5cbc337e4e1b98e65a2d7c2d6d2a0318cd57","title":"Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks"},{"paperId":"598a2ee223e2949c3b28389e922c1892b4717d2a","title":"Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal Transformers"},{"paperId":"56cafbac34f2bb3f6a9828cd228ff281b810d6bb","title":"KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation"},{"paperId":"6007bd2a34385132a7885b934d90b519a1f65bba","title":"ZEN: Pre-training Chinese Text Encoder Enhanced by N-gram Representations"},{"paperId":"33301b25a297b701bdc287e985c006375cb7bb21","title":"Overcoming Data Limitation in Medical Visual Question Answering"},{"paperId":"dfc7b58b67c31932b48586b3e23a43cc94695290","title":"UNITER: UNiversal Image-TExt Representation Learning"},{"paperId":"06a73ad09664435f8b3cd90293f4e05a047cf375","title":"K-BERT: Enabling Language Representation with Knowledge Graph"},{"paperId":"bfeb827d06c1a3583b5cc6d25241203a81f6af09","title":"Knowledge Enhanced Contextual Word Representations"},{"paperId":"4aa6298b606941a282d735fa3143da293199d2ca","title":"VL-BERT: Pre-training of Generic Visual-Linguistic Representations"},{"paperId":"79c93274429d6355959f1e4374c2147bb81ea649","title":"LXMERT: Learning Cross-Modality Encoder Representations from Transformers"},{"paperId":"5aec474c31a2f4b74703c6f786c0a8ff85c450da","title":"VisualBERT: A Simple and Performant Baseline for Vision and Language"},{"paperId":"65a9c7b0800c86a196bc14e7621ff895cc6ab287","title":"ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"5f994dc8cae24ca9d1ed629e517fcc652660ddde","title":"ERNIE: Enhanced Language Representation with Informative Entities"},{"paperId":"031e4e43aaffd7a479738dcea69a2d5be7957aa3","title":"ERNIE: Enhanced Representation through Knowledge Integration"},{"paperId":"156d217b0a911af97fa1b5a71dc909ccef7a8028","title":"SciBERT: A Pretrained Language Model for Scientific Text"},{"paperId":"de28ec1d7bd38c8fc4e8ac59b6133800818b4e29","title":"ScispaCy: Fast and Robust Models for Biomedical Natural Language Processing"},{"paperId":"3d29ce781f297dc543e44dfb39990baff3a3acca","title":"MIMIC-CXR-JPG, a large publicly available database of labeled chest radiographs"},{"paperId":"a564fabf130ff6e2742cfba90c7a4018937d764d","title":"Radiology Objects in COntext (ROCO): A Multimodal Image Dataset"},{"paperId":"a5d10341717c0519cf63151b496a6d2ed67aa05f","title":"Bilinear Attention Networks"},{"paperId":"ff65e3bf34e892ef75d91c5e3d7294e0b64d867d","title":"Zero-Shot Recognition via Semantic Embeddings and Knowledge Graphs"},{"paperId":"d07284a6811f1b2745d91bdb06b040b57f226882","title":"Decoupled Weight Decay Regularization"},{"paperId":"33998aff64ce51df8dee45989cdca4b6b1329ec4","title":"Graph Attention Networks"},{"paperId":"8e9ad6f8b2bc97f0412fa0cc243ac6975864534a","title":"Multi-modal Factorized Bilinear Pooling with Co-attention Learning for Visual Question Answering"},{"paperId":"79baf8cf6be6510f69be8c515516136138678cf5","title":"The More You Know: Using Knowledge Graphs for Image Classification"},{"paperId":"c6850869aa5e78a107c378d2e8bfa39633158c0c","title":"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"},{"paperId":"2c03df8b48bf3fa39054345bafabfeff15bfd11d","title":"Deep Residual Learning for Image Recognition"},{"paperId":"2c1890864c1c2b750f48316dc8b650ba4772adc5","title":"Stacked Attention Networks for Image Question Answering"},{"paperId":"2582ab7c70c9e7fcb84545944eba8f3a7f253248","title":"Translating Embeddings for Modeling Multi-relational Data"},{"paperId":"519799a9e2a72b808d81c6bcba5de263503de053","title":"Contrastive Pre-training and Representation Distillation for Medical Visual Question Answering Based on Radiology Images"},{"paperId":"e5d143ae82ede67726aa1a9aeac3de4bf53d8920","title":"KB-VLP: Knowledge Based Vision and Language Pretraining"},{"paperId":null,"title":"BERT-MK: Integrating Graph Contextualized Knowledge into Pre-trained Language Models"},{"paperId":null,"title":"Neuro-SymbolicVisualReasoning:Disentangling"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"9eeeb23546d3d2bbc73959bffc6819f2335f3c83","title":"VQA-Med: Overview of the Medical Visual Question Answering Task at ImageCLEF 2019"},{"paperId":"18f9a6045ba01cb079c4fa49a630d71bbd27cd92","title":"Descriptor : A dataset of clinically generated visual questions and answers about radiology images"},{"paperId":"441281c07b5e5949aeb56375e25623ddbdab94f4","title":"Intravascular Imaging and Computer Assisted Stenting, and Large-Scale Annotation of Biomedical Data and Expert Label Synthesis"},{"paperId":"0adeb54d32925d200bb313a8e0f06116e49c67fb","title":"The Unified Medical Language System (UMLS): integrating biomedical terminology"},{"paperId":null,"title":"Long short-termmemory"},{"paperId":null,"title":"MM ’22, October 10–14, 2022,"}],"id":"28ff0816f19a5e3e37eac5569de41872fd262f0a","summary":"A systematic and effective approach to enhance Med-VLP by structured medical knowledge from three perspectives is proposed, which align the representations of the vision encoder and the language encoder through knowledge."},{"url":"https://www.semanticscholar.org/paper/da9579539385daedd33a0de0f814e2977ad0d1f5","title":"Towards Unifying Medical Vision-and-Language Pre-training via Soft Prompts","venue":"arXiv.org","year":2023,"referenceCount":72,"citationCount":7,"influentialCitationCount":0,"publicationDate":"17/02/2023","authors":"Zhihong Chen,Shizhe Diao,Benyou Wang,Guanbin Li,Xiang Wan","citations":[{"paperId":"352252231462c24440bc0016638ea5fe8d4c6f7e","title":"UniDCP: Unifying Multiple Medical Vision-language Tasks via Dynamic Cross-modal Learnable Prompts"},{"paperId":"d48fa3ed73817563130ef217d85011ce1fbe7470","title":"BESTMVQA: A Benchmark Evaluation System for Medical Visual Question Answering"},{"paperId":"2c7e346aa311fec4dda04bdf3a214ce2026d8807","title":"Medical Vision Language Pretraining: A survey"},{"paperId":"ad083672bc110b2c86d1461bb4cda3cd3eededee","title":"Multimodal ChatGPT for Medical Applications: an Experimental Study of GPT-4V"},{"paperId":"8e5d42f5b98146d0784fe85e29c768a4989e1478","title":"Foundational Models in Medical Imaging: A Comprehensive Survey and Future Vision"},{"paperId":"b18daa14486920016c4664c3ed1759f2de1ba854","title":"Exploring Transfer Learning in Medical Image Segmentation using Vision-Language Models"},{"paperId":"385376b8aa48c25403f17d6206db7c09b67e1314","title":"Prompt Engineering for Healthcare: Methodologies and Applications"}],"references":[{"paperId":"84729ec815f0607a4a2370c0969e8c3ba82a9411","title":"Learning to Exploit Temporal Structure for Biomedical Vision-Language Processing"},{"paperId":"e3c70b0b71b51872bbdaa0f4bf2b56908f97abec","title":"MedKLIP: Medical Knowledge Enhanced Language-Image Pre-Training for X-ray Diagnosis"},{"paperId":"cdd9c1d23f9e89d5113f3e31821bb174c6a6afed","title":"MedCLIP: Contrastive Learning from Unpaired Medical Images and Text"},{"paperId":"4867e6c7d190e37b9199a67ebeac62180b59aa32","title":"Multi-Granularity Cross-modal Alignment for Generalized Medical Visual Representation Learning"},{"paperId":"81adb80e390f25d4a2d764b1063eeaa2c334d441","title":"Multi-modal Masked Autoencoders for Medical Vision-and-Language Pre-training"},{"paperId":"28ff0816f19a5e3e37eac5569de41872fd262f0a","title":"Align, Reason and Learn: Enhancing Medical Vision-and-Language Pre-training with Knowledge"},{"paperId":"02251886950770e82b3d68564d60cdfe15e73199","title":"Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks"},{"paperId":"db2bd466953f3ea49280988e1659b6ac3f639e45","title":"Cross-modal Memory Networks for Radiology Report Generation"},{"paperId":"1bfa62ddfa3f6691e0e40c06f8ead594b6449cfa","title":"OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework"},{"paperId":"e9581d9758062f76e029bd19a58c4ae976cfb414","title":"SLIP: Self-supervision meets Language-Image Pre-training"},{"paperId":"008721e4f9cb9b2d3242bc31af48db6fb3f8727d","title":"Word Graph Guided Summarization for Radiology Findings"},{"paperId":"2fd6f77540c1cc8e70b96208ccf9971b4251fc02","title":"FLAVA: A Foundational Language And Vision Alignment Model"},{"paperId":"2fca2821ac2beb60fa0e26866e8f063261713951","title":"Joint Learning of Localized Representations from Medical Images and Reports"},{"paperId":"f675c62abfa788ea0be85d3124eba15a14d5e9d6","title":"FILIP: Fine-grained Interactive Language-Image Pre-Training"},{"paperId":"cf7c2e0e4fb2af689aaf4b7a7cddf7b1f4d5e3f0","title":"VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts"},{"paperId":"94ff111c4d81bd03f159321728ceec8b4711c89d","title":"An Empirical Study of Training End-to-End Vision-and-Language Transformers"},{"paperId":"681b16ed7258bf28622f9c835cfe94f195bb5395","title":"MedFuseNet: An attention-based multimodal deep learning model for visual question answering in the medical domain"},{"paperId":"0b500aa5fcc175f07aecf26c0e8ddc4f0c6a931d","title":"GLoRIA: A Multimodal Global-Local Representation Learning Framework for Label-efficient Medical Image Recognition"},{"paperId":"28692beece311a90f5fa1ca2ec9d0c2ce293d069","title":"Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"},{"paperId":"b82c5f9efdb2ae56baa084ca41aeddd8a665c1d1","title":"Align before Fuse: Vision and Language Representation Learning with Momentum Distillation"},{"paperId":"63c74d15940af1af9b386b5762e4445e54c73719","title":"VinVL: Revisiting Visual Representations in Vision-Language Models"},{"paperId":"d9317660e2a538d9c018028956fd114d55330f82","title":"Multi-Modal Understanding and Generation for Medical Images and Text via Vision-Language Pre-Training"},{"paperId":"17c2bb358169541f2d0a769f80779f46d1cd3d37","title":"MMBERT: Multimodal BERT Pretraining for Improved Medical VQA"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"93b6b79b4ef6c345f31722ce7c829385c6dce0d6","title":"Slake: A Semantically-Labeled Knowledge-Enhanced Dataset For Medical Visual Question Answering"},{"paperId":"141a5033d9994242b18bb3b217e79582f1ee9306","title":"Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision"},{"paperId":"cb596bffc5c5042c254058b62317a57fa156fea4","title":"Unifying Vision-and-Language Tasks via Text Generation"},{"paperId":"0839722fb5369c0abaff8515bfc08299efc790a1","title":"ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision"},{"paperId":"19adf1af8daa9551328226fc6c0140e955bf5689","title":"Generating Radiology Reports via Memory-driven Transformer"},{"paperId":"268d347e8a55b5eb82fb5e7d2f800e33c75ab18a","title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"},{"paperId":"9e90c17ef40404b79ad0f12d9b9c94656f12dfcd","title":"Improving Factual Completeness and Consistency of Image-to-Text Radiology Report Generation"},{"paperId":"5ba77a5bdeffb62aa0902ae68997bbc38db8a722","title":"MedICaT: A Dataset of Medical Images, Captions, and Textual References"},{"paperId":"6dd9f99cecd38504b667d320eb2a6267a9fee35d","title":"Contrastive Learning of Medical Visual Representations from Paired Images and Text"},{"paperId":"ed2a06388dd14b052f33bac5e3bfc0fa26243b55","title":"A Comparison of Pre-trained Vision-and-Language Models for Multimodal Representation Learning across Medical Images and Reports"},{"paperId":"bc996a4dbf9d4234eacdd0b930a94de1d158e256","title":"ERNIE-ViL: Knowledge Enhanced Vision-Language Representations Through Scene Graph"},{"paperId":"962dc29fdc3fbdc5930a10aba114050b82fe5a3e","title":"End-to-End Object Detection with Transformers"},{"paperId":"818e5cbc337e4e1b98e65a2d7c2d6d2a0318cd57","title":"Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks"},{"paperId":"598a2ee223e2949c3b28389e922c1892b4717d2a","title":"Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal Transformers"},{"paperId":"43f2ad297941db230c089ba353efc3f281ab678c","title":"5分で分かる!? 有名論文ナナメ読み：Jacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"34733eaf66007516347a40ad5d9bbe1cc9dacb6b","title":"A Simple Framework for Contrastive Learning of Visual Representations"},{"paperId":"add2f205338d70e10ce5e686df4a690e2851bdfc","title":"Momentum Contrast for Unsupervised Visual Representation Learning"},{"paperId":"33301b25a297b701bdc287e985c006375cb7bb21","title":"Overcoming Data Limitation in Medical Visual Question Answering"},{"paperId":"dfc7b58b67c31932b48586b3e23a43cc94695290","title":"UNITER: UNiversal Image-TExt Representation Learning"},{"paperId":"4aa6298b606941a282d735fa3143da293199d2ca","title":"VL-BERT: Pre-training of Generic Visual-Linguistic Representations"},{"paperId":"79c93274429d6355959f1e4374c2147bb81ea649","title":"LXMERT: Learning Cross-Modality Encoder Representations from Transformers"},{"paperId":"5aec474c31a2f4b74703c6f786c0a8ff85c450da","title":"VisualBERT: A Simple and Performant Baseline for Vision and Language"},{"paperId":"65a9c7b0800c86a196bc14e7621ff895cc6ab287","title":"ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks"},{"paperId":"63748e59f4e106cbda6b65939b77589f40e48fcb","title":"Text Summarization with Pretrained Encoders"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"2a567ebd78939d0861d788f0fedff8d40ae62bf2","title":"Publicly Available Clinical BERT Embeddings"},{"paperId":"89a816719613e220a64ab2590c938c23bbfe187e","title":"CheXpert: A Large Chest Radiograph Dataset with Uncertainty Labels and Expert Comparison"},{"paperId":"3d29ce781f297dc543e44dfb39990baff3a3acca","title":"MIMIC-CXR-JPG, a large publicly available database of labeled chest radiographs"},{"paperId":"95b19e31af5385800855f245744aabfb0b0ee74e","title":"Augmenting the National Institutes of Health Chest Radiograph Dataset with Expert Annotations of Possible Pneumonia."},{"paperId":"a564fabf130ff6e2742cfba90c7a4018937d764d","title":"Radiology Objects in COntext (ROCO): A Multimodal Image Dataset"},{"paperId":"f2588de5173fb047192dbb93d62ce6636bdf46bd","title":"Lessons from Natural Language Inference in the Clinical Domain"},{"paperId":"d07284a6811f1b2745d91bdb06b040b57f226882","title":"Decoupled Weight Decay Regularization"},{"paperId":"e7fd6848cb29ca221a7e17d823e06fb566f1f135","title":"Mixed Precision Training"},{"paperId":"c6850869aa5e78a107c378d2e8bfa39633158c0c","title":"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"},{"paperId":"258986132bf17755fe8263e42429fe73218c1534","title":"CIDEr: Consensus-based image description evaluation"},{"paperId":"92b79eab68909c885306a10ffeb31c0b742aff92","title":"“OSCAR”"},{"paperId":"0f8992ee6418d367d8e50ecbb59b08ea15e8431f","title":"Meteor 1.3: Automatic Metric for Reliable Optimization and Evaluation of Machine Translation Systems"},{"paperId":"60b05f32c32519a809f21642ef1eb3eaf3848008","title":"ROUGE: A Package for Automatic Evaluation of Summaries"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":"97da3aaa0dfbd32942ca99c60329d590b1234937","title":"Prefix Language Models are Unified Modal Learners"},{"paperId":"53d8b356551a2361020a948f64454a6d599af69f","title":"Prefix-Tuning: Optimizing Continuous Prompts for Generation"},{"paperId":"519799a9e2a72b808d81c6bcba5de263503de053","title":"Contrastive Pre-training and Representation Distillation for Medical Visual Question Answering Based on Radiology Images"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"9eeeb23546d3d2bbc73959bffc6819f2335f3c83","title":"VQA-Med: Overview of the Medical Visual Question Answering Task at ImageCLEF 2019"},{"paperId":"18f9a6045ba01cb079c4fa49a630d71bbd27cd92","title":"Descriptor : A dataset of clinically generated visual questions and answers about radiology images"},{"paperId":null,"title":"This dataset consists of 315 images and 3,515 questions. We adopt the commonly used version pre-processed by MEVF"},{"paperId":null,"title":"We follow previous studies [50] to prepare and pre-process the dataset by keeping the main three categories of questions: Modality, Plane, and Organ system"},{"paperId":null,"title":"Word graph guided 23354 applicable license agreement with IEEE. Restrictions apply"}],"id":"da9579539385daedd33a0de0f814e2977ad0d1f5","summary":"Experimental results show that the proposed PTUnifier approach achieves state-of-the-art results on a broad range of tasks, spanning uni-modal tasks (\\textit{i.e.}, image-to-text generation and image-text/text-image retrieval), and multi- modal tasks, demonstrating the effectiveness of the approach."},{"url":"https://www.semanticscholar.org/paper/c7a9c7302a72301ed79a7c0696d5af2e03ad3ac4","title":"MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action","venue":"arXiv.org","year":2023,"referenceCount":45,"citationCount":171,"influentialCitationCount":3,"publicationDate":"20/03/2023","authors":"Zhengyuan Yang,Linjie Li,Jianfeng Wang,Kevin Lin,E. Azarnasab,Faisal Ahmed,Zicheng Liu,Ce Liu,Michael Zeng,Lijuan Wang","citations":[{"paperId":"5e7274bcda47b704b6797bb14be8b7a61c047a61","title":"Uncertainty-Aware Evaluation for Vision-Language Models"},{"paperId":"5a20aa49b81b4e14fdb36814e557b3da60259ce9","title":"Chain of Thought Empowers Transformers to Solve Inherently Serial Problems"},{"paperId":"fc3c717987218662f49243e2be6bacc093dd47d8","title":"KG-Agent: An Efficient Autonomous Agent Framework for Complex Reasoning over Knowledge Graph"},{"paperId":"710b1e23b09e0b826f9d47e7cc23b5f4c0808c7e","title":"Multi-modal preference alignment remedies regression of visual instruction tuning on language model"},{"paperId":"73d8b0794b863f16ba63c4e2291a7c914824c0ec","title":"Graph Descriptive Order Improves Reasoning with Large Language Model"},{"paperId":"eaad6e351ab7ddb5a31bce3c5fe8bf38cd08c7f2","title":"Multimodal Query Suggestion with Multi-Agent Reinforcement Learning from Human Feedback"},{"paperId":"798feda076ad710df65d509a7884bd15937c8056","title":"Leak, Cheat, Repeat: Data Contamination and Evaluation Malpractices in Closed-Source LLMs"},{"paperId":"d2d96dc3bbf9d63c85f445e3fa08ad695457a532","title":"LLM Agents in Interaction: Measuring Personality Consistency and Linguistic Alignment in Interacting Populations of Large Language Models"},{"paperId":"b6b6e59f3bfdda9d4a4dfe56c46b30706fd18cf3","title":"Enhance Reasoning for Large Language Models in the Game Werewolf"},{"paperId":"7d9d4595f4cb4fc1521b445a2c4be5735c10c186","title":"EEG-GPT: Exploring Capabilities of Large Language Models for EEG Classification and Interpretation"},{"paperId":"c5db6c2726911b72d534f97bd4d1ed63f6431340","title":"Mobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual Perception"},{"paperId":"a050c9b0c321839e4427ab9defa3463be7825ac4","title":"MM-LLMs: Recent Advances in MultiModal Large Language Models"},{"paperId":"7e6c1bb54bb2e36cc1092b080e9928942f7f8a68","title":"TroVE: Inducing Verifiable and Efficient Toolboxes for Solving Programmatic Tasks"},{"paperId":"140cfda71bfff852c3e205b7ad61854b78c76982","title":"Mastering Text-to-Image Diffusion: Recaptioning, Planning, and Generating with Multimodal LLMs"},{"paperId":"23957040943f883542f47850c709b9e7f9d6fa55","title":"Prompting Large Vision-Language Models for Compositional Reasoning"},{"paperId":"4a48d628e53f554eb6ef09a457ca855188b96171","title":"DoraemonGPT: Toward Understanding Dynamic Scenes with Large Language Models"},{"paperId":"ff61aef2fef3a235bfaa123158a990c4f5f27d1a","title":"Small LLMs Are Weak Tool Learners: A Multi-LLM Agent"},{"paperId":"5502d769595981009e43344f8914e287acca2359","title":"ModaVerse: Efficiently Transforming Modalities with LLMs"},{"paperId":"af5f256e9771bf9cd02451195e3a7ac693fde3ed","title":"Exploring the Reasoning Abilities of Multimodal Large Language Models (MLLMs): A Comprehensive Survey on Emerging Trends in Multimodal Reasoning"},{"paperId":"f04e58782380383f4edaaa899fd85bfb3d20c2e1","title":"SpeechAgents: Human-Communication Simulation with Multi-Modal Multi-Agent Systems"},{"paperId":"9eab4104973f5de650544729a4a69d84c594da92","title":"A Vision Check-up for Language Models"},{"paperId":"516dafd778d6dcce0ef04ef0539257976b897d24","title":"Incorporating Geo-Diverse Knowledge into Prompting for Increased Geographical Robustness in Object Recognition"},{"paperId":"a06d3e9e90008c64c45a0029d580541d5f646771","title":"If LLM Is the Wizard, Then Code Is the Wand: A Survey on How Code Empowers Large Language Models to Serve as Intelligent Agents"},{"paperId":"46f64681d7a0c7f80303bebb0d62a3b7acbcdcd9","title":"An Improved Baseline for Reasoning Segmentation with Large Language Model"},{"paperId":"4599d5af850da482f591a02a3b17d56e0d358771","title":"Plan, Posture and Go: Towards Open-World Text-to-Motion Generation"},{"paperId":"6a33e58ef961a3a0a5657518b2be86395eb7c8d0","title":"InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks"},{"paperId":"24fc9ad715372358bd0108eeb7c944b915963293","title":"ASSISTGUI: Task-Oriented Desktop Graphical User Interface Automation"},{"paperId":"17a32c825bd746a2625eddc2728092171a9ef72a","title":"Jack of All Tasks, Master of Many: Designing General-purpose Coarse-to-Fine Vision-Language Model"},{"paperId":"35a17f896847614a71df772bbe2b66ae231cabc7","title":"CLOVA: A Closed-Loop Visual Assistant with Tool Usage and Update"},{"paperId":"6d2ab31aa75468f5458b9d96192c3f4a28f55d73","title":"DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving"},{"paperId":"55c6d16b550c606d62dd85084f0d373d8f087966","title":"VLAP: Efficient Video-Language Alignment via Frame Prompting and Distilling for Video Question Answering"},{"paperId":"b240a1d8ec2860bdd7370daa3144268ce46ac018","title":"Vary: Scaling up the Vision Vocabulary for Large Vision-Language Models"},{"paperId":"369b34826e23cb43bea9a91395e9603eacfa7420","title":"EgoPlan-Bench: Benchmarking Egocentric Embodied Planning with Multimodal Large Language Models"},{"paperId":"96a7b0fe722e6d2d5167ef25a6aff714a20233a0","title":"Exploring the Limits of ChatGPT in Software Security Applications"},{"paperId":"7e55d8701785818776323b4147cb13354c820469","title":"PaperQA: Retrieval-Augmented Generative Agent for Scientific Research"},{"paperId":"4ece984342174620705c0ba1186ea4a97ff76052","title":"LifelongMemory: Leveraging LLMs for Answering Queries in Egocentric Videos"},{"paperId":"f4e8e66557005af41e101caf9d16d63bf7fe6f9a","title":"LLM as OS, Agents as Apps: Envisioning AIOS, Agents and the AIOS-Agent Ecosystem"},{"paperId":"d77bc1a237b67c57b0c1b99b4802e703747a9688","title":"BenchLMM: Benchmarking Cross-style Visual Capability of Large Multimodal Models"},{"paperId":"f32ea390686b1eee3ba5b53c7a85e9e9385d4b94","title":"Visual Program Distillation: Distilling Tools and Programmatic Reasoning into Vision-Language Models"},{"paperId":"5220687c42520855db240b67415a20e09c137004","title":"Mismatch Quest: Visual and Textual Feedback for Image-Text Misalignment"},{"paperId":"06b2ac5153e3d8d05c13c82f93d7f4e13eee6d0f","title":"Mitigating Fine-Grained Hallucination by Fine-Tuning Large Vision-Language Models with Caption Rewrites"},{"paperId":"246017780386eba39d6cda760a1c2c70356baa50","title":"VIoTGPT: Learning to Schedule Vision Tools towards Intelligent Video Internet of Things"},{"paperId":"4673c2ac4abb4b055da87171231acb60801ffe74","title":"PoseGPT: Chatting about 3D Human Pose"},{"paperId":"8441c30ad4abdca9ee380aa6f22ffd731b10231b","title":"COLE: A Hierarchical Generation Framework for Graphic Design"},{"paperId":"7fb6302976205c75875978b20cb701c3e6d2132f","title":"RoboGPT: an intelligent agent of making embodied long-term decisions for daily instruction tasks"},{"paperId":"ee2c769943f9e46c3bbee117d1ecf14566b7bf1f","title":"Boosting the Power of Small Multimodal Reasoning Models to Match Larger Models with Self-Consistency Training"},{"paperId":"52941cadbd340344f3e0a6f50719fe55b3de5088","title":"Multimodal Large Language Models: A Survey"},{"paperId":"107fb6eec2febbae12db29bf3e311aaf5680027c","title":"Video-LLaVA: Learning United Visual Representation by Alignment Before Projection"},{"paperId":"0f993809c1fe00403ecea66d8f572832f075cfe4","title":"MMC: Advancing Multimodal Chart Understanding with Large-scale Instruction Tuning"},{"paperId":"aad3d2e690f6c73f04a14622ceff51464bbc560e","title":"Chat-UniVi: Unified Visual Representation Empowers Large Language Models with Image and Video Understanding"},{"paperId":"d6de7e7b86717c74a3e54839f6f8c2ee28f52b8e","title":"Towards Understanding the Geospatial Skills of ChatGPT: Taking a Geographic Information Systems (GIS) Exam"},{"paperId":"2fb605f67fee79cad94952ddfe0f686e926f49f5","title":"GPT-4V in Wonderland: Large Multimodal Models for Zero-Shot Smartphone GUI Navigation"},{"paperId":"76a3f4a79ae9a00db2f2b5f6877021d8deb96ada","title":"SPHINX: The Joint Mixing of Weights, Tasks, and Visual Embeddings for Multi-modal Large Language Models"},{"paperId":"ef321c6f174ac59916ac54ec40ad18bca5b58e5c","title":"PerceptionGPT: Effectively Fusing Visual Perception into LLM"},{"paperId":"cf7d69709bdeddd561c183178bbc1f0c2e156a08","title":"Analyzing Modular Approaches for Visual Question Decomposition"},{"paperId":"8ec7d50250203543a0098d99f04957b22bbe2c77","title":"How to Bridge the Gap between Modalities: A Comprehensive Survey on Multimodal Large Language Model"},{"paperId":"ecfff30e570498b6c87c5d1319a0cbcdf7a8b86d","title":"LLaVA-Plus: Learning to Use Tools for Creating Multimodal Agents"},{"paperId":"6ae4705139494fcb6b790b6dd6c4225b40ee40f8","title":"GLaMM: Pixel Grounding Large Multimodal Model"},{"paperId":"c62711f6b5d8620ba36bc2c378ec6ab53f6e197c","title":"RoboGen: Towards Unleashing Infinite Data for Automated Robot Learning via Generative Simulation"},{"paperId":"1672b010fe8abc50744b92a08bc9148c36aebe35","title":"Is GPT Powerful Enough to Analyze the Emotions of Memes?"},{"paperId":"c020f15be1dee20f9e2e0c5a6f05f272b5508325","title":"LLaVA-Interactive: An All-in-One Demo for Image Chat, Segmentation, Generation and Editing"},{"paperId":"e68ab63d505c3ee8d0518c734c3d2b13071cc18d","title":"MM-VID: Advancing Video Understanding with GPT-4V(ision)"},{"paperId":"0212dca18cd0765deed0b6ba80a796f0ad46e066","title":"mPLUG-Octopus: The Versatile Assistant Empowered by A Modularized End-to-End Multimodal LLM"},{"paperId":"807f336176070bd3f95b82a16f125ee99b7d2c80","title":"Woodpecker: Hallucination Correction for Multimodal Large Language Models"},{"paperId":"0b395ed1c8b284e551172b728e83cf257e33729a","title":"HallusionBench: An Advanced Diagnostic Suite for Entangled Language Hallucination&Visual Illusion in Large Vision-Language Models"},{"paperId":"79e7ead8f59b17431de2b86af10dc0c30a1f5a2b","title":"ToolChain*: Efficient Action Space Navigation in Large Language Models with A* Search"},{"paperId":"7451d756118628474dc022813eb952a21d34c5f6","title":"Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V"},{"paperId":"36b923d97d7cfaf73d11c55c15ea46605ba974a5","title":"BiLL-VTG: Bridging Large Language Models and Lightweight Visual Tools for Video-based Texts Generation"},{"paperId":"ac2e5bf716aed246ca8914a6816ef73e00286099","title":"Beyond Segmentation: Road Network Generation with Multi-Modal LLMs"},{"paperId":"03bf1da1caa5f63203d43ed78c12c35a78fc6ed9","title":"EasyGen: Easing Multimodal Generation with a Bidirectional Conditional Diffusion Model and LLMs"},{"paperId":"1d14a708622917da4b9820ada6d32af24fc1651a","title":"Idea2Img: Iterative Self-Refinement with GPT-4V(ision) for Automatic Image Design and Generation"},{"paperId":"a710efa9247207a72f06e0c9db302fd3ecab5fbb","title":"Towards Robust Multi-Modal Reasoning via Model Selection"},{"paperId":"90ff14a19419a0b6bb0965f0ab5e359462556172","title":"How Do Large Language Models Capture the Ever-changing World Knowledge? A Review of Recent Advances"},{"paperId":"7f1ba5630c3baa09b11cc665b3f71cdb117e5ffb","title":"OpenLEAF: Open-Domain Interleaved Image-Text Generation and Evaluation"},{"paperId":"8918e3cc21ecaf81532e452d3b9518360d14860e","title":"Reinforced UI Instruction Grounding: Towards a Generic UI Task Automation API"},{"paperId":"bee68767debbdc96d6f75947e544a8be98b869e3","title":"Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model: Explorations with GPT4-Vision and Beyond"},{"paperId":"b783168c885ecbae0fccdb46ec8e9afd0ef99b7f","title":"Avalon's Game of Thoughts: Battle Against Deception through Recursive Contemplation"},{"paperId":"bfeda6c7aa7899a80adb01894555b09d24756a59","title":"Corex: Pushing the Boundaries of Complex Reasoning through Multi-Model Collaboration"},{"paperId":"e61a96cf602ebff6683929aaf916e25614a475bc","title":"UPAR: A Kantian-Inspired Prompting Framework for Enhancing Large Language Model Capabilities"},{"paperId":"092245d86b77181c36f972b1b7a17a59cd989c4a","title":"Guiding Instruction-based Image Editing via Multimodal Large Language Models"},{"paperId":"7fe071ea76e49bc3e573beb53f07721630954247","title":"Promptbreeder: Self-Referential Self-Improvement Via Prompt Evolution"},{"paperId":"f01281b125128435ad134230c6a41cc55808eaac","title":"Can LLMs Effectively Leverage Graph Structural Information: When and Why"},{"paperId":"11a4284e335ba39330b59d9f42ca3272a6166991","title":"A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future"},{"paperId":"7b689adb8c156d6158660f90d1c86888ee281f63","title":"DreamLLM: Synergistic Multimodal Comprehension and Creation"},{"paperId":"f34b6b4f75fb4e6f2c5654ee13fb2479c6170b3a","title":"Kosmos-2.5: A Multimodal Literate Model"},{"paperId":"7a7128e9696dfedc24bf432c4b4c3aafa5e95a1a","title":"An Empirical Study of Scaling Instruct-Tuned Large Multimodal Models"},{"paperId":"3ec464696db25acc2c39a6d967ec3df09e06f633","title":"Multimodal Multi-Hop Question Answering Through a Conversation Between Tools and Efficiently Finetuned Large Language Models"},{"paperId":"4eb87eaa193929dbef93fa2db9419245a8e8916f","title":"TextBind: Multi-turn Interleaved Multimodal Instruction-following in the Wild"},{"paperId":"d39182113cd4176ead48027b4fc05fe06ec6aaca","title":"Language Models as Black-Box Optimizers for Vision-Language Models"},{"paperId":"09e55d5223104aa5726ff0fcf4043a1beeee410e","title":"Beyond Traditional Teaching: The Potential of Large Language Models and Chatbots in Graduate Engineering Education"},{"paperId":"c237a22698223e4060d83027f399f4fb2aa24291","title":"Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"},{"paperId":"bb1083425517bdac8d9a6438fcf5032543acb20e","title":"Evaluation and Analysis of Hallucination in Large Vision-Language Models"},{"paperId":"3b36d16985286b03e06e8404a7be49a9713d37b9","title":"Confucius: Iterative Tool Learning from Introspection Feedback by Easy-to-Difficult Curriculum"},{"paperId":"894ed1aba8e42a4ec27ba53ecde383b14c5128ca","title":"Examining User-Friendly and Open-Sourced Large GPT Models: A Survey on Language, Multimodal, and Scientific GPT Models"},{"paperId":"ef1ebdb24ce45fb57e271783a0c9a877fe0e79c3","title":"Position-Enhanced Visual Instruction Tuning for Multimodal Large Language Models"},{"paperId":"28c6ac721f54544162865f41c5692e70d61bccab","title":"A Survey on Large Language Model based Autonomous Agents"},{"paperId":"5e4597eb21a393b23e473cf66cb5ae8b27cab03e","title":"ExpeL: LLM Agents Are Experiential Learners"},{"paperId":"eb5cf10406a8ad31e0ebe56b36571d5db4758a62","title":"PUMGPT: A Large Vision-Language Model for Product Understanding"},{"paperId":"d53945d4afb4528590d79e20de52883d29037e86","title":"FashionLOGO: Prompting Multimodal Large Language Models for Fashion Logo Embeddings"},{"paperId":"1fd31b74f5e1eeb67341982fd35a613c6fad10e0","title":"Link-Context Learning for Multimodal LLMs"},{"paperId":"d6c2523ab97416c2692cbbeab082ed1790e8e55e","title":"VisIT-Bench: A Benchmark for Vision-Language Instruction Following Inspired by Real-World Use"},{"paperId":"4f2be887e991efa85f7b874e7ab871080a745c39","title":"CAESURA: Language Models as Multi-Modal Query Planners"},{"paperId":"dd0612ce863f64b0f69d0d9f708c52e829f6f859","title":"TPTU: Large Language Model-based AI Agents for Task Planning and Tool Usage"},{"paperId":"94972e30504017156ef5b5debc419bf6edc67384","title":"MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities"},{"paperId":"659a12d71d8709c132ccd9ccd235f0024cae0239","title":"The All-Seeing Project: Towards Panoptic Visual Recognition and Understanding of the Open World"},{"paperId":"446fb5dead075a1a08862662738f462e9a0e91c8","title":"Tool Documentation Enables Zero-Shot Tool-Usage with Large Language Models"},{"paperId":"ad113d8b6f8f6dace7abd12dc88af520aaaf3fc7","title":"LISA: Reasoning Segmentation via Large Language Model"},{"paperId":"bbcd5cc4bf6c77282e88cae07f7f2adb1da818ca","title":"Testing the Depth of ChatGPT's Comprehension via Cross-Modal Tasks Based on ASCII-Art: GPT3.5's Abilities in Regard to Recognizing and Generating ASCII-Art Are Not Totally Lacking"},{"paperId":"584ca135b61482fd89247113da87d784f738dbfa","title":"Foundational Models Defining a New Era in Vision: A Survey and Outlook"},{"paperId":"2e52178416b93a0a7dc1cb7d21d04c7e72c2ce0f","title":"ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring Instruction Tuning"},{"paperId":"ca31b8584b6c022ef15ddfe994fe361e002b7729","title":"A Comprehensive Overview of Large Language Models"},{"paperId":"094883e42bb9a41f602c0715c1059bc431e33fb2","title":"GPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest"},{"paperId":"ef4b604fca0c62dcd0d5caf7ca24ad74e285632d","title":"MultiQG-TI: Towards Question Generation from Multi-modal Sources"},{"paperId":"ebddfdc5d845a788e8062eddbbf7a335737cb99b","title":"What Matters in Training a GPT4-Style Language Model with Multimodal Inputs?"},{"paperId":"ea566f87f6253bb2d32cf7b61cd3e2535a0c3f42","title":"mPLUG-DocOwl: Modularized Multimodal Large Language Model for Document Understanding"},{"paperId":"82a9b8984e26fdf234431459bdb445fbcfc3cb76","title":"Visual Instruction Tuning with Polite Flamingo"},{"paperId":"ebedc4d7a2356090904baba4104ef0832bc236df","title":"A Survey on Multimodal Large Language Models"},{"paperId":"966852963a88a28786b798c91b6662d6e501e590","title":"AssistGPT: A General Multi-modal Assistant that can Plan, Execute, Inspect, and Learn"},{"paperId":"051549d8ef56937b2f4d113afdcf8c7586d3770b","title":"Towards AGI in Computer Vision: Lessons Learned from GPT and Large Language Models"},{"paperId":"fe50667e1bea4c6f63909b90986231240818c1d6","title":"ChatGPT vs Human-authored Text: Insights into Controllable Text Summarization and Sentence Style Transfer"},{"paperId":"d98536f24272e258b1d399074b64284d64786099","title":"AVIS: Autonomous Visual Information Seeking with Large Language Models"},{"paperId":"4c4d176c6e28f48041f215d563f6ee8633534cff","title":"Valley: Video Assistant with Large Language model Enhanced abilitY"},{"paperId":"fd755dc7b5b206c17fd953db04e1c888d45b6e4e","title":"LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark"},{"paperId":"d47524cd5c3c4b57af2e5a29f6f91c420310f236","title":"MIMIC-IT: Multi-Modal In-Context Instruction Tuning"},{"paperId":"ccef05840870479632f9055479a1269d53033b7b","title":"Certified Reasoning with Language Models"},{"paperId":"f9e33614577f03ddfda62c3122186407aa6be7d3","title":"Certified Deductive Reasoning with Language Models"},{"paperId":"615962d8969c8e0ffe43319689dce6c50cbf1f29","title":"Responsible Task Automation: Empowering Large Language Models as Responsible Task Automators"},{"paperId":"811115d36e1eabe2cef03b38a0809514e40b658e","title":"Chain-Of-Thought Prompting Under Streaming Batch: A Case Study"},{"paperId":"b458fc5261595f44b36325e5eaea1f874d65138f","title":"GPT4Tools: Teaching Large Language Model to Use Tools via Self-instruction"},{"paperId":"af705d648b5b16daa3dcc593bc593f2574d76c07","title":"Grammar Prompting for Domain-Specific Language Generation with Large Language Models"},{"paperId":"e45036dddb5f27d3e87d2f14a2d9e6a402e7b5b7","title":"SheetCopilot: Bringing Software Productivity to the Next Level through Large Language Models"},{"paperId":"50c1414fe41d0cb9db6f0933c9319aa124beac5d","title":"Contextual Object Detection with Multimodal Large Language Models"},{"paperId":"5ff2f5212713ec424662ac3c9e4aa5a8790d40cf","title":"ANPL: Towards Natural Programming with Interactive Decomposition"},{"paperId":"8ecdbfe011b7189fa0ee49ffc4e42a93d728a371","title":"On Evaluating Adversarial Robustness of Large Vision-Language Models"},{"paperId":"32dcd0887537cece54e214f531d2c384470b023f","title":"Large Language Models as Tool Makers"},{"paperId":"c6ac708b65b24c20f80831d518c1795ce8133ad5","title":"ChatBridge: Bridging Modalities with Large Language Model as a Language Catalyst"},{"paperId":"9c3a9b4821daa03cb5369041d59d2714329a3811","title":"Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models"},{"paperId":"9837349417e36ef5be06da0fd6c74042148bdaa2","title":"Visual Programming for Text-to-Image Generation and Evaluation"},{"paperId":"66d755730f5d08a6f4fcc5e81f24982ba389dca9","title":"LayoutGPT: Compositional Visual Planning and Generation with Large Language Models"},{"paperId":"00cb69a9f280317d1c59ac5827551ee9b10642b8","title":"EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought"},{"paperId":"7cf64070fd3d7e53d80f260c10e6bd7018d580e1","title":"IdealGPT: Iteratively Decomposing Vision and Language Reasoning via Large Language Models"},{"paperId":"3c50ef336232da0885ef61da386c98eac964b7cd","title":"PathAsst: A Generative Foundation AI Assistant Towards Artificial General Intelligence of Pathology"},{"paperId":"90027ca7802645671a69b00b65e1fa94e6b63544","title":"ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models"},{"paperId":"f9bfc6d9ba1665b73af3323d46c7642b852759ef","title":"VideoLLM: Modeling Video Sequence with Large Language Models"},{"paperId":"6fe2a4f7594abadafc9feb05e96ab7e9e634ce8f","title":"Album Storytelling with Iterative Story-aware Captioning and Large Language Models"},{"paperId":"ca055cfb9d4d47124cc035c346f38577825fcacf","title":"Enhance Reasoning Ability of Visual-Language Models via Large Language Models"},{"paperId":"6a5525c316b9be7909c433a79e090ed731425083","title":"What Makes for Good Visual Tokenizers for Large Language Models?"},{"paperId":"42a30dc5470f54ec249f25d3c31e05d7c376c8e3","title":"VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks"},{"paperId":"692bc40edf4785d88c39e0c0fe9f270541fecf8a","title":"Towards Generalist Robots: A Promising Paradigm via Generative Simulation"},{"paperId":"d48cb91b9e555194f7494c4d4bb9815021d3ee45","title":"VideoChat: Chat-Centric Video Understanding"},{"paperId":"54a8b153ed04a872da878d695239bdc413dc782c","title":"InternGPT: Solving Vision-Centric Tasks by Interacting with Chatbots Beyond Language"},{"paperId":"d6d3604f369bb0415cbe814e43ca3131323b03e2","title":"Otter: A Multi-Modal Model with In-Context Instruction Tuning"},{"paperId":"570079bbdd8758dfe865097e05719313c9c1301a","title":"LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model"},{"paperId":"7e32aac43e9f1df49e116add03327ee6f365dbf3","title":"mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality"},{"paperId":"ca6a2bc279be5a3349a22bfd6866ed633d18734b","title":"MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models"},{"paperId":"170c97c7215f42edfb20c2248f954879e91ef86e","title":"Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models"},{"paperId":"352420ee61a8da783ca7750170793613b18b8d9c","title":"Tool Learning with Foundation Models"},{"paperId":"a5036f31f0e629dc661f120b8c3b1f374d479ab8","title":"Visual Instruction Tuning"},{"paperId":"354dcdebf3f8b5feeed5c62090e0bc1f0c28db06","title":"ChemCrow: Augmenting large-language models with chemistry tools"},{"paperId":"0ebc861f5478561f12941e6b48aad30574e996d8","title":"Video ChatCaptioner: Towards Enriched Spatiotemporal Descriptions"},{"paperId":"b61f4b2c0d20a6595cf1456d08e5e2770a5913ef","title":"Artificial intelligence—friend or foe in fake news campaigns"},{"paperId":"ac7771c332da42b29a913b116bd6ef622cbf89cf","title":"TaskMatrix.AI: Completing Tasks by Connecting Foundation Models with Millions of APIs"},{"paperId":"53df959bcf6499c45e316086a96a624389a39a52","title":"Accountable Textual-Visual Chat Learns to Reject Human Instructions in Image Re-creation"},{"paperId":"6845bea94b2fb17d4377b3bb2bd10f73a959f9cc","title":"Reasoning with Language Model Prompting: A Survey"},{"paperId":"a3711dbf296b5ddd97ba93826660cd3995611625","title":"Towards A Foundation Model for Generalist Robots: Diverse Skill Learning at Scale via Automated Task and Scene Generation"},{"paperId":"0167f681bac9b7f6cf396e8a5fc6e46c62fd1896","title":"Towards Understanding the Spatial Literacy of ChatGPT ∗ Taking a Geographic Information Systems (GIS) Exam"},{"paperId":"96a6df2b4aa50cfbd8984933e9c66b0763fc08a6","title":"Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V"},{"paperId":"65d5728ea17f016382870aa27aac1e78d590b50c","title":"HallusionBench: You See What You Think? Or You Think What You See? An Image-Context Reasoning Benchmark Challenging for GPT-4V(ision), LLaVA-1.5, and Other Multi-modality Models"},{"paperId":"bca0bbd01ea917b7a9fe369288ea3ba03d3b1ff3","title":"A Survey of Large Language Models in Medicine: Progress, Application, and Challenge"},{"paperId":"5ce94181ea702f69c3651dce721d6bd8026b8106","title":"TPTU: Task Planning and Tool Usage of Large Language Model-based AI Agents"},{"paperId":"44ccf252018f71898d52d89539f17d77a4f8d548","title":"Chart Understanding with Large Language Model"}],"references":[{"paperId":"d62c4d00b277e948956b6610ce2644e88fe1577b","title":"Large Language Models"},{"paperId":"ace2b6367a067898f66a33fca19581ebe71fccc5","title":"GPT-4 Technical Report"},{"paperId":"6e754273d54a91371efbc928cd6b156364d517da","title":"ViperGPT: Visual Inference via Python Execution for Reasoning"},{"paperId":"af997821231898a5f8d0fd78dad4eec526acabe5","title":"Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models"},{"paperId":"38fe8f324d2162e63a967a9ac6648974fc4c66f3","title":"PaLM-E: An Embodied Multimodal Language Model"},{"paperId":"fbfef4723d8c8467d7bd523e1d0b703cce0e0f9c","title":"Language Is Not All You Need: Aligning Perception with Language Models"},{"paperId":"61e721334296ebfbbf6443b5ed9eb8c83b708c95","title":"Scaling Vision Transformers to 22 Billion Parameters"},{"paperId":"53d128ea815bcc0526856eb5a9c42cc977cb36a7","title":"Toolformer: Language Models Can Teach Themselves to Use Tools"},{"paperId":"780a7f5e8ba9b4b451e3dfee1bcfb0f68aba5050","title":"Multimodal Chain-of-Thought Reasoning in Language Models"},{"paperId":"3f5b31c4f7350dc88002c121aecbdc82f86eb5bb","title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models"},{"paperId":"967907503b24423b9b74621051811fcf684e3957","title":"Generalized Decoding for Pixel, Image, and Language"},{"paperId":"f208ea909fa7f54fea82def9a92fd81dfc758c39","title":"Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions"},{"paperId":"6c1e1cc1e0e1f8fd026fe517607b2d4535565fa7","title":"PAL: Program-aided Language Models"},{"paperId":"a5cb8f26acb71edd77ff9a143d3ddaab2367eb40","title":"PromptCap: Prompt-Guided Task-Aware Image Captioning"},{"paperId":"99832586d55f540f603637e458a292406a0ed75d","title":"ReAct: Synergizing Reasoning and Acting in Language Models"},{"paperId":"d3135733aa39dec20ce72aa138589dda27c8406d","title":"Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering"},{"paperId":"60ee030773ba1b68eb222a265b052ca028353362","title":"GIT: A Generative Image-to-text Transformer for Vision and Language"},{"paperId":"e7ad08848d5d7c5c47673ffe0da06af443643bda","title":"Large Language Models are Zero-Shot Reasoners"},{"paperId":"c1ace33daf974d3d16752c7a8565f32a63b09c49","title":"Language Models with Image Descriptors are Strong Few-Shot Video-Language Learners"},{"paperId":"26218bdcc3945c7edae7aa2adbfba4cd820a2df3","title":"Flamingo: a Visual Language Model for Few-Shot Learning"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"cb5e3f085caefd1f3d5e08637ab55d39e61234fc","title":"Do As I Can, Not As I Say: Grounding Language in Robotic Affordances"},{"paperId":"ada81a4de88a6ce474df2e2446ad11fea480616e","title":"Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language"},{"paperId":"1b6e810ce0afd0dd093f789d2b2742d047e316d5","title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"92a8f7f09f3705cb5a6009a42220a6f01ea084e8","title":"Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents"},{"paperId":"2f3efe44083af91cef562c1a3451eee2f8601d22","title":"WebGPT: Browser-assisted question-answering with human feedback"},{"paperId":"21ec90872abd986c12afe39bebe807732ffa70c9","title":"Florence: A New Foundation Model for Computer Vision"},{"paperId":"2672777d25562c9df6fc13b653181db62d39bece","title":"An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA"},{"paperId":"01b5412f3d17e90e09226d7c40ad4d4468a1414d","title":"Multimodal Few-Shot Learning with Frozen Language Models"},{"paperId":"268d347e8a55b5eb82fb5e7d2f800e33c75ab18a","title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"962dc29fdc3fbdc5930a10aba114050b82fe5a3e","title":"End-to-End Object Detection with Transformers"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"2c03df8b48bf3fa39054345bafabfeff15bfd11d","title":"Deep Residual Learning for Image Recognition"},{"paperId":"6424b69f3ff4d35249c0bb7ef912fbc2c86f4ff4","title":"Deep Learning Face Attributes in the Wild"},{"paperId":"71b7178df5d2b112d07e45038cb5637208659ff7","title":"Microsoft COCO: Common Objects in Context"},{"paperId":"b80a580a6f2eca77524302acd944fd6edf0a0611","title":"NUS-WIDE: a real-world web image database from National University of Singapore"},{"paperId":"f79131806747fce087d0fe73d0867cc621547b2a","title":"The MIR flickr retrieval evaluation"},{"paperId":null,"title":"Langchain. https://langchain. readthedocs.io/, 2023"},{"paperId":null,"title":"Case studies of MM-REACT's extensibility. With the new release of GPT-4 (Language-only), we can upgrade ChatGPT in MM-REACT to GPT-4 (Langauge-only) and further improve the results of MM-REACT"},{"paperId":null,"title":"Here's a breakdown of the start and end times for each step in the video: 1"},{"paperId":null,"title":"GPT-4 (Multimodal)"},{"paperId":null,"title":"I am trying to create a short version of the video, can you suggest a shorter segment for each step? Here's a suggested shorter segment for each step: 1"},{"paperId":null,"title":"Ours w/ ChatGPT Ours w/ GPT-4 (Language-only)"},{"paperId":null,"title":"Can you write a template for this website in HTML? Replace the joke and punchline with a real one"}],"id":"c7a9c7302a72301ed79a7c0696d5af2e03ad3ac4","summary":"This paper defines and explores a comprehensive list of advanced vision tasks that are intriguing to solve, but may exceed the capabilities of existing vision and vision-language models and proposes MM-REACT, a system paradigm that integrates ChatGPT with a pool of vision experts to achieve multimodal reasoning and action."},{"url":"https://www.semanticscholar.org/paper/2ea26b243171e37ef20af269942ffde414f9f8cc","title":"UnICLAM: Contrastive Representation Learning with Adversarial Masking for Unified and Interpretable Medical Vision Question Answering","venue":"arXiv.org","year":2022,"referenceCount":46,"citationCount":3,"influentialCitationCount":0,"publicationDate":"21/12/2022","authors":"Chenlu Zhan,Peng Peng,Hongsen Wang,Tao Chen,Hongwei Wang","citations":[{"paperId":"2c7e346aa311fec4dda04bdf3a214ce2026d8807","title":"Medical Vision Language Pretraining: A survey"},{"paperId":"1df754afa6d27b630ecf91d15bb1ae4ed12a194e","title":"A Survey on Interpretable Cross-modal Reasoning"},{"paperId":"baa1dc079d98ca76b0173c8d653fed759fd0a371","title":"A scoping review on multimodal deep learning in biomedical images and texts"}],"references":[{"paperId":"cfca7eedc6ede9d363d1662280a74d78dcdc9d4a","title":"Scaling Language-Image Pre-Training via Masking"},{"paperId":"6e7763ec04906726377953cc85f31a1a0c889001","title":"Anomaly Matters: An Anomaly-Oriented Model for Medical Visual Question Answering"},{"paperId":"e9480d62e216f77d5556b7eda769daa4c92d004d","title":"VQAMix: Conditional Triplet Mixup for Medical Visual Question Answering"},{"paperId":"8af37e55e7994860e6eeb839fd06ec271619a241","title":"SemMAE: Semantic-Guided Masking for Learning Masked Autoencoders"},{"paperId":"004b97aea43f9f62cc49dec20f449abfbae28811","title":"Masked Autoencoders As Spatiotemporal Learners"},{"paperId":"213738a30bc7283cc4447ac87fe783a03a7aae5d","title":"UTC: A Unified Transformer with Inter-Task Contrastive Learning for Visual Dialog"},{"paperId":"4b8d8d36a18fd61a6eda3322d8dd3baad2819600","title":"Unified Contrastive Learning in Image-Text-Label Space"},{"paperId":"9b3fa3a80afdb6d34984307e457656420e60e7e7","title":"Should You Mask 15% in Masked Language Modeling?"},{"paperId":"76d3b9d8685b88866abd19615ac0868061ced7e6","title":"Adversarial Masking for Self-Supervised Learning"},{"paperId":"a3b42a83669998f65df60d7c065a70d07ca95e99","title":"BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation"},{"paperId":"4ef3d9e492479e28fa57d107e52acc6a0c803de2","title":"Does CLIP Benefit Visual Question Answering in the Medical Domain as Much as it Does in the General Domain?"},{"paperId":"6351ebb4a3287f5f3e1273464b3b91e5df5a16d7","title":"Masked Autoencoders Are Scalable Vision Learners"},{"paperId":"cf7c2e0e4fb2af689aaf4b7a7cddf7b1f4d5e3f0","title":"VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts"},{"paperId":"b82c5f9efdb2ae56baa084ca41aeddd8a665c1d1","title":"Align before Fuse: Vision and Language Representation Learning with Momentum Distillation"},{"paperId":"722ad6ac92286507437b31486f47987d6ece05c9","title":"BEiT: BERT Pre-Training of Image Transformers"},{"paperId":"5bd42c29a5ba8a6c39547db89023d879e98a6b32","title":"Multiple Meta-model Quantifying for Medical Visual Question Answering"},{"paperId":"3c83f80f06633ff4598d33c2959f8e4cdcad3e93","title":"Cross-Modal Self-Attention with Multi-Task Pre-Training for Medical Visual Question Answering"},{"paperId":"f0524b3005720bcff886bcb0227f7f0dd924ff07","title":"VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text"},{"paperId":"166e98317ed9c4687e71bef55a6800431e00b8fa","title":"SiT: Self-supervised vIsion Transformer"},{"paperId":"17c2bb358169541f2d0a769f80779f46d1cd3d37","title":"MMBERT: Multimodal BERT Pretraining for Improved Medical VQA"},{"paperId":"93b6b79b4ef6c345f31722ce7c829385c6dce0d6","title":"Slake: A Semantically-Labeled Knowledge-Enhanced Dataset For Medical Visual Question Answering"},{"paperId":"141a5033d9994242b18bb3b217e79582f1ee9306","title":"Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision"},{"paperId":"5e5fbc41106db9acaaf3a365801051e477f0e984","title":"UNIMO: Towards Unified-Modal Understanding and Generation via Cross-Modal Contrastive Learning"},{"paperId":"0e23d2f14e7e56e81538f4a63e11689d8ac1eb9d","title":"Exploring Simple Siamese Representation Learning"},{"paperId":"b8d5b853f2212cbb48a43f1edec9b96d76d388ec","title":"Medical Visual Question Answering via Conditional Reasoning"},{"paperId":"a1b8a8df281bbaec148a897927a49ea47ea31515","title":"Improved Baselines with Momentum Contrastive Learning"},{"paperId":"6adb61121ca4560915ade532910acde56440b88f","title":"A Question-Centric Model for Visual Question Answering in Medical Imaging"},{"paperId":"34733eaf66007516347a40ad5d9bbe1cc9dacb6b","title":"A Simple Framework for Contrastive Learning of Visual Representations"},{"paperId":"6d38bdd172cdbccb11745f5f031f848679117f25","title":"CHAOS Challenge - Combined (CT-MR) Healthy Abdominal Organ Segmentation"},{"paperId":"33301b25a297b701bdc287e985c006375cb7bb21","title":"Overcoming Data Limitation in Medical Visual Question Answering"},{"paperId":"4654aa505e5bcdb089d0df202cd7ceabc9d2d41f","title":"A large annotated medical image dataset for the development and evaluation of segmentation algorithms"},{"paperId":"a564fabf130ff6e2742cfba90c7a4018937d764d","title":"Radiology Objects in COntext (ROCO): A Multimodal Image Dataset"},{"paperId":"a5d10341717c0519cf63151b496a6d2ed67aa05f","title":"Bilinear Attention Networks"},{"paperId":"8e9ad6f8b2bc97f0412fa0cc243ac6975864534a","title":"Multi-modal Factorized Bilinear Pooling with Co-attention Learning for Visual Question Answering"},{"paperId":"c889d6f98e6d79b89c3a6adf8a921f88fa6ba518","title":"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks"},{"paperId":"5582bebed97947a41e3ddd9bd1f284b73f1648c2","title":"Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization"},{"paperId":"12f7de07f9b00315418e381b2bd797d21f12b419","title":"Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding"},{"paperId":"2c1890864c1c2b750f48316dc8b650ba4772adc5","title":"Stacked Attention Networks for Image Question Answering"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"f37e1b62a767a307c046404ca96bc140b3e68cb5","title":"GloVe: Global Vectors for Word Representation"},{"paperId":"519799a9e2a72b808d81c6bcba5de263503de053","title":"Contrastive Pre-training and Representation Distillation for Medical Visual Question Answering Based on Radiology Images"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"692534d0d47974ba2d4d230a80a7efe763b33ad0","title":"Advances in Neural Information Processing Systems 31"},{"paperId":"18f9a6045ba01cb079c4fa49a630d71bbd27cd92","title":"Descriptor : A dataset of clinically generated visual questions and answers about radiology images"},{"paperId":"441281c07b5e5949aeb56375e25623ddbdab94f4","title":"Intravascular Imaging and Computer Assisted Stenting, and Large-Scale Annotation of Biomedical Data and Expert Label Synthesis"},{"paperId":"13167f9cd8c7906ca808b01d28dca6dd951da8a5","title":"of the Association for Computational Linguistics"}],"id":"2ea26b243171e37ef20af269942ffde414f9f8cc","summary":"Experimental results on VQA-RAD and SLAKE public benchmarks demonstrate that UnICLAM outperforms existing 11 state-of-the-art Medical-VQA models and makes an additional discussion about the performance of UnicLAM in diagnosing heart failure, verifying that it exhibits superior few-shot adaption performance in practical disease diagnosis."},{"url":"https://www.semanticscholar.org/paper/3d45e69557f0c6a54ec698304c2e27ec29bc1c2b","title":"PMC-CLIP: Contrastive Language-Image Pre-training using Biomedical Documents","venue":"arXiv.org","year":2023,"referenceCount":33,"citationCount":23,"influentialCitationCount":0,"publicationDate":"13/03/2023","authors":"Weixiong Lin,Ziheng Zhao,Xiaoman Zhang,Chaoyi Wu,Ya Zhang,Yanfeng Wang,Weidi Xie","citations":[{"paperId":"a3d418b4e35a02e4306505ab660a6bcd44c3c752","title":"Asclepius: A Spectrum Evaluation Benchmark for Medical Multi-Modal Large Language Models"},{"paperId":"ae7bf155b421e10d8772a5eb722c6e4cc69a3566","title":"Mind the Modality Gap: Towards a Remote Sensing Vision-Language Model via Cross-modal Alignment"},{"paperId":"7580327ffc9bd5daef83fe8285c0476ca074051d","title":"OmniMedVQA: A New Large-Scale Comprehensive Evaluation Benchmark for Medical LVLM"},{"paperId":"2cd8c14938aed3f1de4ccbe722dc5241bf62079a","title":"RAD-DINO: Exploring Scalable Medical Image Encoders Beyond Text Supervision"},{"paperId":"18d9b13e3383d98c181f4d7a2b3ca1503ed707a0","title":"No-boundary thinking for artificial intelligence in bioinformatics and education"},{"paperId":"93886752191db25efd096a65af7b09df5c0a64e0","title":"Data-Centric Foundation Models in Computational Healthcare: A Survey"},{"paperId":"b1721374889899950994f67029fe899de257c140","title":"A Foundational Multimodal Vision Language AI Assistant for Human Pathology"},{"paperId":"6bdfffbf92d01c8b543088d40d46233610e469a8","title":"CLIP in Medical Imaging: A Comprehensive Survey"},{"paperId":"43989495f64ca97a1ac0a5d9efad0c2d871c01ed","title":"TransMed: Large Language Models Enhance Vision Transformer for Biomedical Image Classification"},{"paperId":"2c7e346aa311fec4dda04bdf3a214ce2026d8807","title":"Medical Vision Language Pretraining: A survey"},{"paperId":"59ba440bdce4b9b963124a46ee87b63b67c4c58c","title":"Training CLIP models on Data from Scientific Papers"},{"paperId":"8d2709ed1788a67e64425fb410bb49f3ee49e088","title":"Large Language Models Illuminate a Progressive Pathway to Artificial Healthcare Assistant: A Review"},{"paperId":"c67a58bb5eb9cb6557a6032bb058a5cab978907f","title":"Qilin-Med-VL: Towards Chinese Large Vision-Language Model for General Healthcare"},{"paperId":"7bf2733f3e98c0e334a040186886bca8d1b238f7","title":"Uni-Dual: A Generic Unified Dual-Task Medical Self-Supervised Learning Framework"},{"paperId":"c7492913370b5726eaa6ced163a60de6c9d4bb7f","title":"A Survey of Large Language Models for Healthcare: from Data, Technology, and Applications to Accountability and Ethics"},{"paperId":"f0950a3f27c0fefffba60ae1c9a8ee360d5eb55f","title":"Instruction Tuning for Large Language Models: A Survey"},{"paperId":"df0ddb588a200d095743e9d26fc4a9318619766e","title":"Towards Generalist Foundation Model for Radiology"},{"paperId":"c9dbdae8146b9f97e254f5d26fd6efde96eaa703","title":"Med-Flamingo: a Multimodal Medical Few-shot Learner"},{"paperId":"813ba033b8f593c98f9af44c5b4901408ba6f70a","title":"Towards a Visual-Language Foundation Model for Computational Pathology"},{"paperId":"b0883ddf7fb07aabfeb2b2f593e7ac302aa42372","title":"PRIOR: Prototype Representation Joint Learning from Medical Images and Reports"},{"paperId":"3c50ef336232da0885ef61da386c98eac964b7cd","title":"PathAsst: A Generative Foundation AI Assistant Towards Artificial General Intelligence of Pathology"},{"paperId":"f4793adffd6f67ffcb93ccfc5672ab301b8a2b96","title":"PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering"},{"paperId":"bca0bbd01ea917b7a9fe369288ea3ba03d3b1ff3","title":"A Survey of Large Language Models in Medicine: Progress, Application, and Challenge"}],"references":[{"paperId":"fa988afd9889451e5fcc46a316bd1e2a6abda367","title":"A Self-Adaptive Discriminative Autoencoder for Medical Applications"},{"paperId":"81adb80e390f25d4a2d764b1063eeaa2c334d441","title":"Multi-modal Masked Autoencoders for Medical Vision-and-Language Pre-training"},{"paperId":"28ff0816f19a5e3e37eac5569de41872fd262f0a","title":"Align, Reason and Learn: Enhancing Medical Vision-and-Language Pre-training with Knowledge"},{"paperId":"02251886950770e82b3d68564d60cdfe15e73199","title":"Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks"},{"paperId":"75bb9eda70751c63fc54dbe63377c673b7dbdb15","title":"CogView2: Faster and Better Text-to-Image Generation via Hierarchical Transformers"},{"paperId":"c57293882b2561e1ba03017902df9fc2f289dea2","title":"Hierarchical Text-Conditional Image Generation with CLIP Latents"},{"paperId":"38d089e36d630189eb6c5274066d57efd48a187d","title":"DWT-CV: Dense weight transfer-based cross validation strategy for model selection in biomedical data analysis"},{"paperId":"d766bffc357127e0dc86dd69561d5aeb520d6f4c","title":"Training language models to follow instructions with human feedback"},{"paperId":"94ff111c4d81bd03f159321728ceec8b4711c89d","title":"An Empirical Study of Training End-to-End Vision-and-Language Transformers"},{"paperId":"a66606a0bb11a1d5a7a14ec09df8a3481121ad6c","title":"MedMNIST v2 - A large-scale lightweight benchmark for 2D and 3D biomedical image classification"},{"paperId":"65a57c948755cffad38de4230937ffbff8a19783","title":"MedMNIST v2: A Large-Scale Lightweight Benchmark for 2D and 3D Biomedical Image Classification"},{"paperId":"b82c5f9efdb2ae56baa084ca41aeddd8a665c1d1","title":"Align before Fuse: Vision and Language Representation Learning with Momentum Distillation"},{"paperId":"ef39d344161d2af825b650168aa332f2217c406a","title":"EXSCLAIM! - An automated pipeline for the construction of labeled materials imaging datasets from literature"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"93b6b79b4ef6c345f31722ce7c829385c6dce0d6","title":"Slake: A Semantically-Labeled Knowledge-Enhanced Dataset For Medical Visual Question Answering"},{"paperId":"0839722fb5369c0abaff8515bfc08299efc790a1","title":"ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision"},{"paperId":"5ba77a5bdeffb62aa0902ae68997bbc38db8a722","title":"MedICaT: A Dataset of Medical Images, Captions, and Textual References"},{"paperId":"a2f38d03fd363e920494ad65a5f0ad8bd18cd60b","title":"Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"962dc29fdc3fbdc5930a10aba114050b82fe5a3e","title":"End-to-End Object Detection with Transformers"},{"paperId":"d1f407b16fb8d99487baee37ed0805676c58e7ac","title":"MIMIC-CXR, a de-identified publicly available database of chest radiographs with free-text reports"},{"paperId":"33301b25a297b701bdc287e985c006375cb7bb21","title":"Overcoming Data Limitation in Medical Visual Question Answering"},{"paperId":"da076089117ca1e8bce65dfa848d23da914b63c5","title":"DocFigure: A Dataset for Scientific Document Figure Classification"},{"paperId":"a564fabf130ff6e2742cfba90c7a4018937d764d","title":"Radiology Objects in COntext (ROCO): A Multimodal Image Dataset"},{"paperId":"b227f3e4c0dc96e5ac5426b85485a70f2175a205","title":"Representation Learning with Contrastive Predictive Coding"},{"paperId":"d07284a6811f1b2745d91bdb06b040b57f226882","title":"Decoupled Weight Decay Regularization"},{"paperId":"2c03df8b48bf3fa39054345bafabfeff15bfd11d","title":"Deep Residual Learning for Image Recognition"},{"paperId":"f7609414ae3a117cd2828c7dae19ad34ff7d72e6","title":"PubMed Central: The GenBank of the published literature."},{"paperId":"519799a9e2a72b808d81c6bcba5de263503de053","title":"Contrastive Pre-training and Representation Distillation for Medical Visual Question Answering Based on Radiology Images"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"18f9a6045ba01cb079c4fa49a630d71bbd27cd92","title":"Descriptor : A dataset of clinically generated visual questions and answers about radiology images"},{"paperId":"0adeb54d32925d200bb313a8e0f06116e49c67fb","title":"The Unified Medical Language System (UMLS): integrating biomedical terminology"},{"paperId":null,"title":"Authors Suppressed Due to Excessive Length"}],"id":"3d45e69557f0c6a54ec698304c2e27ec29bc1c2b","summary":"PMC-OA, a biomedical dataset with 1.6M image-caption pairs collected from PubMedCentral's OpenAccess subset, is built and released, which is 8 times larger than before and achieves state-of-the-art results on various downstream tasks."},{"url":"https://www.semanticscholar.org/paper/170667a96f04adf3b3b83526f75fe8d1063e0f7a","title":"Self-supervised vision-language pretraining for Medical visual question answering","venue":"arXiv.org","year":2022,"referenceCount":26,"citationCount":8,"influentialCitationCount":0,"publicationDate":"24/11/2022","authors":"Pengfei Li,Gang Liu,Lin Tan,Jinying Liao,Shenjun Zhong","citations":[{"paperId":"bc431ad78e91a73a69580b7d256d18777dcda313","title":"Prompt-based Personalized Federated Learning for Medical Visual Question Answering"},{"paperId":"420087f314633a381e61e6c5cd73ccc2070a749e","title":"PeFoMed: Parameter Efficient Fine-tuning on Multimodal Large Language Models for Medical Visual Question Answering"},{"paperId":"2c7e346aa311fec4dda04bdf3a214ce2026d8807","title":"Medical Vision Language Pretraining: A survey"},{"paperId":"1f280fe1f800bae53ada94b30f244fab0cd9f795","title":"KI-MAG: A knowledge-infused abstractive question answering system in medical domain"},{"paperId":"baa1dc079d98ca76b0173c8d653fed759fd0a371","title":"A scoping review on multimodal deep learning in biomedical images and texts"},{"paperId":"0f8d12775a4685575f1489796b5dee9e11fbdfb5","title":"OphGLM: Training an Ophthalmology Large Language-and-Vision Assistant based on Instructions and Dialogue"},{"paperId":"f22d71c7ce9720ba1f717a4f1181488200e78198","title":"LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day"},{"paperId":"07d45ce7de598ef03b400f8ddba7d2e055e77a08","title":"BiomedGPT: A Unified and Generalist Biomedical Generative Pre-trained Transformer for Vision, Language, and Multimodal Tasks"}],"references":[{"paperId":"e9480d62e216f77d5556b7eda769daa4c92d004d","title":"VQAMix: Conditional Triplet Mixup for Medical Visual Question Answering"},{"paperId":"ed1d78633567b82e2626c4af6f3bbbf9c420d5aa","title":"Type-Aware Medical Visual Question Answering"},{"paperId":"4ef3d9e492479e28fa57d107e52acc6a0c803de2","title":"Does CLIP Benefit Visual Question Answering in the Medical Domain as Much as it Does in the General Domain?"},{"paperId":"6351ebb4a3287f5f3e1273464b3b91e5df5a16d7","title":"Masked Autoencoders Are Scalable Vision Learners"},{"paperId":"b82c5f9efdb2ae56baa084ca41aeddd8a665c1d1","title":"Align before Fuse: Vision and Language Representation Learning with Momentum Distillation"},{"paperId":"5bd42c29a5ba8a6c39547db89023d879e98a6b32","title":"Multiple Meta-model Quantifying for Medical Visual Question Answering"},{"paperId":"3c83f80f06633ff4598d33c2959f8e4cdcad3e93","title":"Cross-Modal Self-Attention with Multi-Task Pre-Training for Medical Visual Question Answering"},{"paperId":"17c2bb358169541f2d0a769f80779f46d1cd3d37","title":"MMBERT: Multimodal BERT Pretraining for Improved Medical VQA"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"93b6b79b4ef6c345f31722ce7c829385c6dce0d6","title":"Slake: A Semantically-Labeled Knowledge-Enhanced Dataset For Medical Visual Question Answering"},{"paperId":"268d347e8a55b5eb82fb5e7d2f800e33c75ab18a","title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"},{"paperId":"9fe3eeafbe022de014aeb54d0b55502e2a2e46fe","title":"Hierarchical Deep Multi-modal Network for Medical Visual Question Answering"},{"paperId":"ed6ce80789889c0fd56c8117f85079c1c31fe426","title":"CGMVQA: A New Classification and Generative Model for Medical Visual Question Answering"},{"paperId":"add2f205338d70e10ce5e686df4a690e2851bdfc","title":"Momentum Contrast for Unsupervised Visual Representation Learning"},{"paperId":"87f6a7c014ce206ac5b57299c07e10667d194b39","title":"Randaugment: Practical automated data augmentation with a reduced search space"},{"paperId":"33301b25a297b701bdc287e985c006375cb7bb21","title":"Overcoming Data Limitation in Medical Visual Question Answering"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"5582bebed97947a41e3ddd9bd1f284b73f1648c2","title":"Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization"},{"paperId":"2c03df8b48bf3fa39054345bafabfeff15bfd11d","title":"Deep Residual Learning for Image Recognition"},{"paperId":"eb42cf88027de515750f230b23b1a057dc782108","title":"Very Deep Convolutional Networks for Large-Scale Image Recognition"},{"paperId":"d2c733e34d48784a37d717fe43d9e93277a8c53e","title":"ImageNet: A large-scale hierarchical image database"},{"paperId":"2e9d221c206e9503ceb452302d68d10e293f2a10","title":"Long Short-Term Memory"},{"paperId":"6b0df129ed1930525c5ebe57ccabfb3d23f89be8","title":"Overview of ImageCLEFmedical 2022 - Caption Prediction and Concept Detection"},{"paperId":"2551990a1ccdffb1a4d1d9040b2d493ba6d26dd1","title":"Towards Visual Question Answering on Pathology Images"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"18f9a6045ba01cb079c4fa49a630d71bbd27cd92","title":"Descriptor : A dataset of clinically generated visual questions and answers about radiology images"}],"id":"170667a96f04adf3b3b83526f75fe8d1063e0f7a","summary":"A self-supervised method that applies Masked image modeling, Masked language modeling, Image text matching and Image text alignment via contrastive learning (M2I2) for pretraining on medical image caption dataset, and finetunes to downstream medical VQA tasks is proposed."}]}
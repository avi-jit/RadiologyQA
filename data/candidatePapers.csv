"id","url","score","title","summary","venue","year","authors","citationCount","referenceCount","influentialCitationCount"
"584ca135b61482fd89247113da87d784f738dbfa","https://www.semanticscholar.org/paper/584ca135b61482fd89247113da87d784f738dbfa",7,"Foundational Models Defining a New Era in Vision: A Survey and Outlook","A comprehensive review of emerging foundational models in computer vision, including typical architecture designs to combine different modalities, training objectives, pre-training datasets, fine-tuning mechanisms, and the common prompting patterns; textual, visual, and heterogeneous.","arXiv.org",2023,"Muhammad Awais,Muzammal Naseer,Salman Siddique Khan,R. Anwer,Hisham Cholakkal,M. Shah,Ming Yang,F. Khan",3,365,1
"d6d3604f369bb0415cbe814e43ca3131323b03e2","https://www.semanticscholar.org/paper/d6d3604f369bb0415cbe814e43ca3131323b03e2",6,"Otter: A Multi-Modal Model with In-Context Instruction Tuning","Otter, a multi-modal model based on OpenFlamingo (open-sourced version of DeepMind's Flamingo), trained on MIMIC-IT and showcasing improved instruction-following ability and in-context learning is introduced.","arXiv.org",2023,"Bo Li,Yuanhan Zhang,Liangyu Chen,Jinghao Wang,Jingkang Yang,Ziwei Liu",43,38,11
"86cbd30d1096b0c7e4ac6b03d97a8df12fd21457","https://www.semanticscholar.org/paper/86cbd30d1096b0c7e4ac6b03d97a8df12fd21457",6,"PathAsst: Redefining Pathology through Generative Foundation AI Assistant for Pathology","The PathAsst is presented, which is a generative foundation AI assistant to revolutionize diagnostic and predictive analytics in pathology, trained based on Vicuna-13B language model in coordination with the CLIP vision encoder.","arXiv.org",2023,"Yuxuan Sun,Chenglu Zhu,S. Zheng,Kai Zhang,Zhongyi Shui,Xiaoxuan Yu,Yi-Lei Zhao,Honglin Li,Yunlong Zhang,Ruojia Zhao,Xinheng Lyu,Lin Yang",3,47,0
"ebedc4d7a2356090904baba4104ef0832bc236df","https://www.semanticscholar.org/paper/ebedc4d7a2356090904baba4104ef0832bc236df",6,"A Survey on Multimodal Large Language Models","This paper presents the formulation of MLLM and delineate its related concepts, and discusses the key techniques and applications, including Multimodal Instruction Tuning (M-IT), Multimmodal In-Context Learning (M -ICL), MultIModal Chain of Thought (m-CoT), and LLM-Aided Visual Reasoning (LAVR).","arXiv.org",2023,"Shukang Yin,Chaoyou Fu,Sirui Zhao,Ke Li,Xing Sun,Tong Xu,Enhong Chen",13,108,0
"570079bbdd8758dfe865097e05719313c9c1301a","https://www.semanticscholar.org/paper/570079bbdd8758dfe865097e05719313c9c1301a",5,"LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model","This work augments LLaMA-Adapter by unlocking more learnable parameters and proposes an early fusion strategy to feed visual tokens only into the early LLM layers, contributing to better visual knowledge incorporation and achieves strong multi-modal reasoning with only a small-scale image-text and instruction dataset.","arXiv.org",2023,"Peng Gao,Jiaming Han,Renrui Zhang,Ziyi Lin,Shijie Geng,Aojun Zhou,W. Zhang,Pan Lu,Conghui He,Xiangyu Yue,Hongsheng Li,Y. Qiao",52,79,9
"7cf64070fd3d7e53d80f260c10e6bd7018d580e1","https://www.semanticscholar.org/paper/7cf64070fd3d7e53d80f260c10e6bd7018d580e1",5,"IdealGPT: Iteratively Decomposing Vision and Language Reasoning via Large Language Models","The IdealGPT framework is presented, a framework that iteratively decomposes VL reasoning using large language models (LLMs) and outperforms the best existing GPT-4-like models by an absolute 10% on VCR and 15% on SNLI-VE.","arXiv.org",2023,"Haoxuan You,Rui Sun,Zhecan Wang,Long Chen,Gengyu Wang,Hammad A. Ayyubi,Kai-Wei Chang,Shih-Fu Chang",4,46,1
"fd755dc7b5b206c17fd953db04e1c888d45b6e4e","https://www.semanticscholar.org/paper/fd755dc7b5b206c17fd953db04e1c888d45b6e4e",5,"LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark","This work extends the research of MLLMs to point clouds and presents the LAMM-Dataset and LAMm-Benchmark for 2D image and 3D point cloud understanding and establishes an extensible framework to facilitate the extension of M LLMs to additional modalities.","arXiv.org",2023,"Zhen-fei Yin,Jiong Wang,Jianjian Cao,Zhelun Shi,Dingning Liu,Mukai Li,Lu Sheng,Lei Bai,Xiaoshui Huang,Zhiyong Wang,Wanli Ouyang,Jing Shao",11,98,4
"051549d8ef56937b2f4d113afdcf8c7586d3770b","https://www.semanticscholar.org/paper/051549d8ef56937b2f4d113afdcf8c7586d3770b",5,"Towards AGI in Computer Vision: Lessons Learned from GPT and Large Language Models","It is pointed out that the essential weakness of CV lies in lacking a paradigm to learn from environments, yet NLP has accomplished the task in the text world and is still far from a system like GPT that naturally integrates all tasks.","arXiv.org",2023,"Lingxi Xie,Longhui Wei,Xiaopeng Zhang,Kaifeng Bi,Xiaotao Gu,Jianlong Chang,Qi Tian",2,169,0
"966852963a88a28786b798c91b6662d6e501e590","https://www.semanticscholar.org/paper/966852963a88a28786b798c91b6662d6e501e590",5,"AssistGPT: A General Multi-modal Assistant that can Plan, Execute, Inspect, and Learn","A multi-modal AI assistant with an interleaved code and language reasoning approach called Plan, Execute, Inspect, and Learn (PEIL) to integrate LLMs with various tools, and a Learner is designed to enable the model to autonomously explore and discover the optimal solution.","arXiv.org",2023,"Difei Gao,Lei Ji,Luowei Zhou,Kevin Lin,Joya Chen,Zihan Fan,Mike Zheng Shou",2,73,0
"584f1b12552cef8516ae330e9527897deadd0cd2","https://www.semanticscholar.org/paper/584f1b12552cef8516ae330e9527897deadd0cd2",5,"VisIT-Bench: A Benchmark for Vision-Language Instruction Following Inspired by Real-World Use","This work introduces VisIT-Bench (Visual InsTruction Benchmark), a benchmark for evaluation of instruction-following vision-language models for real-world use, and curates 70 'instruction families' that it envision instruction tuned vision- language models should be able to address.","arXiv.org",2023,"Yonatan Bitton,Hritik Bansal,Jack Hessel,Rulin Shao,Wanrong Zhu,Anas Awadalla,Josh Gardner,Rohan Taori,L. Schimdt",0,95,0
"0ebc861f5478561f12941e6b48aad30574e996d8","https://www.semanticscholar.org/paper/0ebc861f5478561f12941e6b48aad30574e996d8",5,"Video ChatCaptioner: Towards Enriched Spatiotemporal Descriptions","This work introduces Video ChatCaptioner, an innovative approach for creating more comprehensive spatiotemporal video descriptions, specifically designed to select frames for posing video content-driven questions and shows promise as a method for enhancing video content.","arXiv.org",2023,"Jun Chen,Deyao Zhu,Kilichbek Haydarov,Xiang Li,Mohamed Elhoseiny",6,44,0
"1d80b1906455b91535cebf893263fe2e7e030eda","https://www.semanticscholar.org/paper/1d80b1906455b91535cebf893263fe2e7e030eda",5,"LMEye: An Interactive Perception Network for Large Language Models","Interactive Perception Network (IPN), aiming to achieve a LVLM by incorporating the image understanding capability into Large Language Models (LLMs), significantly improves the zero-shot performance of LVLMs on various multimodal tasks compared to previous methods.","arXiv.org",2023,"Yunxin Li,Baotian Hu,Xinyu Chen,Lin Ma,M. Zhang",4,58,0
"42a30dc5470f54ec249f25d3c31e05d7c376c8e3","https://www.semanticscholar.org/paper/42a30dc5470f54ec249f25d3c31e05d7c376c8e3",5,"VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks","This work presents an LLM-based framework for vision-centric tasks, termed VisionLLM, which provides a unified perspective for vision and language tasks by treating images as a foreign language and aligning vision-focused tasks with language tasks that can be flexibly defined and managed using language instructions.","arXiv.org",2023,"Wen Wang,Zhe Chen,Xiaokang Chen,Jiannan Wu,Xizhou Zhu,Gang Zeng,Ping Luo,Tong Lu,Jie Zhou,Y. Qiao,Jifeng Dai",23,78,3
"06091944b864d6dc473cab63321a95fb9c4067cc","https://www.semanticscholar.org/paper/06091944b864d6dc473cab63321a95fb9c4067cc",5,"ChatCAD+: Towards a Universal and Reliable Interactive CAD using LLMs","ChatCAD+, which is designed to be universal and reliable, is introduced, capable of handling medical images from diverse domains and leveraging up-to-date information from reputable medical websites to provide reliable medical advice.","arXiv.org",2023,"Zihao Zhao,Sheng Wang,Jinchen Gu,Yitao Zhu,Lanzhuju Mei,Zixu Zhuang,Zhiming Cui,Qian Wang,Dinggang Shen",1,51,0
"ad113d8b6f8f6dace7abd12dc88af520aaaf3fc7","https://www.semanticscholar.org/paper/ad113d8b6f8f6dace7abd12dc88af520aaaf3fc7",5,"LISA: Reasoning Segmentation via Large Language Model","This work proposes a new segmentation task -- reasoning segmentation, designed to output a segmentation mask given a complex and implicit query text, and presents LISA, which inherits the language generation capabilities of the multi-modal Large Language Model (LLM) while also possessing the ability to produce segmentation masks.","arXiv.org",2023,"Xin Lai,Zhuotao Tian,Yukang Chen,Yanwei Li,Yuhui Yuan,Shu Liu,Jiaya Jia",0,64,0
"31a7d8c4a5ab6bab522494b57270249105c8748e","https://www.semanticscholar.org/paper/31a7d8c4a5ab6bab522494b57270249105c8748e",5,"BiomedGPT: A Unified and Generalist Biomedical Generative Pre-trained Transformer for Vision, Language, and Multimodal Tasks","A unified and generalist Biomedical Generative Pre-trained Transformer model, which leverages self-supervision on large and diverse datasets to accept multi-modal inputs and perform a range of downstream tasks, with far-reaching implications for improving healthcare outcomes.","arXiv.org",2023,"Kaiyuan Zhang,Jun Yu,Zhilin Yan,Yixin Liu,Eashan Adhikarla,S. Fu,Xun Chen,Chen Chen,Yuyin Zhou,Xiang Li,Lifang He,B. Davison,Quanzheng Li,Yong Chen,Hongfang Liu,Lichao Sun",5,147,1
"e34c956ab10c931a374ce829a8a667ba6fd7a816","https://www.semanticscholar.org/paper/e34c956ab10c931a374ce829a8a667ba6fd7a816",5,"Instruction Tuning for Large Language Models: A Survey","A systematic review of the literature, including the general methodology of IT, the construction of IT datasets, the training of IT models, and applications to different modalities, domains and applications, along with an analysis on aspects that influence the outcome of IT.","",2023,"Shengyu Zhang,Linfeng Dong,Xiaoya Li,Sen Zhang,Xiaofei Sun,Shuhe Wang,Jiwei Li,Runyi Hu,Tianwei Zhang,Fei Wu,Guoyin Wang",0,145,0
"3d02e5503caa2f444cbd61778c7cdc00a5b2e98d","https://www.semanticscholar.org/paper/3d02e5503caa2f444cbd61778c7cdc00a5b2e98d",4,"GPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest","This paper proposes instruction tuning on region-of-interest, and proposes a region-level vision-language model, termed as GPT4RoI, which brings brand new conversational and interactive experience beyond image-level understanding.","arXiv.org",2023,"Shilong Zhang,Pei Sun,Shoufa Chen,Min Xiao,Wenqi Shao,Wenwei Zhang,Kai Chen,Ping Luo",5,72,0
"7e32aac43e9f1df49e116add03327ee6f365dbf3","https://www.semanticscholar.org/paper/7e32aac43e9f1df49e116add03327ee6f365dbf3",4,"mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality",,"arXiv.org",2023,"Qinghao Ye,Haiyang Xu,Guohai Xu,Jiabo Ye,Ming Yan,Yi Zhou,Junyan Wang,Anwen Hu,Pengcheng Shi,Yaya Shi,Chenliang Li,Yuanhong Xu,Hehong Chen,Junfeng Tian,Qiang Qi,Ji Zhang,Feiyan Huang",67,36,12
"54a8b153ed04a872da878d695239bdc413dc782c","https://www.semanticscholar.org/paper/54a8b153ed04a872da878d695239bdc413dc782c",4,"InternGPT: Solving Vision-Centric Tasks by Interacting with Chatbots Beyond Language","By incorporating pointing instructions, the proposed iGPT significantly improves the efficiency of communication between users and chatbots, as well as the accuracy of chatbots in vision-centric tasks, especially in complicated visual scenarios where the number of objects is greater than 2.","arXiv.org",2023,"Zhaoyang Liu,Yinan He,Wenhai Wang,Weiyun Wang,Yi Wang,Shoufa Chen,Qing-Long Zhang,Yang Yang,Qingyun Li,Jiashuo Yu,Kunchang Li,Zhe Chen,Xuecheng Yang,Xizhou Zhu,Yali Wang,Limin Wang,Ping Luo,Jifeng Dai,Yu Qiao",16,84,1
"d48cb91b9e555194f7494c4d4bb9815021d3ee45","https://www.semanticscholar.org/paper/d48cb91b9e555194f7494c4d4bb9815021d3ee45",4,"VideoChat: Chat-Centric Video Understanding","VideoChat is introduced, an end-to-end chat-centric video understanding system that integrates video foundation models and large language models via a learnable neural interface, excelling in spatiotemporal reasoning, event localization, and causal relationship inference.","arXiv.org",2023,"Kunchang Li,Yinan He,Yi Wang,Yizhuo Li,Wen Wang,Ping Luo,Yali Wang,Limin Wang,Yu Qiao",29,59,9
"9837349417e36ef5be06da0fd6c74042148bdaa2","https://www.semanticscholar.org/paper/9837349417e36ef5be06da0fd6c74042148bdaa2",4,"Visual Programming for Text-to-Image Generation and Evaluation","This work proposes two novel interpretable/explainable visual programming frameworks for text-to-image (T2I) generation and evaluation and introduces VPEval, an interpretable and explainable evaluation framework for T2I generation based on visual programming.","arXiv.org",2023,"Jaemin Cho,Abhaysinh Zala,Mohit Bansal",2,62,1
"6238fd213197ccf0d79191662828e38118a06d79","https://www.semanticscholar.org/paper/6238fd213197ccf0d79191662828e38118a06d79",4,"ECHo: Event Causality Inference via Human-centric Reasoning","A unified framework aligned with the Chain-of-Thought (CoT) paradigm is proposed to assess the reasoning capability of current AI systems and scrutinize the advanced large language and multimodal models via three complementary human-centric ECHo tasks.","arXiv.org",2023,"Yuxi Xie,Guanzhen Li,MingSung Kan",0,68,0
"00fcc983728346a5f3f8f005f1365be54456728e","https://www.semanticscholar.org/paper/00fcc983728346a5f3f8f005f1365be54456728e",4,"EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought","This work introduces EmbodiedGPT, an end-to-end multi-modal foundation model for embodied AI, empowering embodied agents with multi- modal understanding and execution capabilities, and significantly enhances the success rate of the embodied control task by extracting more effective features.","arXiv.org",2023,"Yao Mu,Qinglong Zhang,Mengkang Hu,Wen Wang,Mingyu Ding,Jun Jin,Bin Wang,Jifeng Dai,Y. Qiao,Ping Luo",14,68,1
"c5c5b9c6660bb2089b781d851cb3fd0ba271d742","https://www.semanticscholar.org/paper/c5c5b9c6660bb2089b781d851cb3fd0ba271d742",4,"Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models","A novel and affordable solution for the effective VL adaption of LLMs, called Mixture-of-Modality Adaptation (MMA), which adopts lightweight modules, i.e., adapters, to bridge the gap between LLMs and VL tasks, which also enables the joint optimization of the image and language models","arXiv.org",2023,"Gen Luo,Yiyi Zhou,Tianhe Ren,Shen Chen,Xiaoshuai Sun,Rongrong Ji",8,48,2
"f8f8267a2acd7598de6c15327f3953241901a62d","https://www.semanticscholar.org/paper/f8f8267a2acd7598de6c15327f3953241901a62d",4,"On Evaluating Adversarial Robustness of Large Vision-Language Models","Evaluating the robustness of open-source large VLMs in the most realistic and high-risk setting, where adversaries have only black-box system access and seek to deceive the model into returning the targeted responses is proposed.","arXiv.org",2023,"Yunqing Zhao,Tianyu Pang,Chao Du,Xiao Yang,Chongxuan Li,Ngai-Man Cheung,Min Lin",6,102,1
"b458fc5261595f44b36325e5eaea1f874d65138f","https://www.semanticscholar.org/paper/b458fc5261595f44b36325e5eaea1f874d65138f",4,"GPT4Tools: Teaching Large Language Model to Use Tools via Self-instruction","The GPT4Tools based on self-instruct to enable open-source LLMs, such as LLaMA and OPT, to use tools, generates an instruction-following dataset by prompting an advanced teacher with various multi-modal contexts using the Low-Rank Adaptation (LoRA) optimization.","arXiv.org",2023,"Rui Yang,Lin Song,Yanwei Li,Sijie Zhao,Yixiao Ge,Xiu Li,Ying Shan",11,63,2
"d47524cd5c3c4b57af2e5a29f6f91c420310f236","https://www.semanticscholar.org/paper/d47524cd5c3c4b57af2e5a29f6f91c420310f236",4,"MIMIC-IT: Multi-Modal In-Context Instruction Tuning","MultI-Modal In-Context Instruction Tuning (MIMIC-IT), a dataset comprising 2.8 million multimodal instruction-response pairs, with 2.2 million unique instructions derived from images and videos, is presented and a large VLM named Otter is trained.","arXiv.org",2023,"Bo Li,Yuanhan Zhang,Liangyu Chen,Jinghao Wang,Fanyi Pu,Jingkang Yang,C. Li,Ziwei Liu",16,55,5
"74538984e72a26254697d4e7eeeb169000cf762a","https://www.semanticscholar.org/paper/74538984e72a26254697d4e7eeeb169000cf762a",4,"Valley: Video Assistant with Large Language model Enhanced abilitY","The proposed Valley model is designed with a simple projection module that bridges video, image, and language modalities, and is further unified with a multi-lingual LLM, and has the potential to function as a highly effective multilingual video assistant that can make complex video understanding scenarios easy.","arXiv.org",2023,"Ruipu Luo,Ziwang Zhao,Min Yang,Junwei Dong,Ming-Hui Qiu,Pengcheng Lu,Tao Wang,Zhongyu Wei",7,27,2
"e8cc6a50bc9c6a1e1ed7fdde0e7ccbb4efd4b505","https://www.semanticscholar.org/paper/e8cc6a50bc9c6a1e1ed7fdde0e7ccbb4efd4b505",4,"AVIS: Autonomous Visual Information Seeking with Large Language Models","An autonomous information seeking visual question answering framework that leverages a Large Language Model to dynamically strategize the utilization of external tools and to investigate their outputs, thereby acquiring the indispensable knowledge needed to provide answers to the posed questions is proposed.","arXiv.org",2023,"Ziniu Hu,Ahmet Iscen,Chen Sun,Kai-Wei Chang,Yizhou Sun,David A. Ross,C. Schmid,A. Fathi",0,127,0
"efc694164312006c543ef745611348ef64e68dda","https://www.semanticscholar.org/paper/efc694164312006c543ef745611348ef64e68dda",4,"Towards Language Models That Can See: Computer Vision Through the LENS of Natural Language","This work proposes LENS, a modular approach for tackling computer vision problems by leveraging the power of large language models (LLMs) with a language model to reason over outputs from a set of independent and highly descriptive vision modules that provide exhaustive information about an image.","arXiv.org",2023,"William M. Berrios,Gautam Mittal,Tristan Thrush,Douwe Kiela,Amanpreet Singh",2,64,0
"4ad771a10145e5e3b7e74bf6e98b165d7258889f","https://www.semanticscholar.org/paper/4ad771a10145e5e3b7e74bf6e98b165d7258889f",4,"What Matters in Training a GPT4-Style Language Model with Multimodal Inputs?","Lynx is presented, which performs the most accurate multi-modal understanding while keeping the best multi- modal generation ability compared to existing open-sourced GPT4-style models.","arXiv.org",2023,"Yan Zeng,Hanbo Zhang,Jiani Zheng,Jiangnan Xia,Guoqiang Wei,Yang Wei,Yuchen Zhang,Tao Kong",3,107,0
"2e52178416b93a0a7dc1cb7d21d04c7e72c2ce0f","https://www.semanticscholar.org/paper/2e52178416b93a0a7dc1cb7d21d04c7e72c2ce0f",4,"ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring Instruction Tuning","This study proposes ChatSpot, a unified end-to-end multimodal large language model that supports diverse forms of interactivity including mouse clicks, drag-and-drop, and drawing boxes, which provides a more flexible and seamless interactive experience.","arXiv.org",2023,"Liang Zhao,En Yu,Zheng Ge,Jinrong Yang,Hao-Ran Wei,Hongyu Zhou,Jian‐Yuan Sun,Yuang Peng,Runpei Dong,Chunrui Han,Xiangyu Zhang",2,39,1
"d53945d4afb4528590d79e20de52883d29037e86","https://www.semanticscholar.org/paper/d53945d4afb4528590d79e20de52883d29037e86",4,"FashionLOGO: Prompting Multimodal Large Language Models for Fashion Logo Embeddings","This work explores how MLLMs can improve logo embedding by prompting them to generate explicit textual knowledge through three types of prompts, including image OCR, brief captions, and detailed descriptions prompts, in a zero-shot setting and adopt a cross-attention transformer to enable image embedding queries to learn supplementary knowledge from textual embeddings automatically.","arXiv.org",2023,"Yulin Su,Min Yang,Minghui Qiu,Jing Wang,Tao Wang",0,44,0
"eb5cf10406a8ad31e0ebe56b36571d5db4758a62","https://www.semanticscholar.org/paper/eb5cf10406a8ad31e0ebe56b36571d5db4758a62",4,"PUMGPT: A Large Vision-Language Model for Product Understanding","This paper presents PUMGPT, a large vision-language model that aims at unifying all product understanding tasks under a singular model structure, and proposes Layer-wise Adapters (LA), an approach that provides enhanced alignment with fewer visual tokens and enables parameter-efficient fine-tuning.","arXiv.org",2023,"Shuhui Wu,Zengming Tang,Zongyi Guo,Weiwei Zhang,Baoliang Cui,Haihong Tang,Weiming Lu",0,38,0
"13b5b69355555e0c8b702261c5de3b4172ba653c","https://www.semanticscholar.org/paper/13b5b69355555e0c8b702261c5de3b4172ba653c",4,"The Art of SOCRATIC QUESTIONING: Zero-shot Multimodal Reasoning with Recursive Thinking and Self-Questioning","Qualitative analysis clearly demonstrates the intermediate thinking steps elicited by Socratic Questioning are similar to the human's recursively thinking process of a complex reasoning problem.","arXiv.org",2023,"Jingyuan Qi,Zhiyang Xu,Ying Shen,Minqian Liu,dingnan jin,Qifan Wang,Lifu Huang",0,27,0
"f1892409fdbb396b00bb180891bb1c130fe3c7f4","https://www.semanticscholar.org/paper/f1892409fdbb396b00bb180891bb1c130fe3c7f4",4,"Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding","Video-LLaMA showcases the ability to perceive and comprehend video content, generating meaningful responses that are grounded in the visual and auditory information presented in the videos, highlighting the potential of Video- LLaMA as a promising prototype for audio-visual AI assistants.","arXiv.org",2023,"Hang Zhang,Xin Li,Lidong Bing",21,32,2
"659a12d71d8709c132ccd9ccd235f0024cae0239","https://www.semanticscholar.org/paper/659a12d71d8709c132ccd9ccd235f0024cae0239",4,"The All-Seeing Project: Towards Panoptic Visual Recognition and Understanding of the Open World","The All-Seeing model (ASM), a unified framework for panoptic visual recognition and understanding, is developed with open-ended language prompts and locations, which allows it to generalize to various vision and language tasks with remarkable zero-shot performance.","arXiv.org",2023,"Weiyun Wang,Min Shi,Qingyun Li,Wen Wang,Zhenhang Huang,Linjie Xing,Zhe Chen,Hao Li,Xizhou Zhu,Zhiguo Cao,Yushi Chen,Tong Lu,Jifeng Dai,Y. Qiao",0,113,0
"1a8eb2cae1833df3bf12fe3b41b03d60b4a4a98d","https://www.semanticscholar.org/paper/1a8eb2cae1833df3bf12fe3b41b03d60b4a4a98d",4,"Visual Instruction Tuning","This paper presents LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding and introduces GPT-4 generated visual instruction tuning data, the model and code base publicly available.","arXiv.org",2023,"Haotian Liu,Chunyuan Li,Qingyang Wu,Yong Jae Lee",142,58,52
"0fea183c4f49a015a8ee7d89ef2e6885b7023c10","https://www.semanticscholar.org/paper/0fea183c4f49a015a8ee7d89ef2e6885b7023c10",4,"SVIT: Scaling up Visual Instruction Tuning","It is empirically verified that training multimodal models on SVIT can significantly improve the multi-modal performance in terms of visual perception, reasoning and planing.","arXiv.org",2023,"Bo Zhao,Boya Wu,Tiejun Huang",1,35,0
"f4793adffd6f67ffcb93ccfc5672ab301b8a2b96","https://www.semanticscholar.org/paper/f4793adffd6f67ffcb93ccfc5672ab301b8a2b96",4,"PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering","This paper proposes a generative-based model for medical visual understanding by aligning visual information from a pre-trained vision encoder with a large language model, and establishes a scalable pipeline to construct a large-scale medical visual question-answering dataset.","arXiv.org",2023,"Xiaoman Zhang,Chaoyi Wu,Ziheng Zhao,Weixiong Lin,Ya Zhang,Yanfeng Wang,Weidi Xie",10,38,2
"bf40c9e7832e1b2887cbf5798455f91705ea11ba","https://www.semanticscholar.org/paper/bf40c9e7832e1b2887cbf5798455f91705ea11ba",4,"Masked Vision and Language Pre-training with Unimodal and Multimodal Contrastive Losses for Medical Visual Question Answering","This paper presents a novel self-supervised approach that learns unimodal and multimodal feature representations of input images and text using medical image caption datasets, by leveraging both unimmodal and multi-modal contrastive losses, along with masked language modeling and image text matching as pretraining objectives.","arXiv.org",2023,"Pengfei Li,Gang Liu,Jinlong He,Zixu Zhao,Shenjun Zhong",0,27,0
"2b98346cf8e9d05146f4753a77a1ef2aa0453739","https://www.semanticscholar.org/paper/2b98346cf8e9d05146f4753a77a1ef2aa0453739",4,"Towards Generalist Foundation Model for Radiology","The experimental results confirm that RadFM significantly outperforms existing multi-modal foundation models and proposes a new evaluation benchmark that comprises five tasks, aiming to comprehensively assess the capability of foundation models in handling practical clinical problems.","arXiv.org",2023,"Chaoyi Wu,Xiaoman Zhang,Ya Zhang,Yanfeng Wang,Weidi Xie",0,46,0
"ea566f87f6253bb2d32cf7b61cd3e2535a0c3f42","https://www.semanticscholar.org/paper/ea566f87f6253bb2d32cf7b61cd3e2535a0c3f42",3,"mPLUG-DocOwl: Modularized Multimodal Large Language Model for Document Understanding","Experimental results show that the proposed mPLUG-DocOwl model outperforms existing multi-modal models, demonstrating its strong ability of document understanding, and also generalizes well on various downstream tasks.","arXiv.org",2023,"Jiabo Ye,Anwen Hu,Haiyang Xu,Qinghao Ye,Mingshi Yan,Yuhao Dan,Chenlin Zhao,Guohai Xu,Chenliang Li,Junfeng Tian,Qiang Qi,Ji Zhang,Feiyan Huang",2,37,0
"a3711dbf296b5ddd97ba93826660cd3995611625","https://www.semanticscholar.org/paper/a3711dbf296b5ddd97ba93826660cd3995611625",3,"Towards A Foundation Model for Generalist Robots: Diverse Skill Learning at Scale via Automated Task and Scene Generation",,"arXiv.org",2023,"Zhou Xian,Théophile Gervet,Zhenjia Xu,Yi-Ling Qiao,Tsun-Hsuan Wang",0,111,0
"97708ebb3ea40a634c746e4095bcfb670672faa5","https://www.semanticscholar.org/paper/97708ebb3ea40a634c746e4095bcfb670672faa5",3,"Reasoning with Language Model Prompting: A Survey","This paper provides a comprehensive survey of cutting-edge research on reasoning with language model prompting with comparisons and summaries and provides systematic resources to help beginners.","Annual Meeting of the Association for Computational Linguistics",2022,"Shuofei Qiao,Yixin Ou,Ningyu Zhang,Xiang Chen,Yunzhi Yao,Shumin Deng,Chuanqi Tan,Fei Huang,Huajun Chen",46,212,2
"170c97c7215f42edfb20c2248f954879e91ef86e","https://www.semanticscholar.org/paper/170c97c7215f42edfb20c2248f954879e91ef86e",3,"Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models","This paper presents Chameleon, an AI system that mitigates LLM limitations by augmenting LLMs with plug-and-play modules for compositional reasoning, and shows that the GPT-4-powered planner exhibits more consistent and rational tool selection via inferring potential constraints from instructions, compared to a ChatGPT- powered planner.","arXiv.org",2023,"Pan Lu,Baolin Peng,Hao Cheng,Michel Galley,Kai-Wei Chang,Y. Wu,Song-Chun Zhu,Jianfeng Gao",45,65,7
"70681d0a441381ec862979aabe47530b515ade5a","https://www.semanticscholar.org/paper/70681d0a441381ec862979aabe47530b515ade5a",3,"Towards Generalist Robots: A Promising Paradigm via Generative Simulation","This document presents a specific idea for mining knowledge in the latest large-scale foundation models for robotics research, which uses a fully automated generative pipeline which uses these models to generate diversified tasks, scenes and training supervisions at scale, thereby scaling up low-level skill learning and ultimately leading to a foundation model for robotics that empowers generalist robots.","",2023,"Zhou Xian,Théophile Gervet,Zhenjia Xu,Yi-Ling Qiao,Tsun-Hsuan Wang,Yian Wang",0,123,0
"405bc18b9d2f783f22f50d5feb02c51b4b34655f","https://www.semanticscholar.org/paper/405bc18b9d2f783f22f50d5feb02c51b4b34655f",3,"LayoutGPT: Compositional Visual Planning and Generation with Large Language Models","This work proposes LayoutGPT, a method to compose in-context visual demonstrations in style sheet language to enhance the visual planning skills of LLMs, and shows superior performance in converting challenging language concepts like numerical and spatial relations to layout arrangements for faithful text-to-image generation.","arXiv.org",2023,"Weixi Feng,Wanrong Zhu,Tsu-Jui Fu,Varun Jampani,Arjun Reddy Akula,Xuehai He,Sugato Basu,X. Wang,William Yang Wang",2,61,0
"1a28e9c62eeb76a1a77dc152197027c15310927b","https://www.semanticscholar.org/paper/1a28e9c62eeb76a1a77dc152197027c15310927b",3,"ANPL: Compiling Natural Programs with Interactive Decomposition","ANPL, a programming system that allows users to decompose user-specific tasks, is introduced and deployed on the Abstraction and Reasoning Corpus (ARC), a set of unique tasks that are challenging for state-of-the-art AI systems.","arXiv.org",2023,"Di Huang,Ziyuan Nan,Xingui Hu,Pengwei Jin,Shaohui Peng,Yuanbo Wen,Rui Zhang,Zidong Du,Qi Guo,Yewen Pu,Yunji Chen",1,67,0
"af705d648b5b16daa3dcc593bc593f2574d76c07","https://www.semanticscholar.org/paper/af705d648b5b16daa3dcc593bc593f2574d76c07",3,"Grammar Prompting for Domain-Specific Language Generation with Large Language Models","Experiments demonstrate that grammar prompting can enable LLMs to perform competitively on a diverse set of DSL generation tasks, including semantic parsing, Overnight, GeoQuery, PDDL planning, and even molecule generation (SMILES).","arXiv.org",2023,"Bailin Wang,Zimu Wang,Xuezhi Wang,Yuan Cao,R. Saurous,Yoon Kim",2,93,0
"446fb5dead075a1a08862662738f462e9a0e91c8","https://www.semanticscholar.org/paper/446fb5dead075a1a08862662738f462e9a0e91c8",3,"Tool Documentation Enables Zero-Shot Tool-Usage with Large Language Models","This work advocates the use of tool documentation, descriptions for the individual tool usage, over demonstrations, and shows that tool documentation is significantly more valuable than demonstrations, with zero-shot documentation significantly outperforming few-shot without documentation.","arXiv.org",2023,"Cheng-Yu Hsieh,Sibei Chen,Chun-Liang Li,Yasuhisa Fujii,Alexander J. Ratner,Chen-Yu Lee,Ranjay Krishna,Tomas Pfister",2,85,0
"5ce94181ea702f69c3651dce721d6bd8026b8106","https://www.semanticscholar.org/paper/5ce94181ea702f69c3651dce721d6bd8026b8106",3,"TPTU: Task Planning and Tool Usage of Large Language Model-based AI Agents","A structured framework tailored for LLM-based AI Agents is proposed and two distinct types of agents are designed to execute the inference process, which emphasizes the substantial potential of these models, while also identifying areas that need more investigation and improvement.","arXiv.org",2023,"Jingqing Ruan,Yihong Chen,Bin Zhang,Zhiwei Xu,Tianpeng Bao,Guoqing Du,Shiwei Shi,Hangyu Mao,Xingyu Zeng,Rui Zhao",1,90,0
"d1120d67b700e4dfe8b39eb1e48fbdea4e1a0c43","https://www.semanticscholar.org/paper/d1120d67b700e4dfe8b39eb1e48fbdea4e1a0c43",3,"HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face","HuggingGPT is a framework that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities to solve AI tasks and is able to cover numerous sophisticated AI tasks in different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks.","arXiv.org",2023,"Yongliang Shen,Kaitao Song,Xu Tan,D. Li,Weiming Lu,Y. Zhuang",168,43,23
"43e6e8d6663d83f1b74cf5a2be7b040b0928f867","https://www.semanticscholar.org/paper/43e6e8d6663d83f1b74cf5a2be7b040b0928f867",3,"X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages","X-LLM is proposed, which converts Multi-modalities into foreign languages using X2L interfaces and inputs them into a large Language model (ChatGLM), and demonstrates impressive multimodel chat abilities.","arXiv.org",2023,"Feilong Chen,Minglun Han,Haozhi Zhao,Qingyang Zhang,Jing Shi,Shuang Xu,Bo Xu",17,62,4
"6a5525c316b9be7909c433a79e090ed731425083","https://www.semanticscholar.org/paper/6a5525c316b9be7909c433a79e090ed731425083",3,"What Makes for Good Visual Tokenizers for Large Language Models?","A new MLLM equipped with a tailored Good Visual Tokenizer (GVT), which exhibits strong visual comprehension capability at multiple scales, without introducing extra parameters and task-specific fine-tuning.","arXiv.org",2023,"Guangzhi Wang,Yixiao Ge,Xiaohan Ding,Mohan S. Kankanhalli,Ying Shan",4,55,0
"ca055cfb9d4d47124cc035c346f38577825fcacf","https://www.semanticscholar.org/paper/ca055cfb9d4d47124cc035c346f38577825fcacf",3,"Enhance Reasoning Ability of Visual-Language Models via Large Language Models","A method called TReE is proposed, which transfers the reasoning ability of a large language model to a visual language model in zero-shot scenarios, and contains three stages: observation, thinking, and re-thinking.","arXiv.org",2023,"Yueting Yang,Xintong Zhang,Wenjuan Han",0,40,0
"c6ac708b65b24c20f80831d518c1795ce8133ad5","https://www.semanticscholar.org/paper/c6ac708b65b24c20f80831d518c1795ce8133ad5",3,"ChatBridge: Bridging Modalities with Large Language Model as a Language Catalyst","It is shown that only language-paired two-modality data is sufficient to connect all modalities and ChatBridge, a novel multimodal language model that leverages the expressive capabilities of language as the catalyst to bridge the gap between various modalities, is presented.","arXiv.org",2023,"Zijia Zhao,Longteng Guo,Tongtian Yue,Si-Qing Chen,Shuai Shao,Xinxin Zhu,Zehuan Yuan,Jing Liu",5,72,1
"50c1414fe41d0cb9db6f0933c9319aa124beac5d","https://www.semanticscholar.org/paper/50c1414fe41d0cb9db6f0933c9319aa124beac5d",3,"Contextual Object Detection with Multimodal Large Language Models","The ContextDET is presented, a unified multimodal model that is capable of end-to-end differentiable modeling of visual-language contexts, so as to locate, identify, and associate visual objects with language inputs for human-AI interaction.","arXiv.org",2023,"Yuhang Zang,Wei Li,Jun Han,Kaiyang Zhou,Chen Change Loy",4,88,0
"705e0f1887d76c956e3a1750f0176f2b8fe121ff","https://www.semanticscholar.org/paper/705e0f1887d76c956e3a1750f0176f2b8fe121ff",3,"Zero-Shot 3D Shape Correspondence","This work introduces a fully automatic method that exploits the exceptional reasoning capabilities of recent foundation models in language and vision to tackle difficult shape correspondence problems and produces highly plausible results in a zero-shot manner, especially between strongly non-isometric shapes.","arXiv.org",2023,"Ahmed Abdelreheem,A. Eldesokey,M. Ovsjanikov,Peter Wonka",1,52,0
"fed150a219f9c31bdb4920e615c7c9264c634736","https://www.semanticscholar.org/paper/fed150a219f9c31bdb4920e615c7c9264c634736",3,"On the Challenges and Perspectives of Foundation Models for Medical Image Analysis","The opportunities, applications and future directions of large-scale pre-trained models, i.e., foundation models, for analyzing medical images, and how foundation models can be leveraged in downstream medical tasks to enhance the accuracy and efficiency of medical image analysis are discussed.","arXiv.org",2023,"Shaoting Zhang,Dimitris N. Metaxas",3,46,0
"79150cb420d15830c8d36f0e91eea1b02e177f0f","https://www.semanticscholar.org/paper/79150cb420d15830c8d36f0e91eea1b02e177f0f",3,"Sticker820K: Empowering Interactive Retrieval with Stickers","The StickerCLIP is proposed as a benchmark model on the Sticker820K dataset, demonstrating strong superiority over the CLIP for the text-to-image retrieval task, and the recently popularized LLM is extended by means of prompt tuning, integrating its ability for sticker retrieval and allowing users to retrieve stickers through instructions.","arXiv.org",2023,"Sijie Zhao,Yixiao Ge,Zhongang Qi,Lin Song,Xiaohan Ding,Zehua Xie,Ying Shan",0,36,0
"697e0add95e880bd42e00bef838181e105f91981","https://www.semanticscholar.org/paper/697e0add95e880bd42e00bef838181e105f91981",3,"MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models","The first MLLM Evaluation benchmark MME is presented, which suggests that existing MLLMs still have a large room for improvement, but also reveals the potential directions for the subsequent model optimization.","arXiv.org",2023,"Chaoyou Fu,Peixian Chen,Yunhang Shen,Yulei Qin,Mengdan Zhang,Xu Lin,Zhenyu Qiu,Wei Lin,Jinrui Yang,Xiawu Zheng,Ke Li,Xing Sun,Rongrong Ji",11,43,4
"85d9151aa2efd0cbe822e403138cfe49f9536703","https://www.semanticscholar.org/paper/85d9151aa2efd0cbe822e403138cfe49f9536703",3,"SITTA: A Semantic Image-Text Alignment for Image Captioning","This work introduces two novel ways of constructing a linear mapping that successfully transfers semantics between the embedding spaces of the two pretrained models, and makes image captioning more accessible for institutions with restricted computational resources.","arXiv.org",2023,"Fabian Paischer,Thomas Adler,M. Hofmarcher,S. Hochreiter",0,116,0
"962ccf1fc49c83817fb031e5b24b81b19cdfb89d","https://www.semanticscholar.org/paper/962ccf1fc49c83817fb031e5b24b81b19cdfb89d",3,"BuboGPT: Enabling Visual Grounding in Multi-Modal LLMs","BozoGPT is a multi-modal LLM with visual grounding that can perform cross-modAL interaction between vision, audio and language, providing fine-grained understanding of visual objects and other given modalities and performs consistently well when provided by arbitrary modalities.","arXiv.org",2023,"Yang Zhao,Zhijie Lin,Daquan Zhou,Zilong Huang,Jiashi Feng,Bingyi Kang",3,33,0
"5c6abca085e6a0047339d57d849886e693fe62bf","https://www.semanticscholar.org/paper/5c6abca085e6a0047339d57d849886e693fe62bf",3,"MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities","MM-Vet is proposed, an evaluation benchmark that examines large multimodal models (LMMs) on complicated multimodAL tasks and proposes an LLM-based evaluator for open-ended outputs that enables the evaluation across different question types and answer styles, resulting in a unified scoring metric.","arXiv.org",2023,"Weihao Yu,Zhengyuan Yang,Linjie Li,Jianfeng Wang,Kevin Lin,Zicheng Liu,Xinchao Wang,Lijuan Wang",0,83,0
"4f2be887e991efa85f7b874e7ab871080a745c39","https://www.semanticscholar.org/paper/4f2be887e991efa85f7b874e7ab871080a745c39",3,"CAESURA: Language Models as Multi-Modal Query Planners","This paper proposes Language-Model-Driven Query Planning, a new paradigm of query planning that uses Language Models to translate natural language queries into executable query plans that can contain complex operators that are able to process arbitrary modalities.","arXiv.org",2023,"Matthias Urban,Carsten Binnig",0,19,0
"1fd31b74f5e1eeb67341982fd35a613c6fad10e0","https://www.semanticscholar.org/paper/1fd31b74f5e1eeb67341982fd35a613c6fad10e0",3,"Link-Context Learning for Multimodal LLMs","This work proposes link-context learning (LCL), which emphasizes ""reasoning from cause and effect"" to augment the learning capabilities of MLLMs and introduces the ISEKAI dataset, comprising exclusively of unseen generated image-label pairs designed for link- context learning.","arXiv.org",2023,"Yan Tai,Weichen Fan,Zhao Zhang,Feng Zhu,Rui Zhao,Ziwei Liu",0,33,0
"9a22b33b529484c912d1ea9f8698369d4546a1c1","https://www.semanticscholar.org/paper/9a22b33b529484c912d1ea9f8698369d4546a1c1",3,"Transfer Visual Prompt Generator across LLMs","This work investigates the VPG transferability across LLMs, and designs a two-stage transfer framework named VPGTrans, which is simple yet highly effective and demonstrated to significantly speed up the transfer learning process without compromising performance.","arXiv.org",2023,"Ao Zhang,Hao Fei,Yuan Yao,Wei Ji,Li Li,Zhiyuan Liu,Tat-Seng Chua",20,35,5
"d886fc1b43b1c14b1c82ce8e4eab7c48e2c6d7af","https://www.semanticscholar.org/paper/d886fc1b43b1c14b1c82ce8e4eab7c48e2c6d7af",3,"Paxion: Patching Action Knowledge in Video-Language Foundation Models","This work proposes a novel framework, Paxion, along with a new Discriminative Video Dynamics Modeling (DVDM) objective, which effectively fills the gap in action knowledge understanding, while maintaining or improving performance on a wide spectrum of both object- and action-centric downstream tasks.","arXiv.org",2023,"Zhenhailong Wang,Ansel Blume,Sha Li,Genglin Liu,Jaemin Cho,Zineng Tang,Mohit Bansal,Heng Ji",1,62,0
"bfa9df38bd8597d8cdf0c3497fe5e5a5e31f646f","https://www.semanticscholar.org/paper/bfa9df38bd8597d8cdf0c3497fe5e5a5e31f646f",3,"ONE-PEACE: Exploring One General Representation Model Toward Unlimited Modalities","This work releases ONE-PEACE, a highly extensible model with 4B parameters that can seamlessly align and integrate representations across vision, audio, and language modalities, and develops two modality-agnostic pretraining tasks, which align the semantic space of different modalities and capture fine-grained details within modalities concurrently.","arXiv.org",2023,"Peng Wang,Shijie Wang,Junyang Lin,Shuai Bai,Xiaohuan Zhou,Jingren Zhou,Xinggang Wang,Chang Zhou",8,172,1
"6fe2a4f7594abadafc9feb05e96ab7e9e634ce8f","https://www.semanticscholar.org/paper/6fe2a4f7594abadafc9feb05e96ab7e9e634ce8f",3,"Album Storytelling with Iterative Story-aware Captioning and Large Language Models","This work proposes a new iterative album storytelling pipeline, which starts with an initial story and builds a story-aware caption model to refine the captions using the whole story as guidance, then feeds into the LLMs to generate a new refined story.","arXiv.org",2023,"Munan Ning,Yujia Xie,Dongdong Chen,Zeyin Song,Lu Yuan,Yonghong Tian,Qixiang Ye,Liuliang Yuan",1,58,0
"065dcc6074ffc9e314799d97c1757e5d23e7e2b1","https://www.semanticscholar.org/paper/065dcc6074ffc9e314799d97c1757e5d23e7e2b1",3,"S-CLIP: Semi-supervised Vision-Language Pre-training using Few Specialist Captions","S-CLIP is proposed, a semi-supervised learning method for training CLIP that utilizes additional unpaired images and employs two pseudo-labeling strategies specifically designed for contrastive learning and the language modality.","arXiv.org",2023,"Sangwoo Mo,Min-Kyung Kim,Kyungmin Lee,Jinwoo Shin",0,100,0
"f45a3474bd38d65c1b2cc3342a64dacbf07f445a","https://www.semanticscholar.org/paper/f45a3474bd38d65c1b2cc3342a64dacbf07f445a",3,"Cream: Visually-Situated Natural Language Understanding with Contrastive Reading Model and Frozen Large Language Models","The Contrastive Reading Model (Cream) is proposed, a novel neural architecture designed to enhance the language-image understanding capability of LLMs by capturing intricate details typically overlooked by existing methods.","arXiv.org",2023,"Geewook Kim,Hodong Lee,D. Kim,Haeji Jung,S. Park,Yoon Kim,Sangdoo Yun,T. Kil,Bado Lee,Seunghyun Park",2,69,0
"0983883619a0ca597d055d0e58da2f514052913d","https://www.semanticscholar.org/paper/0983883619a0ca597d055d0e58da2f514052913d",3,"Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration","This work proposes Macaw-LLM, a novel multi-modal LLM that seamlessly integrates visual, audio, and textual information and builds on the work of previous work on instruction-tuned large language models to handle diverse data modalities and address complex real-world scenarios.","arXiv.org",2023,"Chenyang Lyu,Minghao Wu,Longyue Wang,Xinting Huang,Bingshuai Liu,Zefeng Du,Shuming Shi,Zhaopeng Tu",9,54,3
"8efc20988021ce3b4b05dd44b13e27260ee9b99b","https://www.semanticscholar.org/paper/8efc20988021ce3b4b05dd44b13e27260ee9b99b",3,"Investigating Prompting Techniques for Zero- and Few-Shot Visual Question Answering","It is demonstrated that carefully designed question templates and the integration of additional visual cues, like image captions, can contribute to improved VQA performance, especially when used in conjunction with few-shot examples, however, a limitation in the use of chain-of-thought rationalization is identified, which negatively affects V QA accuracy.","arXiv.org",2023,"Rabiul Awal,Le Zhang,Aishwarya Agrawal",0,45,0
"94053805cd59f2e9a47fe3f080c7e7afefb337cc","https://www.semanticscholar.org/paper/94053805cd59f2e9a47fe3f080c7e7afefb337cc",3,"Generative Pretraining in Multimodality","Emu, a Transformer-based multimodal foundation model, is presented, which can seamlessly generate images and texts in multi-modality context through a one-model-for-all autoregressive training process and demonstrates superb performance compared to state-of-the-art large multimodAL models.","arXiv.org",2023,"Quan Sun,Qiying Yu,Yufeng Cui,Fan Zhang,Xiaosong Zhang,Yueze Wang,Hongcheng Gao,Jingjing Liu,Tiejun Huang,Xinlong Wang",5,74,1
"d58b89b64a0565e77d9b9734d871c58e4a7af6d8","https://www.semanticscholar.org/paper/d58b89b64a0565e77d9b9734d871c58e4a7af6d8",3,"mBLIP: Efficient Bootstrapping of Multilingual Vision-LLMs",,"arXiv.org",2023,"Gregor Geigle,Abhay Jain,R. Timofte,Goran Glavavs",0,68,0
"2e3dcf5a5d58ac210d0d87e9f918540a8373211a","https://www.semanticscholar.org/paper/2e3dcf5a5d58ac210d0d87e9f918540a8373211a",3,"GIT-Mol: A Multi-modal Large Language Model for Molecular Science with Graph, Image, and Text","GIT-Mol is introduced, a multi-modal large language model that integrates the structure Graph, Image, and Text information, including the Simplified Molecular Input Line Entry System (SMILES) and molecular captions, and proposes GIT-Former, a novel architecture capable of mapping all modalities into a unified latent space.","arXiv.org",2023,"Peng Liu,Yiming Ren,Zhixiang Ren",0,59,0
"c9dbdae8146b9f97e254f5d26fd6efde96eaa703","https://www.semanticscholar.org/paper/c9dbdae8146b9f97e254f5d26fd6efde96eaa703",3,"Med-Flamingo: a Multimodal Medical Few-shot Learner","Med-Flamingo improves performance in generative medical VQA by up to 20\% in clinician's rating and firstly enables multimodal medical few-shot adaptations, such as rationale generation, as well as releasing the model, code, and evaluation app.","arXiv.org",2023,"Michael Moor,Qian Huang,Shirley Wu,Michihiro Yasunaga,C. Zakka,Yashodhara Dalmia,E. Reis,P. Rajpurkar,J. Leskovec",3,30,1
"82e849a32601090fbf820f5d381dba43b52a8ed5","https://www.semanticscholar.org/paper/82e849a32601090fbf820f5d381dba43b52a8ed5",3,"Do DALL-E and Flamingo Understand Each Other?","A reconstruction task where Flamingo generates a description for a given image and DALL-E uses this description as input to synthesize a new image finds that an optimal description of an image is one that gives rise to a generated image similar to the original one.","arXiv.org",2022,"Hang Li,Jindong Gu,Rajat Koner,Sahand Sharifzadeh,Volker Tresp",6,94,0
"0fe88452660cb8a0e37f54bcd44f3cd6504354b5","https://www.semanticscholar.org/paper/0fe88452660cb8a0e37f54bcd44f3cd6504354b5",3,"Unified Model for Image, Video, Audio and Language Tasks","The proposed UnIVAL model is efficiently pretrained on many tasks, based on task balancing and multimodal curriculum learning, and unifies text, images, video, and audio into a single model, allowing for out-of-distribution generalization.","arXiv.org",2023,"Mustafa Shukor,Corentin Dancette,Alexandre Ramé,M. Cord",3,158,0
"cb5f53cd7d43ac6f48abf4dde6fc58e53a083b0e","https://www.semanticscholar.org/paper/cb5f53cd7d43ac6f48abf4dde6fc58e53a083b0e",3,"Accelerating the integration of ChatGPT and other large‐scale AI models into biomedical research and healthcare","","MedComm – Future Medicine",2023,"Ding‐Qiao Wang,Long‐Yu Feng,Jin‐Guo Ye,Jin‐Gen Zou,Yingfeng Zheng",6,99,0
"d92c797f587ce7f1b001920ab9e6b7d31960bd77","https://www.semanticscholar.org/paper/d92c797f587ce7f1b001920ab9e6b7d31960bd77",3,"RemoteCLIP: A Vision Language Foundation Model for Remote Sensing","RemoteCLIP is proposed, the first vision-language foundation model for remote sensing that aims to learn robust visual features with rich semantics, as well as aligned text embeddings for seamless downstream application and consistently outperforms baseline foundation models across different model scales.","arXiv.org",2023,"F. Liu,Delong Chen,Zhan-Rong Guan,Xiaocong Zhou,Jiale Zhu,Jun Zhou",3,109,0
"4df485f8ad1c55f8a3fd3f06b5cc46bcb4f0ddc1","https://www.semanticscholar.org/paper/4df485f8ad1c55f8a3fd3f06b5cc46bcb4f0ddc1",3,"A Foundation LAnguage-Image model of the Retina (FLAIR): Encoding expert knowledge in text supervision","Interestingly, FLAIR outperforms by a large margin more generalist, larger-scale image-language models, which emphasizes the potential of embedding experts' domain knowledge and the limitations of generalist models in medical imaging.","arXiv.org",2023,"Julio Silva-Rodríguez,H. Chakor,R. Kobbi,J. Dolz,Ismail Ben Ayed",0,108,0
"e9480d62e216f77d5556b7eda769daa4c92d004d","https://www.semanticscholar.org/paper/e9480d62e216f77d5556b7eda769daa4c92d004d",3,"VQAMix: Conditional Triplet Mixup for Medical Visual Question Answering","This paper proposes a simple yet effective data augmentation method, VQAMix, which generates more labeled training samples by linearly combining a pair of VQA samples, which can be easily embedded into any visual-language model to boost performance.","IEEE Transactions on Medical Imaging",2022,"Haifan Gong,Guanqi Chen,Mingzhi Mao,Z. Li,Guanbin Li",9,55,3
"f7ea746cd2cc25628a7a553ac27d228198be42cb","https://www.semanticscholar.org/paper/f7ea746cd2cc25628a7a553ac27d228198be42cb",3,"Pre-trained multilevel fuse network based on vision-conditioned reasoning and bilinear attentions for medical image visual question answering","This paper proposes a new pre-trained multilevel fusion network based on Vision-conditioned reasoning and Bilinear attentions for Med-VQA (VB-MVZA), which achieves more significant accuracy than the baseline models for open-ended questions and more powerful for language-bias Med- VQA datasets.","Journal of Supercomputing",2023,"Linqin Cai,Haodu Fang,Zhiqing Li",0,47,0
"ac4d13b6a4f9fb67337099f4602135a0351f5c99","https://www.semanticscholar.org/paper/ac4d13b6a4f9fb67337099f4602135a0351f5c99",3,"Towards Medical Artificial General Intelligence via Knowledge-Enhanced Multimodal Pretraining","The proposed MOTOR successfully mimics the human practice of fulfilling a""medical student"" to accelerate the process of becoming a""specialist"" and believes that this work makes a significant stride in realizing MAGI.","arXiv.org",2023,"Bingqian Lin,Zicong Chen,Mingjie Li,Haokun Lin,Hang Xu,Yi Zhu,Jian-zhuo Liu,Wenjia Cai,Lei Yang,Shen Zhao,Chenfei Wu,Ling Chen,Xiaojun Chang,Yi Yang,L. Xing,Xiaodan Liang",0,60,0
"64fa56962dd0f4bbe206be6142fbe0315c4e7c2f","https://www.semanticscholar.org/paper/64fa56962dd0f4bbe206be6142fbe0315c4e7c2f",3,"Multi-modal Pre-training for Medical Vision-language Understanding and Generation: An Empirical Study with A New Benchmark","RadioGraphy Captions (RGC), a high-quality, multi-modality radiographic dataset containing 18,434 image-caption pairs collected from an open-access online database MedPix, is proposed, which can be used as a pre-training dataset or a new benchmark for medical report generation and medical image-text retrieval.","arXiv.org",2023,"Li Xu,Bo Liu,Ameer Hamza Khan,Lu Fan,Xiao-Ming Wu",0,71,0
"534675abb9d72fc0c08d080d4f73335ceb75902c","https://www.semanticscholar.org/paper/534675abb9d72fc0c08d080d4f73335ceb75902c",3,"Multimodal Prompt Retrieval for Generative Visual Question Answering","A novel generative model enhanced by multi-modal prompt retrieval (MPR) that integrates retrieved prompts and multimodal features to generate answers in free text that enables rapid zero-shot dataset adaptation to unseen data distributions and open-set answer labels across datasets.","arXiv.org",2023,"Timothy Ossowski,Junjie Hu",0,46,0
"e0e9ba0c01d441e1fdcb8628d3f743d387b0b017","https://www.semanticscholar.org/paper/e0e9ba0c01d441e1fdcb8628d3f743d387b0b017",3,"UIT-Saviors at MEDVQA-GI 2023: Improving Multimodal Learning with Image Enhancement for Gastrointestinal Visual Question Answering","This study highlights the dominance of Transformer-based vision models over the CNNs and demonstrates the effectiveness of the image enhancement process, with six out of the eight vision models achieving better F1-Score.","arXiv.org",2023,"T. M. Thai,A. T. Vo,Hao K. Tieu,Linh P Bui,T. Nguyen",0,43,0
"304f8b4edea01fdb5a2f7f8b998c83188deeccff","https://www.semanticscholar.org/paper/304f8b4edea01fdb5a2f7f8b998c83188deeccff",3,"Towards Generalist Biomedical AI","Med-PaLM M is a large multimodal generative model that flexibly encodes and interprets biomedical data including clinical language, imaging, and genomics with the same set of model weights and reaches performance competitive with or exceeding the state of the art on all MultiMedBench tasks, often surpassing specialist models by a wide margin.","arXiv.org",2023,"Tao Tu,Shekoofeh Azizi,Danny Driess,M. Schaekermann,Mohamed Amin,Pi-Chuan Chang,Andrew Carroll,Chuck Lau,Ryutaro Tanno,Ira Ktena,B. Mustafa,Aakanksha Chowdhery,Yun Liu,Simon Kornblith,David J. Fleet,P. A. Mansfield,Sushant Prakash,Renee C Wong,S. Virmani,Christopher Semturs,S. S. Mahdavi,Bradley Green,Ewa Dominowska,B. A. Y. Arcas,J. Barral,D. Webster,G. Corrado,Y. Matias,K. Singhal,Peter R. Florence,A. Karthikesalingam,Vivek Natarajan",1,122,0
"53df959bcf6499c45e316086a96a624389a39a52","https://www.semanticscholar.org/paper/53df959bcf6499c45e316086a96a624389a39a52",2,"Accountable Textual-Visual Chat Learns to Reject Human Instructions in Image Re-creation","This paper constructs two new multimodal datasets and proposes a two-state training procedure to train the image auto-encoder and auto-regressive transformer from scratch, and provides comprehensive analyses of experimental results in terms of re-created image quality, answer accuracy, and the model behavior when faced with uncertainty and imperfect user queries.","",2023,"Zhiwei Zhang,Yuliang Liu",0,122,0
"ac7771c332da42b29a913b116bd6ef622cbf89cf","https://www.semanticscholar.org/paper/ac7771c332da42b29a913b116bd6ef622cbf89cf",2,"TaskMatrix.AI: Completing Tasks by Connecting Foundation Models with Millions of APIs","The vision of how to build such an ecosystem is presented, each key component is explained, and study cases are used to illustrate both the feasibility of this vision and the main challenges the authors need to address next.","arXiv.org",2023,"Yaobo Liang,Chenfei Wu,Ting Song,Wenshan Wu,Yan Xia,Yu Liu,Yangyiwen Ou,Shuai Lu,Lei Ji,Shaoguang Mao,Yun Wang,Linjun Shou,Ming Gong,Nan Duan",47,27,3
"352420ee61a8da783ca7750170793613b18b8d9c","https://www.semanticscholar.org/paper/352420ee61a8da783ca7750170793613b18b8d9c",2,"Tool Learning with Foundation Models","A systematic investigation of tool learning is presented, including its cognitive origins, the paradigm shift of foundation models, and the complementary roles of tools and models to inspire future research in integrating tools with foundation models.","arXiv.org",2023,"Yujia Qin,Shengding Hu,Yankai Lin,Weize Chen,Ning Ding,Ganqu Cui,Zheni Zeng,Yufei Huang,Chaojun Xiao,Chi Han,Y. Fung,Yusheng Su,Huadong Wang,Cheng Qian,Runchu Tian,Kunlun Zhu,Shi Liang,Xingyu Shen,Bokai Xu,Zhen Zhang,Yining Ye,Bo Li,Ziwei Tang,Jing Yi,Yu Zhu,Zhenning Dai,Lan Yan,Xin Cong,Ya-Ting Lu,Weilin Zhao,Yuxiang Huang,Jun-Han Yan,Xu Han,Xian Sun,Dahai Li,Jason Phang,Cheng Yang,Tongshuang Wu,Heng Ji,Zhiyuan Liu,Maosong Sun",28,238,3
"13a5140fc0b269c408ecfc666cb297410bc753c5","https://www.semanticscholar.org/paper/13a5140fc0b269c408ecfc666cb297410bc753c5",2,"Matcher: Segment Anything with One Shot Using All-Purpose Feature Matching","This work presents Matcher, which segments anything with one shot by integrating an all-purpose feature extraction model and a class-agnostic segmentation model, and proposes a novel instance-level matching strategy for controllable mask merging.","arXiv.org",2023,"Yang Liu,Muzhi Zhu,Hengtao Li,Hao Chen,Xinlong Wang,Chunhua Shen",5,44,0
"90027ca7802645671a69b00b65e1fa94e6b63544","https://www.semanticscholar.org/paper/90027ca7802645671a69b00b65e1fa94e6b63544",2,"ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models","This study proposes a modular paradigm ReWOO (Reasoning WithOut Observation) that detaches the reasoning process from external observations, thus significantly reducing token consumption and demonstrating robustness under tool-failure scenarios.","arXiv.org",2023,"Binfeng Xu,Zhiyuan Peng,Bowen Lei,Subhabrata Mukherjee,Yuchen Liu,Dongkuan Xu",6,41,0
"615962d8969c8e0ffe43319689dce6c50cbf1f29","https://www.semanticscholar.org/paper/615962d8969c8e0ffe43319689dce6c50cbf1f29",2,"Responsible Task Automation: Empowering Large Language Models as Responsible Task Automators","This paper presents Responsible Task Automation (ResponsibleTA) as a fundamental framework to facilitate responsible collaboration between LLM-based coordinators and executors for task automation with three empowered capabilities: predicting the feasibility of the commands for executors, verifying the completeness of executors and enhancing the security.","arXiv.org",2023,"Zhizheng Zhang,Xiaoyi Zhang,Wenxuan Xie,Yan Lu",0,46,0
"9dbb39eccbcd31b8f6b4ff0a2c96f61a7c34e54b","https://www.semanticscholar.org/paper/9dbb39eccbcd31b8f6b4ff0a2c96f61a7c34e54b",2,"RM-PRT: Realistic Robotic Manipulation Simulator and Benchmark with Progressive Reasoning Tasks","A realistic robotic manipulation simulator is introduced and a Robotic Manipulation with Progressive Reasoning Tasks (RM-PRT) benchmark is built on this basis, which evaluates the robot's ability to understand natural language instructions in two modes of adsorption and grasping.","arXiv.org",2023,"Pengzhen Ren,Kaiwen Zhang,Hetao Zheng,Zixuan Li,Yuhang Wen,Fengda Zhu,Mas Ma,Xiaodan Liang",0,63,0
"bbcd5cc4bf6c77282e88cae07f7f2adb1da818ca","https://www.semanticscholar.org/paper/bbcd5cc4bf6c77282e88cae07f7f2adb1da818ca",2,"Testing the Depth of ChatGPT's Comprehension via Cross-Modal Tasks Based on ASCII-Art: GPT3.5's Abilities in Regard to Recognizing and Generating ASCII-Art Are Not Totally Lacking","This work examines GPT3.5's aptitude for visual tasks, where the inputs feature content provided as ASCII-art without overt distillation into a lingual summary, and conducts experiments analyzing the model's performance on image recognition tasks after various transforms typical in visual settings.","arXiv.org",2023,"David Bayani",0,85,0
"da96ec9c32d63292e506ba8f8ea8e838df998c02","https://www.semanticscholar.org/paper/da96ec9c32d63292e506ba8f8ea8e838df998c02",2,"StableLLaVA: Enhanced Visual Instruction Tuning with Synthesized Image-Dialogue Data","This work proposes a novel data collection methodology that synchronously synthesizes images and dialogues for visual instruction tuning, marrying the abilities of ChatGPT and text-to-image generative models to yield a diverse and controllable dataset with varied image content.","",2023,"Yanda Li,Chi Zhang,Gang Yu,Zhibin Wang,Bin Fu,Guosheng Lin,Chunhua Shen,Ling Chen,Yunchao Wei",0,29,0
"0f8d12775a4685575f1489796b5dee9e11fbdfb5","https://www.semanticscholar.org/paper/0f8d12775a4685575f1489796b5dee9e11fbdfb5",2,"OphGLM: Training an Ophthalmology Large Language-and-Vision Assistant based on Instructions and Dialogue","This paper establishes a new ophthalmic multimodal instruction-following and dialogue fine-tuning dataset based on disease-related knowledge data and publicly available real-world medical dialogue, and introduces visual ability into the large language model to complete the OphGLM.","arXiv.org",2023,"Weihao Gao,Zhuo Deng,Zhiyuan Niu,Fuju Rong,Chucheng Chen,Zheng Gong,Wenze Zhang,Daimin Xiao,Fangjun Li,Zhenjie Cao,Zhaoyi Ma,Wenbin Wei,Lan Ma",1,25,0
"f33fd49b0b761dff7e96ff782814d3542029ce87","https://www.semanticscholar.org/paper/f33fd49b0b761dff7e96ff782814d3542029ce87",2,"Exploring Transfer Learning in Medical Image Segmentation using Vision-Language Models","This study comprehensively evaluates existing vision language models across multiple datasets to assess their transferability from the open domain to the medical field and reports the zero-shot and finetuned segmentation performance of 4 Vision Language Models on 11 medical datasets using 9 types of prompts derived from 14 attributes.","arXiv.org",2023,"K. Poudel,Manish Dhakal,Prasiddha Bhandari,Rabin Adhikari,Safal Thapaliya,Bishesh Khanal",0,58,0
"2195676f111ad492c50f4d4c96abb2bd3d72f7fc","https://www.semanticscholar.org/paper/2195676f111ad492c50f4d4c96abb2bd3d72f7fc",2,"Instruct2Act: Mapping Multi-modality Instructions to Robotic Actions with Large Language Model","This paper presents Instruct2Act, a framework that utilizes Large Language Models to map multi-modal instructions to sequential actions for robotic manipulation tasks, employing the LLM model to generate Python programs that constitute a comprehensive perception, planning, and action loop for robotic tasks.","arXiv.org",2023,"Siyuan Huang,Zhengkai Jiang,Hao-Wen Dong,Y. Qiao,Peng Gao,Hongsheng Li",5,55,1
"ba704774f194938b04b1e2be40b1d111a4ca08e1","https://www.semanticscholar.org/paper/ba704774f194938b04b1e2be40b1d111a4ca08e1",2,"CREATOR: Disentangling Abstract and Concrete Reasonings of Large Language Models through Tool Creation",,"arXiv.org",2023,"Cheng Qian,Chi Han,Y. Fung,Yujia Qin,Zhiyuan Liu,Heng Ji",7,32,1
"b595b55ed27935d306b0a5e0b06a3b0a771275b1","https://www.semanticscholar.org/paper/b595b55ed27935d306b0a5e0b06a3b0a771275b1",2,"SheetCopilot: Bringing Software Productivity to the Next Level through Large Language Models","This work proposes a SheetCopilot agent which takes natural language task and control spreadsheet to fulfill the requirements, and proposes a set of atomic actions as an abstraction of spreadsheet software functionalities.","arXiv.org",2023,"Hongxin Li,Jingran Su,Yuntao Chen,Qing Li,Zhaoxiang Zhang",3,75,1
"ed9943d73eb42116fe33564b5065c78b5ca0b16e","https://www.semanticscholar.org/paper/ed9943d73eb42116fe33564b5065c78b5ca0b16e",2,"RestGPT: Connecting Large Language Models with Real-World Applications via RESTful APIs","This paper introduces RestGPT, which leverages LLMs to solve user requests by connecting with RESTful APIs and proposes a coarse-to-fine online planning mechanism to enhance the ability of planning and API selection.","arXiv.org",2023,"Yifan Song,Weimin Xiong,Dawei Zhu,Chengzu Li,Ke Wang,Ye Tian,Sujian Li",2,18,0
"473eb062612a17c965eaa62136322f0dec6b1f8e","https://www.semanticscholar.org/paper/473eb062612a17c965eaa62136322f0dec6b1f8e",2,"Data-Copilot: Bridging Billions of Data and Humans with Autonomous Workflow","This work proposes Data-Copilot, an LLM-based system that connects numerous data sources on one end and caters to diverse human demands on the other end, and autonomously transforms raw data into visualization results that best match the user's intent.","arXiv.org",2023,"Wenqi Zhang,Yongliang Shen,Weiming Lu,Y. Zhuang",2,31,1
"7562e25b666cba841b1dd5cf6e700978922beb04","https://www.semanticscholar.org/paper/7562e25b666cba841b1dd5cf6e700978922beb04",2,"SkinGPT: A Dermatology Diagnostic System with Vision Large Language Model","The ability to deploy it locally and protect user privacy makes SkinGPT an attractive option for patients seeking an accurate and reliable diagnosis of their skin conditions and the system can autonomously determine the characteristics and categories of skin conditions, perform analysis, and provide treatment recommendations.","arXiv.org",2023,"Juexiao Zhou,Xin Gao",1,35,1
"06d8562831c32844285a691c5250d04726df3c61","https://www.semanticscholar.org/paper/06d8562831c32844285a691c5250d04726df3c61",2,"A Systematic Survey of Prompt Engineering on Vision-Language Foundation Models","This paper aims to provide a comprehensive survey of cutting-edge research in prompt engineering on three types of vision-language models: multimodal-to-text generation models, image-text matching models, and text- to-image generation models.","arXiv.org",2023,"Jindong Gu,Zhen Han,Shuo Chen,Ahmad Beirami,Bailan He,Gengyuan Zhang,Ruotong Liao,Yao Qin,Volker Tresp,Philip H. S. Torr",2,215,0
"d0c87ca688547f5e63fd4900300474980d900b57","https://www.semanticscholar.org/paper/d0c87ca688547f5e63fd4900300474980d900b57",2,"Towards Inadequately Pre-trained Models in Transfer Learning","It is found that during the same pre-training process, models at middle epochs, which is inadequately pre-trained, can outperform fully trained models when used as feature extractors (FE), while the fine-tuning (FT) performance still grows with the source performance.","",2022,"Andong Deng,Xingjian Li,Di Hu,Tianyang Wang,H. Xiong,Chengzhong Xu",0,86,0
"30c0cdc414f68211d5d0514df027cec22e005174","https://www.semanticscholar.org/paper/30c0cdc414f68211d5d0514df027cec22e005174",2,"A Survey on In-context Learning","This paper presents a formal definition of ICL and clarify its correlation to related studies, and organizes and discusses advanced techniques, including training strategies, demonstration designing strategies, as well as related analysis.","",2022,"Qingxiu Dong,Lei Li,Damai Dai,Ce Zheng,Zhiyong Wu,Baobao Chang,Xu Sun,Jingjing Xu,Zhifang Sui",26,127,0
"27d0d2923a42bd2bced1b100844e232ff87368e3","https://www.semanticscholar.org/paper/27d0d2923a42bd2bced1b100844e232ff87368e3",2,"SkinGPT-4: An Interactive Dermatology Diagnostic System with Visual Large Language Model","Though SkinGPT-4 is not a substitute for doctors, it could enhance users' comprehension of their medical conditions, facilitate improve communication between patients and doctors, expedite the diagnostic process for dermatologists, and potentially promote human-centred care and healthcare equity in underdeveloped areas.","medRxiv",2023,"Juexiao Zhou,Xiao-Zhen He,Liyuan Sun,Jian-Hui Xu,Xiuying Chen,Yuetan Chu,Longxi Zhou,Xingyu Liao,Bin Zhang,Xin Gao",1,68,0
"c56a51728678e5b2e3ff95e51caf21d267439c36","https://www.semanticscholar.org/paper/c56a51728678e5b2e3ff95e51caf21d267439c36",2,"ChatVideo: A Tracklet-centric Multimodal and Versatile Video Understanding System","The vision for multimodal and versatile video understanding is presented and a prototype system, built upon a tracklet-centric paradigm, which treats tracklets as the basic video unit and employs various Video Foundation Models to annotate their properties e.g., appearance, motion, etc.","arXiv.org",2023,"Junke Wang,Dongdong Chen,Chong Luo,Xiyang Dai,Lu Yuan,Zuxuan Wu,Yu-Gang Jiang",5,43,0
"6f8b9192b1f215254ee7625d752710182c05d2f9","https://www.semanticscholar.org/paper/6f8b9192b1f215254ee7625d752710182c05d2f9",2,"Caption Anything: Interactive Image Description with Diverse Multimodal Controls","Caption AnyThing (CAT) is presented, a foundation model augmented image captioning framework supporting a wide range of multimodel controls: 1) visual controls, including points, boxes, and trajectories; 2) language controls, such as sentiment, length, language, and factuality.","arXiv.org",2023,"Teng Wang,Jinrui Zhang,Junjie Fei,Yixiao Ge,Hao Zheng,Yun-Qiu Tang,Zhe Li,Mingqi Gao,Shanshan Zhao,Ying Shan,Feng Zheng",9,49,3
"80c44fab16852ea9599411da14de7079c4514172","https://www.semanticscholar.org/paper/80c44fab16852ea9599411da14de7079c4514172",2,"Vision-Language Models in Remote Sensing: Current Progress and Future Trends","A comprehensive review of the research on vision-language models in remote sensing, summarizing the latest progress, highlighting the current challenges, and identifying potential research opportunities is provided.","arXiv.org",2023,"Congcong Wen,Yuan Hu,Xiang Li,Zhenghang Yuan,Xiao Xiang Zhu",2,195,0
"8bd6a2a89503be083176f2cc26fabedb79238cbd","https://www.semanticscholar.org/paper/8bd6a2a89503be083176f2cc26fabedb79238cbd",2,"InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning","This paper conducts a systematic and comprehensive study on vision-language instruction tuning based on the pretrained BLIP-2 models, and introduces an instruction-aware Query Transformer, which extracts informative features tailored to the given instruction.","arXiv.org",2023,"Wenliang Dai,Junnan Li,Dongxu Li,A. M. H. Tiong,Junqi Zhao,Weisheng Wang,Boyang Li,Pascale Fung,Steven C. H. Hoi",72,49,18
"abac9af9174e8be2f605453695d98e3686768a27","https://www.semanticscholar.org/paper/abac9af9174e8be2f605453695d98e3686768a27",2,"ArtGPT-4: Artistic Vision-Language Understanding with Adapter-enhanced MiniGPT-4","A novel multimodal model called ArtGPT-4 has been proposed to address some challenges in image understanding, particularly in artistic pictures, and novel benchmarks for evaluating the performance of vision-language models are proposed.","arXiv.org",2023,"Zheng Yuan,HU Xue,Xinyi Wang,Yongming Liu,Zhuanzhe Zhao,Kun Wang",1,44,0
"848e690a62c327e1210532d58a6b914097cac763","https://www.semanticscholar.org/paper/848e690a62c327e1210532d58a6b914097cac763",2,"On the Hidden Mystery of OCR in Large Multimodal Models","This study conducted a comprehensive study of existing publicly available multimodal models, evaluating their performance in text recognition, text-based visual question answering, key information extraction, and handwritten mathematical expression recognition.","arXiv.org",2023,"Yuliang Liu,Zhang Li,Hongliang Li,Wenwen Yu,Mingxin Huang,Dezhi Peng,Mingyu Liu,Mingrui Chen,Chunyuan Li,Lianwen Jin,Xiang Bai",12,67,3
"3f0c4d50050e8d74993b020897abaee8d1e8054d","https://www.semanticscholar.org/paper/3f0c4d50050e8d74993b020897abaee8d1e8054d",2,"Evaluating Object Hallucination in Large Vision-Language Models","This work presents the first systematic study on object hallucination of LVLMs, and designs an improved evaluation method by proposing a polling-based query method called POPE, which can evaluate the object hallucinated objects in a more stable and flexible way.","arXiv.org",2023,"Yifan Li,Yifan Du,Kun Zhou,Jinpeng Wang,Wayne Xin Zhao,Ji-rong Wen",14,41,5
"a979975d1a0aea0e01423f092249cc3de575b6cd","https://www.semanticscholar.org/paper/a979975d1a0aea0e01423f092249cc3de575b6cd",2,"X-IQE: eXplainable Image Quality Evaluation for Text-to-Image Generation with Visual Large Language Models","A novel explainable image quality evaluation approach called X-IQE, which leverages visual large language models (LLMs) to evaluate text-to-image generation methods by generating textual explanations by utilizing a hierarchical Chain of Thought to enable MiniGPT-4 to produce self-consistent, unbiased texts that are highly correlated with human evaluation.","arXiv.org",2023,"Yixiong Chen,Li Liu,C. Ding",2,56,0
"1bdd5fc17cc580efe998304692639c57c857cc84","https://www.semanticscholar.org/paper/1bdd5fc17cc580efe998304692639c57c857cc84",2,"Going Denser with Open-Vocabulary Part Segmentation","A detector with the ability to predict both open-vocabulary objects and their part segmentation and training on the joint of part-level, object-level and image-level data to build the multi-granularity alignment between language and image.","arXiv.org",2023,"Pei Sun,Shoufa Chen,Chenchen Zhu,Fanyi Xiao,Ping Luo,Saining Xie,Zhicheng Yan",2,94,0
"33f9ddca2469bf4831dcab085e1620792b1a6a80","https://www.semanticscholar.org/paper/33f9ddca2469bf4831dcab085e1620792b1a6a80",2,"LLM Itself Can Read and Generate CXR Images","This work presents a novel method to fine-tune a pre-trained LLM to read and generate images like text without any structural changes, extra training objectives, or the need for training an ad-hoc network while still preserving the of the instruction-following capability of the LLM.","arXiv.org",2023,"Suhyeon Lee,Won Jun Kim,Jong-Chul Ye",1,49,0
"f9bfc6d9ba1665b73af3323d46c7642b852759ef","https://www.semanticscholar.org/paper/f9bfc6d9ba1665b73af3323d46c7642b852759ef",2,"VideoLLM: Modeling Video Sequence with Large Language Models","This work proposes a novel framework called VideoLLM that leverages the sequence reasoning capabilities of pre-trained LLMs from natural language processing (NLP) for video sequence understanding and demonstrates that the understanding and Reasoning capabilities of LLMs can be effectively transferred to video understanding tasks.","arXiv.org",2023,"Guo Chen,Yin-Dong Zheng,Jiahao Wang,Jilan Xu,Yifei Huang,Junting Pan,Yi Wang,Yali Wang,Y. Qiao,Tong Lu,Limin Wang",7,103,0
"43a55dbd95c9d5cd82de8db276f41adeec4a937d","https://www.semanticscholar.org/paper/43a55dbd95c9d5cd82de8db276f41adeec4a937d",2,"Interactive Data Synthesis for Systematic Vision Adaptation via LLMs-AIGCs Collaboration","This work extensively study how LLMs communicate with AIGC model to achieve more controllable image generation and makes the first attempt to collaborate them for automatic data augmentation for a variety of downstream tasks.","arXiv.org",2023,"Qifan Yu,Juncheng Li,Wentao Ye,Siliang Tang,Yueting Zhuang",4,39,0
"2ad8183c72a90511383a32ccaeea313eb85f4085","https://www.semanticscholar.org/paper/2ad8183c72a90511383a32ccaeea313eb85f4085",2,"DetGPT: Detect What You Need via Reasoning","This paper introduces a new paradigm for object detection that is called reasoning-based object detection, and leverages state-of-the-art multi-modal models and open-vocabulary object detectors to perform reasoning within the context of the user's instructions and the visual scene.","arXiv.org",2023,"Renjie Pi,Jiahui Gao,Shizhe Diao,Rui Pan,Hanze Dong,Jipeng Zhang,Lewei Yao,Jianhua Han,Hang Xu,Lingpeng Kong Tong Zhang",7,56,1
"e9a37d881abf7d94cb2c586f1cb26978343750ba","https://www.semanticscholar.org/paper/e9a37d881abf7d94cb2c586f1cb26978343750ba",2,"ReSee: Responding through Seeing Fine-grained Visual Knowledge in Open-domain Dialogue","Empirical, encouraging results not only demonstrate the effectiveness of introducing visual knowledge at both entity and turn level but also verify the proposed model ReSee outperforms several state-of-the-art methods on automatic and human evaluations.","arXiv.org",2023,"Haoqin Tu,Yitong Li,Fei Mi,Zhongliang Yang",0,61,0
"b82c1b0512d25307e3c81bb8d9df1607267a7a52","https://www.semanticscholar.org/paper/b82c1b0512d25307e3c81bb8d9df1607267a7a52",2,"MemeCap: A Dataset for Captioning and Interpreting Memes","The task of meme captioning is presented and a new dataset, MemeCap, containing 6.3K memes along with the title of the post containing the meme, the meme captions, the literal image caption, and the visual metaphors is released.","arXiv.org",2023,"EunJeong Hwang,V. Shwartz",1,41,0
"d3f79210b54e168c76b8c311488f42d7d1048b81","https://www.semanticscholar.org/paper/d3f79210b54e168c76b8c311488f42d7d1048b81",2,"PandaGPT: One Model To Instruction-Follow Them All","PandaGPT is an approach to emPower large lANguage moDels with visual and Auditory instruction-following capabilities that can perform complex tasks such as detailed image description generation, writing stories inspired by videos, and answering questions about audios.","arXiv.org",2023,"Yixuan Su,Tian Lan,Huayang Li,Jialu Xu,Yan Wang,Deng Cai",25,29,4
"b13242323021bc1483a0d76e23428e324d409315","https://www.semanticscholar.org/paper/b13242323021bc1483a0d76e23428e324d409315",2,"NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models","The NavGPT is introduced, a purely LLM-based instruction-following navigation agent, to reveal the reasoning capability of GPT models in complex embodied scenes by performing zero-shot sequential action prediction for vision-and-language navigation (VLN).","arXiv.org",2023,"Gengze Zhou,Yicong Hong,Qi Wu",6,72,0
"5fb7afae5fcacae1d40f109a348b43e00aa5d486","https://www.semanticscholar.org/paper/5fb7afae5fcacae1d40f109a348b43e00aa5d486",2,"Dense and Aligned Captions (DAC) Promote Compositional Reasoning in VL Models","A fine-tuning approach is proposed for automatically treating two factors limiting the VL models' compositional reasoning performance: the caption quality, or in other words `image-alignment', of the texts and the `density' of the captions in the sense of mentioning all the details appearing on the image.","arXiv.org",2023,"Sivan Doveh,Assaf Arbelle,Sivan Harary,Roei Herzig,Donghyun Kim,Paola Cascante-Bonilla,Amit Alfassy,R. Panda,R. Giryes,R. Feris,S. Ullman,Leonid Karlinsky",2,86,0
"7e3bbd7be60bb50a8093152795f269a69a4a0fd9","https://www.semanticscholar.org/paper/7e3bbd7be60bb50a8093152795f269a69a4a0fd9",2,"Chatting Makes Perfect - Chat-based Image Retrieval","This work introduces ChatIR: a chat-based image retrieval system that engages in a conversation with the user to elicit information, in addition to an initial query, in order to clarify the user's search intent.","arXiv.org",2023,"Matan Levy,Rami Ben-Ari,N. Darshan,D. Lischinski",0,55,0
"0867f7029b3726740fb41ca8171833bf6f82e483","https://www.semanticscholar.org/paper/0867f7029b3726740fb41ca8171833bf6f82e483",2,"Exploring Open-Vocabulary Semantic Segmentation without Human Labels","This work presents ZeroSeg, a novel method that leverages the existing pretrained vision-language (VL) model to train open-vocabulary zero-shot semantic segmentation models, achieving state-of-the-art performance when compared to other zero- shot segmentation methods under the same training data, while also performing competitively compared to strongly supervised methods.","arXiv.org",2023,"Jun Chen,Deyao Zhu,Guocheng Qian,Bernard Ghanem,Zhicheng Yan,Chenchen Zhu,Fanyi Xiao,Mohamed Elhoseiny,S. Culatana",0,58,0
"31a68755ca6899e6c360ec8568704ae74f223a25","https://www.semanticscholar.org/paper/31a68755ca6899e6c360ec8568704ae74f223a25",2,"GPT4Image: Can Large Pre-trained Models Help Vision Models on Perception Tasks?","This paper presents a new learning paradigm in which the knowledge extracted from large pre-trained models are utilized to help models like CNN and ViT learn enhanced representations and achieve better performance.","arXiv.org",2023,"N. Ding,Yehui Tang,Zhongqian Fu,Chaoting Xu,Kai Han,Yunhe Wang",0,50,0
"d7a4b09a0e2c2d7b118144cf09895c640896da7b","https://www.semanticscholar.org/paper/d7a4b09a0e2c2d7b118144cf09895c640896da7b",2,"Youku-mPLUG: A 10 Million Large-scale Chinese Video-Language Dataset for Pre-training and Benchmarks","The zero-shot instruction understanding experiment indicates that pretraining with Youku-mPLUG can enhance the ability to comprehend overall and detailed visual semantics, recognize scene text, and leverage open-domain knowledge.","arXiv.org",2023,"Haiyang Xu,Qinghao Ye,Xuan-Wei Wu,Mingshi Yan,Yuan Miao,Jiabo Ye,Guohai Xu,Anwen Hu,Yaya Shi,Guangwei Xu,Chenliang Li,Qingfang Qian,Maofei Que,Ji Zhang,Xiaoyan Zeng,Feiyan Huang",0,44,0
"6a2a756c60dbc99f666ae6e32b0dd1a58e1e2de8","https://www.semanticscholar.org/paper/6a2a756c60dbc99f666ae6e32b0dd1a58e1e2de8",2,"M3IT: A Large-Scale Dataset towards Multi-Modal Multilingual Instruction Tuning","Ying-VLM, a VLM model trained on the M$^3$IT dataset, is developed, showcasing its potential to answer complex questions requiring world knowledge, generalize to unseen video tasks, and comprehend unseen instructions in Chinese.","arXiv.org",2023,"Lei Li,Yuwei Yin,Shicheng Li,Liang Chen,Peiyi Wang,Shuhuai Ren,Mukai Li,Yazheng Yang,Jingjing Xu,Xu Sun,Lingpeng Kong,Qi Liu",7,64,3
"d818f40ea693a335e02f32dab520351d271c58bf","https://www.semanticscholar.org/paper/d818f40ea693a335e02f32dab520351d271c58bf",2,"Artificial General Intelligence for Medical Imaging","In this review, the importance of integrating clinical expertise, domain knowledge, and multimodal capabilities into AGI models, focusing on foundational Large Language Models, Large Vision Models, and Large Multimodal Models, is emphasized.","arXiv.org",2023,"Xiang Li,Lu Zhang,Zihao Wu,Zheng Liu,Lin Zhao,Yixuan Yuan,Jun Liu,Gang Li,Dajiang Zhu,Pingkuan Yan,Quanzheng Li,W. Liu,Tianming Liu,Dinggang Shen",8,166,0
"da061a6e0016d6b625a8e86d64a797ca8ddb92a5","https://www.semanticscholar.org/paper/da061a6e0016d6b625a8e86d64a797ca8ddb92a5",2,"Modular Visual Question Answering via Code Generation","This work presents a framework that formulates visual question answering as modular code generation that requires no additional training and relies on pre- trained language models, visual models pre-trained on image-caption pairs, and fifty VQA examples used for in-context learning.","Annual Meeting of the Association for Computational Linguistics",2023,"Sanjay Subramanian,Medhini G. Narasimhan,Kushal Khangaonkar,Kevin Yang,Arsha Nagrani,C. Schmid,Andy Zeng,Trevor Darrell,D. Klein",1,42,0
"bf7025a2e5dbb3c09deae02a1aa98a256ca559e2","https://www.semanticscholar.org/paper/bf7025a2e5dbb3c09deae02a1aa98a256ca559e2",2,"Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models","This work introduces Video-ChatGPT, a multimodal model that merges a video-adapted visual encoder with a LLM, and develops a quantiative evaluation framework for video-based dialogue models to objectively analyse the strengths and weaknesses of proposed models.","arXiv.org",2023,"Muhammad Maaz,Hanoona Rasheed,Salman Khan,F. Khan",12,36,5
"2933acb28b7369c7ea5b8728f6d8cb55e1beef98","https://www.semanticscholar.org/paper/2933acb28b7369c7ea5b8728f6d8cb55e1beef98",2,"Customizing General-Purpose Foundation Models for Medical Report Generation","This work proposes customizing off-the-shelf general-purpose large-scale pre-trained models, i.e., foundation models (FMs), in computer vision and natural language processing with a specific focus on medical report generation and demonstrates that unfreezing EVA-ViT-g to learn medical image representations, followed by parameter-efficient training of ChatGLM-6B to capture the writing styles of medical reports, is essential for achieving optimal results.","arXiv.org",2023,"Bang Yang,Asif Raza,Yuexian Zou,Tong Zhang",2,68,0
"b937b5ad3c1ebe6007e744fa7864ec095e0070ab","https://www.semanticscholar.org/paper/b937b5ad3c1ebe6007e744fa7864ec095e0070ab",2,"Tell Me Where to Go: A Composable Framework for Context-Aware Embodied Robot Navigation","NavCon is developed, a low-bandwidth framework that solves the lack of real-world generalization in Large Language Models by creating an intermediate layer between an LLM and a robot navigation framework in the form of Python code.","arXiv.org",2023,"Harel Biggie,Ajay Narasimha Mopidevi,Dusty Woods,C. Heckman",0,71,0
"a8d02ff6d075c3cc48f0b97801cc52765c8f9ac9","https://www.semanticscholar.org/paper/a8d02ff6d075c3cc48f0b97801cc52765c8f9ac9",2,"LVLM-eHub: A Comprehensive Evaluation Benchmark for Large Vision-Language Models","A comprehensive evaluation of publicly available large multimodal models by building a LVLM evaluation Hub (LVLM-eHub), which provides a foundational framework for the conception and assessment of innovative strategies aimed at enhancing zero-shot multimodals techniques.","arXiv.org",2023,"Peng Xu,Wenqi Shao,Kaipeng Zhang,Peng Gao,Shuo Liu,Meng Lei,Fanqing Meng,Siyuan Huang,Y. Qiao,Ping Luo",15,84,3
"bc2333c9a667af90ee7ce52b911d2e04aed01526","https://www.semanticscholar.org/paper/bc2333c9a667af90ee7ce52b911d2e04aed01526",2,"MotionGPT: Finetuned LLMs are General-Purpose Motion Generators","This paper presents a Motion General-Purpose generaTor (MotionGPT) that can use multimodal control signals, e.g., text and single-frame poses, for generating consecutive human motions by treating multimmodal signals as special input tokens in large language models (LLMs).","arXiv.org",2023,"Yaqi Zhang,Di Huang,B. Liu,Shixiang Tang,Yan Lu,Lu Chen,Lei Bai,Q. Chu,Nenghai Yu,Wanli Ouyang",5,53,1
"7839d037bb0e41f8a9898f177d2710cfe23633fc","https://www.semanticscholar.org/paper/7839d037bb0e41f8a9898f177d2710cfe23633fc",2,"Path to Medical AGI: Unify Domain-specific Medical LLMs with the Lowest Cost","This work proposes Medical AGI (MedAGI), a paradigm to unify domain-specific medical LLMs with the lowest cost, and suggests a possible path to achieve medical AGI.","medRxiv",2023,"Juexiao Zhou,Xiuying Chen,Xin Gao",0,47,0
"8724579d3f126e753a0451d98ff57b165f722e72","https://www.semanticscholar.org/paper/8724579d3f126e753a0451d98ff57b165f722e72",2,"Are aligned neural networks adversarially aligned?","It is shown that existing NLP-based optimization attacks are insufficiently powerful to reliably attack aligned text models, and conjecture that improved NLP attacks may demonstrate this same level of adversarial control over text-only models.","arXiv.org",2023,"Nicholas Carlini,Milad Nasr,Christopher A. Choquette-Choo,Matthew Jagielski,Irena Gao,Anas Awadalla,Pang Wei Koh,Daphne Ippolito,Katherine Lee,Florian Tramèr,Ludwig Schmidt",10,53,1
"84dc889beff9d51fe429cff8c92735e7410ee3c2","https://www.semanticscholar.org/paper/84dc889beff9d51fe429cff8c92735e7410ee3c2",2,"Aligning Large Multi-Modal Model with Robust Instruction Tuning","By finetuning MiniGPT4 on LRV-Instruction, this paper successfully mitigate hallucination while improving performance on public datasets using less training data compared to state-of-the-art methods and observing that a balanced ratio of positive and negative instances in the training data leads to a more robust model.","arXiv.org",2023,"Fuxiao Liu,Kevin Lin,Linjie Li,Jianfeng Wang,Y. Yacoob,Lijuan Wang",8,39,1
"cde934546bbdb19094d8a53cc047d002c827f884","https://www.semanticscholar.org/paper/cde934546bbdb19094d8a53cc047d002c827f884",2,"Large Multimodal Models: Notes on CVPR 2023 Tutorial",,"arXiv.org",2023,"Chunyuan Li",3,65,0
"def6c12724dec95ec1276a77fd1cf7e200883bdb","https://www.semanticscholar.org/paper/def6c12724dec95ec1276a77fd1cf7e200883bdb",2,"Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic","This paper proposes an MLLM called Shikra, which can handle spatial coordinate inputs and outputs in natural language, and can naturally handle location-related tasks like REC and PointQA, as well as conventional VL tasks such as Image Captioning and VQA.","arXiv.org",2023,"Ke Chen,Zhao Zhang,Weili Zeng,Richong Zhang,Feng Zhu,Rui Zhao",8,57,4
"a9d5d97733ccb15002ff3cfb95b0a7d8ba5236e3","https://www.semanticscholar.org/paper/a9d5d97733ccb15002ff3cfb95b0a7d8ba5236e3",2,"LLaVAR: Enhanced Visual Instruction Tuning for Text-Rich Image Understanding","This work enhances the current visual instruction tuning pipeline with text-rich images by substantially improves the LLaVA model's capability on text-based VQA datasets and shows promising interaction skills with humans based on the latest real-world online content that combines text and images.","arXiv.org",2023,"Yanzhe Zhang,Ruiyi Zhang,Jiuxiang Gu,Yufan Zhou,Nedim Lipka,Diyi Yang,Tongfei Sun",8,54,0
"72160b3c0f73c968fcb903db71817d1bed695f4d","https://www.semanticscholar.org/paper/72160b3c0f73c968fcb903db71817d1bed695f4d",2,"Look, Remember and Reason: Visual Reasoning with Grounded Rationales","This work introduces rationales over the visual input that allow for low-level visual capabilities, such as object recognition and tracking, as surrogate tasks to solve visual reasoning problems.","arXiv.org",2023,"Apratim Bhattacharyya,Sunny Panchal,Mingu Lee,R. Pourreza,Pulkit Madan,R. Memisevic",1,78,0
"16160a4bdd0f239e47f120547e6ecee44636d5e8","https://www.semanticscholar.org/paper/16160a4bdd0f239e47f120547e6ecee44636d5e8",2,"JourneyDB: A Benchmark for Generative Image Understanding","A large-scale dataset, JourneyDB, for multi-modal visual understanding in generative images, and 4 benchmarks to quantify the performance of generated image understanding in terms of both content and style interpretation are presented.","arXiv.org",2023,"Junting Pan,Keqiang Sun,Yuying Ge,Hao Li,Haodong Duan,Xiaoshi Wu,Renrui Zhang,Aojun Zhou,Zipeng Qin,Yi Wang,Jifeng Dai,Y. Qiao,Hongsheng Li",0,49,0
"df710c46594c04fb59ef9a93d3b4e1cb387a1b2b","https://www.semanticscholar.org/paper/df710c46594c04fb59ef9a93d3b4e1cb387a1b2b",2,"Embodied Task Planning with Large Language Models","Experimental results show that the generated plan from the TaPA framework can achieve higher success rate than LLaVA and GPT-3.5 by a sizable margin, which indicates the practicality of embodied task planning in general and complex environments.","arXiv.org",2023,"Zhenyu Wu,Ziwei Wang,Xiuwei Xu,Jiwen Lu,H. Yan",2,52,0
"ef4b604fca0c62dcd0d5caf7ca24ad74e285632d","https://www.semanticscholar.org/paper/ef4b604fca0c62dcd0d5caf7ca24ad74e285632d",2,"MultiQG-TI: Towards Question Generation from Multi-modal Sources","A simple solution is proposed for the new problem of automatic question generation from multi-modal sources containing images and texts, called MultiQG-TI, which enables a text-only question generator to process visual input in addition to textual input.","Workshop on Innovative Use of NLP for Building Educational Applications",2023,"Zichao Wang,Richard Baraniuk",0,38,0
"f5945b1421ee8e0cd59e674b73f4ad82a71f66c7","https://www.semanticscholar.org/paper/f5945b1421ee8e0cd59e674b73f4ad82a71f66c7",2,"T2I-CompBench: A Comprehensive Benchmark for Open-world Compositional Text-to-image Generation","This work proposes T2I-CompBench, a comprehensive benchmark for open-world compositional text-to-image generation, and introduces a new approach, Generative mOdel fine-tuning with Reward-driven Sample selection (GORS), to boost the compositionalText to image generation abilities of pretrained text- to-image models.","arXiv.org",2023,"Kaiyi Huang,Kaiyue Sun,Enze Xie,Zhenguo Li,Xihui Liu",0,49,0
"b37b1dc72b1882858f5120f2cd6883134089a6ed","https://www.semanticscholar.org/paper/b37b1dc72b1882858f5120f2cd6883134089a6ed",2,"MMBench: Is Your Multi-modal Model an All-around Player?","MMBench is a systematically-designed objective benchmark for robustly evaluating the various abilities of vision-language models, and it is hoped MMBench will assist the research community in better evaluating their models and encourage future advancements in this domain.","arXiv.org",2023,"Yuanzhan Liu,Haodong Duan,Yuanhan Zhang,Bo Li,Songyang Zhang,Wangbo Zhao,Yike Yuan,Jiaqi Wang,Conghui He,Ziwei Liu,Kai Chen,Dahua Lin",4,47,1
"ba63203d7f91d4135d2a392e841ce532c006e31c","https://www.semanticscholar.org/paper/ba63203d7f91d4135d2a392e841ce532c006e31c",2,"VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models","This work aims to synthesize robot trajectories, i.e., a dense sequence of 6-DoF end-effector waypoints, for a large variety of manipulation tasks given an open set of instructions and an open-set of objects, and presents a large-scale study of the proposed method in both simulated and real-robot environments.","arXiv.org",2023,"Wenlong Huang,Chen Wang,Ruohan Zhang,Yunzhu Li,Jiajun Wu,Li Fei-Fei",0,136,0
"369b449415d50387fba048bbd4d26ee890df84b5","https://www.semanticscholar.org/paper/369b449415d50387fba048bbd4d26ee890df84b5",2,"InternVid: A Large-scale Video-Text Dataset for Multimodal Understanding and Generation","A scalable approach to autonomously build a high-quality video-text dataset with large language models (LLM), thereby showcasing its efficacy in learning video-language representation at scale and introducing ViCLIP, a video- Text representation learning model based on ViT-L.","arXiv.org",2023,"Yi Wang,Yinan He,Yizhuo Li,Kunchang Li,Jiashuo Yu,X. Ma,Xinyuan Chen,Yaohui Wang,Ping Luo,Ziwei Liu,Yali Wang,Limin Wang,Y. Qiao",0,79,0
"40298b8d50109c52fc10763eddc64a07cf8acb31","https://www.semanticscholar.org/paper/40298b8d50109c52fc10763eddc64a07cf8acb31",2,"Planting a SEED of Vision in Large Language Model","This study identifies two crucial principles for the architecture and training of SEED that effectively ease subsequent alignment with LLMs, and emphasizes the great potential of discrete visual tokens in versatile multimodal LLMs and the importance of proper image tokenizers in broader research.","arXiv.org",2023,"Yuying Ge,Yixiao Ge,Ziyun Zeng,Xintao Wang,Ying Shan",1,31,0
"813ba033b8f593c98f9af44c5b4901408ba6f70a","https://www.semanticscholar.org/paper/813ba033b8f593c98f9af44c5b4901408ba6f70a",2,"Towards a Visual-Language Foundation Model for Computational Pathology","This work introduces CONtrastive learning from Captions for Histopathology (CONCH), a visual-language foundation model developed using diverse sources of histopathology images, biomedical text, and notably over 1.17 million image-caption pairs via task-agnostic pretraining, with the potential to directly facilitate a wide array of machine learning-based workflows requiring minimal or no further supervised fine-tuning.","arXiv.org",2023,"Ming Y. Lu,Bowen Chen,Drew F. K. Williamson,Richard J. Chen,Ivy Liang,Tong Ding,Guillaume Jaume,I. Odintsov,Andrew Zhang,L. Le,G. Gerber,Anil Parwani,Faisal Mahmood",0,106,0
"872c111c4bed5aba086cc023ce6279edb469220a","https://www.semanticscholar.org/paper/872c111c4bed5aba086cc023ce6279edb469220a",2,"RSGPT: A Remote Sensing Vision Language Model and Benchmark","A high-quality Remote Sensing Image Captioning dataset (RSICap) is built that facilitates the development of large VLMs in the RS field and provides a benchmark evaluation dataset called RSIEval, which consists of human-annotated captions and visual question-answer pairs, allowing for a comprehensive assessment of VL Ms in the context of RS.","arXiv.org",2023,"Yuan Hu,Jianlong Yuan,Congcong Wen,Xiaonan Lu,Xiang Li",0,85,0
"4309d572a37d655779f9dce6a2c98c66334132de","https://www.semanticscholar.org/paper/4309d572a37d655779f9dce6a2c98c66334132de",2,"SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension","This work addresses the evaluation of generative comprehension in MLLMs as a preliminary step towards a comprehensive assessment ofGenerative models, by introducing a benchmark named SEED-Bench, which spans 12 evaluation dimensions including the comprehension of both the image and video modality.","arXiv.org",2023,"Bohao Li,Rui Wang,Guangzhi Wang,Yuying Ge,Yixiao Ge,Ying Shan",4,38,0
"6f9b7c8cde1be2e62a503c31cac883c6d44c9d0d","https://www.semanticscholar.org/paper/6f9b7c8cde1be2e62a503c31cac883c6d44c9d0d",2,"MovieChat: From Dense Token to Sparse Memory for Long Video Understanding","Inspired by Atkinson-Shiffrin memory model, this work develops an memory mechanism including a rapidly updated short-term memory and a compact thus sustained long- term memory that achieves state-of-the-art performace in long video understanding.","arXiv.org",2023,"Enxin Song,Wenhao Chai,Guanhong Wang,Yucheng Zhang,Haoyang Zhou,Feiyang Wu,Xun Guo,Tianbo Ye,Yang Lu,Jenq-Neng Hwang,Gaoang Wang",0,113,0
"6f425ba8c1fe3139fcb886d9dda30cd6520517ac","https://www.semanticscholar.org/paper/6f425ba8c1fe3139fcb886d9dda30cd6520517ac",2,"More Context, Less Distraction: Visual Classification by Inferring and Conditioning on Contextual Attributes","A training-free, two-step zero-shot classification method named PerceptionCLIP, which first infers contextual attributes and then performs object classification conditioning on them and achieves better generalization, group robustness, and better interpretability.","arXiv.org",2023,"Bang An,Sicheng Zhu,Michael Panaitescu-Liess,Chaithanya Kumar Mummadi,Furong Huang",0,59,0
"e1266706b4f83130a170e2b066bf65a1e6d72387","https://www.semanticscholar.org/paper/e1266706b4f83130a170e2b066bf65a1e6d72387",2,"Improving Generalization of Image Captioning with Unsupervised Prompt Learning","An unsupervised prompt learning method is proposed to improve Generalization of Image Captioning (GeneIC), which learns a domain-specific prompt vector for the target domain without requiring annotated data.","arXiv.org",2023,"Hongchen Wei,Zhenzhong Chen",0,72,0
"1b656e3cdc49afab9ab9c1ec264850ef70153ecb","https://www.semanticscholar.org/paper/1b656e3cdc49afab9ab9c1ec264850ef70153ecb",2,"Tiny LVLM-eHub: Early Multimodal Experiments with Bard","It is demonstrated that Bard outperforms previous LVLMs in most multimodal capabilities except object hallucination, to which Bard is still susceptible, which leads to a robust and accurate evaluation and exhibits improved alignment with human evaluation compared to the word matching approach.","arXiv.org",2023,"Wenqi Shao,Yutao Hu,Peng Gao,Meng Lei,Kaipeng Zhang,Fanqing Meng,Peng Xu,Siyuan Huang,Hongsheng Li,Y. Qiao,Ping Luo",0,63,0
"d2a0216ecfff5be849c2c9ee389d4a2f3f4aecca","https://www.semanticscholar.org/paper/d2a0216ecfff5be849c2c9ee389d4a2f3f4aecca",2,"Empowering Vision-Language Models to Follow Interleaved Vision-Language Instructions","This paper presents Cheetor, a Transformer-based MLLM that can effectively handle a wide variety of interleaved vision-language instructions and achieves state-of-the-art zero-shot performance across all tasks of I4, without high-quality multimodal instruction tuning data.","arXiv.org",2023,"Juncheng Li,Kaihang Pan,Zhiqi Ge,Minghe Gao,Hanwang Zhang,Wei Ji,Wenqiao Zhang,Tat-Seng Chua,Siliang Tang,Yueting Zhuang",2,64,0
"74d245de70e9a9f11d6eaa72439004e5cc2fabaf","https://www.semanticscholar.org/paper/74d245de70e9a9f11d6eaa72439004e5cc2fabaf",2,"Prompting In-Context Operator Learning with Sensor Data, Equations, and Natural Language","This work proposes the use of ""captions"" to integrate human knowledge about the operator, expressed through natural language descriptions and equations, and introduces a more efficient neural network architecture for multi-modal in-context operator learning, referred to as ""ICON-LM"", based on a language-model-like architecture.","arXiv.org",2023,"Liu Yang,Tingwei Meng,Siting Liu,S. Osher",0,61,0
"2854e5bab8e6f36e54c64456628a9559bf67019e","https://www.semanticscholar.org/paper/2854e5bab8e6f36e54c64456628a9559bf67019e",2,"IP-Adapter: Text Compatible Image Prompt Adapter for Text-to-Image Diffusion Models","The proposed IP-Adapter is an effective and lightweight adapter to achieve image prompt capability for the pretrained text-to-image diffusion models and has the benefit of the decoupled cross-attention strategy, the image prompt can also work well with the text prompt to achieve multimodal image generation.","arXiv.org",2023,"Hu Ye,Jun Zhang,Siyi Liu,Xiao Han,Wei Yang",0,51,0
"30cfc4e7174211aa48c965826d51db773f0d37c7","https://www.semanticscholar.org/paper/30cfc4e7174211aa48c965826d51db773f0d37c7",2,"Chat-3D: Data-efficiently Tuning Large Language Model for Universal Dialogue of 3D Scenes","Chat-3D is presented, which combines the 3D visual perceptual ability of pre-trained 3D representations and the impressive reasoning and conversation capabilities of advanced LLMs to achieve the first universal dialogue systems for 3D scenes.","arXiv.org",2023,"Zehan Wang,Haifeng Huang,Yang Zhao,Ziang Zhang,Zhou Zhao",0,59,0
"e58b0ee9a1fdb15a72ee721053df3569127cde42","https://www.semanticscholar.org/paper/e58b0ee9a1fdb15a72ee721053df3569127cde42",2,"Tackling Vision Language Tasks Through Learning Inner Monologues","Inner Monologue Multi-Modal Optimization is proposed, to solve complex vision language problems by simulating inner monologue processes, a cognitive process in which an individual engages in silent verbal communication with themselves, promising wider applicability to many different AI problems beyond vision language tasks.","",2023,"Diji Yang,Kezhen Chen,Jinmeng Rao,Xiao-Yu Guo,Yawen Zhang,Jie Yang,Y. Zhang",0,46,0
"27e45a8aecc1fec246fd70c80d8f5104807cf0dd","https://www.semanticscholar.org/paper/27e45a8aecc1fec246fd70c80d8f5104807cf0dd",2,"ViT-Lens: Towards Omni-modal Representations","ViT-Lens is presented that facilitates efficient omni-modal representation learning by perceiving novel modalities with a pretrained ViT and aligning to a pre-defined space, and enables zero-shot 3D question-answering by simply integrating the trained 3D lens into the InstructBLIP model without any adaptation.","",2023,"Weixian Lei,Yixiao Ge,Jianfeng Zhang,Dylan Sun,Kun Yi,Ying Shan,Mike Zheng Shou",0,80,0
"66d3b7a6561148fd21c364315e67bf9373f50ef7","https://www.semanticscholar.org/paper/66d3b7a6561148fd21c364315e67bf9373f50ef7",2,"An Examination of the Compositionality of Large Generative Vision-Language Models","This study provides the first unbiased benchmark for the compositionality of GVLMs, facilitating future research in this direction and defines a MorphoBias Score to quantify the morphological bias and proposes a novel LLM-based strategy to calibrate the bias.","",2023,"Teli Ma,Rong Li,Junwei Liang",0,48,0
"110ccbe25193fa3df9ae073205dee9da8b05cd01","https://www.semanticscholar.org/paper/110ccbe25193fa3df9ae073205dee9da8b05cd01",2,"Compositional Reasoning in Vision-Language Models","This paper decomposes “com-positional” questions from the CLEVR dataset and confirms that state-of-the-art VLMs struggle with composition, revealing that these models can accurately identify pieces of information but struggle to relate them together to answer compositional questions.","",2023,"Jessica Li,Apoorv Khandelwal,Elizabeth-Jane Pavlick",0,23,0
"508d9b43832790b4d35f4ae1fa76e9712859d6aa","https://www.semanticscholar.org/paper/508d9b43832790b4d35f4ae1fa76e9712859d6aa",2,"Vision and Structured-Language Pretraining for Cross-Modal Food Retrieval","VLPCook outperforms current SoTA by a significant margin on the task of Cross-Modal Food Retrieval on the large Recipe1M dataset and validates the generalization of the approach to other tasks and domains with structured text such as the Medical domain on the ROCO dataset.","",2022,"Mustafa Shukor,Nicolas Thome,M. Cord",0,85,0
"3f5b31c4f7350dc88002c121aecbdc82f86eb5bb","https://www.semanticscholar.org/paper/3f5b31c4f7350dc88002c121aecbdc82f86eb5bb",2,"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models","BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods, and is demonstrated's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.","International Conference on Machine Learning",2023,"Junnan Li,Dongxu Li,S. Savarese,Steven C. H. Hoi",385,46,100
"64caaab51d8339f1b99874d3bddb79debbe661ca","https://www.semanticscholar.org/paper/64caaab51d8339f1b99874d3bddb79debbe661ca",2,"mPLUG-2: A Modularized Multi-modal Foundation Model Across Text, Image and Video",,"International Conference on Machine Learning",2023,"Haiyang Xu,Qinghao Ye,Mingshi Yan,Yaya Shi,Jiabo Ye,Yuanhong Xu,Chenliang Li,Bin Bi,Qiuchen Qian,Wei Wang,Guohai Xu,Ji Zhang,Songfang Huang,Feiran Huang,Jingren Zhou",22,128,1
"fbfef4723d8c8467d7bd523e1d0b703cce0e0f9c","https://www.semanticscholar.org/paper/fbfef4723d8c8467d7bd523e1d0b703cce0e0f9c",2,"Language Is Not All You Need: Aligning Perception with Language Models","This work introduces Kosmos-1, a Multimodal Large Language Model (MLLM) that can perceive general modalities, learn in context, and follow instructions, and shows that MLLMs can benefit from cross-modal transfer, i.e., transfer knowledge from language to multimodals, and from multimodal to language.","arXiv.org",2023,"Shaohan Huang,Li Dong,Wenhui Wang,Y. Hao,Saksham Singhal,Shuming Ma,Tengchao Lv,Lei Cui,O. Mohammed,Qiang Liu,Kriti Aggarwal,Zewen Chi,Johan Bjorck,Vishrav Chaudhary,Subhojit Som,Xia Song,Furu Wei",112,70,17
"69cfdc8df16ae63b7acba4ac6f727f78b86893c3","https://www.semanticscholar.org/paper/69cfdc8df16ae63b7acba4ac6f727f78b86893c3",2,"ChatGPT Asks, BLIP-2 Answers: Automatic Questioning Towards Enriched Visual Descriptions","This paper introduces ChatCaptioner, a novel automatic-questioning method deployed in image captioning that identifies 53% more objects within the image than BLIP-2 alone measured by WordNet synset matching.","arXiv.org",2023,"Deyao Zhu,Jun Chen,Kilichbek Haydarov,Xiaoqian Shen,Wenxuan Zhang,Mohamed Elhoseiny",19,51,3
"9d12916dd46df7a6446cbec0bc4d054f7dafcdab","https://www.semanticscholar.org/paper/9d12916dd46df7a6446cbec0bc4d054f7dafcdab",2,"Scaling Vision-Language Models with Sparse Mixture of Experts","The effectiveness of MoE in scaling vision-language models is explored, demonstrating its potential to achieve state-of-the-art performance on a range of benchmarks over dense models of equivalent computational cost.","arXiv.org",2023,"Sheng Shen,Z. Yao,Chunyuan Li,Trevor Darrell,K. Keutzer,Yuxiong He",4,74,0
"4396e30f28eb49bb07c63cf62ca90415ebbe43d4","https://www.semanticscholar.org/paper/4396e30f28eb49bb07c63cf62ca90415ebbe43d4",2,"IRGen: Generative Modeling for Image Retrieval","The framework, IRGen, is a unified model that enables end-to-end differentiable search, thus achieving superior performance thanks to direct optimization and tackling the key technical challenge of converting an image into quite a short sequence of semantic units in order to enable efficient and effective retrieval.","arXiv.org",2023,"Yidan Zhang,Ting Zhang,Dong Chen,Yujing Wang,Qi Chen,Xingxu Xie,Hao Sun,Weiwei Deng,Qi Zhang,Fan Yang,Mao Yang,Q. Liao,B. Guo",2,115,0
"3049c992adbd56e29c4d957ee0c4e9d05fe3c6d1","https://www.semanticscholar.org/paper/3049c992adbd56e29c4d957ee0c4e9d05fe3c6d1",2,"EVA-02: A Visual Representation for Neon Genesis","EVA-02, a next-generation Transformer-based visual representation pre-trained to reconstruct strong and robust language-aligned vision features via masked image modeling, is launched, demonstrating superior performance compared to prior state-of-the-art approaches across various representative vision tasks, while utilizing significantly fewer parameters and compute budgets.","arXiv.org",2023,"Yuxin Fang,Quan Sun,Xinggang Wang,Tiejun Huang,Xinlong Wang,Yue Cao",14,147,4
"d064075c47e358f604034d06df4b985356757c71","https://www.semanticscholar.org/paper/d064075c47e358f604034d06df4b985356757c71",2,"Equivariant Similarity for Vision-Language Foundation Models","This study proposes EqSim, a regularization loss that can be efficiently calculated from any two matched training pairs and easily pluggable into existing image-text retrieval fine-tuning and presents a new challenging benchmark EqBen, the first to focus on ""visual-minimal change"".","arXiv.org",2023,"Tan Wang,Kevin Lin,Linjie Li,Chung-Ching Lin,Zhengyuan Yang,Hanwang Zhang,Zicheng Liu,Lijuan Wang",3,75,1
"a757999ed260d7bc45484dc6b4456bf33fe6f679","https://www.semanticscholar.org/paper/a757999ed260d7bc45484dc6b4456bf33fe6f679",2,"LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention","A zero-initialized attention mechanism with zero gating is proposed, which adaptively injects the new instructional cues into LLaMA, while effectively preserves its pre-trained knowledge on traditional vision and language tasks, demonstrating the superior generalization capacity of the approach.","arXiv.org",2023,"Renrui Zhang,Jiaming Han,Aojun Zhou,Xiangfei Hu,Shilin Yan,Pan Lu,Hongsheng Li,Peng Gao,Y. Qiao",82,100,10
"26eb6745006e94317cfb634123fe3015702fb224","https://www.semanticscholar.org/paper/26eb6745006e94317cfb634123fe3015702fb224",2,"MaMMUT: A Simple Architecture for Joint Learning for MultiModal Tasks","A novel paradigm of training with a decoder-only model for multimodal tasks, which is surprisingly effective in jointly learning of these disparate vision-language tasks, and achieves the state of the art on image-text and text-image retrieval, video question answering and open-vocabulary detection tasks.","arXiv.org",2023,"Weicheng Kuo,A. Piergiovanni,Dahun Kim,Xiyang Luo,Benjamin Caine,W. Li,A. Ogale,Luowei Zhou,Andrew M. Dai,Zhifeng Chen,Claire Cui,A. Angelova",4,76,0
"c84e2801512069acbc63f1a7f73273281939428c","https://www.semanticscholar.org/paper/c84e2801512069acbc63f1a7f73273281939428c",2,"A Study of Autoregressive Decoders for Multi-Tasking in Computer Vision","This work takes a close look at autoregressive decoders for multi-task learning in multimodal computer vision, including classification, captioning, visual question answering, and optical character recognition and compares these to well-tuned single-task baselines to highlight the cost incurred by multi-tasking.","arXiv.org",2023,"L. Beyer,Bo Wan,Gagan Madan,Filip Pavetic,A. Steiner,Alexander Kolesnikov,André Susano Pinto,Emanuele Bugliarello,Xiao Wang,Qihang Yu,Liang-Chieh Chen,Xiaohua Zhai",3,98,0
"84b6fecf016d74512869c698c66c83729abdf359","https://www.semanticscholar.org/paper/84b6fecf016d74512869c698c66c83729abdf359",2,"Self-Supervised Multimodal Learning: A Survey","A comprehensive review of the state-of-the-art in SSML, in which three major challenges intrinsic to self-supervised learning with multimodal data are elucidated and existing solutions to these challenges are detailed.","arXiv.org",2023,"Yongshuo Zong,Oisin Mac Aodha,Timothy M. Hospedales",4,231,0
"db1c83ef73d2f7731b0dd255835f2f26db749e17","https://www.semanticscholar.org/paper/db1c83ef73d2f7731b0dd255835f2f26db749e17",2,"Not All Features Matter: Enhancing Few-shot CLIP with Adaptive Prior Refinement","This paper proposes APE, an Adaptive Prior rEfinement method for CLIP's pre-trained knowledge, which achieves superior accuracy with high computational efficiency and introduces two model variants, a training-free APE and aTraining-required APE-T.","arXiv.org",2023,"Xiang-yu Zhu,Renrui Zhang,Bowei He,A-Long Zhou,D. Wang,Bingyan Zhao,Peng Gao",2,103,0
"a43a3fadc9190e61b34f59a913f1716e443519e4","https://www.semanticscholar.org/paper/a43a3fadc9190e61b34f59a913f1716e443519e4",2,"On the Opportunities and Challenges of Foundation Models for Geospatial Artificial Intelligence","It is proposed that one of the major challenges of developing a FM for GeoAI is to address the multimodality nature of geospatial tasks and the possibility of a multimodal foundation model which can reason over various types ofGeoAI data through geosp spatial alignments is suggested.","arXiv.org",2023,"Gengchen Mai,Weiming Huang,Jin Sun,Suhang Song,Deepak Mishra,Ninghao Liu,Song Gao,Tianming Liu,G. Cong,Yingjie Hu,Chris Cundy,Ziyuan Li,Rui Zhu,Ni Lao",20,158,2
"b7d73f22d861f526541575a3b17449bd3c58ca74","https://www.semanticscholar.org/paper/b7d73f22d861f526541575a3b17449bd3c58ca74",2,"MVP-SEG: Multi-View Prompt Learning for Open-Vocabulary Semantic Segmentation","Experiments show that the multi-view prompts learned from seen categories have strong generalization to unseen categories, and MVP-SEG+ which combines the knowledge transfer stage significantly outperforms previous methods on several benchmarks, justifying that MVP- SEG does lead to better focus on different local parts.","arXiv.org",2023,"Jie Guo,Qimeng Wang,Yan Gao,Xiaolong Jiang,Xu Tang,Yao Hu,Baochang Zhang",2,56,0
"c3068e2a9f4cd374c7ff3be1b8f877b3d653e880","https://www.semanticscholar.org/paper/c3068e2a9f4cd374c7ff3be1b8f877b3d653e880",2,"Multimodal Neural Databases","This paper proposes a new framework, inspired by recent work on neural databases, which is named Multimodal Neural Databases (MMNDBs), which can answer complex database-like queries that involve reasoning over different input modalities, such as text and images, at scale.","Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",2023,"Giovanni Trappolini,Andrea Santilli,E. Rodolà,A. Halevy,F. Silvestri",3,40,0
"b07fa63a6d2f39900f0f2cae8f58cd5507010aad","https://www.semanticscholar.org/paper/b07fa63a6d2f39900f0f2cae8f58cd5507010aad",2,"Multi-Prompt with Depth Partitioned Cross-Modal Learning","This study introduces the Partitioned Multi-modal Prompt (PMPO), a multi- modal prompting technique that extends the soft prompt from a single learnable prompt to multiple prompts, and incorporates prior information from manually designed templates and learnable multi-prompts, thus improving the generalization capabilities of the approach.","arXiv.org",2023,"Yiqi Wang,Xianda Guo,Zheng Hua Zhu,Yingjie Tian",0,43,0
"0340c850e033abbf71c7214e403c8fe2be5ef91f","https://www.semanticscholar.org/paper/0340c850e033abbf71c7214e403c8fe2be5ef91f",2,"Visual Tuning","This survey characterizes a large and thoughtful selection of recent works, providing a systematic and comprehensive overview of existing work and models and categorizes recent visual tuning techniques into five groups: prompt tuning, adapter tuning, parameter tuning, and remapping tuning.","arXiv.org",2023,"Bruce X. B. Yu,Jianlong Chang,Haixin Wang,Lin Liu,Shijie Wang,Zhiyu Wang,Junfan Lin,Lingxi Xie,Haojie Li,Zhouchen Lin,Qi Tian,Chang Wen Chen",3,291,0
"8badb0587fef2ffc078b0cec549eb8ec96ed3ad4","https://www.semanticscholar.org/paper/8badb0587fef2ffc078b0cec549eb8ec96ed3ad4",2,"Self-Chained Image-Language Model for Video Localization and Question Answering","Self-Chained Video Localization-Answering (SeViLA), a novel framework that leverages a single image-language model (BLIP-2) to tackle both temporal keyframe localization and QA on videos, outperforms several strong baselines/previous works on five video QA and event prediction tasks, and achieves the state-of-the-art in both fine-tuning and zero-shot settings.","arXiv.org",2023,"Shoubin Yu,Jaemin Cho,Prateek Yadav,Mohit Bansal",1,80,0
"38be7643bcad936739550a1802220eb53ca9b1df","https://www.semanticscholar.org/paper/38be7643bcad936739550a1802220eb53ca9b1df",2,"Simple Token-Level Confidence Improves Caption Correctness","This work explores Token-Level Confidence, or TLC, as a simple yet surprisingly effective method to assess caption correctness, and fine-tune a vision-language model on image captioning, input an image and proposed caption to the model, and aggregate either algebraic or learned token confidences over words or sequences to estimate image-caption consistency.","arXiv.org",2023,"Suzanne Petryk,Spencer Whitehead,Joseph Gonzalez,Trevor Darrell,Anna Rohrbach,Marcus Rohrbach",0,73,0
"1b0841c955ffc296348ab2bb1e98fdb0995e928b","https://www.semanticscholar.org/paper/1b0841c955ffc296348ab2bb1e98fdb0995e928b",2,"Enhancing Vision-Language Pre-Training with Jointly Learned Questioner and Dense Captioner","A novel method called Joint QA and DC GEneration (JADE), which utilizes a pre-trained multimodal model and easily-crawled image-text pairs to automatically generate and filter large-scale VQA and dense captioning datasets.","arXiv.org",2023,"Zikang Liu,Sihan Chen,Longteng Guo,Handong Li,Xingjian He,J. Liu",0,64,0
"6118eb18023429fa8bad64b7a1d95533127a62d7","https://www.semanticscholar.org/paper/6118eb18023429fa8bad64b7a1d95533127a62d7",2,"Prompt ChatGPT In MNER: Improved multimodal named entity recognition method based on auxiliary refining knowledge from ChatGPT","This paper uses ChatGPT as an implicit knowledge engine to acquire auxiliary refined knowledge, thereby bolstering the model's performance in MNER tasks and significantly outperforms all existing state-of-the-art methods on two classic MNER datasets.","arXiv.org",2023,"Jinyuan Li,Han Li,Zhufeng Pan,Gang Pan",1,48,0
"d57caa06a42baa90eb741a9afb10fe4fff8be82a","https://www.semanticscholar.org/paper/d57caa06a42baa90eb741a9afb10fe4fff8be82a",2,"SmartTrim: Adaptive Tokens and Parameters Pruning for Efficient Vision-Language Models","This work proposes an adaptive acceleration method SmartTrim for VLMs, which adjusts the inference overhead based on the complexity of instances, and incorporates lightweight trimming modules into the backbone to perform task-specific pruning on redundant inputs and parameters without the need for additional pre-training or data augmentation.","arXiv.org",2023,"Zekun Wang,Jingchang Chen,Wangchunshu Zhou,Ming Liu,Bing Qin",0,87,0
"c2c7ad3112c4b575e5d8163a0e574f9eb743cb52","https://www.semanticscholar.org/paper/c2c7ad3112c4b575e5d8163a0e574f9eb743cb52",2,"Zero-shot Visual Question Answering with Language Model Feedback","A novel language model guided captioning approach, LAMOC, for knowledge-based visual question answering (VQA), which outperforms several competitive zero-shot methods and even achieves comparable results to a fine-tuned VLP model.","Annual Meeting of the Association for Computational Linguistics",2023,"Yifan Du,Junyi Li,Tianyi Tang,Wayne Xin Zhao,Ji-rong Wen",0,42,0
"5c183d241fe54a6d67e21eea48fbd5ea6b31ec1c","https://www.semanticscholar.org/paper/5c183d241fe54a6d67e21eea48fbd5ea6b31ec1c",2,"VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset","Based on the proposed VAST-27M dataset, an omni-modality video-text foundational model named VAST is trained, which can perceive and process vision, audio, and subtitle modalities from video, and better support various tasks including vision- Text, audio-text, and multi-modal video- text tasks (retrieval, captioning and QA).","arXiv.org",2023,"Sihan Chen,Handong Li,Qunbo Wang,Zijia Zhao,Ming-Ting Sun,Xinxin Zhu,J. Liu",0,101,0
"fd928577d67dd01048d13f284a6256164bbcf2f0","https://www.semanticscholar.org/paper/fd928577d67dd01048d13f284a6256164bbcf2f0",2,"Learning without Forgetting for Vision-Language Models","ProjectiOn Fusion (PROOF) is proposed that enables VLMs to learn without forgetting and jointly adjusting visual and textual features, the model can capture semantic information with stronger representation ability.","arXiv.org",2023,"Da-Wei Zhou,Yuanhan Zhang,Jingyi Ning,Han-Jia Ye,De-chuan Zhan,Ziwei Liu",0,84,0
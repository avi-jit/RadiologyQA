"id","url","score","title","summary","venue","year","authors","citationCount","referenceCount","influentialCitationCount"
"c7492913370b5726eaa6ced163a60de6c9d4bb7f","https://www.semanticscholar.org/paper/c7492913370b5726eaa6ced163a60de6c9d4bb7f",5,"A Survey of Large Language Models for Healthcare: from Data, Technology, and Applications to Accountability and Ethics","It is contended that a significant paradigm shift is underway, transitioning from PLMs to LLMs, which encompasses a move from discriminative AI approaches to generativeAI approaches, as well as a shift from model-centered methodologies to datacentered methodologies.","arXiv.org",2023,"Kai He,Rui Mao,Qika Lin,Yucheng Ruan,Xiang Lan,Mengling Feng,Erik Cambria",27,377,2
"4d4a96708fc67403176bb2b891b564af7a20c148","https://www.semanticscholar.org/paper/4d4a96708fc67403176bb2b891b564af7a20c148",4,"RAMM: Retrieval-augmented Biomedical Visual Question Answering with Multi-modal Pre-training","This paper collects a new biomedical dataset named PMCPM which offers patient-based image-text pairs containing diverse patient situations from PubMed and proposes a retrieval-augmented pretrain-and-finetune paradigm named RAMM for biomedical VQA to overcome the data limitation issue.","ACM Multimedia",2023,"Zheng Yuan,Qiao Jin,Chuanqi Tan,Zhengyun Zhao,Hongyi Yuan,Fei Huang,Songfang Huang",4,49,0
"ebedc4d7a2356090904baba4104ef0832bc236df","https://www.semanticscholar.org/paper/ebedc4d7a2356090904baba4104ef0832bc236df",4,"A Survey on Multimodal Large Language Models","This paper presents the formulation of MLLM and delineate its related concepts, and discusses the key techniques and applications, including Multimodal Instruction Tuning (M-IT), Multimmodal In-Context Learning (M -ICL), MultIModal Chain of Thought (m-CoT), and LLM-Aided Visual Reasoning (LAVR).","arXiv.org",2023,"Shukang Yin,Chaoyou Fu,Sirui Zhao,Ke Li,Xing Sun,Tong Xu,Enhong Chen",110,108,2
"2c7e346aa311fec4dda04bdf3a214ce2026d8807","https://www.semanticscholar.org/paper/2c7e346aa311fec4dda04bdf3a214ce2026d8807",4,"Medical Vision Language Pretraining: A survey","This paper reviews existing works through the lens of different pretraining objectives, architectures, downstream evaluation tasks, and datasets utilized for pretraining and downstream tasks, then dives into current challenges in medical VLP, discussing existing and potential solutions, and concludes by highlighting future directions.","arXiv.org",2023,"Prashant Shrestha,Sanskar Amgain,Bidur Khanal,C. Linte,Binod Bhattarai",2,161,0
"5ce94181ea702f69c3651dce721d6bd8026b8106","https://www.semanticscholar.org/paper/5ce94181ea702f69c3651dce721d6bd8026b8106",3,"TPTU: Task Planning and Tool Usage of Large Language Model-based AI Agents","A structured framework tailored for LLM-based AI Agents is proposed and two distinct types of agents are designed to execute the inference process, which emphasizes the substantial potential of these models, while also identifying areas that need more investigation and improvement.","arXiv.org",2023,"Jingqing Ruan,Yihong Chen,Bin Zhang,Zhiwei Xu,Tianpeng Bao,Guoqing Du,Shiwei Shi,Hangyu Mao,Xingyu Zeng,Rui Zhao",34,91,5
"6845bea94b2fb17d4377b3bb2bd10f73a959f9cc","https://www.semanticscholar.org/paper/6845bea94b2fb17d4377b3bb2bd10f73a959f9cc",3,"Reasoning with Language Model Prompting: A Survey","This paper provides a comprehensive survey of cutting-edge research on reasoning with language model prompting with comparisons and summaries and provides systematic resources to help beginners.","Annual Meeting of the Association for Computational Linguistics",2022,"Shuofei Qiao,Yixin Ou,Ningyu Zhang,Xiang Chen,Yunzhi Yao,Shumin Deng,Chuanqi Tan,Fei Huang,Huajun Chen",126,219,2
"0ebc861f5478561f12941e6b48aad30574e996d8","https://www.semanticscholar.org/paper/0ebc861f5478561f12941e6b48aad30574e996d8",3,"Video ChatCaptioner: Towards Enriched Spatiotemporal Descriptions","This work introduces Video ChatCaptioner, an innovative approach for creating more comprehensive spatiotemporal video descriptions, specifically designed to select frames for posing video content-driven questions and shows promise as a method for enhancing video content.","arXiv.org",2023,"Jun Chen,Deyao Zhu,Kilichbek Haydarov,Xiang Li,Mohamed Elhoseiny",11,44,0
"a5036f31f0e629dc661f120b8c3b1f374d479ab8","https://www.semanticscholar.org/paper/a5036f31f0e629dc661f120b8c3b1f374d479ab8",3,"Visual Instruction Tuning","This paper presents LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding and introduces GPT-4 generated visual instruction tuning data, the model and code base publicly available.","Neural Information Processing Systems",2023,"Haotian Liu,Chunyuan Li,Qingyang Wu,Yong Jae Lee",791,63,255
"170c97c7215f42edfb20c2248f954879e91ef86e","https://www.semanticscholar.org/paper/170c97c7215f42edfb20c2248f954879e91ef86e",3,"Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models","This paper demonstrates the effectiveness of Chameleon on two multi-modal knowledge-intensive reasoning tasks: ScienceQA and TabMWP and shows that the GPT-4-powered planner exhibits more consistent and rational tool selection via inferring potential constraints from instructions, compared to a ChatGPT- powered planner.","Neural Information Processing Systems",2023,"Pan Lu,Baolin Peng,Hao Cheng,Michel Galley,Kai-Wei Chang,Y. Wu,Song-Chun Zhu,Jianfeng Gao",133,72,17
"ca6a2bc279be5a3349a22bfd6866ed633d18734b","https://www.semanticscholar.org/paper/ca6a2bc279be5a3349a22bfd6866ed633d18734b",3,"MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models","MiniGPT-4 is presented, which aligns a frozen visual encoder with a frozen advanced LLM, Vicuna, using one projection layer to uncovers that properly aligning the visual features with an advanced large language model can possess numerous advanced multi-modal abilities demonstrated by G PT-4.","arXiv.org",2023,"Deyao Zhu,Jun Chen,Xiaoqian Shen,Xiang Li,Mohamed Elhoseiny",622,49,134
"570079bbdd8758dfe865097e05719313c9c1301a","https://www.semanticscholar.org/paper/570079bbdd8758dfe865097e05719313c9c1301a",3,"LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model","This work augments LLaMA-Adapter by unlocking more learnable parameters and proposes an early fusion strategy to feed visual tokens only into the early LLM layers, contributing to better visual knowledge incorporation and achieves strong multi-modal reasoning with only a small-scale image-text and instruction dataset.","arXiv.org",2023,"Peng Gao,Jiaming Han,Renrui Zhang,Ziyi Lin,Shijie Geng,Aojun Zhou,W. Zhang,Pan Lu,Conghui He,Xiangyu Yue,Hongsheng Li,Y. Qiao",219,79,30
"d6d3604f369bb0415cbe814e43ca3131323b03e2","https://www.semanticscholar.org/paper/d6d3604f369bb0415cbe814e43ca3131323b03e2",3,"Otter: A Multi-Modal Model with In-Context Instruction Tuning","Otter, a multi-modal model based on OpenFlamingo (open-sourced version of DeepMind's Flamingo), trained on MIMIC-IT and showcasing improved instruction-following ability and in-context learning is introduced.","arXiv.org",2023,"Bo Li,Yuanhan Zhang,Liangyu Chen,Jinghao Wang,Jingkang Yang,Ziwei Liu",228,39,34
"66d755730f5d08a6f4fcc5e81f24982ba389dca9","https://www.semanticscholar.org/paper/66d755730f5d08a6f4fcc5e81f24982ba389dca9",3,"LayoutGPT: Compositional Visual Planning and Generation with Large Language Models","This work proposes LayoutGPT, a method to compose in-context visual demonstrations in style sheet language to enhance the visual planning skills of LLMs, and shows superior performance in converting challenging language concepts like numerical and spatial relations to layout arrangements for faithful text-to-image generation.","Neural Information Processing Systems",2023,"Weixi Feng,Wanrong Zhu,Tsu-Jui Fu,Varun Jampani,Arjun Reddy Akula,Xuehai He,Sugato Basu,X. Wang,William Yang Wang",30,65,4
"9837349417e36ef5be06da0fd6c74042148bdaa2","https://www.semanticscholar.org/paper/9837349417e36ef5be06da0fd6c74042148bdaa2",3,"Visual Programming for Text-to-Image Generation and Evaluation","This work proposes two novel interpretable/explainable visual programming frameworks for text-to-image (T2I) generation and evaluation and introduces VPEval, an interpretable and explainable evaluation framework for T2I generation based on visual programming.","arXiv.org",2023,"Jaemin Cho,Abhaysinh Zala,Mohit Bansal",23,68,4
"7cf64070fd3d7e53d80f260c10e6bd7018d580e1","https://www.semanticscholar.org/paper/7cf64070fd3d7e53d80f260c10e6bd7018d580e1",3,"IdealGPT: Iteratively Decomposing Vision and Language Reasoning via Large Language Models","The IdealGPT framework is presented, a framework that iteratively decomposes VL reasoning using large language models (LLMs) and outperforms the best existing GPT-4-like models by an absolute 10% on VCR and 15% on SNLI-VE.","Conference on Empirical Methods in Natural Language Processing",2023,"Haoxuan You,Rui Sun,Zhecan Wang,Long Chen,Gengyu Wang,Hammad A. Ayyubi,Kai-Wei Chang,Shih-Fu Chang",16,51,3
"5ff2f5212713ec424662ac3c9e4aa5a8790d40cf","https://www.semanticscholar.org/paper/5ff2f5212713ec424662ac3c9e4aa5a8790d40cf",3,"ANPL: Towards Natural Programming with Interactive Decomposition","This paper introduces ANPL, an interactive programming system that ensures users can always refine the generated code towards their specific programmatic intents via structured decompositions, and deploys ANPL on the Abstraction and Reasoning Corpus, a set of unique tasks that are challenging for state-of-the-art AI systems.","Neural Information Processing Systems",2023,"Di Huang,Ziyuan Nan,Xingui Hu,Pengwei Jin,Shaohui Peng,Yuanbo Wen,Rui Zhang,Zidong Du,Qi Guo,Yewen Pu,Yunji Chen",1,71,0
"af705d648b5b16daa3dcc593bc593f2574d76c07","https://www.semanticscholar.org/paper/af705d648b5b16daa3dcc593bc593f2574d76c07",3,"Grammar Prompting for Domain-Specific Language Generation with Large Language Models","Experiments demonstrate that grammar prompting can enable LLMs to perform competitively on a diverse set of DSL generation tasks, including semantic parsing, Overnight, GeoQuery, PDDL planning, and even molecule generation (SMILES).","Neural Information Processing Systems",2023,"Bailin Wang,Zi Wang,Xuezhi Wang,Yuan Cao,R. Saurous,Yoon Kim",6,102,0
"fd755dc7b5b206c17fd953db04e1c888d45b6e4e","https://www.semanticscholar.org/paper/fd755dc7b5b206c17fd953db04e1c888d45b6e4e",3,"LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark","This work extends the research of MLLMs to point clouds and presents the LAMM-Dataset and LAMm-Benchmark for 2D image and 3D point cloud understanding and establishes an extensible framework to facilitate the extension of M LLMs to additional modalities.","Neural Information Processing Systems",2023,"Zhen-fei Yin,Jiong Wang,Jianjian Cao,Zhelun Shi,Dingning Liu,Mukai Li,Lu Sheng,Lei Bai,Xiaoshui Huang,Zhiyong Wang,Wanli Ouyang,Jing Shao",42,99,8
"d98536f24272e258b1d399074b64284d64786099","https://www.semanticscholar.org/paper/d98536f24272e258b1d399074b64284d64786099",3,"AVIS: Autonomous Visual Information Seeking with Large Language Models","An autonomous information seeking visual question answering framework that leverages a Large Language Model to dynamically strategize the utilization of external tools and to investigate their outputs, thereby acquiring the indispensable knowledge needed to provide answers to the posed questions is proposed.","Neural Information Processing Systems",2023,"Ziniu Hu,Ahmet Iscen,Chen Sun,Kai-Wei Chang,Yizhou Sun,David A. Ross,C. Schmid,A. Fathi",10,133,1
"051549d8ef56937b2f4d113afdcf8c7586d3770b","https://www.semanticscholar.org/paper/051549d8ef56937b2f4d113afdcf8c7586d3770b",3,"Towards AGI in Computer Vision: Lessons Learned from GPT and Large Language Models","It is pointed out that the essential weakness of CV lies in lacking a paradigm to learn from environments, yet NLP has accomplished the task in the text world and is still far from a system like GPT that naturally integrates all tasks.","arXiv.org",2023,"Lingxi Xie,Longhui Wei,Xiaopeng Zhang,Kaifeng Bi,Xiaotao Gu,Jianlong Chang,Qi Tian",2,169,0
"966852963a88a28786b798c91b6662d6e501e590","https://www.semanticscholar.org/paper/966852963a88a28786b798c91b6662d6e501e590",3,"AssistGPT: A General Multi-modal Assistant that can Plan, Execute, Inspect, and Learn","A multi-modal AI assistant with an interleaved code and language reasoning approach called Plan, Execute, Inspect, and Learn (PEIL) to integrate LLMs with various tools, and a Learner is designed to enable the model to autonomously explore and discover the optimal solution.","arXiv.org",2023,"Difei Gao,Lei Ji,Luowei Zhou,Kevin Lin,Joya Chen,Zihan Fan,Mike Zheng Shou",26,73,3
"094883e42bb9a41f602c0715c1059bc431e33fb2","https://www.semanticscholar.org/paper/094883e42bb9a41f602c0715c1059bc431e33fb2",3,"GPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest","Spatial instruction tuning is proposed, which introduces the reference to the region-of-interest (RoI) in the instruction, which achieves a remarkable accuracy of 81.6%, surpassing all existing models by a significant margin and almost reaching human-level performance of 85.0%.","arXiv.org",2023,"Shilong Zhang,Pei Sun,Shoufa Chen,Min Xiao,Wenqi Shao,Wenwei Zhang,Kai Chen,Ping Luo",74,96,6
"ca31b8584b6c022ef15ddfe994fe361e002b7729","https://www.semanticscholar.org/paper/ca31b8584b6c022ef15ddfe994fe361e002b7729",3,"A Comprehensive Overview of Large Language Models","A self-contained comprehensive overview of the existing literature on a broad range of LLM-related concepts discusses relevant background concepts along with covering the advanced topics at the frontier of research in LLMs.","arXiv.org",2023,"Humza Naveed,Asad Ullah Khan,Shi Qiu,Muhammad Saqib,Saeed Anwar,Muhammad Usman,Nick Barnes,A. Mian",58,447,2
"584ca135b61482fd89247113da87d784f738dbfa","https://www.semanticscholar.org/paper/584ca135b61482fd89247113da87d784f738dbfa",3,"Foundational Models Defining a New Era in Vision: A Survey and Outlook","A comprehensive review of emerging foundational models in computer vision, including typical architecture designs to combine different modalities, training objectives, pre-training datasets, fine-tuning mechanisms, and the common prompting patterns; textual, visual, and heterogeneous.","arXiv.org",2023,"Muhammad Awais,Muzammal Naseer,Salman Siddique Khan,R. Anwer,Hisham Cholakkal,M. Shah,Ming Yang,F. Khan",22,365,4
"446fb5dead075a1a08862662738f462e9a0e91c8","https://www.semanticscholar.org/paper/446fb5dead075a1a08862662738f462e9a0e91c8",3,"Tool Documentation Enables Zero-Shot Tool-Usage with Large Language Models","This work advocates the use of tool documentation, descriptions for the individual tool usage, over demonstrations, and shows that tool documentation is significantly more valuable than demonstrations, with zero-shot documentation significantly outperforming few-shot without documentation.","arXiv.org",2023,"Cheng-Yu Hsieh,Sibei Chen,Chun-Liang Li,Yasuhisa Fujii,Alexander J. Ratner,Chen-Yu Lee,Ranjay Krishna,Tomas Pfister",16,85,0
"dd0612ce863f64b0f69d0d9f708c52e829f6f859","https://www.semanticscholar.org/paper/dd0612ce863f64b0f69d0d9f708c52e829f6f859",3,"TPTU: Large Language Model-based AI Agents for Task Planning and Tool Usage","A structured framework tailored for LLM-based AI Agents is proposed and two distinct types of agents are designed to execute the inference process, which emphasizes the substantial potential of these models, while also identifying areas that need more investigation and improvement.","",2023,"Jingqing Ruan,Yihong Chen,Bin Zhang,Zhiwei Xu,Tianpeng Bao,Guoqing Du,Shiwei Shi,Hangyu Mao,Xingyu Zeng,Rui Zhao",7,95,0
"d6c2523ab97416c2692cbbeab082ed1790e8e55e","https://www.semanticscholar.org/paper/d6c2523ab97416c2692cbbeab082ed1790e8e55e",3,"VisIT-Bench: A Benchmark for Vision-Language Instruction Following Inspired by Real-World Use","This work introduces VisIT-Bench (Visual InsTruction Benchmark), a benchmark for evaluation of instruction-following vision-language models for real-world use, and curates 70 'instruction families' that it envision instruction tuned vision- language models should be able to address.","arXiv.org",2023,"Yonatan Bitton,Hritik Bansal,Jack Hessel,Rulin Shao,Wanrong Zhu,Anas Awadalla,Josh Gardner,Rohan Taori,L. Schimdt",23,99,1
"894ed1aba8e42a4ec27ba53ecde383b14c5128ca","https://www.semanticscholar.org/paper/894ed1aba8e42a4ec27ba53ecde383b14c5128ca",3,"Examining User-Friendly and Open-Sourced Large GPT Models: A Survey on Language, Multimodal, and Scientific GPT Models","This survey paper provides an examination of alternative open-sourced models of large GPTs, focusing on user-friendly and relatively small models that facilitate easier deployment and accessibility, and aims to equip researchers, practitioners, and enthusiasts with a thorough understanding of these models.","arXiv.org",2023,"Kaiyuan Gao,Su He,Zhenyu He,Jiacheng Lin,Qizhi Pei,Jie Shao,Wei Zhang",2,178,0
"4eb87eaa193929dbef93fa2db9419245a8e8916f","https://www.semanticscholar.org/paper/4eb87eaa193929dbef93fa2db9419245a8e8916f",3,"TextBind: Multi-turn Interleaved Multimodal Instruction-following in the Wild","This work introduces TextBind, an almost annotation-free framework for empowering larger language models with the multi-turn interleaved multimodal instruction-following capabilities, and devise MIM, a language model-centric architecture that seamlessly integrates image encoder and decoder models.","arXiv.org",2023,"Huayang Li,Siheng Li,Deng Cai,Longyue Wang,Lemao Liu,Taro Watanabe,Yujiu Yang,Shuming Shi",4,67,1
"3ec464696db25acc2c39a6d967ec3df09e06f633","https://www.semanticscholar.org/paper/3ec464696db25acc2c39a6d967ec3df09e06f633",3,"Multimodal Multi-Hop Question Answering Through a Conversation Between Tools and Efficiently Finetuned Large Language Models","A tool-interacting divide-and-conquer strategy enabling large language models (LLMs) to answer complex multimodal multi-hop questions, demonstrating the efficacy and generality of this approach.","arXiv.org",2023,"Hossein Rajabzadeh,Suyuchen Wang,Hyock Ju Kwon,Bang Liu",1,33,0
"f34b6b4f75fb4e6f2c5654ee13fb2479c6170b3a","https://www.semanticscholar.org/paper/f34b6b4f75fb4e6f2c5654ee13fb2479c6170b3a",3,"Kosmos-2.5: A Multimodal Literate Model","Kosmos-2.5 can be readily adapted for any text-intensive image understanding task with different prompts through supervised fine-tuning, making it a general-purpose tool for real-world applications involving text-rich images and paves the way for the future scaling of multimodal large language models.","arXiv.org",2023,"Tengchao Lv,Yupan Huang,Jingye Chen,Lei Cui,Shuming Ma,Ya-Chi Chang,Shaohan Huang,Wenhui Wang,Li Dong,Weiyao Luo,Shaoxiang Wu,Guoxin Wang,Cha Zhang,Furu Wei",13,84,0
"7b689adb8c156d6158660f90d1c86888ee281f63","https://www.semanticscholar.org/paper/7b689adb8c156d6158660f90d1c86888ee281f63",3,"DreamLLM: Synergistic Multimodal Comprehension and Creation","A learning framework that first achieves versatile Multimodal Large Language Models (MLLMs) empowered with frequently overlooked synergy between multimodal comprehension and creation, reaping from the enhanced learning synergy.","arXiv.org",2023,"Runpei Dong,Chunrui Han,Yuang Peng,Zekun Qi,Zheng Ge,Jinrong Yang,Liang Zhao,Jian‐Yuan Sun,Hongyu Zhou,Hao-Ran Wei,Xiangwen Kong,Xiangyu Zhang,Kaisheng Ma,Li Yi",38,169,4
"092245d86b77181c36f972b1b7a17a59cd989c4a","https://www.semanticscholar.org/paper/092245d86b77181c36f972b1b7a17a59cd989c4a",3,"Guiding Instruction-based Image Editing via Multimodal Large Language Models","This work investigates how MLLMs facilitate edit instructions and presents MLLM-Guided Image Editing (MGIE), which learns to derive expressive instructions and provides explicit guidance and can lead to a notable improvement in automatic metrics and human evaluation while maintaining competitive inference efficiency.","arXiv.org",2023,"Tsu-Jui Fu,Wenze Hu,Xianzhi Du,William Yang Wang,Yinfei Yang,Zhe Gan",10,67,1
"7f1ba5630c3baa09b11cc665b3f71cdb117e5ffb","https://www.semanticscholar.org/paper/7f1ba5630c3baa09b11cc665b3f71cdb117e5ffb",3,"OpenLEAF: Open-Domain Interleaved Image-Text Generation and Evaluation","A new interleaved generation framework based on prompting large-language models (LLMs) and pre-trained text-to-image (T2I) models, namely OpenLEAF is proposed, which can generate high-quality image-text content for various domains and applications.","arXiv.org",2023,"Jie An,Zhengyuan Yang,Linjie Li,Jianfeng Wang,K. Lin,Zicheng Liu,Lijuan Wang,Jiebo Luo",3,40,0
"a710efa9247207a72f06e0c9db302fd3ecab5fbb","https://www.semanticscholar.org/paper/a710efa9247207a72f06e0c9db302fd3ecab5fbb",3,"Towards Robust Multi-Modal Reasoning via Model Selection","This framework improves model selection and bolsters the robustness of multi-modal agents in multi-step reasoning, and enables dynamic model selection, considering both user inputs and subtask dependencies, thereby robustifying the overall reasoning process.","arXiv.org",2023,"Xiangyan Liu,Rongxue Li,Wei Ji,Tao Lin",1,56,0
"1d14a708622917da4b9820ada6d32af24fc1651a","https://www.semanticscholar.org/paper/1d14a708622917da4b9820ada6d32af24fc1651a",3,"Idea2Img: Iterative Self-Refinement with GPT-4V(ision) for Automatic Image Design and Generation",,"arXiv.org",2023,"Zhengyuan Yang,Jianfeng Wang,Linjie Li,Kevin Lin,Chung-Ching Lin,Zicheng Liu,Lijuan Wang",6,56,0
"c020f15be1dee20f9e2e0c5a6f05f272b5508325","https://www.semanticscholar.org/paper/c020f15be1dee20f9e2e0c5a6f05f272b5508325",3,"LLaVA-Interactive: An All-in-One Demo for Image Chat, Segmentation, Generation and Editing","The development of LLaVA-Interactive is extremely cost-efficient as the system combines three multimodal skills of pre-built AI models without additional model training: visual chat of L LaVA, image segmentation from SEEM, as well as image generation and editing from GLIGEN.","arXiv.org",2023,"Wei-Ge Chen,Irina Spiridonova,Jianwei Yang,Jianfeng Gao,Chun-yue Li",6,36,1
"ecfff30e570498b6c87c5d1319a0cbcdf7a8b86d","https://www.semanticscholar.org/paper/ecfff30e570498b6c87c5d1319a0cbcdf7a8b86d",3,"LLaVA-Plus: Learning to Use Tools for Creating Multimodal Agents","LLaVA-Plus is distinct in that the image query is directly grounded and actively engaged throughout the entire human-AI interaction sessions, significantly improving tool use performance and enabling new scenarios.","arXiv.org",2023,"Shilong Liu,Hao Cheng,Haotian Liu,Hao Zhang,Feng Li,Tianhe Ren,Xueyan Zou,Jianwei Yang,Hang Su,Jun-Juan Zhu,Lei Zhang,Jianfeng Gao,Chun-yue Li",17,52,4
"2fb605f67fee79cad94952ddfe0f686e926f49f5","https://www.semanticscholar.org/paper/2fb605f67fee79cad94952ddfe0f686e926f49f5",3,"GPT-4V in Wonderland: Large Multimodal Models for Zero-Shot Smartphone GUI Navigation","The findings demonstrate that large multimodal models, specifically GPT-4V, excel in zero-shot GUI navigation through its advanced screen interpretation, action reasoning, and precise action localization capabilities.","arXiv.org",2023,"An Yan,Zhengyuan Yang,Wanrong Zhu,K. Lin,Linjie Li,Jianfeng Wang,Jianwei Yang,Yiwu Zhong,Julian McAuley,Jianfeng Gao,Zicheng Liu,Lijuan Wang",17,58,0
"aad3d2e690f6c73f04a14622ceff51464bbc560e","https://www.semanticscholar.org/paper/aad3d2e690f6c73f04a14622ceff51464bbc560e",3,"Chat-UniVi: Unified Visual Representation Empowers Large Language Models with Image and Video Understanding","This work introduces Chat-UniVi, a unified vision-language model capable of comprehending and engaging in conversations involving images and videos through a unified visual representation that consistently outperforms even existing methods exclusively designed for either images or videos.","arXiv.org",2023,"Peng Jin,Ryuichi Takanobu,Caiwan Zhang,Xiaochun Cao,Li Yuan",11,74,2
"107fb6eec2febbae12db29bf3e311aaf5680027c","https://www.semanticscholar.org/paper/107fb6eec2febbae12db29bf3e311aaf5680027c",3,"Video-LLaVA: Learning United Visual Representation by Alignment Before Projection","This work unify visual representation into the language feature space to advance the foundational LLM towards a unified LVLM, and establishes a simple but robust LVLM baseline, Video-LLaVA, which learns from a mixed dataset of images and videos, mutually enhancing each other.","arXiv.org",2023,"Bin Lin,Bin Zhu,Yang Ye,Munan Ning,Peng Jin,Li Yuan",17,54,1
"246017780386eba39d6cda760a1c2c70356baa50","https://www.semanticscholar.org/paper/246017780386eba39d6cda760a1c2c70356baa50",3,"VIoTGPT: Learning to Schedule Vision Tools towards Intelligent Video Internet of Things","VIoTGPT is built, the framework based on LLMs to correctly interact with humans, query knowledge videos, and invoke vision models to accomplish complicated tasks to address the challenges posed by the fine-grained and interrelated vision tool usage of VIoT.","arXiv.org",2023,"Yaoyao Zhong,Mengshi Qi,Rui Wang,Yuhan Qiu,Yang Zhang,Huadong Ma",0,78,0
"6d2ab31aa75468f5458b9d96192c3f4a28f55d73","https://www.semanticscholar.org/paper/6d2ab31aa75468f5458b9d96192c3f4a28f55d73",3,"DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving","DriveMLM is introduced, an LLM-based AD framework that can perform close-loop autonomous driving in realistic simulators and can plug-and-play in existing AD systems such as Apollo for close-loop driving.","arXiv.org",2023,"Wenhai Wang,Jiangwei Xie,ChuanYang Hu,Haoming Zou,Jianan Fan,Wenwen Tong,Yang Wen,Silei Wu,Hanming Deng,Zhiqi Li,Hao Tian,Lewei Lu,Xizhou Zhu,Xiaogang Wang,Yu Qiao,Jifeng Dai",6,80,2
"35a17f896847614a71df772bbe2b66ae231cabc7","https://www.semanticscholar.org/paper/35a17f896847614a71df772bbe2b66ae231cabc7",3,"CLOVA: A Closed-Loop Visual Assistant with Tool Usage and Update","Experiments show that CLOVA outperforms tool-usage methods by 5% in visual question answering and multiple-image reasoning tasks, by 10% in knowledge tagging tasks, and by 20% in image editing tasks, highlighting the significance of the learning capability for general visual assistants.","arXiv.org",2023,"Zhi Gao,Yuntao Du,Xintong Zhang,Xiaojian Ma,Wenjuan Han,Song-Chun Zhu,Qing Li",3,83,0
"24fc9ad715372358bd0108eeb7c944b915963293","https://www.semanticscholar.org/paper/24fc9ad715372358bd0108eeb7c944b915963293",3,"ASSISTGUI: Task-Oriented Desktop Graphical User Interface Automation","An advanced Actor-Critic Embodied Agent framework is proposed, which incorporates a sophisticated GUI parser driven by an LLM-agent and an enhanced reasoning mechanism adept at handling lengthy procedural tasks that outshine existing methods in performance.","arXiv.org",2023,"Difei Gao,Lei Ji,Zechen Bai,Mingyu Ouyang,Peiran Li,Dongxing Mao,Qinchen Wu,Weichen Zhang,Peiyi Wang,Xiangwu Guo,Hengxu Wang,Luowei Zhou,Mike Zheng Shou",4,53,0
"6a33e58ef961a3a0a5657518b2be86395eb7c8d0","https://www.semanticscholar.org/paper/6a33e58ef961a3a0a5657518b2be86395eb7c8d0",3,"InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks","A large-scale vision-language foundation model (InternVL), which scales up the vision foundation model to 6 billion parameters and progressively aligns it with the LLM, using web-scale image-text data from various sources is designed.","arXiv.org",2023,"Zhe Chen,Jiannan Wu,Wenhai Wang,Weijie Su,Guo Chen,Sen Xing,Zhong Muyan,Qinglong Zhang,Xizhou Zhu,Lewei Lu,Bin Li,Ping Luo,Tong Lu,Yu Qiao,Jifeng Dai",8,185,3
"a06d3e9e90008c64c45a0029d580541d5f646771","https://www.semanticscholar.org/paper/a06d3e9e90008c64c45a0029d580541d5f646771",3,"If LLM Is the Wizard, Then Code Is the Wand: A Survey on How Code Empowers Large Language Models to Serve as Intelligent Agents","An overview of the various benefits of integrating code into LLMs' training data and how these profound capabilities of LLMs, brought by code, have led to their emergence as intelligent agents in situations where the ability to understand instructions, decompose goals, plan and execute actions, and refine from feedback are crucial to their success on downstream tasks are traced.","arXiv.org",2024,"Ke Yang,Jiateng Liu,John Wu,Chaoqi Yang,Y. Fung,Sha Li,Zixuan Huang,Xu Cao,Xingyao Wang,Yiquan Wang,Heng Ji,Chengxiang Zhai",8,168,0
"4a48d628e53f554eb6ef09a457ca855188b96171","https://www.semanticscholar.org/paper/4a48d628e53f554eb6ef09a457ca855188b96171",3,"DoraemonGPT: Toward Understanding Dynamic Scenes with Large Language Models","DoraemonGPT, a comprehensive and conceptually elegant system driven by LLMs to handle dynamic video tasks, and a novel LLM-driven planner based on Monte Carlo Tree Search is introduced to explore the large planning space for scheduling various tools.","arXiv.org",2024,"Zongxin Yang,Guikun Chen,Xiaodi Li,Wenguan Wang,Yi Yang",3,123,1
"23957040943f883542f47850c709b9e7f9d6fa55","https://www.semanticscholar.org/paper/23957040943f883542f47850c709b9e7f9d6fa55",3,"Prompting Large Vision-Language Models for Compositional Reasoning","This paper makes an exploratory step using a novel generative method that prompts large vision-language models (e.g., GPT-4) to depict images and perform compositional reasoning, and outperforms other embedding-based methods on the Winoground dataset, and obtains further improvement of up to 10% accuracy when enhanced with the optimal description.","arXiv.org",2024,"Timothy Ossowski,Ming Jiang,Junjie Hu",0,34,0
"140cfda71bfff852c3e205b7ad61854b78c76982","https://www.semanticscholar.org/paper/140cfda71bfff852c3e205b7ad61854b78c76982",3,"Mastering Text-to-Image Diffusion: Recaptioning, Planning, and Generating with Multimodal LLMs","This paper proposes a brand new training-free text-to-image generation/editing framework, namely Recaption, Plan and Generate (RPG), harnessing the powerful chain-of-thought reasoning ability of multimodal LLMs to enhance the compositionality of text-to-image diffusion models.","arXiv.org",2024,"Ling Yang,Zhaochen Yu,Chenlin Meng,Minkai Xu,Stefano Ermon,Bin Cui",7,77,0
"a050c9b0c321839e4427ab9defa3463be7825ac4","https://www.semanticscholar.org/paper/a050c9b0c321839e4427ab9defa3463be7825ac4",3,"MM-LLMs: Recent Advances in MultiModal Large Language Models","A taxonomy encompassing $122$ MM-LLMs, each characterized by its specific formulations is introduced and a review of selected MM-LLMs on mainstream benchmarks and key training recipes to enhance the potency of MM-LLMs are summarized.","arXiv.org",2024,"Duzhen Zhang,Yahan Yu,Chenxing Li,Jiahua Dong,Dan Su,Chenhui Chu,Dong Yu",3,254,0
"2cea424c7dce71042c24d43317521abdc4c0ffb4","https://www.semanticscholar.org/paper/2cea424c7dce71042c24d43317521abdc4c0ffb4",3,"Large Multimodal Agents: A Survey","This paper conducts a systematic review of LLM-driven multimodal agents, and introduces the essential components involved in developing LMAs and categorizes the current body of research into four distinct types.","",2024,"Junlin Xie,Zhihong Chen,Ruifei Zhang,Xiang Wan,Guanbin Li",0,80,0
"74c68aed85f2fe8019113bbdb533fcba7e3ce0bd","https://www.semanticscholar.org/paper/74c68aed85f2fe8019113bbdb533fcba7e3ce0bd",3,"ShapeLLM: Universal 3D Object Understanding for Embodied Interaction","ShapeLLM is the first 3D Multimodal Large Language Model designed for embodied interaction, exploring a universal 3D object understanding with 3D point clouds and languages, built upon an improved 3D encoder by extending ReCon to ReCon++ that benefits from multi-view image distillation for enhanced geometry understanding.","",2024,"Zekun Qi,Runpei Dong,Shaochen Zhang,Haoran Geng,Chunrui Han,Zheng Ge,Li Yi,Kaisheng Ma",0,196,0
"6bdfffbf92d01c8b543088d40d46233610e469a8","https://www.semanticscholar.org/paper/6bdfffbf92d01c8b543088d40d46233610e469a8",3,"CLIP in Medical Imaging: A Comprehensive Survey","This survey offers an in-depth exploration of the CLIP paradigm within the domain of medical imaging, regarding both refined CLIP pre-training and CLIP-driven applications, and investigates the adaptation of CLIP pre-training in the medical domain.","arXiv.org",2023,"Zihao Zhao,Yuxiao Liu,Han Wu,Yonghao Li,Sheng Wang,L. Teng,Disheng Liu,Xiang Li,Zhiming Cui,Qian Wang,Dinggang Shen",3,218,0
"8ecdbfe011b7189fa0ee49ffc4e42a93d728a371","https://www.semanticscholar.org/paper/8ecdbfe011b7189fa0ee49ffc4e42a93d728a371",3,"On Evaluating Adversarial Robustness of Large Vision-Language Models","Evaluating the robustness of open-source large VLMs in the most realistic and high-risk setting, where adversaries have only black-box system access and seek to deceive the model into returning the targeted responses is proposed.","Neural Information Processing Systems",2023,"Yunqing Zhao,Tianyu Pang,Chao Du,Xiao Yang,Chongxuan Li,Ngai-Man Cheung,Min Lin",36,108,3
"52941cadbd340344f3e0a6f50719fe55b3de5088","https://www.semanticscholar.org/paper/52941cadbd340344f3e0a6f50719fe55b3de5088",3,"Multimodal Large Language Models: A Survey","A range of multimodal products are introduced, focusing on the efforts of major technology companies, and a compilation of the latest algorithms and commonly used datasets are presented, providing researchers with valuable resources for experimentation and evaluation.","BigData Congress [Services Society]",2023,"Jiayang Wu,Wensheng Gan,Zefeng Chen,Shicheng Wan,Philip S. Yu",13,75,0
"88bddfb7d1e0462be8fe99fdbd71c658140cb17b","https://www.semanticscholar.org/paper/88bddfb7d1e0462be8fe99fdbd71c658140cb17b",3,"From Image to Language: A Critical Analysis of Visual Question Answering (VQA) Approaches, Challenges, and Opportunities","This work presents a survey in the domain of VQA that delves into the intricacies of V QA datasets and methods over the field's history, introduces a detailed taxonomy to categorize the facets of VZA, and highlights the recent trends, challenges, and scopes for improvement.","arXiv.org",2023,"Md Farhan Ishmam,Md Sakib Hossain Shovon,M. F. Mridha,Nilanjan Dey",1,304,0
"da9579539385daedd33a0de0f814e2977ad0d1f5","https://www.semanticscholar.org/paper/da9579539385daedd33a0de0f814e2977ad0d1f5",3,"Towards Unifying Medical Vision-and-Language Pre-training via Soft Prompts","This work unify the input format by introducing visual and textual prompts, which serve as DETR-like queries that assist in extracting features when one of the modalities is missing, and proposes an effective yet straightforward scheme named PTUnifier to unify the two types.","IEEE International Conference on Computer Vision",2023,"Zhihong Chen,Shizhe Diao,Benyou Wang,Guanbin Li,Xiang Wan",7,72,1
"8f3138f7ee5127faab265793be8ae278bc49d9b1","https://www.semanticscholar.org/paper/8f3138f7ee5127faab265793be8ae278bc49d9b1",3,"PMC-CLIP: Contrastive Language-Image Pre-training using Biomedical Documents","PMC-OA, a biomedical dataset with 1.6M image-caption pairs collected from PubMedCentral's OpenAccess subset, is built and released, which is 8 times larger than before and achieves state-of-the-art results on various downstream tasks.","International Conference on Medical Image Computing and Computer-Assisted Intervention",2023,"Weixiong Lin,Ziheng Zhao,Xiaoman Zhang,Chaoyi Wu,Ya Zhang,Yanfeng Wang,Weidi Xie",25,33,2
"ac4d13b6a4f9fb67337099f4602135a0351f5c99","https://www.semanticscholar.org/paper/ac4d13b6a4f9fb67337099f4602135a0351f5c99",3,"Towards Medical Artificial General Intelligence via Knowledge-Enhanced Multimodal Pretraining","The proposed MOTOR successfully mimics the human practice of fulfilling a""medical student"" to accelerate the process of becoming a""specialist"" and believes that this work makes a significant stride in realizing MAGI.","arXiv.org",2023,"Bingqian Lin,Zicong Chen,Mingjie Li,Haokun Lin,Hang Xu,Yi Zhu,Jian-zhuo Liu,Wenjia Cai,Lei Yang,Shen Zhao,Chenfei Wu,Ling Chen,Xiaojun Chang,Yi Yang,L. Xing,Xiaodan Liang",3,59,0
"f4793adffd6f67ffcb93ccfc5672ab301b8a2b96","https://www.semanticscholar.org/paper/f4793adffd6f67ffcb93ccfc5672ab301b8a2b96",3,"PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering","This paper proposes a generative-based model for medical visual understanding by aligning visual information from a pre-trained vision encoder with a large language model, and establishes a scalable pipeline to construct a large-scale medical visual question-answering dataset.","arXiv.org",2023,"Xiaoman Zhang,Chaoyi Wu,Ziheng Zhao,Weixiong Lin,Ya Zhang,Yanfeng Wang,Weidi Xie",36,40,3
"07d45ce7de598ef03b400f8ddba7d2e055e77a08","https://www.semanticscholar.org/paper/07d45ce7de598ef03b400f8ddba7d2e055e77a08",3,"BiomedGPT: A Unified and Generalist Biomedical Generative Pre-trained Transformer for Vision, Language, and Multimodal Tasks","BiomedGPT is proposed, the first open-source and generalist visual language AI for diverse biomedical tasks and facilitates zero-shot transfer learning, greatly enhancing its utility as a biomedical assistant, similar to ChatGPT.","",2023,"Kai Zhang,Jun Yu,Eashan Adhikarla,Rong Zhou,Zhiling Yan,Yixin Liu,Zheng Liu,Lifang He,Brian D. Davison,Xiang Li,Hui Ren,S. Fu,James Zou,Wei Liu,Jing Huang,Chen Chen,Yuyin Zhou,Tianming Liu,Xun Chen,Yong Chen,Quanzheng Li,Hongfang Liu,Lichao Sun",0,131,0
"f22d71c7ce9720ba1f717a4f1181488200e78198","https://www.semanticscholar.org/paper/f22d71c7ce9720ba1f717a4f1181488200e78198",3,"LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day","This paper proposes a cost-efficient approach for training a vision-language conversational assistant that can answer open-ended research questions of biomedical images, and releases instruction-following data and the LLaVA-Med model, which exhibits excellent multimodal conversational capability.","Neural Information Processing Systems",2023,"Chunyuan Li,Cliff Wong,Sheng Zhang,Naoto Usuyama,Haotian Liu,Jianwei Yang,Tristan Naumann,Hoifung Poon,Jianfeng Gao",107,46,15
"64fa56962dd0f4bbe206be6142fbe0315c4e7c2f","https://www.semanticscholar.org/paper/64fa56962dd0f4bbe206be6142fbe0315c4e7c2f",3,"Multi-modal Pre-training for Medical Vision-language Understanding and Generation: An Empirical Study with A New Benchmark","RadioGraphy Captions (RGC), a high-quality, multi-modality radiographic dataset containing 18,434 image-caption pairs collected from an open-access online database MedPix, is proposed, which can be used as a pre-training dataset or a new benchmark for medical report generation and medical image-text retrieval.","arXiv.org",2023,"Li Xu,Bo Liu,Ameer Hamza Khan,Lu Fan,Xiao-Ming Wu",2,71,1
"e0e9ba0c01d441e1fdcb8628d3f743d387b0b017","https://www.semanticscholar.org/paper/e0e9ba0c01d441e1fdcb8628d3f743d387b0b017",3,"UIT-Saviors at MEDVQA-GI 2023: Improving Multimodal Learning with Image Enhancement for Gastrointestinal Visual Question Answering","This study highlights the dominance of Transformer-based vision models over the CNNs and demonstrates the effectiveness of the image enhancement process, with six out of the eight vision models achieving better F1-Score.","Conference and Labs of the Evaluation Forum",2023,"T. M. Thai,A. T. Vo,Hao K. Tieu,Linh Bui,T. Nguyen",2,44,0
"baa1dc079d98ca76b0173c8d653fed759fd0a371","https://www.semanticscholar.org/paper/baa1dc079d98ca76b0173c8d653fed759fd0a371",3,"A scoping review on multimodal deep learning in biomedical images and texts","This study reviewed the current uses of multimodal deep learning on five tasks: report generation, Visual question answering, Cross-modal retrieval, computer-aided diagnosis, and Semantic segmentation, and highlighted the diverse applications and potential of MDL.","Journal of Biomedical Informatics",2023,"Zhaoyi Sun,Mingquan Lin,Qingqing Zhu,Qianqian Xie,Fei Wang,Zhiyong Lu,Yifan Peng",5,148,0
"df0ddb588a200d095743e9d26fc4a9318619766e","https://www.semanticscholar.org/paper/df0ddb588a200d095743e9d26fc4a9318619766e",3,"Towards Generalist Foundation Model for Radiology","This study constructs a large-scale Medical Multi-modal Dataset, MedMD, which consists of 16M 2D and 3D medical scans with high-quality text descriptions or reports across various data formats, modalities, and tasks, covering over 5000 distinct diseases, and proposes a new evaluation benchmark, RadBench, aiming to comprehensively assess the capability of foundation models in handling practical clinical problems.","arXiv.org",2023,"Chaoyi Wu,Xiaoman Zhang,Ya Zhang,Yanfeng Wang,Weidi Xie",26,59,2
"f0950a3f27c0fefffba60ae1c9a8ee360d5eb55f","https://www.semanticscholar.org/paper/f0950a3f27c0fefffba60ae1c9a8ee360d5eb55f",3,"Instruction Tuning for Large Language Models: A Survey","A systematic review of the literature, including the general methodology of IT, the construction of IT datasets, the training of IT models, and applications to different modalities, domains and applications, along with an analysis on aspects that influence the outcome of IT.","arXiv.org",2023,"Shengyu Zhang,Linfeng Dong,Xiaoya Li,Sen Zhang,Xiaofei Sun,Shuhe Wang,Jiwei Li,Runyi Hu,Tianwei Zhang,Fei Wu,Guoyin Wang",101,150,3
"a0476578761e983d5ab2083abab07b81236c1d58","https://www.semanticscholar.org/paper/a0476578761e983d5ab2083abab07b81236c1d58",3,"Asymmetric cross-modal attention network with multimodal augmented mixup for medical visual question answering","A new Asymmetric Cross Modal Attention network called ACMA is proposed, which constructs an image- guided attention and a question-guided attention to improve multimodal interactions from insufficient data.","Artif. Intell. Medicine",2023,"Yong Li,Qihao Yang,Fu Lee Wang,Lap-Kei Lee,Yingying Qu,Tianyong Hao",0,59,0
"da9134f694959b68027c33c8e998ffb3d41305da","https://www.semanticscholar.org/paper/da9134f694959b68027c33c8e998ffb3d41305da",3,"Exploring Question Decomposition for Zero-Shot VQA","A model-driven selective decomposition approach for second-guessing predictions and correcting errors is introduced, and its effectiveness on eight VQA tasks across three domains is validated, showing consistent improvements in accuracy.","Neural Information Processing Systems",2023,"Zaid Khan,B. Vijaykumar,S. Schulter,Manmohan Chandraker,Yun Fu",0,60,0
"749104d1a207f5bc192c7d95a12856b5e7f84d1f","https://www.semanticscholar.org/paper/749104d1a207f5bc192c7d95a12856b5e7f84d1f",3,"Mapping medical image-text to a joint space via masked modeling","A self-supervised learning paradigm, multi-modal masked autoencoders (M3AE) is introduced, which learns to map medical images and texts to a joint space by reconstructing pixels and tokens from randomly masked images andtext.","Medical Image Anal.",2023,"Zhihong Chen,Yuhao Du,Jinpeng Hu,Yang Liu,Guanbin Li,Xiang Wan,Tsung-Hui Chang",0,43,0
"8d2709ed1788a67e64425fb410bb49f3ee49e088","https://www.semanticscholar.org/paper/8d2709ed1788a67e64425fb410bb49f3ee49e088",3,"Large Language Models Illuminate a Progressive Pathway to Artificial Healthcare Assistant: A Review","This review offers an extensive analysis on the transformative potential of LLMs in modern medicine and highlights the pivotal need for continuous optimizations and ethical oversight before these models can be effectively integrated into clinical practice.","arXiv.org",2023,"Mingze Yuan,Peng Bao,J. Yuan,Yunhao Shen,Zi Chen,Yi Xie,Jie Zhao,Yang Chen,Li Zhang,Lin Shen,Bin Dong",1,186,0
"352252231462c24440bc0016638ea5fe8d4c6f7e","https://www.semanticscholar.org/paper/352252231462c24440bc0016638ea5fe8d4c6f7e",3,"UniDCP: Unifying Multiple Medical Vision-language Tasks via Dynamic Cross-modal Learnable Prompts","UniDCP is the first Med-VLP model capable of performing all 8 medical uni-modal and cross-modal tasks over 14 corresponding datasets, consistently yielding superior results over diverse state-of-the-art methods.","arXiv.org",2023,"Chenlu Zhan,Yufei Zhang,Yu Lin,Gaoang Wang,Hongwei Wang",0,67,0
"63de69245502d9a22de04581a4b5c0168d596aa3","https://www.semanticscholar.org/paper/63de69245502d9a22de04581a4b5c0168d596aa3",3,"Generalizing Visual Question Answering from Synthetic to Human-Written Questions via a Chain of QA with a Large Language Model","The effectiveness of CoQAH is tested on two types of human-written VQA datasets for 3D-rendered and chest X-ray images and found that it achieved state-of-the-art accuracy in both types of data.","arXiv.org",2024,"Taehee Kim,Yeongjae Cho,Heejun Shin,Yohan Jo,Dongmyung Shin",0,26,0
"61cadcfa555cbef120df7c017ef02e87f19900b7","https://www.semanticscholar.org/paper/61cadcfa555cbef120df7c017ef02e87f19900b7",3,"Free Form Medical Visual Question Answering in Radiology","This research delves into the effective representation of radiology images and the joint learning of multimodal representations, surpassing existing methods and innovatively augment the SLAKE dataset, enabling the model to respond to a more diverse array of questions.","arXiv.org",2024,"Abhishek Narayanan,Rushabh Musthyala,Rahul Sankar,Anirudh Prasad Nistala,P. Singh,Jacopo Cirrone",0,48,0
"7580327ffc9bd5daef83fe8285c0476ca074051d","https://www.semanticscholar.org/paper/7580327ffc9bd5daef83fe8285c0476ca074051d",3,"OmniMedVQA: A New Large-Scale Comprehensive Evaluation Benchmark for Medical LVLM","OmniMedVQA is introduced, a novel comprehensive medical Visual Question Answering (VQA) benchmark collected from 75 different medical datasets, including 12 different modalities and covering more than 20 distinct anatomical regions, calling for a more versatile and robust LVLM in the biomedical field.","arXiv.org",2024,"Yutao Hu,Tian-Xin Li,Quanfeng Lu,Wenqi Shao,Junjun He,Yu Qiao,Ping Luo",0,119,0
"a3d418b4e35a02e4306505ab660a6bcd44c3c752","https://www.semanticscholar.org/paper/a3d418b4e35a02e4306505ab660a6bcd44c3c752",3,"Asclepius: A Spectrum Evaluation Benchmark for Medical Multi-Modal Large Language Models","This work introduces Asclepius, a novel Med-MLLM benchmark that rigorously and comprehensively assesses model capability in terms of distinct medical specialties and different diagnostic capacities, and sets a precedent for future evaluations and the safe deployment of these models in clinical environments.","arXiv.org",2024,"Wenxuan Wang,Yihang Su,Jingyuan Huan,Jie Liu,Wenting Chen,Yudi Zhang,Cheng-Yi Li,Kao-Jung Chang,Xiaohan Xin,LinLin Shen,Michael R. Lyu",0,42,0
"bca0bbd01ea917b7a9fe369288ea3ba03d3b1ff3","https://www.semanticscholar.org/paper/bca0bbd01ea917b7a9fe369288ea3ba03d3b1ff3",2,"A Survey of Large Language Models in Medicine: Progress, Application, and Challenge","A detailed overview of the development and deployment of LLMs in medicine, including the challenges and opportunities they face, is provided and a detailed introduction to the principles of existing medical LLMs are provided, including their basic model structures, number of parameters, and sources and scales of data used for model development.","arXiv.org",2023,"Hongjian Zhou,Boyang Gu,Xinyu Zou,Yiru Li,Sam S. Chen,Peilin Zhou,Junling Liu,Y. Hua,Chengfeng Mao,Xian Wu,Zheng Li,Fenglin Liu",5,183,0
"3c50ef336232da0885ef61da386c98eac964b7cd","https://www.semanticscholar.org/paper/3c50ef336232da0885ef61da386c98eac964b7cd",2,"PathAsst: A Generative Foundation AI Assistant Towards Artificial General Intelligence of Pathology","The experimental results of PathAsst show the potential of harnessing AI-powered generative foundation model to improve pathology diagnosis and treatment processes.","",2023,"Yuxuan Sun,Chenglu Zhu,S. Zheng,Kai Zhang,Zhongyi Shui,Xiaoxuan Yu,Yi-Lei Zhao,Honglin Li,Yunlong Zhang,Ruojia Zhao,Xinheng Lyu,Lin Yang",10,41,2
"a3711dbf296b5ddd97ba93826660cd3995611625","https://www.semanticscholar.org/paper/a3711dbf296b5ddd97ba93826660cd3995611625",2,"Towards A Foundation Model for Generalist Robots: Diverse Skill Learning at Scale via Automated Task and Scene Generation",,"arXiv.org",2023,"Zhou Xian,Théophile Gervet,Zhenjia Xu,Yi-Ling Qiao,Tsun-Hsuan Wang",4,115,0
"692bc40edf4785d88c39e0c0fe9f270541fecf8a","https://www.semanticscholar.org/paper/692bc40edf4785d88c39e0c0fe9f270541fecf8a",2,"Towards Generalist Robots: A Promising Paradigm via Generative Simulation","This document presents a specific idea for mining knowledge in the latest large-scale foundation models for robotics research, which uses a fully automated generative pipeline which uses these models to generate diversified tasks, scenes and training supervisions at scale, thereby scaling up low-level skill learning and ultimately leading to a foundation model for robotics that empowers generalist robots.","",2023,"Zhou Xian,Théophile Gervet,Zhenjia Xu,Yi-Ling Qiao,Tsun-Hsuan Wang,Yian Wang",3,124,0
"e45036dddb5f27d3e87d2f14a2d9e6a402e7b5b7","https://www.semanticscholar.org/paper/e45036dddb5f27d3e87d2f14a2d9e6a402e7b5b7",2,"SheetCopilot: Bringing Software Productivity to the Next Level through Large Language Models","This work proposes a SheetCopilot agent that takes natural language task and control spreadsheet to fulfill the requirements, and proposes a set of atomic actions as an abstraction of spreadsheet software functionalities.","Neural Information Processing Systems",2023,"Hongxin Li,Jingran Su,Yuntao Chen,Qing Li,Zhaoxiang Zhang",11,93,2
"28c6ac721f54544162865f41c5692e70d61bccab","https://www.semanticscholar.org/paper/28c6ac721f54544162865f41c5692e70d61bccab",2,"A Survey on Large Language Model based Autonomous Agents","A systematic review of the field of LLM-based autonomous agents from a holistic perspective, and proposes a unified framework that encompasses a majority of the previous work.","arXiv.org",2023,"Lei Wang,Chengbang Ma,Xueyang Feng,Zeyu Zhang,Hao-ran Yang,Jingsen Zhang,Zhi-Yang Chen,Jiakai Tang,Xu Chen,Yankai Lin,Wayne Xin Zhao,Zhewei Wei,Ji-rong Wen",205,184,18
"c62711f6b5d8620ba36bc2c378ec6ab53f6e197c","https://www.semanticscholar.org/paper/c62711f6b5d8620ba36bc2c378ec6ab53f6e197c",2,"RoboGen: Towards Unleashing Infinite Data for Automated Robot Learning via Generative Simulation","RoboGen is presented, a generative robotic agent that automatically learns diverse robotic skills at scale via generative simulation and attempts to extract the extensive and versatile knowledge embedded in large-scale models and transfer them to the field of robotics.","arXiv.org",2023,"Yufei Wang,Zhou Xian,Feng Chen,Tsun-Hsuan Wang,Yian Wang,Zackory M. Erickson,David Held,Chuang Gan",10,127,0
"8ec7d50250203543a0098d99f04957b22bbe2c77","https://www.semanticscholar.org/paper/8ec7d50250203543a0098d99f04957b22bbe2c77",2,"How to Bridge the Gap between Modalities: A Comprehensive Survey on Multimodal Large Language Model","This paper aims to explore modality alignment methods for LLMs and their existing capabilities, and surveys existing modal alignment methods in MLLMs into four groups: Multimodal Converters that change data into something LLMs can understand, and Data-Driven methods that teach LLMs to understand specific types of data in a dataset.","arXiv.org",2023,"Shezheng Song,Xiaopeng Li,Shasha Li",1,117,0
"cf7d69709bdeddd561c183178bbc1f0c2e156a08","https://www.semanticscholar.org/paper/cf7d69709bdeddd561c183178bbc1f0c2e156a08",2,"Analyzing Modular Approaches for Visual Question Decomposition","ViperGPT's reported gains over BLIP-2 can be attributed to its selection of task-specific modules, and when it is run using a more task-agnostic selection of modules, these gains go away, but on some benchmarks, modular approaches significantly benefit by representing subtasks with natural language, instead of code.","Conference on Empirical Methods in Natural Language Processing",2023,"Apoorv Khandelwal,Ellie Pavlick,Chen Sun",0,41,0
"ef321c6f174ac59916ac54ec40ad18bca5b58e5c","https://www.semanticscholar.org/paper/ef321c6f174ac59916ac54ec40ad18bca5b58e5c",2,"PerceptionGPT: Effectively Fusing Visual Perception into LLM","A novel end-to-end framework named PerceptionGPT, which efficiently and effectively equips the VLLMs with visual perception abilities by leveraging the representation power of LLMs' token embedding and demonstrates significant improvements over previous methods with much fewer trainable parameters and GPU hours.","arXiv.org",2023,"Renjie Pi,Lewei Yao,Jiahui Gao,Jipeng Zhang,Tong Zhang",3,50,0
"f32ea390686b1eee3ba5b53c7a85e9e9385d4b94","https://www.semanticscholar.org/paper/f32ea390686b1eee3ba5b53c7a85e9e9385d4b94",2,"Visual Program Distillation: Distilling Tools and Programmatic Reasoning into Vision-Language Models","Visual Program Distillation (VPD) is proposed, an instruction tuning framework that produces a vision-language model (VLM) capable of solving complex visual tasks with a single forward pass and improves the VLM's ability to count, understand spatial relations, and reason compositionally.","arXiv.org",2023,"Yushi Hu,Otilia Stretcu,Chun-Ta Lu,Krishnamurthy Viswanathan,K. Hata,Enming Luo,Ranjay Krishna,Ariel Fuxman",0,59,0
"5502d769595981009e43344f8914e287acca2359","https://www.semanticscholar.org/paper/5502d769595981009e43344f8914e287acca2359",2,"ModaVerse: Efficiently Transforming Modalities with LLMs","ModaVerse is introduced, a Multi-modal Large Language Model (MLLM) capable of comprehending and transforming content across various modalities including images, videos, and audio, and a novel Input/Output (I/O) alignment mechanism that operates directly at the level of natural language.","arXiv.org",2024,"Xinyu Wang,Bohan Zhuang,Qi Wu",0,56,0
"7e6c1bb54bb2e36cc1092b080e9928942f7f8a68","https://www.semanticscholar.org/paper/7e6c1bb54bb2e36cc1092b080e9928942f7f8a68",2,"TroVE: Inducing Verifiable and Efficient Toolboxes for Solving Programmatic Tasks","TROVE is presented, a training-free method of inducing a verifiable and efficient toolbox of functions, by generating via using, growing, and periodically trimming the toolbox.","arXiv.org",2024,"Zhiruo Wang,Daniel Fried,Graham Neubig",0,33,0
"5e7274bcda47b704b6797bb14be8b7a61c047a61","https://www.semanticscholar.org/paper/5e7274bcda47b704b6797bb14be8b7a61c047a61",2,"Uncertainty-Aware Evaluation for Vision-Language Models","It is shown that models with the highest accuracy may also have the highest uncertainty, which confirms the importance of measuring it for VLMs, and a correlation between model uncertainty and its language model part is revealed.","",2024,"Vasily Kostumov,Bulat Nutfullin,Oleg Pilipenko,Eugene Ilyushin",0,64,0
"1b5e69a5b0f179e90f356a9c8cc1a39f77471dab","https://www.semanticscholar.org/paper/1b5e69a5b0f179e90f356a9c8cc1a39f77471dab",2,"Selective""Selective Prediction"": Reducing Unnecessary Abstention in Vision-Language Reasoning","ReCoVERR, an inference-time algorithm to reduce the over-abstention of a selective vision-language system without decreasing prediction accuracy, enables two VLMs, BLIP2 and InstructBLIP, to answer up to 20% more questions on the A-OKVQA task than vanilla selective prediction without decreasing system accuracy, thus improving overall system reliability.","",2024,"Tejas Srinivasan,Jack Hessel,Tanmay Gupta,Bill Yuchen Lin,Yejin Choi,Jesse Thomason,Khyathi Raghavi Chandu",0,38,0
"44ccf252018f71898d52d89539f17d77a4f8d548","https://www.semanticscholar.org/paper/44ccf252018f71898d52d89539f17d77a4f8d548",2,"Chart Understanding with Large Language Model","A baseline multimodal model is introduced that integrates text and charts to enhance the chart comprehension capabilities of existing models, offering more pertinent insights and information related to the depicted charts.","",,"Yaser James,Will Li,John Feng",0,39,0
"96a6df2b4aa50cfbd8984933e9c66b0763fc08a6","https://www.semanticscholar.org/paper/96a6df2b4aa50cfbd8984933e9c66b0763fc08a6",2,"Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V",,"",2023,"Jianwei Yang,Hao Zhang,Feng Li,Xueyan Zou,Chun-yue Li,Jianfeng Gao",26,88,5
"ed9943d73eb42116fe33564b5065c78b5ca0b16e","https://www.semanticscholar.org/paper/ed9943d73eb42116fe33564b5065c78b5ca0b16e",2,"RestGPT: Connecting Large Language Models with Real-World Applications via RESTful APIs","This paper introduces RestGPT, which leverages LLMs to solve user requests by connecting with RESTful APIs and proposes a coarse-to-fine online planning mechanism to enhance the ability of planning and API selection.","arXiv.org",2023,"Yifan Song,Weimin Xiong,Dawei Zhu,Cheng Li,Ke Wang,Ye Tian,Sujian Li",23,20,1
"d1120d67b700e4dfe8b39eb1e48fbdea4e1a0c43","https://www.semanticscholar.org/paper/d1120d67b700e4dfe8b39eb1e48fbdea4e1a0c43",2,"HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face","HuggingGPT is an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities to solve AI tasks and can tackle a wide range of sophisticated AI tasks spanning different modalities and domains.","Neural Information Processing Systems",2023,"Yongliang Shen,Kaitao Song,Xu Tan,D. Li,Weiming Lu,Y. Zhuang",409,43,48
"13b5b69355555e0c8b702261c5de3b4172ba653c","https://www.semanticscholar.org/paper/13b5b69355555e0c8b702261c5de3b4172ba653c",2,"The Art of SOCRATIC QUESTIONING: Zero-shot Multimodal Reasoning with Recursive Thinking and Self-Questioning","Qualitative analysis clearly demonstrates the intermediate thinking steps elicited by S OCRATIC Q UESTIONING are similar to the human’s recursively thinking process of a complex reasoning problem.","arXiv.org",2023,"Jingyuan Qi,Zhiyang Xu,Ying Shen,Minqian Liu,dingnan jin,Qifan Wang,Lifu Huang",8,27,0
"53df959bcf6499c45e316086a96a624389a39a52","https://www.semanticscholar.org/paper/53df959bcf6499c45e316086a96a624389a39a52",2,"Accountable Textual-Visual Chat Learns to Reject Human Instructions in Image Re-creation","This paper introduces two novel multimodal datasets: the synthetic CLEVR-ATVC dataset (620K) and the manually pictured Fruit-ATVC dataset (50K) and introduces specific rules as supervisory signals within the datasets to facilitate the accountability of multimodal systems in rejecting human requests.","",2023,"Zhiwei Zhang,Yuliang Liu",0,130,0
"ac7771c332da42b29a913b116bd6ef622cbf89cf","https://www.semanticscholar.org/paper/ac7771c332da42b29a913b116bd6ef622cbf89cf",2,"TaskMatrix.AI: Completing Tasks by Connecting Foundation Models with Millions of APIs","The vision of how to build such an ecosystem is presented, each key component is explained, and study cases are used to illustrate both the feasibility of this vision and the main challenges the authors need to address next.","Intelligent Computing",2023,"Yaobo Liang,Chenfei Wu,Ting Song,Wenshan Wu,Yan Xia,Yu Liu,Yangyiwen Ou,Shuai Lu,Lei Ji,Shaoguang Mao,Yuntao Wang,Linjun Shou,Ming Gong,Nan Duan",107,30,4
"352420ee61a8da783ca7750170793613b18b8d9c","https://www.semanticscholar.org/paper/352420ee61a8da783ca7750170793613b18b8d9c",2,"Tool Learning with Foundation Models","A systematic investigation of tool learning is presented, including its cognitive origins, the paradigm shift of foundation models, and the complementary roles of tools and models to inspire future research in integrating tools with foundation models.","arXiv.org",2023,"Yujia Qin,Shengding Hu,Yankai Lin,Weize Chen,Ning Ding,Ganqu Cui,Zheni Zeng,Yufei Huang,Chaojun Xiao,Chi Han,Y. Fung,Yusheng Su,Huadong Wang,Cheng Qian,Runchu Tian,Kunlun Zhu,Shi Liang,Xingyu Shen,Bokai Xu,Zhen Zhang,Yining Ye,Bo Li,Ziwei Tang,Jing Yi,Yu Zhu,Zhenning Dai,Lan Yan,Xin Cong,Ya-Ting Lu,Weilin Zhao,Yuxiang Huang,Jun-Han Yan,Xu Han,Xian Sun,Dahai Li,Jason Phang,Cheng Yang,Tongshuang Wu,Heng Ji,Zhiyuan Liu,Maosong Sun",107,238,6
"7e32aac43e9f1df49e116add03327ee6f365dbf3","https://www.semanticscholar.org/paper/7e32aac43e9f1df49e116add03327ee6f365dbf3",2,"mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality",,"arXiv.org",2023,"Qinghao Ye,Haiyang Xu,Guohai Xu,Jiabo Ye,Ming Yan,Yi Zhou,Junyan Wang,Anwen Hu,Pengcheng Shi,Yaya Shi,Chenliang Li,Yuanhong Xu,Hehong Chen,Junfeng Tian,Qiang Qi,Ji Zhang,Feiyan Huang",325,36,40
"54a8b153ed04a872da878d695239bdc413dc782c","https://www.semanticscholar.org/paper/54a8b153ed04a872da878d695239bdc413dc782c",2,"InternGPT: Solving Vision-Centric Tasks by Interacting with Chatbots Beyond Language","By incorporating pointing instructions, the proposed iGPT significantly improves the efficiency of communication between users and chatbots, as well as the accuracy of chatbots in vision-centric tasks, especially in complicated visual scenarios where the number of objects is greater than 2.","arXiv.org",2023,"Zhaoyang Liu,Yinan He,Wenhai Wang,Weiyun Wang,Yi Wang,Shoufa Chen,Qing-Long Zhang,Yang Yang,Qingyun Li,Jiashuo Yu,Kunchang Li,Zhe Chen,Xuecheng Yang,Xizhou Zhu,Yali Wang,Limin Wang,Ping Luo,Jifeng Dai,Yu Qiao",39,83,0
"d48cb91b9e555194f7494c4d4bb9815021d3ee45","https://www.semanticscholar.org/paper/d48cb91b9e555194f7494c4d4bb9815021d3ee45",2,"VideoChat: Chat-Centric Video Understanding","An end-to-end chat-centric video understanding system that integrates video foundation models and large language models via a learnable neural interface, excelling in spatiotemporal reasoning, event localization, and causal relationship inference is initiated.","arXiv.org",2023,"Kunchang Li,Yinan He,Yi Wang,Yizhuo Li,Wen Wang,Ping Luo,Yali Wang,Limin Wang,Yu Qiao",130,65,21
"42a30dc5470f54ec249f25d3c31e05d7c376c8e3","https://www.semanticscholar.org/paper/42a30dc5470f54ec249f25d3c31e05d7c376c8e3",2,"VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks","This work presents an LLM-based framework for vision-centric tasks, termed VisionLLM, which provides a unified perspective for vision and language tasks by treating images as a foreign language and aligning vision-focused tasks with language tasks that can be flexibly defined and managed using language instructions.","Neural Information Processing Systems",2023,"Wen Wang,Zhe Chen,Xiaokang Chen,Jiannan Wu,Xizhou Zhu,Gang Zeng,Ping Luo,Tong Lu,Jie Zhou,Y. Qiao,Jifeng Dai",127,81,7
"2195676f111ad492c50f4d4c96abb2bd3d72f7fc","https://www.semanticscholar.org/paper/2195676f111ad492c50f4d4c96abb2bd3d72f7fc",2,"Instruct2Act: Mapping Multi-modality Instructions to Robotic Actions with Large Language Model","This paper presents Instruct2Act, a framework that utilizes Large Language Models to map multi-modal instructions to sequential actions for robotic manipulation tasks, employing the LLM model to generate Python programs that constitute a comprehensive perception, planning, and action loop for robotic tasks.","arXiv.org",2023,"Siyuan Huang,Zhengkai Jiang,Hao Dong,Y. Qiao,Peng Gao,Hongsheng Li",33,55,3
"ca055cfb9d4d47124cc035c346f38577825fcacf","https://www.semanticscholar.org/paper/ca055cfb9d4d47124cc035c346f38577825fcacf",2,"Enhance Reasoning Ability of Visual-Language Models via Large Language Models","A method called TReE is proposed, which transfers the reasoning ability of a large language model to a visual language model in zero-shot scenarios, and contains three stages: observation, thinking, and re-thinking.","arXiv.org",2023,"Yueting Yang,Xintong Zhang,Wenjuan Han",0,40,0
"6fe2a4f7594abadafc9feb05e96ab7e9e634ce8f","https://www.semanticscholar.org/paper/6fe2a4f7594abadafc9feb05e96ab7e9e634ce8f",2,"Album Storytelling with Iterative Story-aware Captioning and Large Language Models","This work proposes a new iterative album storytelling pipeline, which starts with an initial story and builds a story-aware caption model to refine the captions using the whole story as guidance, then feeds into the LLMs to generate a new refined story.","arXiv.org",2023,"Munan Ning,Yujia Xie,Dongdong Chen,Zeyin Song,Lu Yuan,Yonghong Tian,Qixiang Ye,Liuliang Yuan",3,58,1
"8da9b1436212b233fc49c7daf1ba15c22874ff5a","https://www.semanticscholar.org/paper/8da9b1436212b233fc49c7daf1ba15c22874ff5a",2,"CREATOR: Tool Creation for Disentangling Abstract and Concrete Reasoning of Large Language Models","The proposed CREATOR, a novel framework that enables LLMs to create their own tools using documentation and code realization, disentangles abstract tool creation and concrete decision execution, resulting in improved performance.","Conference on Empirical Methods in Natural Language Processing",2023,"Cheng Qian,Chi Han,Y. Fung,Yujia Qin,Zhiyuan Liu,Heng Ji",5,36,1
"90027ca7802645671a69b00b65e1fa94e6b63544","https://www.semanticscholar.org/paper/90027ca7802645671a69b00b65e1fa94e6b63544",2,"ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models","This study proposes a modular paradigm ReWOO (Reasoning WithOut Observation) that detaches the reasoning process from external observations, thus significantly reducing token consumption and demonstrating robustness under tool-failure scenarios.","arXiv.org",2023,"Binfeng Xu,Zhiyuan Peng,Bowen Lei,Subhabrata Mukherjee,Yuchen Liu,Dongkuan Xu",34,41,2
"69335077fcacbff7a7cf25697da1949e6bdfa968","https://www.semanticscholar.org/paper/69335077fcacbff7a7cf25697da1949e6bdfa968",2,"The Art of SOCRATIC QUESTIONING: Recursive Thinking with Large Language Models","The qualitative analysis clearly shows that the intermediate reasoning steps elicited by SOCRATIC QUESTIONING are similar to humans' recursively thinking process of complex reasoning problems.","Conference on Empirical Methods in Natural Language Processing",2023,"Jingyuan Qi,Zhiyang Xu,Ying Shen,Minqian Liu,dingnan jin,Qifan Wang,Lifu Huang",3,44,1
"00cb69a9f280317d1c59ac5827551ee9b10642b8","https://www.semanticscholar.org/paper/00cb69a9f280317d1c59ac5827551ee9b10642b8",2,"EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought","This work introduces EmbodiedGPT, an end-to-end multi-modal foundation model for embodied AI, empowering embodied agents with multi- modal understanding and execution capabilities, and significantly enhances the success rate of the embodied control task by extracting more effective features.","Neural Information Processing Systems",2023,"Yao Mu,Qinglong Zhang,Mengkang Hu,Wen Wang,Mingyu Ding,Jun Jin,Bin Wang,Jifeng Dai,Y. Qiao,Ping Luo",64,73,3
"9c3a9b4821daa03cb5369041d59d2714329a3811","https://www.semanticscholar.org/paper/9c3a9b4821daa03cb5369041d59d2714329a3811",2,"Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models","A novel and affordable solution for the effective VL adaption of LLMs, called Mixture-of-Modality Adaptation (MMA), which adopts lightweight modules, i.e., adapters, to bridge the gap between LLMs and VL tasks, which also enables the joint optimization of the image and language models","Neural Information Processing Systems",2023,"Gen Luo,Yiyi Zhou,Tianhe Ren,Shen Chen,Xiaoshuai Sun,Rongrong Ji",36,53,4
"c6ac708b65b24c20f80831d518c1795ce8133ad5","https://www.semanticscholar.org/paper/c6ac708b65b24c20f80831d518c1795ce8133ad5",2,"ChatBridge: Bridging Modalities with Large Language Model as a Language Catalyst","It is shown that only language-paired two-modality data is sufficient to connect all modalities and ChatBridge, a novel multimodal language model that leverages the expressive capabilities of language as the catalyst to bridge the gap between various modalities, is presented.","arXiv.org",2023,"Zijia Zhao,Longteng Guo,Tongtian Yue,Si-Qing Chen,Shuai Shao,Xinxin Zhu,Zehuan Yuan,Jing Liu",23,72,4
"50c1414fe41d0cb9db6f0933c9319aa124beac5d","https://www.semanticscholar.org/paper/50c1414fe41d0cb9db6f0933c9319aa124beac5d",2,"Contextual Object Detection with Multimodal Large Language Models","The ContextDET is presented, a unified multimodal model that is capable of end-to-end differentiable modeling of visual-language contexts, so as to locate, identify, and associate visual objects with language inputs for human-AI interaction.","arXiv.org",2023,"Yuhang Zang,Wei Li,Jun Han,Kaiyang Zhou,Chen Change Loy",22,87,0
"b458fc5261595f44b36325e5eaea1f874d65138f","https://www.semanticscholar.org/paper/b458fc5261595f44b36325e5eaea1f874d65138f",2,"GPT4Tools: Teaching Large Language Model to Use Tools via Self-instruction","The GPT4Tools based on self-instruct to enable open-source LLMs, such as LLaMA and OPT, to use tools, generates an instruction-following dataset by prompting an advanced teacher with various multi-modal contexts using the Low-Rank Adaptation (LoRA) optimization.","Neural Information Processing Systems",2023,"Rui Yang,Lin Song,Yanwei Li,Sijie Zhao,Yixiao Ge,Xiu Li,Ying Shan",69,66,9
"615962d8969c8e0ffe43319689dce6c50cbf1f29","https://www.semanticscholar.org/paper/615962d8969c8e0ffe43319689dce6c50cbf1f29",2,"Responsible Task Automation: Empowering Large Language Models as Responsible Task Automators","This paper presents Responsible Task Automation (ResponsibleTA) as a fundamental framework to facilitate responsible collaboration between LLM-based coordinators and executors for task automation with three empowered capabilities: predicting the feasibility of the commands for executors, verifying the completeness of executors and enhancing the security.","arXiv.org",2023,"Zhizheng Zhang,Xiaoyi Zhang,Wenxuan Xie,Yan Lu",4,46,1
"d47524cd5c3c4b57af2e5a29f6f91c420310f236","https://www.semanticscholar.org/paper/d47524cd5c3c4b57af2e5a29f6f91c420310f236",2,"MIMIC-IT: Multi-Modal In-Context Instruction Tuning","MultI-Modal In-Context Instruction Tuning (MIMIC-IT), a dataset comprising 2.8 million multimodal instruction-response pairs, with 2.2 million unique instructions derived from images and videos, is presented and a large VLM named Otter is trained.","arXiv.org",2023,"Bo Li,Yuanhan Zhang,Liangyu Chen,Jinghao Wang,Fanyi Pu,Jingkang Yang,C. Li,Ziwei Liu",75,55,10
"ed30969f0e4811473144ffe83c1baa6d54f02202","https://www.semanticscholar.org/paper/ed30969f0e4811473144ffe83c1baa6d54f02202",2,"RestGPT: Connecting Large Language Models with Real-World RESTful APIs","This paper proposes RestGPT, which exploits the power of LLMs and conducts a coarse-to-fine online planning mechanism to enhance the abilities of task decomposition and API selection and paves a new way towards AGI.","",2023,"Yifan Song,Weimin Xiong,Dawei Zhu,Wenhao Wu,Han Qian,Mingbo Song,Hailiang Huang,Cheng Li,Ke Wang,Rong Yao,Ye Tian,Sujian Li",12,25,1
"4c4d176c6e28f48041f215d563f6ee8633534cff","https://www.semanticscholar.org/paper/4c4d176c6e28f48041f215d563f6ee8633534cff",2,"Valley: Video Assistant with Large Language model Enhanced abilitY","A novel multi-modal foundation model capable of comprehending video, image, and language within a general framework is developed, and Qualitative experiments demonstrate that Valley has the potential to function as a highly effective video assistant that can make complex video understanding scenarios easy.","arXiv.org",2023,"Ruipu Luo,Ziwang Zhao,Min Yang,Junwei Dong,Ming-Hui Qiu,Pengcheng Lu,Tao Wang,Zhongyu Wei",40,39,9
"473eb062612a17c965eaa62136322f0dec6b1f8e","https://www.semanticscholar.org/paper/473eb062612a17c965eaa62136322f0dec6b1f8e",2,"Data-Copilot: Bridging Billions of Data and Humans with Autonomous Workflow","This work proposes Data-Copilot, an LLM-based system that connects numerous data sources on one end and caters to diverse human demands on the other end, and autonomously transforms raw data into visualization results that best match the user's intent.","arXiv.org",2023,"Wenqi Zhang,Yongliang Shen,Weiming Lu,Y. Zhuang",21,37,1
"ea566f87f6253bb2d32cf7b61cd3e2535a0c3f42","https://www.semanticscholar.org/paper/ea566f87f6253bb2d32cf7b61cd3e2535a0c3f42",2,"mPLUG-DocOwl: Modularized Multimodal Large Language Model for Document Understanding","Experimental results show that the proposed mPLUG-DocOwl model outperforms existing multi-modal models, demonstrating its strong ability of document understanding, and also generalizes well on various downstream tasks.","arXiv.org",2023,"Jiabo Ye,Anwen Hu,Haiyang Xu,Qinghao Ye,Mingshi Yan,Yuhao Dan,Chenlin Zhao,Guohai Xu,Chenliang Li,Junfeng Tian,Qiang Qi,Ji Zhang,Feiyan Huang",29,37,5
"ebddfdc5d845a788e8062eddbbf7a335737cb99b","https://www.semanticscholar.org/paper/ebddfdc5d845a788e8062eddbbf7a335737cb99b",2,"What Matters in Training a GPT4-Style Language Model with Multimodal Inputs?","Lynx is presented, which performs the most accurate multi-modal understanding while keeping the best multi- modal generation ability compared to existing open-sourced GPT4-style models.","arXiv.org",2023,"Yan Zeng,Hanbo Zhang,Jiani Zheng,Jiangnan Xia,Guoqiang Wei,Yang Wei,Yuchen Zhang,Tao Kong",34,108,3
"2e52178416b93a0a7dc1cb7d21d04c7e72c2ce0f","https://www.semanticscholar.org/paper/2e52178416b93a0a7dc1cb7d21d04c7e72c2ce0f",2,"ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring Instruction Tuning","This study proposes ChatSpot, a unified end-to-end multimodal large language model that supports diverse forms of interactivity including mouse clicks, drag-and-drop, and drawing boxes, which provides a more flexible and seamless interactive experience.","arXiv.org",2023,"Liang Zhao,En Yu,Zheng Ge,Jinrong Yang,Hao-Ran Wei,Hongyu Zhou,Jian‐Yuan Sun,Yuang Peng,Runpei Dong,Chunrui Han,Xiangyu Zhang",18,39,3
"bbcd5cc4bf6c77282e88cae07f7f2adb1da818ca","https://www.semanticscholar.org/paper/bbcd5cc4bf6c77282e88cae07f7f2adb1da818ca",2,"Testing the Depth of ChatGPT's Comprehension via Cross-Modal Tasks Based on ASCII-Art: GPT3.5's Abilities in Regard to Recognizing and Generating ASCII-Art Are Not Totally Lacking","This work examines GPT3.5's aptitude for visual tasks, where the inputs feature content provided as ASCII-art without overt distillation into a lingual summary, and conducts experiments analyzing the model's performance on image recognition tasks after various transforms typical in visual settings.","arXiv.org",2023,"David Bayani",0,86,0
"ad113d8b6f8f6dace7abd12dc88af520aaaf3fc7","https://www.semanticscholar.org/paper/ad113d8b6f8f6dace7abd12dc88af520aaaf3fc7",2,"LISA: Reasoning Segmentation via Large Language Model","This work proposes a new segmentation task -- reasoning segmentation, designed to output a segmentation mask given a complex and implicit query text, and presents LISA, which inherits the language generation capabilities of the multi-modal Large Language Model (LLM) while also possessing the ability to produce segmentation masks.","arXiv.org",2023,"Xin Lai,Zhuotao Tian,Yukang Chen,Yanwei Li,Yuhui Yuan,Shu Liu,Jiaya Jia",71,64,16
"4f2be887e991efa85f7b874e7ab871080a745c39","https://www.semanticscholar.org/paper/4f2be887e991efa85f7b874e7ab871080a745c39",2,"CAESURA: Language Models as Multi-Modal Query Planners","This paper proposes Language-Model-Driven Query Planning, a new paradigm of query planning that uses Language Models to translate natural language queries into executable query plans that can contain complex operators that are able to process arbitrary modalities.","arXiv.org",2023,"Matthias Urban,Carsten Binnig",0,30,0
"d53945d4afb4528590d79e20de52883d29037e86","https://www.semanticscholar.org/paper/d53945d4afb4528590d79e20de52883d29037e86",2,"FashionLOGO: Prompting Multimodal Large Language Models for Fashion Logo Embeddings","This work explores how MLLMs can improve logo embedding by prompting them to generate explicit textual knowledge through three types of prompts, including image OCR, brief captions, and detailed descriptions prompts, in a zero-shot setting and adopt a cross-attention transformer to enable image embedding queries to learn supplementary knowledge from textual embeddings automatically.","arXiv.org",2023,"Yulin Su,Min Yang,Minghui Qiu,Jing Wang,Tao Wang",0,44,0
"eb5cf10406a8ad31e0ebe56b36571d5db4758a62","https://www.semanticscholar.org/paper/eb5cf10406a8ad31e0ebe56b36571d5db4758a62",2,"PUMGPT: A Large Vision-Language Model for Product Understanding","This paper presents PUMGPT, a large vision-language model that aims at unifying all product understanding tasks under a singular model structure, and proposes Layer-wise Adapters (LA), an approach that provides enhanced alignment with fewer visual tokens and enables parameter-efficient fine-tuning.","arXiv.org",2023,"Shuhui Wu,Zengming Tang,Zongyi Guo,Weiwei Zhang,Baoliang Cui,Haihong Tang,Weiming Lu",1,38,0
"ef1ebdb24ce45fb57e271783a0c9a877fe0e79c3","https://www.semanticscholar.org/paper/ef1ebdb24ce45fb57e271783a0c9a877fe0e79c3",2,"Position-Enhanced Visual Instruction Tuning for Multimodal Large Language Models","This paper proposes Position-enhanced Visual Instruction Tuning (PVIT), which extends the functionality of MLLMs by integrating an additional region-level vision encoder, which promotes a more detailed comprehension of images for the MLLM.","arXiv.org",2023,"Chi Chen,Ruoyu Qin,Fuwen Luo,Xiaoyue Mi,Peng Li,Maosong Sun,Yang Liu",21,34,2
"3b36d16985286b03e06e8404a7be49a9713d37b9","https://www.semanticscholar.org/paper/3b36d16985286b03e06e8404a7be49a9713d37b9",2,"Confucius: Iterative Tool Learning from Introspection Feedback by Easy-to-Difficult Curriculum","The Confucius is proposed, a novel tool learning framework to train large language models to use complicated tools in real-world scenarios, which contains two main phases: a multi-stage learning method to teach the LLM to use various tools from an easy-to-difficult curriculum and the Iterative Self-instruct from Introspective Feedback to dynamically construct the dataset to improve the ability to use the complicated tool.","arXiv.org",2023,"Shen Gao,Zhengliang Shi,Minghang Zhu,Bowen Fang,Xin Xin,Pengjie Ren,Zhumin Chen,Jun Ma",6,44,0
"6bcc6ab9c28805d4067e99b2cdc7524550fe80e1","https://www.semanticscholar.org/paper/6bcc6ab9c28805d4067e99b2cdc7524550fe80e1",2,"PointLLM: Empowering Large Language Models to Understand Point Clouds","Experimental results reveal PointLLM's superior performance over existing 2D and 3D baselines, with a notable achievement in human-evaluated object captioning tasks where it surpasses human annotators in over 50% of the samples.","arXiv.org",2023,"Runsen Xu,Xiaolong Wang,Tai Wang,Yilun Chen,Jiangmiao Pang,Dahua Lin",31,72,4
"c237a22698223e4060d83027f399f4fb2aa24291","https://www.semanticscholar.org/paper/c237a22698223e4060d83027f399f4fb2aa24291",2,"Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations","InteRecAgent enables traditional recommender systems, such as those ID-based matrix factorization models, to become interactive systems with a natural language interface through the integration of LLMs, and introduces an efficient framework, which employs LLMs as the brain and recommender models as tools.","arXiv.org",2023,"Xu Huang,Jianxun Lian,Yuxuan Lei,Jing Yao,Defu Lian,Xing Xie",17,58,1
"d39182113cd4176ead48027b4fc05fe06ec6aaca","https://www.semanticscholar.org/paper/d39182113cd4176ead48027b4fc05fe06ec6aaca",2,"Language Models as Black-Box Optimizers for Vision-Language Models","This work proposes employing chat-based LLMs to search for the best text prompt for VLMs and highlights the advantage of conversational feedback that incorporates both positive and negative prompts, suggesting that LLMs can utilize the implicit gradient direction in textual feedback for a more efficient search.","arXiv.org",2023,"Samuel Yu,Shihong Liu,Zhiqiu Lin,Deepak Pathak,Deva Ramanan",4,107,0
"a1426b13b74dbad17b34606d25aabe1d61f6e11a","https://www.semanticscholar.org/paper/a1426b13b74dbad17b34606d25aabe1d61f6e11a",2,"CRAFT: Customizing LLMs by Creating and Retrieving from Specialized Toolsets","CRAFT is designed to be flexible and offers a plug-and-play approach to adapt off-the-shelf LLMs to unseen domains and modalities, without any finetuning, and achieves substantial improvements compared to strong baselines.","arXiv.org",2023,"Lifan Yuan,Yangyi Chen,Xingyao Wang,Y. Fung,Hao Peng,Heng Ji",17,61,2
"bee68767debbdc96d6f75947e544a8be98b869e3","https://www.semanticscholar.org/paper/bee68767debbdc96d6f75947e544a8be98b869e3",2,"Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model: Explorations with GPT4-Vision and Beyond","This study introduces a new benchmark called PCA-EVAL, which evaluates embodied decision-making from the perspectives of Perception, Cognition, and Action and proposes HOLMES, a multi-agent cooperation framework that allows LLMs to leverage MLLMs and APIs to gather multimodal information for informed decision- making.","arXiv.org",2023,"Liang Chen,Yichi Zhang,Shuhuai Ren,Haozhe Zhao,Zefan Cai,Yuchi Wang,Tianyu Liu,Baobao Chang",13,60,2
"8918e3cc21ecaf81532e452d3b9518360d14860e","https://www.semanticscholar.org/paper/8918e3cc21ecaf81532e452d3b9518360d14860e",2,"Reinforced UI Instruction Grounding: Towards a Generic UI Task Automation API","This work builds a multimodal model to ground natural language instructions in given UI screenshots as a generic UI task automation executor and proposes an innovative Reinforcement Learning (RL) based algorithm to supervise the tokens in such sequence jointly with visually semantic metrics, which effectively strengthens the spatial decoding capability of the pixel-to-sequence paradigm.","arXiv.org",2023,"Zhizheng Zhang,Wenxuan Xie,Xiaoyi Zhang,Yan Lu",2,48,0
"84f9bc5f89dac53662fb467b6af8ff26415ca3e7","https://www.semanticscholar.org/paper/84f9bc5f89dac53662fb467b6af8ff26415ca3e7",2,"InstructDET: Diversifying Referring Object Detection with Generalized Instructions","A data-centric method for referring object detection (ROD) that localizes target objects based on user instructions and shows that a conventional ROD model surpasses existing methods on standard REC datasets and the authors' InDET test set.","arXiv.org",2023,"Ronghao Dang,Jiangyan Feng,Haodong Zhang,Chongjian Ge,Lin Song,Lijun Gong,Chengju Liu,Qi Chen,Feng Zhu,Rui Zhao,Yibing Song",2,62,0
"33095b1334bed852e3652bd9d7da3f4df0cdf485","https://www.semanticscholar.org/paper/33095b1334bed852e3652bd9d7da3f4df0cdf485",2,"ViCor: Bridging Visual Understanding and Commonsense Reasoning with Large Language Models","This work explores the synergistic capabilities of pre-trained vision-and-language models (VLMs) and large language models (LLMs) for visual commonsense reasoning (VCR) and suggests a collaborative approach where LLMs, when uncertain about their reasoning, actively direct VLMs to concentrate on and gather relevant visual elements to support potential commonsense inferences.","arXiv.org",2023,"KAI-QING Zhou,Kwonjoon Lee,Teruhisa Misu,Xin Eric Wang",1,34,0
"03bf1da1caa5f63203d43ed78c12c35a78fc6ed9","https://www.semanticscholar.org/paper/03bf1da1caa5f63203d43ed78c12c35a78fc6ed9",2,"EasyGen: Easing Multimodal Generation with a Bidirectional Conditional Diffusion Model and LLMs","Comprehensive quantitative and qualitative experiments show that EasyGen excels in data-efficient training, high-quality image generation, and extendibility, effectively addressing the challenges in multimodal generation.","",2023,"Xiangyu Zhao,Bo Liu,Qijiong Liu,Guangyuan Shi,Xiao-Ming Wu",3,59,0
"36b923d97d7cfaf73d11c55c15ea46605ba974a5","https://www.semanticscholar.org/paper/36b923d97d7cfaf73d11c55c15ea46605ba974a5",2,"BiLL-VTG: Bridging Large Language Models and Lightweight Visual Tools for Video-based Texts Generation","BiLL-VTG is introduced, a fast adaptive framework that leverages large language models (LLMs) to reasoning on videos based on essential lightweight visual tools and an Instruction-oriented Video Events Recognition (InsOVER) algorithm based on the efficient Hungarian matching to localize corresponding video events using linguistic instructions, enabling LLMs to interact with long videos.","arXiv.org",2023,"Ji Qi,Kaixuan Ji,Jifan Yu,Duokang Wang,Bin Xu,Lei Hou,Juanzi Li",0,68,0
"7451d756118628474dc022813eb952a21d34c5f6","https://www.semanticscholar.org/paper/7451d756118628474dc022813eb952a21d34c5f6",2,"Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V","The experiments show that GPT-4V with SoM in zero-shot setting outperforms the state-of-the-art fully-finetuned referring expression comprehension and segmentation model on RefCOCOg, and the effectiveness of SoM on a wide range of fine-grained vision and multimodal tasks is validated.","arXiv.org",2023,"Jianwei Yang,Hao Zhang,Feng Li,Xueyan Zou,Chun-yue Li,Jianfeng Gao",8,48,3
"79e7ead8f59b17431de2b86af10dc0c30a1f5a2b","https://www.semanticscholar.org/paper/79e7ead8f59b17431de2b86af10dc0c30a1f5a2b",2,"ToolChain*: Efficient Action Space Navigation in Large Language Models with A* Search","This work proposes ToolChain*, an efficient tree search-based planning algorithm for LLM-based agents that formulates the entire action space as a decision tree, where each node represents a possible API function call involved in a solution plan.","arXiv.org",2023,"Yuchen Zhuang,Xiang Chen,Tong Yu,Saayan Mitra,Victor S. Bursztyn,Ryan A. Rossi,Somdeb Sarkhel,Chao Zhang",10,60,0
"f90c522b284a6c065fa5126216a26a7415a2b9fa","https://www.semanticscholar.org/paper/f90c522b284a6c065fa5126216a26a7415a2b9fa",2,"MoqaGPT : Zero-Shot Multi-modal Open-domain Question Answering with Large Language Model","MoqaGPT, a straightforward and flexible framework built upon LLMs, retrieves and extracts answers from each modality separately, then fuses this multi-modal information using LLMs to produce a final answer.","Conference on Empirical Methods in Natural Language Processing",2023,"Le Zhang,Yihong Wu,Fengran Mo,Jian-Yun Nie,Aishwarya Agrawal",0,30,0
"8e3e7deb95d2a984cba615ec847e64f354626cdf","https://www.semanticscholar.org/paper/8e3e7deb95d2a984cba615ec847e64f354626cdf",2,"WebWISE: Web Interface Control and Sequential Exploration with Large Language Models","This paper investigates using a Large Language Model (LLM) to automatically perform web software tasks using click, scroll, and text input operations using filtered Document Object Model elements as observations and performs tasks step-by-step, sequentially generating small programs based on the current observations.","arXiv.org",2023,"Heyi Tao,TV Sethuraman,Michal Shlapentokh-Rothman,Derek Hoiem",1,35,0
"807f336176070bd3f95b82a16f125ee99b7d2c80","https://www.semanticscholar.org/paper/807f336176070bd3f95b82a16f125ee99b7d2c80",2,"Woodpecker: Hallucination Correction for Multimodal Large Language Models","Implemented in a post-remedy manner, Woodpecker can easily serve different MLLMs, while being interpretable by accessing intermediate outputs of the five stages, and shows the huge potential of this new paradigm.","arXiv.org",2023,"Shukang Yin,Chaoyou Fu,Sirui Zhao,Tong Xu,Hao Wang,Dianbo Sui,Yunhang Shen,Ke Li,Xingguo Sun,Enhong Chen",16,49,2
"0212dca18cd0765deed0b6ba80a796f0ad46e066","https://www.semanticscholar.org/paper/0212dca18cd0765deed0b6ba80a796f0ad46e066",2,"mPLUG-Octopus: The Versatile Assistant Empowered by A Modularized End-to-End Multimodal LLM",,"ACM Multimedia",2023,"Qinghao Ye,Haiyang Xu,Mingshi Yan,Chenlin Zhao,Junyang Wang,Xiaoshan Yang,Ji Zhang,Fei Huang,J. Sang,Changsheng Xu",0,10,0
"76a3f4a79ae9a00db2f2b5f6877021d8deb96ada","https://www.semanticscholar.org/paper/76a3f4a79ae9a00db2f2b5f6877021d8deb96ada",2,"SPHINX: The Joint Mixing of Weights, Tasks, and Visual Embeddings for Multi-modal Large Language Models","SPHINX, a versatile multi-modal large language model (MLLM) with a joint mixing of model weights, tuning tasks, and visual embeddings is presented, and an efficient strategy aiming to better capture fine-grained appearances of high-resolution images is proposed.","arXiv.org",2023,"Ziyi Lin,Chris Liu,Renrui Zhang,Peng Gao,Longtian Qiu,Han Xiao,Han Qiu,Chen Lin,Wenqi Shao,Keqin Chen,Jiaming Han,Siyuan Huang,Yichi Zhang,Xuming He,Hongsheng Li,Y. Qiao",23,90,2
"0f993809c1fe00403ecea66d8f572832f075cfe4","https://www.semanticscholar.org/paper/0f993809c1fe00403ecea66d8f572832f075cfe4",2,"MMC: Advancing Multimodal Chart Understanding with Large-scale Instruction Tuning","This work provides an instruction-tuning methodology and benchmark to advance multimodal understanding of charts and develops MultiModal Chart Assistant (MMCA), an LMM that achieves state-of-the-art performance on existing chart QA benchmarks.","arXiv.org",2023,"Fuxiao Liu,Xiaoyang Wang,Wenlin Yao,Jianshu Chen,Kaiqiang Song,Sangwoo Cho,Yaser Yacoob,Dong Yu",8,38,0
"ab8169d6e4dfabfe7c30ebec1bb871bf3e1551cd","https://www.semanticscholar.org/paper/ab8169d6e4dfabfe7c30ebec1bb871bf3e1551cd",2,"GAIA: a benchmark for General AI Assistants","GAIA proposes real-world questions that require a set of fundamental abilities such as reasoning, multi-modality handling, web browsing, and generally tool-use proficiency, and shows that human respondents obtain 92% vs. 15% for GPT-4 equipped with plugins.","arXiv.org",2023,"Grégoire Mialon,Clémentine Fourrier,Craig Swift,Thomas Wolf,Y. LeCun,Thomas Scialom",25,40,4
"ee2c769943f9e46c3bbee117d1ecf14566b7bf1f","https://www.semanticscholar.org/paper/ee2c769943f9e46c3bbee117d1ecf14566b7bf1f",2,"Boosting the Power of Small Multimodal Reasoning Models to Match Larger Models with Self-Consistency Training","This work proposes MC-CoT, a self-consistency training strategy that generates multiple rationales and answers, subsequently selecting the most accurate through a voting process, and demonstrates that this approach significantly improves model performance across various benchmarks.","arXiv.org",2023,"Cheng Tan,Jingxuan Wei,Zhangyang Gao,Linzhuang Sun,Siyuan Li,Xihong Yang,Stan Z. Li",0,62,0
"7b0a186b0140ee91fb13991c9c7187f3dc3b0670","https://www.semanticscholar.org/paper/7b0a186b0140ee91fb13991c9c7187f3dc3b0670",2,"Visual Programming for Zero-shot Open-Vocabulary 3D Visual Grounding","This work proposes a novel visual programming approach for zero-shot open-vocabulary 3DVG, leveraging the capabilities of large language models (LLMs) and develops an innovative language-object correlation module to extend the scope of existing 3D object detectors into open- Vocabulary scenarios.","arXiv.org",2023,"Zhihao Yuan,Jinke Ren,Chun-Mei Feng,Hengshuang Zhao,Shuguang Cui,Zhen Li",0,67,0
"5eea245cc12c55905d4df827d0c9776c5ddfa743","https://www.semanticscholar.org/paper/5eea245cc12c55905d4df827d0c9776c5ddfa743",2,"Compositional Chain-of-Thought Prompting for Large Multimodal Models","The proposed Compositional Chain-of-Thought (CCoT) approach not only improves LMM performance on several vision and language VL compositional benchmarks but also improves the performance of several popular LMMs on general multimodal benchmarks, without the need for fine-tuning or annotated ground-truth SGs.","arXiv.org",2023,"Chancharik Mitra,Brandon Huang,Trevor Darrell,Roei Herzig",4,91,2
"8441c30ad4abdca9ee380aa6f22ffd731b10231b","https://www.semanticscholar.org/paper/8441c30ad4abdca9ee380aa6f22ffd731b10231b",2,"COLE: A Hierarchical Generation Framework for Graphic Design","The key insight is to dissect the complex task of text-to-design generation into a hierarchy of simpler sub-tasks, each addressed by specialized models working collaboratively, to streamline the complex process and significantly enhance generation reliability.","arXiv.org",2023,"Peidong Jia,Chenxuan Li,Zeyu Liu,Yichao Shen,Xingru Chen,Yuhui Yuan,Yinglin Zheng,Dong Chen,Ji Li,Xiaodong Xie,Shanghang Zhang,Baining Guo",0,45,0
"06b2ac5153e3d8d05c13c82f93d7f4e13eee6d0f","https://www.semanticscholar.org/paper/06b2ac5153e3d8d05c13c82f93d7f4e13eee6d0f",2,"Mitigating Fine-Grained Hallucination by Fine-Tuning Large Vision-Language Models with Caption Rewrites","ReCaption is proposed, a framework that consists of two components: rewriting captions using ChatGPT and fine-tuning the instruction-tuned LVLMs on the rewritten captions, and a fine-grained probing-based evaluation method named \textit{Fine-Grained Object Hallucination Evaluation} (FGHE).","Conference on Multimedia Modeling",2023,"Lei Wang,Jiabang He,Shenshen Li,Ning Liu,Ee-Peng Lim",7,39,0
"10578bc0bdb3ebf9232931dd4961f55ba470caad","https://www.semanticscholar.org/paper/10578bc0bdb3ebf9232931dd4961f55ba470caad",2,"LaMPilot: An Open Benchmark Dataset for Autonomous Driving with Language Model Programs","LaMPilot is presented, a novel framework for planning in the field of autonomous driving, rethinking the task as a code-generation process that leverages established behavioral primitives and shows that GPT-4, with human feedback, achieved an impressive task completion rate and a minimal collision rate.","arXiv.org",2023,"Yunsheng Ma,Can Cui,Xu Cao,Wenqian Ye,Peiran Liu,Juanwu Lu,Amr Abdelraouf,Rohit Gupta,Kyungtae Han,Aniket Bera,J. Rehg,Ziran Wang",7,54,0
"96a7b0fe722e6d2d5167ef25a6aff714a20233a0","https://www.semanticscholar.org/paper/96a7b0fe722e6d2d5167ef25a6aff714a20233a0",2,"Exploring the Limits of ChatGPT in Software Security Applications","The exploration reveals that ChatGPT not only excels at generating code, which is the conventional application of language models, but also demonstrates strong capability in understanding user-provided commands in natural languages, reasoning about control and data flows within programs, generating complex data structures, and even decompiling assembly code.","arXiv.org",2023,"Fangzhou Wu,Qingzhao Zhang,Ati Priya Bajaj,Tiffany Bao,Ning Zhang,Ruoyu Wang,Chaowei Xiao",0,153,0
"b240a1d8ec2860bdd7370daa3144268ce46ac018","https://www.semanticscholar.org/paper/b240a1d8ec2860bdd7370daa3144268ce46ac018",2,"Vary: Scaling up the Vision Vocabulary for Large Vision-Language Models","Compared to the popular BLIP-2, MiniGPT4, and LLaVA, Vary can maintain its vanilla capabilities while enjoying more excellent fine-grained perception and understanding ability and is competent in new document parsing features (OCR or markdown conversion).","arXiv.org",2023,"Haoran Wei,Lingyu Kong,Jinyue Chen,Liang Zhao,Zheng Ge,Jinrong Yang,Jian‐Yuan Sun,Chunrui Han,Xiangyu Zhang",6,52,0
"33c3dbf86dd6e338e2a49bba9c2e2193d1eca08a","https://www.semanticscholar.org/paper/33c3dbf86dd6e338e2a49bba9c2e2193d1eca08a",2,"Vista-LLaMA: Reliable Video Narrator via Equal Distance to Visual Tokens","This work proposes Vista-LLaMA, a novel framework that maintains the consistent distance between all visual tokens and any language tokens, irrespective of the generated text length, and presents a sequential visual projector that projects the current video frame into tokens of language space with the assistance of the previous frame.","arXiv.org",2023,"Fan Ma,Xiaojie Jin,Heng Wang,Yuchen Xian,Jiashi Feng,Yi Yang",2,39,1
"55c6d16b550c606d62dd85084f0d373d8f087966","https://www.semanticscholar.org/paper/55c6d16b550c606d62dd85084f0d373d8f087966",2,"VLAP: Efficient Video-Language Alignment via Frame Prompting and Distilling for Video Question Answering","This work proposes an efficient Video-Language Alignment via Frame-Prompting and Distilling (VLAP) network that addresses both efficient frame sampling and effective cross-modal alignment in a unified way and demonstrates the capability of selecting key frames with critical contents, thus improving the video-language alignment accuracy.","arXiv.org",2023,"Xijun Wang,Junbang Liang,Chun-Kai Wang,Kenan Deng,Yu Lou,Ming Lin,Shan Yang",0,66,0
"17a32c825bd746a2625eddc2728092171a9ef72a","https://www.semanticscholar.org/paper/17a32c825bd746a2625eddc2728092171a9ef72a",2,"Jack of All Tasks, Master of Many: Designing General-purpose Coarse-to-Fine Vision-Language Model","This work introduces VistaLLM, a powerful visual system that addresses coarse- and fine-grained VL tasks over single and multiple input images using a unified framework and addresses the lack of multi-image grounding datasets by introducing a novel task, AttCoSeg (Attribute-level Co-Segmentation), which boosts the model's reasoning and grounding capability over multiple input images.","arXiv.org",2023,"Shraman Pramanick,Guangxing Han,Rui Hou,Sayan Nag,Ser-nam Lim,Nicolas Ballas,Qifan Wang,Rama Chellappa,Amjad Almahairi",0,126,0
"4599d5af850da482f591a02a3b17d56e0d358771","https://www.semanticscholar.org/paper/4599d5af850da482f591a02a3b17d56e0d358771",2,"Plan, Posture and Go: Towards Open-World Text-to-Motion Generation","A divide-and-conquer framework named PRO-Motion, which consists of three modules as motion planner, posture-diffuser and go-diffuser, which demonstrates its capability of generating diverse and realistic motions from complex open-world prompts.","arXiv.org",2023,"Jinpeng Liu,Wen-Dao Dai,Chunyu Wang,Yiji Cheng,Yansong Tang,Xin Tong",1,103,0
"9eab4104973f5de650544729a4a69d84c594da92","https://www.semanticscholar.org/paper/9eab4104973f5de650544729a4a69d84c594da92",2,"A Vision Check-up for Language Models","Although LLM-generated images do not look like natural images, results on image generation and the ability of models to correct these generated images indicate that precise modeling of strings can teach language models about numerous aspects of the visual world.","arXiv.org",2024,"Pratyusha Sharma,Tamar Rott Shaham,Manel Baradad,Stephanie Fu,Adrian Rodriguez-Munoz,Shivam Duggal,Phillip Isola,Antonio Torralba",0,56,0
"46f64681d7a0c7f80303bebb0d62a3b7acbcdcd9","https://www.semanticscholar.org/paper/46f64681d7a0c7f80303bebb0d62a3b7acbcdcd9",2,"An Improved Baseline for Reasoning Segmentation with Large Language Model","LISA++ is introduced, an update to the existing LISA model, focusing on improving core functionalities while keeping the base architecture intact, and its adaptability and improved features highlight the versatility of the mask-as-embedding paradigm proposed by LISA and the potential as a foundational model for diverse applications.","arXiv.org",2023,"Senqiao Yang,Tianyuan Qu,Xin Lai,Zhuotao Tian,Bohao Peng,Shu Liu,Jiaya Jia",0,42,0
"ff61aef2fef3a235bfaa123158a990c4f5f27d1a","https://www.semanticscholar.org/paper/ff61aef2fef3a235bfaa123158a990c4f5f27d1a",2,"Small LLMs Are Weak Tool Learners: A Multi-LLM Agent","Evaluation across various tool-use benchmarks illustrates that the proposed multi-LLM framework surpasses the traditional single-LLM approach, highlighting its efficacy and advantages in tool learning.","arXiv.org",2024,"Weizhou Shen,Chenliang Li,Hongzhan Chen,Ming Yan,Xiaojun Quan,Hehong Chen,Ji Zhang,Fei Huang",4,44,0
"4f2a56102bcbf0fe79379c4c27daecbccfb35a26","https://www.semanticscholar.org/paper/4f2a56102bcbf0fe79379c4c27daecbccfb35a26",2,"MLLM-Tool: A Multimodal Large Language Model For Tool Agent Learning","This paper proposes MLLM-Tool, a system incorporating open-source LLMs and multi-modal encoders so that the learnt LLMs can be conscious of multi-modal input instruction and then select the function-matched tool correctly.","arXiv.org",2024,"Chenyu Wang,Weixin Luo,Qianyu Chen,Haonan Mai,Jindi Guo,Sixun Dong,Xiaohua Xuan,Zhengxin Li,Lin Ma,Shenghua Gao",3,44,0
"c5db6c2726911b72d534f97bd4d1ed63f6431340","https://www.semanticscholar.org/paper/c5db6c2726911b72d534f97bd4d1ed63f6431340",2,"Mobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual Perception","Different from previous solutions that rely on XML files of Apps or mobile system metadata, Mobile-Agent allows for greater adaptability across diverse mobile operating environments in a vision-centric way, thereby eliminating the necessity for system-specific customizations.","arXiv.org",2024,"Junyang Wang,Haiyang Xu,Jiabo Ye,Mingshi Yan,Weizhou Shen,Ji Zhang,Fei Huang,Jitao Sang",5,25,0
"eaad6e351ab7ddb5a31bce3c5fe8bf38cd08c7f2","https://www.semanticscholar.org/paper/eaad6e351ab7ddb5a31bce3c5fe8bf38cd08c7f2",2,"Multimodal Query Suggestion with Multi-Agent Reinforcement Learning from Human Feedback","A novel Multimodal Query Suggestion (MMQS) task, which aims to generate query suggestions based on user query images to improve the intentionality and diversity of search results, is introduced and the RL4Sugg framework is presented, leveraging the power of Large Language Models with Multi-Agent Reinforcement Learning from Human Feedback to optimize the generation process.","arXiv.org",2024,"Zheng Wang,Bingzheng Gan,Wei Shi",0,52,0
"710b1e23b09e0b826f9d47e7cc23b5f4c0808c7e","https://www.semanticscholar.org/paper/710b1e23b09e0b826f9d47e7cc23b5f4c0808c7e",2,"Multi-modal preference alignment remedies regression of visual instruction tuning on language model","A distillation-based multi-modal alignment model with fine-grained annotations on a small dataset that reconciles the textual and visual performance of MLLMs, restoring and boosting language capability after visual instruction tuning is proposed.","arXiv.org",2024,"Shengzhi Li,Rongyu Lin,Shichao Pei",0,29,0
"83d201d503b863fec7d1225f00a141e722e03f17","https://www.semanticscholar.org/paper/83d201d503b863fec7d1225f00a141e722e03f17",2,"Using Left and Right Brains Together: Towards Vision and Language Planning","This work introduces a novel vision-language planning framework to perform concurrent visual and language planning for tasks with inputs of any form, and demonstrates the superior performance of this approach.","arXiv.org",2024,"Jun Cen,Chenfei Wu,Xiao Liu,Sheng-Siang Yin,Yixuan Pei,Jinglong Yang,Qifeng Chen,Nan Duan,Jianguo Zhang",0,49,0
"ded3266d047b36a963c1324aa9f98705d598bdcf","https://www.semanticscholar.org/paper/ded3266d047b36a963c1324aa9f98705d598bdcf",2,"From Summary to Action: Enhancing Large Language Models for Complex Tasks with Open World APIs","A novel tool invocation pipeline designed to control massive real-world APIs, which mirrors the human task-solving process, addressing complicated real-life user queries and highlights Sum2Act's effectiveness in enhancing LLMs for complex real-world tasks.","",2024,"Yulong Liu,Yunlong Yuan,Chunwei Wang,Jianhua Han,Yongqiang Ma,Li Zhang,Nanning Zheng,Hang Xu",0,36,0
"ac8089bb7944090cf1de5df25aadf5e6356f3040","https://www.semanticscholar.org/paper/ac8089bb7944090cf1de5df25aadf5e6356f3040",2,"TempCompass: Do Video LLMs Really Understand Videos?","The TempCompass benchmark, which introduces a diversity of temporal aspects and task formats, and comprehensively evaluate 8 state-of-the-art (SOTA) Video LLMs and 3 Image LLMs, reveals the discerning fact that these models exhibit notably poor temporal perception ability.","",2024,"Yuanxin Liu,Shicheng Li,Yi Liu,Yuxiang Wang,Shuhuai Ren,Lei Li,Sishuo Chen,Xu Sun,Lu Hou",0,22,0
"23684a07517870cffd1f97fafbaae16ba22bd2b7","https://www.semanticscholar.org/paper/23684a07517870cffd1f97fafbaae16ba22bd2b7",2,"Large AI Models in Health Informatics: Applications, Challenges, and the Future","Seven key sectors in which large AI models are applicable and might have substantial influence, including: 1) bioinformatics; 2) medical diagnosis; 3) medical imaging; 4) medical informatics; 5) medical education; 6) public health; and 7) medical robotics are identified.","IEEE journal of biomedical and health informatics",2023,"Jianing Qiu,Lin Li,Jiankai Sun,Jiachuan Peng,Peilun Shi,Rui Zhang,Yinzhao Dong,Kyle Lam,F. P. Lo,Bo Xiao,Wu Yuan,Dong Xu,Benny P. L. Lo",34,347,1
"cb5f53cd7d43ac6f48abf4dde6fc58e53a083b0e","https://www.semanticscholar.org/paper/cb5f53cd7d43ac6f48abf4dde6fc58e53a083b0e",2,"Accelerating the integration of ChatGPT and other large‐scale AI models into biomedical research and healthcare","","MedComm – Future Medicine",2023,"Ding‐Qiao Wang,Long‐Yu Feng,Jin‐Guo Ye,Jin‐Gen Zou,Yingfeng Zheng",22,99,0
"51b169701290cd129e0781fc9f3a9918604c89b5","https://www.semanticscholar.org/paper/51b169701290cd129e0781fc9f3a9918604c89b5",2,"Improving accuracy of GPT-3/4 results on biomedical data using a retrieval-augmented language model","It is suggested that RetA models, supplemented with domain-specific corpora, may outperform general-purpose LLMs in accuracy and relevance within specific domains.","arXiv.org",2023,"D. Soong,S. Sridhar,Han Si,J. Wagner,Ana Caroline Costa S'a,Christina Y. Yu,Kubra Karagoz,Meijian Guan,Hisham K Hamadeh,Brandon Higgs",9,54,0
"06091944b864d6dc473cab63321a95fb9c4067cc","https://www.semanticscholar.org/paper/06091944b864d6dc473cab63321a95fb9c4067cc",2,"ChatCAD+: Towards a Universal and Reliable Interactive CAD using LLMs","ChatCAD+, which is designed to be universal and reliable, is introduced, capable of handling medical images from diverse domains and leveraging up-to-date information from reputable medical websites to provide reliable medical advice.","arXiv.org",2023,"Zihao Zhao,Sheng Wang,Jinchen Gu,Yitao Zhu,Lanzhuju Mei,Zixu Zhuang,Zhiming Cui,Qian Wang,Dinggang Shen",10,50,1
"6294f078e79828cac21e717813e8f3d02b18a97c","https://www.semanticscholar.org/paper/6294f078e79828cac21e717813e8f3d02b18a97c",2,"The importance of resource awareness in artificial intelligence for healthcare","It is highlighted that there are resource sustainability issues in AI/ML for healthcare and various algorithm/system innovations that will help address these issues are presented.","Nature Machine Intelligence",2023,"Zhenge Jia,Jianxu Chen,Xiaowei Xu,J. Kheir,Jingtong Hu,Han Xiao,Sui Peng,X. Hu,Danny Chen,Yi Shi",7,161,0
"c9dbdae8146b9f97e254f5d26fd6efde96eaa703","https://www.semanticscholar.org/paper/c9dbdae8146b9f97e254f5d26fd6efde96eaa703",2,"Med-Flamingo: a Multimodal Medical Few-shot Learner","Med-Flamingo improves performance in generative medical VQA by up to 20\% in clinician's rating and firstly enables multimodal medical few-shot adaptations, such as rationale generation, as well as releasing the model, code, and evaluation app.","ML4H@NeurIPS",2023,"Michael Moor,Qian Huang,Shirley Wu,Michihiro Yasunaga,C. Zakka,Yashodhara Dalmia,E. Reis,P. Rajpurkar,J. Leskovec",36,34,6
"7e55d8701785818776323b4147cb13354c820469","https://www.semanticscholar.org/paper/7e55d8701785818776323b4147cb13354c820469",2,"PaperQA: Retrieval-Augmented Generative Agent for Scientific Research","PaperQA is an agent that performs information retrieval across full-text scientific articles, assesses the relevance of sources and passages, and uses RAG to provide answers, and exceeds performance of existing LLMs and LLM agents on current science QA benchmarks.","arXiv.org",2023,"Jakub L'ala,Odhran O'Donoghue,Aleksandar Shtedritski,Sam Cox,Samuel G. Rodriques,Andrew D. White",6,77,1
"93886752191db25efd096a65af7b09df5c0a64e0","https://www.semanticscholar.org/paper/93886752191db25efd096a65af7b09df5c0a64e0",2,"Data-Centric Foundation Models in Computational Healthcare: A Survey","A wide range of data-centric approaches in the FM era (from model pre-training to inference) towards improving the healthcare workflow are investigated and a promising outlook of FM-based analytics to enhance the performance of patient outcome and clinical workflow in the evolving landscape of healthcare and medicine is offered.","arXiv.org",2024,"Yunkun Zhang,Jin Gao,Zheling Tan,Lingfeng Zhou,Kexin Ding,Mu Zhou,Shaoting Zhang,Dequan Wang",3,316,0
"18d9b13e3383d98c181f4d7a2b3ca1503ed707a0","https://www.semanticscholar.org/paper/18d9b13e3383d98c181f4d7a2b3ca1503ed707a0",2,"No-boundary thinking for artificial intelligence in bioinformatics and education","This session addressed various areas of AI in an open discussion and raised some perspectives on how popular tools like ChatGPT can be integrated into bioinformatics, communicating with scientists in different fields to properly utilize the potential of these algorithms, and how to continue educational outreach to further interest of data science and informatics to the next-generation of scientists.","Frontiers in Bioinformatics",2024,"Prajay Patel,Nisha Pillai,Inimary T. Toby",0,26,0
"20fcc01d12a50f1da2af71d85f0a269b3ba48b77","https://www.semanticscholar.org/paper/20fcc01d12a50f1da2af71d85f0a269b3ba48b77",2,"LMEye: An Interactive Perception Network for Large Language Models","LMEye, a human-like eye with a play-and-plug interactive perception network, designed to enable dynamic interaction between LLMs and external vision information, is introduced, demonstrating that it significantly improves the zero-shot performance on various multimodal tasks compared to previous methods, with less parameters.","arXiv.org",2023,"Yunxin Li,Baotian Hu,Xinyu Chen,Lin Ma,M. Zhang",12,57,1
"8efc20988021ce3b4b05dd44b13e27260ee9b99b","https://www.semanticscholar.org/paper/8efc20988021ce3b4b05dd44b13e27260ee9b99b",2,"Investigating Prompting Techniques for Zero- and Few-Shot Visual Question Answering","Light is shed on the intricacies of prompting strategies in VLMs for VQA, emphasizing the synergistic use of captions, templates, and pre-processing to enhance model efficacy.","arXiv.org",2023,"Rabiul Awal,Le Zhang,Aishwarya Agrawal",2,59,0
"efc694164312006c543ef745611348ef64e68dda","https://www.semanticscholar.org/paper/efc694164312006c543ef745611348ef64e68dda",2,"Towards Language Models That Can See: Computer Vision Through the LENS of Natural Language","This work proposes LENS, a modular approach for tackling computer vision problems by leveraging the power of large language models (LLMs) with a language model to reason over outputs from a set of independent and highly descriptive vision modules that provide exhaustive information about an image.","arXiv.org",2023,"William Berrios,Gautam Mittal,Tristan Thrush,Douwe Kiela,Amanpreet Singh",24,64,2
"ac2e5bf716aed246ca8914a6816ef73e00286099","https://www.semanticscholar.org/paper/ac2e5bf716aed246ca8914a6816ef73e00286099",2,"Beyond Segmentation: Road Network Generation with Multi-Modal LLMs","An innovative approach to road network generation through the utilization of a multi-modal Large Language Model (LLM), specifically designed to process aerial images of road layouts and produce detailed, navigable road networks within the input images.","arXiv.org",2023,"Sumedh Rasal,Sanjay K. Boddhu",2,34,0
"beb3e8acd816bac1a5b7fccfd073f79048877e33","https://www.semanticscholar.org/paper/beb3e8acd816bac1a5b7fccfd073f79048877e33",2,"Frozen Transformers in Language Models Are Effective Visual Encoder Layers","This paper reveals that large language models (LLMs), despite being trained solely on textual data, are surprisingly strong encoders for purely visual tasks in the absence of language and proposes the information filtering hypothesis to explain the effectiveness of pre-trained LLMs in visual encoding.","arXiv.org",2023,"Ziqi Pang,Ziyang Xie,Yunze Man,Yu-Xiong Wang",2,83,0
"96d104dfe727f78a35faaafe81481f3672b485ee","https://www.semanticscholar.org/paper/96d104dfe727f78a35faaafe81481f3672b485ee",2,"Large Language Models are Visual Reasoning Coordinators","This work proposes Cola, a novel paradigm that coordinates multiple VLMs for visual reasoning by facilitating natural language communication that leverages their distinct and complementary capabilities, and validate that a coordinator LLM indeed comprehends the instruction prompts as well as the separate functionalities of VL Ms and coordinates them to enable impressive visual reasoning capabilities.","Neural Information Processing Systems",2023,"Liangyu Chen,Boyi Li,Sheng Shen,Jingkang Yang,Chunyuan Li,Kurt Keutzer,Trevor Darrell,Ziwei Liu",16,125,0
"29b3ce4de9dd9d784ca1d876957950f4b2d3796a","https://www.semanticscholar.org/paper/29b3ce4de9dd9d784ca1d876957950f4b2d3796a",2,"Towards Perceiving Small Visual Details in Zero-shot Visual Question Answering with Multimodal LLMs","It is shown that MLLMs zero-shot accuracy in answering visual questions is very sensitive to the size of the visual subject of the question, declining up to 46% with size, and that visual cropping is a promising direction to improve their zero-shot performance.","",2023,"Jiarui Zhang,Mahyar Khayatkhoei,P. Chhikara,Filip Ilievski",0,38,0
"af5f256e9771bf9cd02451195e3a7ac693fde3ed","https://www.semanticscholar.org/paper/af5f256e9771bf9cd02451195e3a7ac693fde3ed",2,"Exploring the Reasoning Abilities of Multimodal Large Language Models (MLLMs): A Comprehensive Survey on Emerging Trends in Multimodal Reasoning","This survey comprehensively review the existing evaluation protocols of multimodal reasoning, categorize and illustrate the frontiers of MLLMs, introduce recent trends in applications of MLLMs on reasoning-intensive tasks, and finally discuss current practices and future directions.","arXiv.org",2024,"Yiqi Wang,Wentao Chen,Xiaotian Han,Xudong Lin,Haiteng Zhao,Yongfei Liu,Bohan Zhai,Jianbo Yuan,Quanzeng You,Hongxia Yang",2,178,0
"fed3376de52d70ba83050182e79466dddde45746","https://www.semanticscholar.org/paper/fed3376de52d70ba83050182e79466dddde45746",2,"On the Safety Concerns of Deploying LLMs/VLMs in Robotics: Highlighting the Risks and Vulnerabilities","It is shown that it is easy to manipulate or misguide the robot's actions, leading to safety hazards, and the critical need for robust countermeasures to ensure the safe and reliable deployment of the advanced LLM/VLM-based robotic systems.","arXiv.org",2024,"Xiyang Wu,Ruiqi Xian,Tianrui Guan,Jing Liang,Souradip Chakraborty,Fuxiao Liu,Brian M. Sadler,Dinesh Manocha,A. S. Bedi",0,53,0
"86188727c4d4f3eb064ae7ff0d9a3483b4ef47c1","https://www.semanticscholar.org/paper/86188727c4d4f3eb064ae7ff0d9a3483b4ef47c1",2,"Typographic Attacks in Large Multimodal Models Can be Alleviated by More Informative Prompts","It is proved that CLIP's performance of zero-shot classification on typo-ridden images can be significantly improved by providing more informative texts to match images, and it is proved that LMMs can utilize more informative prompts to leverage information in embeddings to differentiate between visual content and typos.","",2024,"Hao Cheng,Erjia Xiao,Renjing Xu",0,46,0
"17ca48ad1b944c897863f04ba9ffa72674dce1ce","https://www.semanticscholar.org/paper/17ca48ad1b944c897863f04ba9ffa72674dce1ce",2,"Parallel multi-head attention and term-weighted question embedding for medical visual question answering","The proposed MaMVQA model achieved significantly increased accuracy in predicting answers to both close-ended and open-ended questions and outperforms previous state-of-the-art methods in terms of accuracy while requiring no external data to train the model.","Multimedia tools and applications",2023,"Sruthy Manmadhan,Binsu C. Kovoor",2,59,0
"420087f314633a381e61e6c5cd73ccc2070a749e","https://www.semanticscholar.org/paper/420087f314633a381e61e6c5cd73ccc2070a749e",2,"PeFoMed: Parameter Efficient Fine-tuning on Multimodal Large Language Models for Medical Visual Question Answering","A parameter efficient framework for fine-tuning MLLM specifically tailored to Med-VQA applications is proposed and empirically validate it on a public benchmark dataset, revealing that it outperforms the GPT-4v model by a significant margin of 26% absolute accuracy on closed-ended questions.","arXiv.org",2024,"Jinlong He,Pengfei Li,Gang Liu,Zixu Zhao,Shenjun Zhong",0,43,0
"6ed96d6822a06ad9a735bc09e301bf41df61c534","https://www.semanticscholar.org/paper/6ed96d6822a06ad9a735bc09e301bf41df61c534",2,"CheXagent: Towards a Foundation Model for Chest X-Ray Interpretation","This work designs a clinical large language model (LLM) for parsing radiology reports, a vision encoder for representing CXR images, and a network to bridge the vision and language modalities and introduces a novel benchmark designed to systematically evaluate FMs across 8 clinically-relevant CXR interpretation tasks.","arXiv.org",2024,"Zhihong Chen,Maya Varma,Jean-Benoit Delbrouck,Magdalini Paschali,Louis Blankemeier,Dave Van Veen,Jeya Maria Jose Valanarasu,Alaa Youssef,Joseph Paul Cohen,Eduardo Pontes Reis,Emily B. Tsai,Andrew Johnston,Cameron Olsen,Tanishq Mathew Abraham,S. Gatidis,Akshay S Chaudhari,Curtis P. Langlotz",1,97,0
"357fc385caf2e5b9898c9140fa3ac9955e6bb3c6","https://www.semanticscholar.org/paper/357fc385caf2e5b9898c9140fa3ac9955e6bb3c6",2,"TeamS at VQA-Med 2021: BBN-Orchestra for Long-tailed Medical Visual Question Answering","The proposed BBN-Orchestra is an ensemble of bilateral-branch networks (BBN) and successfully reduces overfitting to train and validation data in addition to effectively modeling the imbalanced long-tailed image distribution.","Conference and Labs of the Evaluation Forum",2021,"Sedigheh Eslami,Gerard de Melo,C. Meinel",7,16,1
"31a7d8c4a5ab6bab522494b57270249105c8748e","https://www.semanticscholar.org/paper/31a7d8c4a5ab6bab522494b57270249105c8748e",2,"BiomedGPT: A Unified and Generalist Biomedical Generative Pre-trained Transformer for Vision, Language, and Multimodal Tasks","A unified and generalist BiomedGPT model, which leverages self-supervision on large and diverse datasets to accept multi-modal inputs and perform a range of downstream tasks, which presents a significant step forward in developing unified and generalist models for biomedicine.","arXiv.org",2023,"Kai Zhang,Jun Yu,Zhilin Yan,Yixin Liu,Eashan Adhikarla,S. Fu,Xun Chen,Chen Chen,Yuyin Zhou,Xiang Li,Lifang He,B. Davison,Quanzheng Li,Yong Chen,Hongfang Liu,Lichao Sun",39,147,2
"61c0b6a5e7aea48a1376b61a4a737137d602b242","https://www.semanticscholar.org/paper/61c0b6a5e7aea48a1376b61a4a737137d602b242",2,"PubMedCLIP: How Much Does CLIP Benefit Visual Question Answering in the Medical Domain?","This work presents PubMedCLIP, a fine-tuned version of CLIP for the medical domain based on PubMed articles that achieves superior results improving the overall accuracy up to 3% in comparison to the state-of-the-art Model-Agnostic Meta-Learning (MAML) networks pre-trained only on visual data.","Findings",2023,"Sedigheh Eslami,C. Meinel,Gerard de Melo",17,40,2
"b88f6aa65a4e1faf963494a76d28cc12112c9543","https://www.semanticscholar.org/paper/b88f6aa65a4e1faf963494a76d28cc12112c9543",2,"A Critical Analysis of Benchmarks, Techniques, and Models in Medical Visual Question Answering","The statistical analysis of medical VQA from 2018 to 2023 and individual yearly analyses reveals consistent preferences for LSTM and VGGNet, except in 2018 when ResNet was more commonly used.","IEEE Access",2023,"Suheer Al-Hadhrami,M. Menai,Saad Al-ahmadi,Ahmed Alnafessah",0,232,0
"3c83f80f06633ff4598d33c2959f8e4cdcad3e93","https://www.semanticscholar.org/paper/3c83f80f06633ff4598d33c2959f8e4cdcad3e93",2,"Cross-Modal Self-Attention with Multi-Task Pre-Training for Medical Visual Question Answering","This work reformulates image feature pre-training as a multi-task learning paradigm and witness its extraordinary superiority, forcing it to take into account the applicability of features for the specific image comprehension task.","International Conference on Multimedia Retrieval",2021,"Haifan Gong,Guanqi Chen,Sishuo Liu,Yizhou Yu,Guanbin Li",47,32,2
"e34b699cef0a711a8cb9c39ecea20ac2df1578f5","https://www.semanticscholar.org/paper/e34b699cef0a711a8cb9c39ecea20ac2df1578f5",2,"Medical Visual Question Answering: A Survey","This survey collects and discusses the publicly available medical VQA datasets up-to-date about the data source, data quantity, and task feature, and summarizes the approaches used in medical V QA tasks.","Artif. Intell. Medicine",2021,"Zhihong Lin,Donghao Zhang,Qingyi Tao,Danli Shi,Gholamreza Haffari,Qi Wu,M. He,Z. Ge",34,114,3
"4ef3d9e492479e28fa57d107e52acc6a0c803de2","https://www.semanticscholar.org/paper/4ef3d9e492479e28fa57d107e52acc6a0c803de2",2,"Does CLIP Benefit Visual Question Answering in the Medical Domain as Much as it Does in the General Domain?","A fine-tuned version of CLIP for the medical domain based on PubMed articles is presented, which leads to noticeable improvements for MedVQA and fundamental performance differences of VQA in general versus medical domains are witnessed.","arXiv.org",2021,"Sedigheh Eslami,Gerard de Melo,C. Meinel",59,31,12
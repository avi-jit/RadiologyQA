"id","url","score","title","summary","venue","year","authors","citationCount","referenceCount","influentialCitationCount"
"7cf64070fd3d7e53d80f260c10e6bd7018d580e1","https://www.semanticscholar.org/paper/7cf64070fd3d7e53d80f260c10e6bd7018d580e1",5,"IdealGPT: Iteratively Decomposing Vision and Language Reasoning via Large Language Models","The IdealGPT framework is presented, a framework that iteratively decomposes VL reasoning using large language models (LLMs) and outperforms the best existing GPT-4-like models by an absolute 10% on VCR and 15% on SNLI-VE.","Conference on Empirical Methods in Natural Language Processing",2023,"Haoxuan You,Rui Sun,Zhecan Wang,Long Chen,Gengyu Wang,Hammad A. Ayyubi,Kai-Wei Chang,Shih-Fu Chang",15,51,3
"fd755dc7b5b206c17fd953db04e1c888d45b6e4e","https://www.semanticscholar.org/paper/fd755dc7b5b206c17fd953db04e1c888d45b6e4e",5,"LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark","This work extends the research of MLLMs to point clouds and presents the LAMM-Dataset and LAMm-Benchmark for 2D image and 3D point cloud understanding and establishes an extensible framework to facilitate the extension of M LLMs to additional modalities.","Neural Information Processing Systems",2023,"Zhen-fei Yin,Jiong Wang,Jianjian Cao,Zhelun Shi,Dingning Liu,Mukai Li,Lu Sheng,Lei Bai,Xiaoshui Huang,Zhiyong Wang,Wanli Ouyang,Jing Shao",42,99,8
"966852963a88a28786b798c91b6662d6e501e590","https://www.semanticscholar.org/paper/966852963a88a28786b798c91b6662d6e501e590",5,"AssistGPT: A General Multi-modal Assistant that can Plan, Execute, Inspect, and Learn","A multi-modal AI assistant with an interleaved code and language reasoning approach called Plan, Execute, Inspect, and Learn (PEIL) to integrate LLMs with various tools, and a Learner is designed to enable the model to autonomously explore and discover the optimal solution.","arXiv.org",2023,"Difei Gao,Lei Ji,Luowei Zhou,Kevin Lin,Joya Chen,Zihan Fan,Mike Zheng Shou",25,73,3
"ca31b8584b6c022ef15ddfe994fe361e002b7729","https://www.semanticscholar.org/paper/ca31b8584b6c022ef15ddfe994fe361e002b7729",5,"A Comprehensive Overview of Large Language Models","A self-contained comprehensive overview of the existing literature on a broad range of LLM-related concepts discusses relevant background concepts along with covering the advanced topics at the frontier of research in LLMs.","arXiv.org",2023,"Humza Naveed,Asad Ullah Khan,Shi Qiu,Muhammad Saqib,Saeed Anwar,Muhammad Usman,Nick Barnes,A. Mian",55,447,2
"f34b6b4f75fb4e6f2c5654ee13fb2479c6170b3a","https://www.semanticscholar.org/paper/f34b6b4f75fb4e6f2c5654ee13fb2479c6170b3a",5,"Kosmos-2.5: A Multimodal Literate Model","Kosmos-2.5 can be readily adapted for any text-intensive image understanding task with different prompts through supervised fine-tuning, making it a general-purpose tool for real-world applications involving text-rich images and paves the way for the future scaling of multimodal large language models.","arXiv.org",2023,"Tengchao Lv,Yupan Huang,Jingye Chen,Lei Cui,Shuming Ma,Ya-Chi Chang,Shaohan Huang,Wenhui Wang,Li Dong,Weiyao Luo,Shaoxiang Wu,Guoxin Wang,Cha Zhang,Furu Wei",12,84,0
"092245d86b77181c36f972b1b7a17a59cd989c4a","https://www.semanticscholar.org/paper/092245d86b77181c36f972b1b7a17a59cd989c4a",5,"Guiding Instruction-based Image Editing via Multimodal Large Language Models","This work investigates how MLLMs facilitate edit instructions and presents MLLM-Guided Image Editing (MGIE), which learns to derive expressive instructions and provides explicit guidance and can lead to a notable improvement in automatic metrics and human evaluation while maintaining competitive inference efficiency.","arXiv.org",2023,"Tsu-Jui Fu,Wenze Hu,Xianzhi Du,William Yang Wang,Yinfei Yang,Zhe Gan",9,67,0
"107fb6eec2febbae12db29bf3e311aaf5680027c","https://www.semanticscholar.org/paper/107fb6eec2febbae12db29bf3e311aaf5680027c",5,"Video-LLaVA: Learning United Visual Representation by Alignment Before Projection","This work unify visual representation into the language feature space to advance the foundational LLM towards a unified LVLM, and establishes a simple but robust LVLM baseline, Video-LLaVA, which learns from a mixed dataset of images and videos, mutually enhancing each other.","arXiv.org",2023,"Bin Lin,Bin Zhu,Yang Ye,Munan Ning,Peng Jin,Li Yuan",15,54,1
"6d2ab31aa75468f5458b9d96192c3f4a28f55d73","https://www.semanticscholar.org/paper/6d2ab31aa75468f5458b9d96192c3f4a28f55d73",5,"DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving","DriveMLM is introduced, an LLM-based AD framework that can perform close-loop autonomous driving in realistic simulators and can plug-and-play in existing AD systems such as Apollo for close-loop driving.","arXiv.org",2023,"Wenhai Wang,Jiangwei Xie,ChuanYang Hu,Haoming Zou,Jianan Fan,Wenwen Tong,Yang Wen,Silei Wu,Hanming Deng,Zhiqi Li,Hao Tian,Lewei Lu,Xizhou Zhu,Xiaogang Wang,Yu Qiao,Jifeng Dai",6,80,2
"6a33e58ef961a3a0a5657518b2be86395eb7c8d0","https://www.semanticscholar.org/paper/6a33e58ef961a3a0a5657518b2be86395eb7c8d0",5,"InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks","A large-scale vision-language foundation model (InternVL), which scales up the vision foundation model to 6 billion parameters and progressively aligns it with the LLM, using web-scale image-text data from various sources is designed.","arXiv.org",2023,"Zhe Chen,Jiannan Wu,Wenhai Wang,Weijie Su,Guo Chen,Sen Xing,Zhong Muyan,Qinglong Zhang,Xizhou Zhu,Lewei Lu,Bin Li,Ping Luo,Tong Lu,Yu Qiao,Jifeng Dai",7,185,3
"6bdfffbf92d01c8b543088d40d46233610e469a8","https://www.semanticscholar.org/paper/6bdfffbf92d01c8b543088d40d46233610e469a8",5,"CLIP in Medical Imaging: A Comprehensive Survey","This survey offers an in-depth exploration of the CLIP paradigm within the domain of medical imaging, regarding both refined CLIP pre-training and CLIP-driven applications, and investigates the adaptation of CLIP pre-training in the medical domain.","arXiv.org",2023,"Zihao Zhao,Yuxiao Liu,Han Wu,Yonghao Li,Sheng Wang,L. Teng,Disheng Liu,Xiang Li,Zhiming Cui,Qian Wang,Dinggang Shen",3,217,0
"06091944b864d6dc473cab63321a95fb9c4067cc","https://www.semanticscholar.org/paper/06091944b864d6dc473cab63321a95fb9c4067cc",5,"ChatCAD+: Towards a Universal and Reliable Interactive CAD using LLMs","ChatCAD+, which is designed to be universal and reliable, is introduced, capable of handling medical images from diverse domains and leveraging up-to-date information from reputable medical websites to provide reliable medical advice.","arXiv.org",2023,"Zihao Zhao,Sheng Wang,Jinchen Gu,Yitao Zhu,Lanzhuju Mei,Zixu Zhuang,Zhiming Cui,Qian Wang,Dinggang Shen",10,50,1
"f4793adffd6f67ffcb93ccfc5672ab301b8a2b96","https://www.semanticscholar.org/paper/f4793adffd6f67ffcb93ccfc5672ab301b8a2b96",5,"PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering","This paper proposes a generative-based model for medical visual understanding by aligning visual information from a pre-trained vision encoder with a large language model, and establishes a scalable pipeline to construct a large-scale medical visual question-answering dataset.","arXiv.org",2023,"Xiaoman Zhang,Chaoyi Wu,Ziheng Zhao,Weixiong Lin,Ya Zhang,Yanfeng Wang,Weidi Xie",35,40,3
"baa1dc079d98ca76b0173c8d653fed759fd0a371","https://www.semanticscholar.org/paper/baa1dc079d98ca76b0173c8d653fed759fd0a371",5,"A scoping review on multimodal deep learning in biomedical images and texts","This study reviewed the current uses of multimodal deep learning on five tasks: report generation, Visual question answering, Cross-modal retrieval, computer-aided diagnosis, and Semantic segmentation, and highlighted the diverse applications and potential of MDL.","Journal of Biomedical Informatics",2023,"Zhaoyi Sun,Mingquan Lin,Qingqing Zhu,Qianqian Xie,Fei Wang,Zhiyong Lu,Yifan Peng",5,147,0
"d48fa3ed73817563130ef217d85011ce1fbe7470","https://www.semanticscholar.org/paper/d48fa3ed73817563130ef217d85011ce1fbe7470",5,"BESTMVQA: A Benchmark Evaluation System for Medical Visual Question Answering","A Benchmark Evaluation SysTem for Medical Visual Question Answering, denoted by BESTMVQA, is developed, which provides a useful tool for users to automatically build Med-V QA datasets, which helps overcoming the data insufficient problem.","arXiv.org",2023,"Xiaojie Hong,Zixin Song,Liangzhi Li,Xiaoli Wang,Feiyan Liu",0,23,0
"570079bbdd8758dfe865097e05719313c9c1301a","https://www.semanticscholar.org/paper/570079bbdd8758dfe865097e05719313c9c1301a",4,"LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model","This work augments LLaMA-Adapter by unlocking more learnable parameters and proposes an early fusion strategy to feed visual tokens only into the early LLM layers, contributing to better visual knowledge incorporation and achieves strong multi-modal reasoning with only a small-scale image-text and instruction dataset.","arXiv.org",2023,"Peng Gao,Jiaming Han,Renrui Zhang,Ziyi Lin,Shijie Geng,Aojun Zhou,W. Zhang,Pan Lu,Conghui He,Xiangyu Yue,Hongsheng Li,Y. Qiao",211,79,29
"d6d3604f369bb0415cbe814e43ca3131323b03e2","https://www.semanticscholar.org/paper/d6d3604f369bb0415cbe814e43ca3131323b03e2",4,"Otter: A Multi-Modal Model with In-Context Instruction Tuning","Otter, a multi-modal model based on OpenFlamingo (open-sourced version of DeepMind's Flamingo), trained on MIMIC-IT and showcasing improved instruction-following ability and in-context learning is introduced.","arXiv.org",2023,"Bo Li,Yuanhan Zhang,Liangyu Chen,Jinghao Wang,Jingkang Yang,Ziwei Liu",223,37,34
"d98536f24272e258b1d399074b64284d64786099","https://www.semanticscholar.org/paper/d98536f24272e258b1d399074b64284d64786099",4,"AVIS: Autonomous Visual Information Seeking with Large Language Models","An autonomous information seeking visual question answering framework that leverages a Large Language Model to dynamically strategize the utilization of external tools and to investigate their outputs, thereby acquiring the indispensable knowledge needed to provide answers to the posed questions is proposed.","Neural Information Processing Systems",2023,"Ziniu Hu,Ahmet Iscen,Chen Sun,Kai-Wei Chang,Yizhou Sun,David A. Ross,C. Schmid,A. Fathi",9,133,1
"ebddfdc5d845a788e8062eddbbf7a335737cb99b","https://www.semanticscholar.org/paper/ebddfdc5d845a788e8062eddbbf7a335737cb99b",4,"What Matters in Training a GPT4-Style Language Model with Multimodal Inputs?","Lynx is presented, which performs the most accurate multi-modal understanding while keeping the best multi- modal generation ability compared to existing open-sourced GPT4-style models.","arXiv.org",2023,"Yan Zeng,Hanbo Zhang,Jiani Zheng,Jiangnan Xia,Guoqiang Wei,Yang Wei,Yuchen Zhang,Tao Kong",33,108,3
"6bcc6ab9c28805d4067e99b2cdc7524550fe80e1","https://www.semanticscholar.org/paper/6bcc6ab9c28805d4067e99b2cdc7524550fe80e1",4,"PointLLM: Empowering Large Language Models to Understand Point Clouds","Experimental results reveal PointLLM's superior performance over existing 2D and 3D baselines, with a notable achievement in human-evaluated object captioning tasks where it surpasses human annotators in over 50% of the samples.","arXiv.org",2023,"Runsen Xu,Xiaolong Wang,Tai Wang,Yilun Chen,Jiangmiao Pang,Dahua Lin",31,72,4
"bee68767debbdc96d6f75947e544a8be98b869e3","https://www.semanticscholar.org/paper/bee68767debbdc96d6f75947e544a8be98b869e3",4,"Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model: Explorations with GPT4-Vision and Beyond","This study introduces a new benchmark called PCA-EVAL, which evaluates embodied decision-making from the perspectives of Perception, Cognition, and Action and proposes HOLMES, a multi-agent cooperation framework that allows LLMs to leverage MLLMs and APIs to gather multimodal information for informed decision- making.","arXiv.org",2023,"Liang Chen,Yichi Zhang,Shuhuai Ren,Haozhe Zhao,Zefan Cai,Yuchi Wang,Tianyu Liu,Baobao Chang",12,60,2
"807f336176070bd3f95b82a16f125ee99b7d2c80","https://www.semanticscholar.org/paper/807f336176070bd3f95b82a16f125ee99b7d2c80",4,"Woodpecker: Hallucination Correction for Multimodal Large Language Models","Implemented in a post-remedy manner, Woodpecker can easily serve different MLLMs, while being interpretable by accessing intermediate outputs of the five stages, and shows the huge potential of this new paradigm.","arXiv.org",2023,"Shukang Yin,Chaoyou Fu,Sirui Zhao,Tong Xu,Hao Wang,Dianbo Sui,Yunhang Shen,Ke Li,Xingguo Sun,Enhong Chen",15,49,2
"5eea245cc12c55905d4df827d0c9776c5ddfa743","https://www.semanticscholar.org/paper/5eea245cc12c55905d4df827d0c9776c5ddfa743",4,"Compositional Chain-of-Thought Prompting for Large Multimodal Models","The proposed Compositional Chain-of-Thought (CCoT) approach not only improves LMM performance on several vision and language VL compositional benchmarks but also improves the performance of several popular LMMs on general multimodal benchmarks, without the need for fine-tuning or annotated ground-truth SGs.","arXiv.org",2023,"Chancharik Mitra,Brandon Huang,Trevor Darrell,Roei Herzig",4,91,2
"4f2a56102bcbf0fe79379c4c27daecbccfb35a26","https://www.semanticscholar.org/paper/4f2a56102bcbf0fe79379c4c27daecbccfb35a26",4,"MLLM-Tool: A Multimodal Large Language Model For Tool Agent Learning","This paper proposes MLLM-Tool, a system incorporating open-source LLMs and multi-modal encoders so that the learnt LLMs can be conscious of multi-modal input instruction and then select the function-matched tool correctly.","arXiv.org",2024,"Chenyu Wang,Weixin Luo,Qianyu Chen,Haonan Mai,Jindi Guo,Sixun Dong,Xiaohua Xuan,Zhengxin Li,Lin Ma,Shenghua Gao",3,44,0
"5e7274bcda47b704b6797bb14be8b7a61c047a61","https://www.semanticscholar.org/paper/5e7274bcda47b704b6797bb14be8b7a61c047a61",4,"Uncertainty-Aware Evaluation for Vision-Language Models","It is shown that models with the highest accuracy may also have the highest uncertainty, which confirms the importance of measuring it for VLMs, and a correlation between model uncertainty and its language model part is revealed.","",2024,"Vasily Kostumov,Bulat Nutfullin,Oleg Pilipenko,Eugene Ilyushin",0,64,0
"efc694164312006c543ef745611348ef64e68dda","https://www.semanticscholar.org/paper/efc694164312006c543ef745611348ef64e68dda",4,"Towards Language Models That Can See: Computer Vision Through the LENS of Natural Language","This work proposes LENS, a modular approach for tackling computer vision problems by leveraging the power of large language models (LLMs) with a language model to reason over outputs from a set of independent and highly descriptive vision modules that provide exhaustive information about an image.","arXiv.org",2023,"William Berrios,Gautam Mittal,Tristan Thrush,Douwe Kiela,Amanpreet Singh",24,64,2
"93886752191db25efd096a65af7b09df5c0a64e0","https://www.semanticscholar.org/paper/93886752191db25efd096a65af7b09df5c0a64e0",4,"Data-Centric Foundation Models in Computational Healthcare: A Survey","A wide range of data-centric approaches in the FM era (from model pre-training to inference) towards improving the healthcare workflow are investigated and a promising outlook of FM-based analytics to enhance the performance of patient outcome and clinical workflow in the evolving landscape of healthcare and medicine is offered.","arXiv.org",2024,"Yunkun Zhang,Jin Gao,Zheling Tan,Lingfeng Zhou,Kexin Ding,Mu Zhou,Shaoting Zhang,Dequan Wang",3,316,0
"20fcc01d12a50f1da2af71d85f0a269b3ba48b77","https://www.semanticscholar.org/paper/20fcc01d12a50f1da2af71d85f0a269b3ba48b77",4,"LMEye: An Interactive Perception Network for Large Language Models","LMEye, a human-like eye with a play-and-plug interactive perception network, designed to enable dynamic interaction between LLMs and external vision information, is introduced, demonstrating that it significantly improves the zero-shot performance on various multimodal tasks compared to previous methods, with less parameters.","arXiv.org",2023,"Yunxin Li,Baotian Hu,Xinyu Chen,Lin Ma,M. Zhang",11,57,1
"8badb0587fef2ffc078b0cec549eb8ec96ed3ad4","https://www.semanticscholar.org/paper/8badb0587fef2ffc078b0cec549eb8ec96ed3ad4",4,"Self-Chained Image-Language Model for Video Localization and Question Answering","Self-Chained Video Localization-Answering (SeViLA), a novel framework that leverages a single image-language model (BLIP-2) to tackle both temporal keyframe localization and QA on videos, and achieves the state-of-the-art in both fine-tuning and zero-shot settings.","Neural Information Processing Systems",2023,"Shoubin Yu,Jaemin Cho,Prateek Yadav,Mohit Bansal",27,97,6
"659a12d71d8709c132ccd9ccd235f0024cae0239","https://www.semanticscholar.org/paper/659a12d71d8709c132ccd9ccd235f0024cae0239",4,"The All-Seeing Project: Towards Panoptic Visual Recognition and Understanding of the Open World","The All-Seeing model (ASM), a unified framework for panoptic visual recognition and understanding, is developed with open-ended language prompts and locations, which allows it to generalize to various vision and language tasks with remarkable zero-shot performance.","arXiv.org",2023,"Weiyun Wang,Min Shi,Qingyun Li,Wen Wang,Zhenhang Huang,Linjie Xing,Zhe Chen,Hao Li,Xizhou Zhu,Zhiguo Cao,Yushi Chen,Tong Lu,Jifeng Dai,Y. Qiao",19,115,4
"b1721374889899950994f67029fe899de257c140","https://www.semanticscholar.org/paper/b1721374889899950994f67029fe899de257c140",4,"A Foundational Multimodal Vision Language AI Assistant for Human Pathology","PathChat is presented, a vision-language generalist AI assistant for human pathology using an in-house developed foundational vision encoder pretrained on 100 million histology images from over 100,000 patient cases and 1.18 million pathology image-caption pairs.","arXiv.org",2023,"Ming Y. Lu,Bowen Chen,Drew F. K. Williamson,Richard J. Chen,Kenji Ikamura,Georg Gerber,Ivy Liang,L. Le,Tong Ding,Anil V. Parwani,Faisal Mahmood",1,126,0
"a050c9b0c321839e4427ab9defa3463be7825ac4","https://www.semanticscholar.org/paper/a050c9b0c321839e4427ab9defa3463be7825ac4",4,"MM-LLMs: Recent Advances in MultiModal Large Language Models","A taxonomy encompassing $122$ MM-LLMs, each characterized by its specific formulations is introduced and a review of selected MM-LLMs on mainstream benchmarks and key training recipes to enhance the potency of MM-LLMs are summarized.","arXiv.org",2024,"Duzhen Zhang,Yahan Yu,Chenxing Li,Jiahua Dong,Dan Su,Chenhui Chu,Dong Yu",3,254,0
"31a7d8c4a5ab6bab522494b57270249105c8748e","https://www.semanticscholar.org/paper/31a7d8c4a5ab6bab522494b57270249105c8748e",4,"BiomedGPT: A Unified and Generalist Biomedical Generative Pre-trained Transformer for Vision, Language, and Multimodal Tasks","A unified and generalist BiomedGPT model, which leverages self-supervision on large and diverse datasets to accept multi-modal inputs and perform a range of downstream tasks, which presents a significant step forward in developing unified and generalist models for biomedicine.","arXiv.org",2023,"Kai Zhang,Jun Yu,Zhilin Yan,Yixin Liu,Eashan Adhikarla,S. Fu,Xun Chen,Chen Chen,Yuyin Zhou,Xiang Li,Lifang He,B. Davison,Quanzheng Li,Yong Chen,Hongfang Liu,Lichao Sun",38,147,2
"f22d71c7ce9720ba1f717a4f1181488200e78198","https://www.semanticscholar.org/paper/f22d71c7ce9720ba1f717a4f1181488200e78198",4,"LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day","This paper proposes a cost-efficient approach for training a vision-language conversational assistant that can answer open-ended research questions of biomedical images, and releases instruction-following data and the LLaVA-Med model, which exhibits excellent multimodal conversational capability.","Neural Information Processing Systems",2023,"Chunyuan Li,Cliff Wong,Sheng Zhang,Naoto Usuyama,Haotian Liu,Jianwei Yang,Tristan Naumann,Hoifung Poon,Jianfeng Gao",98,46,15
"df0ddb588a200d095743e9d26fc4a9318619766e","https://www.semanticscholar.org/paper/df0ddb588a200d095743e9d26fc4a9318619766e",4,"Towards Generalist Foundation Model for Radiology","This study constructs a large-scale Medical Multi-modal Dataset, MedMD, which consists of 16M 2D and 3D medical scans with high-quality text descriptions or reports across various data formats, modalities, and tasks, covering over 5000 distinct diseases, and proposes a new evaluation benchmark, RadBench, aiming to comprehensively assess the capability of foundation models in handling practical clinical problems.","arXiv.org",2023,"Chaoyi Wu,Xiaoman Zhang,Ya Zhang,Yanfeng Wang,Weidi Xie",25,59,2
"61cadcfa555cbef120df7c017ef02e87f19900b7","https://www.semanticscholar.org/paper/61cadcfa555cbef120df7c017ef02e87f19900b7",4,"Free Form Medical Visual Question Answering in Radiology","This research delves into the effective representation of radiology images and the joint learning of multimodal representations, surpassing existing methods and innovatively augment the SLAKE dataset, enabling the model to respond to a more diverse array of questions.","arXiv.org",2024,"Abhishek Narayanan,Rushabh Musthyala,Rahul Sankar,Anirudh Prasad Nistala,P. Singh,Jacopo Cirrone",0,48,0
"44ccf252018f71898d52d89539f17d77a4f8d548","https://www.semanticscholar.org/paper/44ccf252018f71898d52d89539f17d77a4f8d548",3,"Chart Understanding with Large Language Model","A baseline multimodal model is introduced that integrates text and charts to enhance the chart comprehension capabilities of existing models, offering more pertinent insights and information related to the depicted charts.","",,"Yaser James,Will Li,John Feng",0,39,0
"54a8b153ed04a872da878d695239bdc413dc782c","https://www.semanticscholar.org/paper/54a8b153ed04a872da878d695239bdc413dc782c",3,"InternGPT: Solving Vision-Centric Tasks by Interacting with Chatbots Beyond Language","By incorporating pointing instructions, the proposed iGPT significantly improves the efficiency of communication between users and chatbots, as well as the accuracy of chatbots in vision-centric tasks, especially in complicated visual scenarios where the number of objects is greater than 2.","arXiv.org",2023,"Zhaoyang Liu,Yinan He,Wenhai Wang,Weiyun Wang,Yi Wang,Shoufa Chen,Qing-Long Zhang,Yang Yang,Qingyun Li,Jiashuo Yu,Kunchang Li,Zhe Chen,Xuecheng Yang,Xizhou Zhu,Yali Wang,Limin Wang,Ping Luo,Jifeng Dai,Yu Qiao",39,83,0
"66d755730f5d08a6f4fcc5e81f24982ba389dca9","https://www.semanticscholar.org/paper/66d755730f5d08a6f4fcc5e81f24982ba389dca9",3,"LayoutGPT: Compositional Visual Planning and Generation with Large Language Models","This work proposes LayoutGPT, a method to compose in-context visual demonstrations in style sheet language to enhance the visual planning skills of LLMs, and shows superior performance in converting challenging language concepts like numerical and spatial relations to layout arrangements for faithful text-to-image generation.","Neural Information Processing Systems",2023,"Weixi Feng,Wanrong Zhu,Tsu-Jui Fu,Varun Jampani,Arjun Reddy Akula,Xuehai He,Sugato Basu,X. Wang,William Yang Wang",27,65,4
"af705d648b5b16daa3dcc593bc593f2574d76c07","https://www.semanticscholar.org/paper/af705d648b5b16daa3dcc593bc593f2574d76c07",3,"Grammar Prompting for Domain-Specific Language Generation with Large Language Models","Experiments demonstrate that grammar prompting can enable LLMs to perform competitively on a diverse set of DSL generation tasks, including semantic parsing, Overnight, GeoQuery, PDDL planning, and even molecule generation (SMILES).","Neural Information Processing Systems",2023,"Bailin Wang,Zi Wang,Xuezhi Wang,Yuan Cao,R. Saurous,Yoon Kim",6,102,0
"dd0612ce863f64b0f69d0d9f708c52e829f6f859","https://www.semanticscholar.org/paper/dd0612ce863f64b0f69d0d9f708c52e829f6f859",3,"TPTU: Large Language Model-based AI Agents for Task Planning and Tool Usage","A structured framework tailored for LLM-based AI Agents is proposed and two distinct types of agents are designed to execute the inference process, which emphasizes the substantial potential of these models, while also identifying areas that need more investigation and improvement.","",2023,"Jingqing Ruan,Yihong Chen,Bin Zhang,Zhiwei Xu,Tianpeng Bao,Guoqing Du,Shiwei Shi,Hangyu Mao,Xingyu Zeng,Rui Zhao",7,95,0
"c5db6c2726911b72d534f97bd4d1ed63f6431340","https://www.semanticscholar.org/paper/c5db6c2726911b72d534f97bd4d1ed63f6431340",3,"Mobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual Perception","Different from previous solutions that rely on XML files of Apps or mobile system metadata, Mobile-Agent allows for greater adaptability across diverse mobile operating environments in a vision-centric way, thereby eliminating the necessity for system-specific customizations.","arXiv.org",2024,"Junyang Wang,Haiyang Xu,Jiabo Ye,Mingshi Yan,Weizhou Shen,Ji Zhang,Fei Huang,Jitao Sang",3,25,0
"ca055cfb9d4d47124cc035c346f38577825fcacf","https://www.semanticscholar.org/paper/ca055cfb9d4d47124cc035c346f38577825fcacf",3,"Enhance Reasoning Ability of Visual-Language Models via Large Language Models","A method called TReE is proposed, which transfers the reasoning ability of a large language model to a visual language model in zero-shot scenarios, and contains three stages: observation, thinking, and re-thinking.","arXiv.org",2023,"Yueting Yang,Xintong Zhang,Wenjuan Han",0,40,0
"50c1414fe41d0cb9db6f0933c9319aa124beac5d","https://www.semanticscholar.org/paper/50c1414fe41d0cb9db6f0933c9319aa124beac5d",3,"Contextual Object Detection with Multimodal Large Language Models","The ContextDET is presented, a unified multimodal model that is capable of end-to-end differentiable modeling of visual-language contexts, so as to locate, identify, and associate visual objects with language inputs for human-AI interaction.","arXiv.org",2023,"Yuhang Zang,Wei Li,Jun Han,Kaiyang Zhou,Chen Change Loy",21,87,0
"79150cb420d15830c8d36f0e91eea1b02e177f0f","https://www.semanticscholar.org/paper/79150cb420d15830c8d36f0e91eea1b02e177f0f",3,"Sticker820K: Empowering Interactive Retrieval with Stickers","The StickerCLIP is proposed as a benchmark model on the Sticker820K dataset, demonstrating strong superiority over the CLIP for the text-to-image retrieval task, and the recently popularized LLM is extended by means of prompt tuning, integrating its ability for sticker retrieval and allowing users to retrieve stickers through instructions.","arXiv.org",2023,"Sijie Zhao,Yixiao Ge,Zhongang Qi,Lin Song,Xiaohan Ding,Zehua Xie,Ying Shan",0,36,0
"697e0add95e880bd42e00bef838181e105f91981","https://www.semanticscholar.org/paper/697e0add95e880bd42e00bef838181e105f91981",3,"MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models","This paper presents the first comprehensive MLLM Evaluation benchmark MME, which measures both perception and cognition abilities on a total of 14 subtasks and suggests that existing MLLMs still have a large room for improvement, but also reveals the potential directions for the subsequent model optimization.","arXiv.org",2023,"Chaoyou Fu,Peixian Chen,Yunhang Shen,Yulei Qin,Mengdan Zhang,Xu Lin,Zhenyu Qiu,Wei Lin,Jinrui Yang,Xiawu Zheng,Ke Li,Xing Sun,Rongrong Ji",148,60,32
"1fd31b74f5e1eeb67341982fd35a613c6fad10e0","https://www.semanticscholar.org/paper/1fd31b74f5e1eeb67341982fd35a613c6fad10e0",3,"Link-Context Learning for Multimodal LLMs","This work proposes link-context learning (LCL), which emphasizes ""reasoning from cause and effect"" to augment the learning capabilities of MLLMs and introduces the ISEKAI dataset, comprising exclusively of unseen generated image-label pairs designed for link- context learning.","arXiv.org",2023,"Yan Tai,Weichen Fan,Zhao Zhang,Feng Zhu,Rui Zhao,Ziwei Liu",3,33,1
"d39182113cd4176ead48027b4fc05fe06ec6aaca","https://www.semanticscholar.org/paper/d39182113cd4176ead48027b4fc05fe06ec6aaca",3,"Language Models as Black-Box Optimizers for Vision-Language Models","This work proposes employing chat-based LLMs to search for the best text prompt for VLMs and highlights the advantage of conversational feedback that incorporates both positive and negative prompts, suggesting that LLMs can utilize the implicit gradient direction in textual feedback for a more efficient search.","arXiv.org",2023,"Samuel Yu,Shihong Liu,Zhiqiu Lin,Deepak Pathak,Deva Ramanan",3,107,0
"7a7128e9696dfedc24bf432c4b4c3aafa5e95a1a","https://www.semanticscholar.org/paper/7a7128e9696dfedc24bf432c4b4c3aafa5e95a1a",3,"An Empirical Study of Scaling Instruct-Tuned Large Multimodal Models","An empirical study of scaling LLaVA up to 33B and 65B/70B and performance of LoRA/QLoRA tuning of LMM are comparable to the performance of full-model fine-tuning, finding that scaling LMM consistently enhances model performance and improves language capabilities.","arXiv.org",2023,"Yadong Lu,Chunyuan Li,Haotian Liu,Jianwei Yang,Jianfeng Gao,Yelong Shen",7,22,0
"6ae4705139494fcb6b790b6dd6c4225b40ee40f8","https://www.semanticscholar.org/paper/6ae4705139494fcb6b790b6dd6c4225b40ee40f8",3,"GLaMM: Pixel Grounding Large Multimodal Model","This work presents Grounding LMM (GLaMM), the first model that can generate natural language responses seamlessly intertwined with corresponding object segmentation masks and is flexible enough to accept both textual and optional visual prompts (region of interest) as input.","arXiv.org",2023,"H. Rasheed,Muhammad Maaz,Sahal Shaji Mullappilly,Abdelrahman M. Shaker,Salman H. Khan,Hisham Cholakkal,R. Anwer,Erix Xing,Ming-Hsuan Yang,F. Khan",15,63,2
"7b0a186b0140ee91fb13991c9c7187f3dc3b0670","https://www.semanticscholar.org/paper/7b0a186b0140ee91fb13991c9c7187f3dc3b0670",3,"Visual Programming for Zero-shot Open-Vocabulary 3D Visual Grounding","This work proposes a novel visual programming approach for zero-shot open-vocabulary 3DVG, leveraging the capabilities of large language models (LLMs) and develops an innovative language-object correlation module to extend the scope of existing 3D object detectors into open- Vocabulary scenarios.","arXiv.org",2023,"Zhihao Yuan,Jinke Ren,Chun-Mei Feng,Hengshuang Zhao,Shuguang Cui,Zhen Li",0,67,0
"769a924d0af014acec326f50c15c5d70d258a969","https://www.semanticscholar.org/paper/769a924d0af014acec326f50c15c5d70d258a969",3,"LLMGA: Multimodal Large Language Model based Generation Assistant","This paper introduces a Multimodal Large Language Model-based Generation Assistant (LLMGA), leveraging the vast reservoir of knowledge and proficiency in reasoning, comprehension, and response inherent in Large Language Models to assist users in image generation and editing.","arXiv.org",2023,"Bin Xia,Shiyin Wang,Yingfan Tao,Yitong Wang,Jiaya Jia",2,65,0
"cc9627c4475b9ad1c7e4105f3bceb1e7b59b7cab","https://www.semanticscholar.org/paper/cc9627c4475b9ad1c7e4105f3bceb1e7b59b7cab",3,"Zero-Shot Video Question Answering with Procedural Programs","This work proposes to answer zero-shot questions about videos by generating short procedural programs that derive a final answer from solving a sequence of visual subtasks, using a large language model to generate such programs from an input question and an API of visual modules in the prompt, then executes them to obtain the output.","arXiv.org",2023,"Rohan Choudhury,Koichiro Niinuma,Kris M. Kitani,László A. Jeni",0,65,0
"c672ec79f55cef8f7a32cd8dddfa981b893f1567","https://www.semanticscholar.org/paper/c672ec79f55cef8f7a32cd8dddfa981b893f1567",3,"V*: Guided Visual Search as a Core Mechanism in Multimodal LLMs","This work introduces V*, an LLM-guided visual search mechanism that employs the world knowledge in LLMs for efficient visual querying and results in a new MLLM meta-architecture, named Show, sEArch, and TelL (SEAL).","arXiv.org",2023,"Penghao Wu,Saining Xie",8,60,1
"5f58863dd6474d6f127be995b5871e7c60f2792f","https://www.semanticscholar.org/paper/5f58863dd6474d6f127be995b5871e7c60f2792f",3,"Video Understanding with Large Language Models: A Survey","The emergent capabilities of Vid-LLMs are surprisingly advanced, particularly their ability for open-ended spatial-temporal reasoning combined with commonsense knowledge, suggesting a promising path for future video understanding.","arXiv.org",2023,"Yunlong Tang,Jing Bi,Siting Xu,Luchuan Song,Susan Liang,Teng Wang,Daoan Zhang,Jie An,Jingyang Lin,Rongyi Zhu,A. Vosoughi,Chao Huang,Zeliang Zhang,Feng Zheng,Jianguo Zhang,Ping Luo,Jiebo Luo,Chenliang Xu",2,218,1
"002d2c4569d070a55fe69c25ebccad8e9ddae572","https://www.semanticscholar.org/paper/002d2c4569d070a55fe69c25ebccad8e9ddae572",3,"Incorporating Visual Experts to Resolve the Information Loss in Multimodal Large Language Models","A novel method is introduced that incorporates multi-task encoders and visual tools into the existing MLLMs training and inference pipeline, aiming to provide a more comprehensive and accurate summarization of visual inputs.","arXiv.org",2024,"Xin He,Longhui Wei,Lingxi Xie,Qi Tian",0,58,0
"0a8a776054a087118f4f9523994ef084b2b2469a","https://www.semanticscholar.org/paper/0a8a776054a087118f4f9523994ef084b2b2469a",3,"Advancing Large Multi-modal Models with Explicit Chain-of-Reasoning and Visual Question Generation","This paper designs an LMM, which has high capabilities on region awareness to address the intricate requirements of image-text alignment, and introduces a system that can ask a question to acquire necessary knowledge, thereby enhancing the robustness and explicability of the reasoning process.","arXiv.org",2024,"Kohei Uehara,Nabarun Goswami,Hanqin Wang,Toshiaki Baba,Kohtaro Tanaka,Tomohiro Hashimoto,Kai Wang,Rei Ito,Takagi Naoya,Ryo Umagami,Yingyi Wen,Tanachai Anakewat,Tatsuya Harada",0,87,0
"83ee82e62f2eae18cc3472120eb9004109895a31","https://www.semanticscholar.org/paper/83ee82e62f2eae18cc3472120eb9004109895a31",3,"Delving into Multi-modal Multi-task Foundation Models for Road Scene Understanding: From Learning Paradigm Perspectives","A systematic analysis of MM-VUFMs specifically designed for road scenes, referring to task-specific models, unified multi-modal models, unified multi-task models, and foundation model prompting techniques to highlight their advanced capabilities in diverse learning paradigms.","arXiv.org",2024,"Sheng Luo,Wei Chen,Wanxin Tian,Rui Liu,Luanxuan Hou,Xiubao Zhang,Haifeng Shen,Ruiqi Wu,Shuyi Geng,Yi Zhou,Ling Shao,Yi Yang,Bojun Gao,Qun Li,Guobin Wu",0,213,0
"28fbbf98bac1bb941162df553ca034d600cb59a6","https://www.semanticscholar.org/paper/28fbbf98bac1bb941162df553ca034d600cb59a6",3,"Rephrase, Augment, Reason: Visual Grounding of Questions for Vision-Language Models","Rephrase, Augment and Reason (RepARe), a gradient-free framework that extracts salient details about the image using the underlying LVLM as a captioner and reasoner, in order to propose modifications to the original question, is presented.","arXiv.org",2023,"Archiki Prasad,Elias Stengel-Eskin,Mohit Bansal",2,108,0
"cb5f53cd7d43ac6f48abf4dde6fc58e53a083b0e","https://www.semanticscholar.org/paper/cb5f53cd7d43ac6f48abf4dde6fc58e53a083b0e",3,"Accelerating the integration of ChatGPT and other large‐scale AI models into biomedical research and healthcare","","MedComm – Future Medicine",2023,"Ding‐Qiao Wang,Long‐Yu Feng,Jin‐Guo Ye,Jin‐Gen Zou,Yingfeng Zheng",21,99,0
"2010e5fb3a804ac376412b4fa65ee83f34d5e1d9","https://www.semanticscholar.org/paper/2010e5fb3a804ac376412b4fa65ee83f34d5e1d9",3,"A Systematic Evaluation of GPT-4V's Multimodal Capability for Medical Image Analysis","An evaluation of GPT-4V's multimodal capability for medical image analysis shows that it excels in understanding medical images and is able to generate high-quality radiology reports and effectively answer questions about medical images, but it is found that its performance for medical visual grounding needs to be substantially improved.","",2023,"Yingshu Li,Yunyi Liu,Zhanyu Wang,Xinyu Liang,Lei Wang,Lingqiao Liu,Leyang Cui,Zhaopeng Tu,Longyue Wang,Luping Zhou",0,65,0
"3130643a5d02f0e849d83bb1f85577a924081f36","https://www.semanticscholar.org/paper/3130643a5d02f0e849d83bb1f85577a924081f36",3,"Paxion: Patching Action Knowledge in Video-Language Foundation Models","This work proposes a novel framework, Paxion, along with a new Discriminative Video Dynamics Modeling (DVDM) objective, and introduces the DVDM objective to train the Knowledge Patcher, which forces the model to encode the correlation between the action text and the correct ordering of video frames.","Neural Information Processing Systems",2023,"Zhenhailong Wang,Ansel Blume,Sha Li,Genglin Liu,Jaemin Cho,Zineng Tang,Mohit Bansal,Heng Ji",8,63,1
"fc6a2f7478f68adefd69e2071f27e38aa1647f2f","https://www.semanticscholar.org/paper/fc6a2f7478f68adefd69e2071f27e38aa1647f2f",3,"Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond","The Qwen-VL series, a set of large-scale vision-language models (LVLMs) designed to perceive and understand both texts and images, set new records for generalist models under similar model scales on a broad range of visual-centric benchmarks.","",2023,"Jinze Bai,Shuai Bai,Shusheng Yang,Shijie Wang,Sinan Tan,Peng Wang,Junyang Lin,Chang Zhou,Jingren Zhou",60,85,15
"96c43227831c4c3b12b7c64809e78674cea3a8a1","https://www.semanticscholar.org/paper/96c43227831c4c3b12b7c64809e78674cea3a8a1",3,"DeepSpeed-VisualChat: Multi-Round Multi-Image Interleave Chat via Multi-Modal Causal Attention","The DeepSpeed-VisualChat framework is presented, designed to optimize Large Language Models by incorporating multi-modal capabilities, with a focus on enhancing the proficiency of Large Vision and Language Models in handling interleaved inputs.","arXiv.org",2023,"Z. Yao,Xiaoxia Wu,Conglong Li,Minjia Zhang,Heyang Qi,Olatunji Ruwase,A. A. Awan,Samyam Rajbhandari,Yuxiong He",3,47,1
"id","url","score","title","summary","venue","year","authors","citationCount","referenceCount","influentialCitationCount"
"2c7e346aa311fec4dda04bdf3a214ce2026d8807","https://www.semanticscholar.org/paper/2c7e346aa311fec4dda04bdf3a214ce2026d8807",8,"Medical Vision Language Pretraining: A survey","This paper reviews existing works through the lens of different pretraining objectives, architectures, downstream evaluation tasks, and datasets utilized for pretraining and downstream tasks, then dives into current challenges in medical VLP, discussing existing and potential solutions, and concludes by highlighting future directions.","arXiv.org",2023,"Prashant Shrestha,Sanskar Amgain,Bidur Khanal,C. Linte,Binod Bhattarai",1,161,0
"584ca135b61482fd89247113da87d784f738dbfa","https://www.semanticscholar.org/paper/584ca135b61482fd89247113da87d784f738dbfa",6,"Foundational Models Defining a New Era in Vision: A Survey and Outlook","A comprehensive review of emerging foundational models in computer vision, including typical architecture designs to combine different modalities, training objectives, pre-training datasets, fine-tuning mechanisms, and the common prompting patterns; textual, visual, and heterogeneous.","arXiv.org",2023,"Muhammad Awais,Muzammal Naseer,Salman Siddique Khan,R. Anwer,Hisham Cholakkal,M. Shah,Ming Yang,F. Khan",21,366,4
"ebedc4d7a2356090904baba4104ef0832bc236df","https://www.semanticscholar.org/paper/ebedc4d7a2356090904baba4104ef0832bc236df",6,"A Survey on Multimodal Large Language Models","This paper presents the formulation of MLLM and delineate its related concepts, and discusses the key techniques and applications, including Multimodal Instruction Tuning (M-IT), Multimmodal In-Context Learning (M -ICL), MultIModal Chain of Thought (m-CoT), and LLM-Aided Visual Reasoning (LAVR).","arXiv.org",2023,"Shukang Yin,Chaoyou Fu,Sirui Zhao,Ke Li,Xing Sun,Tong Xu,Enhong Chen",79,108,2
"c7492913370b5726eaa6ced163a60de6c9d4bb7f","https://www.semanticscholar.org/paper/c7492913370b5726eaa6ced163a60de6c9d4bb7f",6,"A Survey of Large Language Models for Healthcare: from Data, Technology, and Applications to Accountability and Ethics","It is contended that a significant paradigm shift is underway, transitioning from PLMs to LLMs, which encompasses a move from discriminative AI approaches to generativeAI approaches, as well as a shift from model-centered methodologies to datacentered methodologies.","arXiv.org",2023,"Kai He,Rui Mao,Qika Lin,Yucheng Ruan,Xiang Lan,Mengling Feng,Erik Cambria",11,377,2
"8d2709ed1788a67e64425fb410bb49f3ee49e088","https://www.semanticscholar.org/paper/8d2709ed1788a67e64425fb410bb49f3ee49e088",6,"Large Language Models Illuminate a Progressive Pathway to Artificial Healthcare Assistant: A Review","This review offers an extensive analysis on the transformative potential of LLMs in modern medicine and highlights the pivotal need for continuous optimizations and ethical oversight before these models can be effectively integrated into clinical practice.","arXiv.org",2023,"Mingze Yuan,Peng Bao,J. Yuan,Yunhao Shen,Zi Chen,Yi Xie,Jie Zhao,Yang Chen,Li Zhang,Lin Shen,Bin Dong",1,186,0
"420087f314633a381e61e6c5cd73ccc2070a749e","https://www.semanticscholar.org/paper/420087f314633a381e61e6c5cd73ccc2070a749e",6,"PeFoMed: Parameter Efficient Fine-tuning on Multimodal Large Language Models for Medical Visual Question Answering","A parameter efficient framework for fine-tuning MLLM specifically tailored to Med-VQA applications is proposed and empirically validate it on a public benchmark dataset, revealing that it outperforms the GPT-4v model by a significant margin of 26% absolute accuracy on closed-ended questions.","",2024,"Jinlong He,Pengfei Li,Gang Liu,Zixu Zhao,Shenjun Zhong",0,43,0
"570079bbdd8758dfe865097e05719313c9c1301a","https://www.semanticscholar.org/paper/570079bbdd8758dfe865097e05719313c9c1301a",5,"LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model","This work augments LLaMA-Adapter by unlocking more learnable parameters and proposes an early fusion strategy to feed visual tokens only into the early LLM layers, contributing to better visual knowledge incorporation and achieves strong multi-modal reasoning with only a small-scale image-text and instruction dataset.","arXiv.org",2023,"Peng Gao,Jiaming Han,Renrui Zhang,Ziyi Lin,Shijie Geng,Aojun Zhou,W. Zhang,Pan Lu,Conghui He,Xiangyu Yue,Hongsheng Li,Y. Qiao",174,79,24
"d6d3604f369bb0415cbe814e43ca3131323b03e2","https://www.semanticscholar.org/paper/d6d3604f369bb0415cbe814e43ca3131323b03e2",5,"Otter: A Multi-Modal Model with In-Context Instruction Tuning","Otter, a multi-modal model based on OpenFlamingo (open-sourced version of DeepMind's Flamingo), trained on MIMIC-IT and showcasing improved instruction-following ability and in-context learning is introduced.","arXiv.org",2023,"Bo Li,Yuanhan Zhang,Liangyu Chen,Jinghao Wang,Jingkang Yang,Ziwei Liu",181,38,31
"86cbd30d1096b0c7e4ac6b03d97a8df12fd21457","https://www.semanticscholar.org/paper/86cbd30d1096b0c7e4ac6b03d97a8df12fd21457",5,"PathAsst: Redefining Pathology through Generative Foundation AI Assistant for Pathology","The PathAsst is presented, which is a generative foundation AI assistant to revolutionize diagnostic and predictive analytics in pathology, trained based on Vicuna-13B language model in coordination with the CLIP vision encoder.","arXiv.org",2023,"Yuxuan Sun,Chenglu Zhu,S. Zheng,Kai Zhang,Zhongyi Shui,Xiaoxuan Yu,Yi-Lei Zhao,Honglin Li,Yunlong Zhang,Ruojia Zhao,Xinheng Lyu,Lin Yang",10,47,2
"7cf64070fd3d7e53d80f260c10e6bd7018d580e1","https://www.semanticscholar.org/paper/7cf64070fd3d7e53d80f260c10e6bd7018d580e1",5,"IdealGPT: Iteratively Decomposing Vision and Language Reasoning via Large Language Models","The IdealGPT framework is presented, a framework that iteratively decomposes VL reasoning using large language models (LLMs) and outperforms the best existing GPT-4-like models by an absolute 10% on VCR and 15% on SNLI-VE.","Conference on Empirical Methods in Natural Language Processing",2023,"Haoxuan You,Rui Sun,Zhecan Wang,Long Chen,Gengyu Wang,Hammad A. Ayyubi,Kai-Wei Chang,Shih-Fu Chang",14,47,3
"fd755dc7b5b206c17fd953db04e1c888d45b6e4e","https://www.semanticscholar.org/paper/fd755dc7b5b206c17fd953db04e1c888d45b6e4e",5,"LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark","This work extends the research of MLLMs to point clouds and presents the LAMM-Dataset and LAMm-Benchmark for 2D image and 3D point cloud understanding and establishes an extensible framework to facilitate the extension of M LLMs to additional modalities.","arXiv.org",2023,"Zhen-fei Yin,Jiong Wang,Jianjian Cao,Zhelun Shi,Dingning Liu,Mukai Li,Lu Sheng,Lei Bai,Xiaoshui Huang,Zhiyong Wang,Wanli Ouyang,Jing Shao",35,98,8
"051549d8ef56937b2f4d113afdcf8c7586d3770b","https://www.semanticscholar.org/paper/051549d8ef56937b2f4d113afdcf8c7586d3770b",5,"Towards AGI in Computer Vision: Lessons Learned from GPT and Large Language Models","It is pointed out that the essential weakness of CV lies in lacking a paradigm to learn from environments, yet NLP has accomplished the task in the text world and is still far from a system like GPT that naturally integrates all tasks.","arXiv.org",2023,"Lingxi Xie,Longhui Wei,Xiaopeng Zhang,Kaifeng Bi,Xiaotao Gu,Jianlong Chang,Qi Tian",2,169,0
"966852963a88a28786b798c91b6662d6e501e590","https://www.semanticscholar.org/paper/966852963a88a28786b798c91b6662d6e501e590",5,"AssistGPT: A General Multi-modal Assistant that can Plan, Execute, Inspect, and Learn","A multi-modal AI assistant with an interleaved code and language reasoning approach called Plan, Execute, Inspect, and Learn (PEIL) to integrate LLMs with various tools, and a Learner is designed to enable the model to autonomously explore and discover the optimal solution.","arXiv.org",2023,"Difei Gao,Lei Ji,Luowei Zhou,Kevin Lin,Joya Chen,Zihan Fan,Mike Zheng Shou",20,73,2
"ca31b8584b6c022ef15ddfe994fe361e002b7729","https://www.semanticscholar.org/paper/ca31b8584b6c022ef15ddfe994fe361e002b7729",5,"A Comprehensive Overview of Large Language Models","A self-contained comprehensive overview of the existing literature on a broad range of LLM-related concepts discusses relevant background concepts along with covering the advanced topics at the frontier of research in LLMs.","arXiv.org",2023,"Humza Naveed,Asad Ullah Khan,Shi Qiu,Muhammad Saqib,Saeed Anwar,Muhammad Usman,N. Barnes,A. Mian",33,447,2
"d6c2523ab97416c2692cbbeab082ed1790e8e55e","https://www.semanticscholar.org/paper/d6c2523ab97416c2692cbbeab082ed1790e8e55e",5,"VisIT-Bench: A Benchmark for Vision-Language Instruction Following Inspired by Real-World Use","This work introduces VisIT-Bench (Visual InsTruction Benchmark), a benchmark for evaluation of instruction-following vision-language models for real-world use, and curates 70 'instruction families' that it envision instruction tuned vision- language models should be able to address.","arXiv.org",2023,"Yonatan Bitton,Hritik Bansal,Jack Hessel,Rulin Shao,Wanrong Zhu,Anas Awadalla,Josh Gardner,Rohan Taori,L. Schimdt",17,99,1
"894ed1aba8e42a4ec27ba53ecde383b14c5128ca","https://www.semanticscholar.org/paper/894ed1aba8e42a4ec27ba53ecde383b14c5128ca",5,"Examining User-Friendly and Open-Sourced Large GPT Models: A Survey on Language, Multimodal, and Scientific GPT Models","This survey paper provides an examination of alternative open-sourced models of large GPTs, focusing on user-friendly and relatively small models that facilitate easier deployment and accessibility, and aims to equip researchers, practitioners, and enthusiasts with a thorough understanding of these models.","arXiv.org",2023,"Kaiyuan Gao,Su He,Zhenyu He,Jiacheng Lin,Qizhi Pei,Jie Shao,Wei Zhang",0,178,0
"4eb87eaa193929dbef93fa2db9419245a8e8916f","https://www.semanticscholar.org/paper/4eb87eaa193929dbef93fa2db9419245a8e8916f",5,"TextBind: Multi-turn Interleaved Multimodal Instruction-following in the Wild","This work introduces TextBind, an almost annotation-free framework for empowering larger language models with the multi-turn interleaved multimodal instruction-following capabilities, and devise MIM, a language model-centric architecture that seamlessly integrates image encoder and decoder models.","arXiv.org",2023,"Huayang Li,Siheng Li,Deng Cai,Longyue Wang,Lemao Liu,Taro Watanabe,Yujiu Yang,Shuming Shi",4,68,1
"f34b6b4f75fb4e6f2c5654ee13fb2479c6170b3a","https://www.semanticscholar.org/paper/f34b6b4f75fb4e6f2c5654ee13fb2479c6170b3a",5,"Kosmos-2.5: A Multimodal Literate Model","Kosmos-2.5 can be readily adapted for any text-intensive image understanding task with different prompts through supervised fine-tuning, making it a general-purpose tool for real-world applications involving text-rich images and paves the way for the future scaling of multimodal large language models.","arXiv.org",2023,"Tengchao Lv,Yupan Huang,Jingye Chen,Lei Cui,Shuming Ma,Ya-Chi Chang,Shaohan Huang,Wenhui Wang,Li Dong,Weiyao Luo,Shaoxiang Wu,Guoxin Wang,Cha Zhang,Furu Wei",7,84,0
"7b689adb8c156d6158660f90d1c86888ee281f63","https://www.semanticscholar.org/paper/7b689adb8c156d6158660f90d1c86888ee281f63",5,"DreamLLM: Synergistic Multimodal Comprehension and Creation","A learning framework that first achieves versatile Multimodal Large Language Models (MLLMs) empowered with frequently overlooked synergy between multimodal comprehension and creation, reaping from the enhanced learning synergy.","arXiv.org",2023,"Runpei Dong,Chunrui Han,Yuang Peng,Zekun Qi,Zheng Ge,Jinrong Yang,Liang Zhao,Jian‐Yuan Sun,Hongyu Zhou,Hao-Ran Wei,Xiangwen Kong,Xiangyu Zhang,Kaisheng Ma,Li Yi",23,169,1
"092245d86b77181c36f972b1b7a17a59cd989c4a","https://www.semanticscholar.org/paper/092245d86b77181c36f972b1b7a17a59cd989c4a",5,"Guiding Instruction-based Image Editing via Multimodal Large Language Models","This work investigates how MLLMs facilitate edit instructions and presents MLLM-Guided Image Editing (MGIE), which learns to derive expressive instructions and provides explicit guidance and can lead to a notable improvement in automatic metrics and human evaluation while maintaining competitive inference efficiency.","arXiv.org",2023,"Tsu-Jui Fu,Wenze Hu,Xianzhi Du,William Yang Wang,Yinfei Yang,Zhe Gan",6,63,0
"a710efa9247207a72f06e0c9db302fd3ecab5fbb","https://www.semanticscholar.org/paper/a710efa9247207a72f06e0c9db302fd3ecab5fbb",5,"Towards Robust Multi-Modal Reasoning via Model Selection","This framework improves model selection and bolsters the robustness of multi-modal agents in multi-step reasoning, and enables dynamic model selection, considering both user inputs and subtask dependencies, thereby robustifying the overall reasoning process.","arXiv.org",2023,"Xiangyan Liu,Rongxue Li,Wei Ji,Tao Lin",0,57,0
"aad3d2e690f6c73f04a14622ceff51464bbc560e","https://www.semanticscholar.org/paper/aad3d2e690f6c73f04a14622ceff51464bbc560e",5,"Chat-UniVi: Unified Visual Representation Empowers Large Language Models with Image and Video Understanding","This work introduces Chat-UniVi, a unified vision-language model capable of comprehending and engaging in conversations involving images and videos through a unified visual representation that consistently outperforms even existing methods exclusively designed for either images or videos.","arXiv.org",2023,"Peng Jin,Ryuichi Takanobu,Caiwan Zhang,Xiaochun Cao,Li Yuan",4,74,0
"107fb6eec2febbae12db29bf3e311aaf5680027c","https://www.semanticscholar.org/paper/107fb6eec2febbae12db29bf3e311aaf5680027c",5,"Video-LLaVA: Learning United Visual Representation by Alignment Before Projection","This work unify visual representation into the language feature space to advance the foundational LLM towards a unified LVLM, and establishes a simple but robust LVLM baseline, Video-LLaVA, which learns from a mixed dataset of images and videos, mutually enhancing each other.","arXiv.org",2023,"Bin Lin,Bin Zhu,Yang Ye,Munan Ning,Peng Jin,Li Yuan",1,54,0
"6d2ab31aa75468f5458b9d96192c3f4a28f55d73","https://www.semanticscholar.org/paper/6d2ab31aa75468f5458b9d96192c3f4a28f55d73",5,"DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving","DriveMLM is introduced, an LLM-based AD framework that can perform close-loop autonomous driving in realistic simulators and can plug-and-play in existing AD systems such as Apollo for close-loop driving.","arXiv.org",2023,"Wenhai Wang,Jiangwei Xie,ChuanYang Hu,Haoming Zou,Jianan Fan,Wenwen Tong,Yang Wen,Silei Wu,Hanming Deng,Zhiqi Li,Hao Tian,Lewei Lu,Xizhou Zhu,Xiaogang Wang,Yu Qiao,Jifeng Dai",2,80,0
"6a33e58ef961a3a0a5657518b2be86395eb7c8d0","https://www.semanticscholar.org/paper/6a33e58ef961a3a0a5657518b2be86395eb7c8d0",5,"InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks","A large-scale vision-language foundation model (InternVL), which scales up the vision foundation model to 6 billion parameters and progressively aligns it with the LLM, using web-scale image-text data from various sources is designed.","",2023,"Zhe Chen,Jiannan Wu,Wenhai Wang,Weijie Su,Guo Chen,Sen Xing,Zhong Muyan,Qinglong Zhang,Xizhou Zhu,Lewei Lu,Bin Li,Ping Luo,Tong Lu,Yu Qiao,Jifeng Dai",1,185,0
"8ecdbfe011b7189fa0ee49ffc4e42a93d728a371","https://www.semanticscholar.org/paper/8ecdbfe011b7189fa0ee49ffc4e42a93d728a371",5,"On Evaluating Adversarial Robustness of Large Vision-Language Models","Evaluating the robustness of open-source large VLMs in the most realistic and high-risk setting, where adversaries have only black-box system access and seek to deceive the model into returning the targeted responses is proposed.","arXiv.org",2023,"Yunqing Zhao,Tianyu Pang,Chao Du,Xiao Yang,Chongxuan Li,Ngai-Man Cheung,Min Lin",25,108,2
"0ebc861f5478561f12941e6b48aad30574e996d8","https://www.semanticscholar.org/paper/0ebc861f5478561f12941e6b48aad30574e996d8",5,"Video ChatCaptioner: Towards Enriched Spatiotemporal Descriptions","This work introduces Video ChatCaptioner, an innovative approach for creating more comprehensive spatiotemporal video descriptions, specifically designed to select frames for posing video content-driven questions and shows promise as a method for enhancing video content.","arXiv.org",2023,"Jun Chen,Deyao Zhu,Kilichbek Haydarov,Xiang Li,Mohamed Elhoseiny",11,44,0
"42a30dc5470f54ec249f25d3c31e05d7c376c8e3","https://www.semanticscholar.org/paper/42a30dc5470f54ec249f25d3c31e05d7c376c8e3",5,"VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks","This work presents an LLM-based framework for vision-centric tasks, termed VisionLLM, which provides a unified perspective for vision and language tasks by treating images as a foreign language and aligning vision-focused tasks with language tasks that can be flexibly defined and managed using language instructions.","arXiv.org",2023,"Wen Wang,Zhe Chen,Xiaokang Chen,Jiannan Wu,Xizhou Zhu,Gang Zeng,Ping Luo,Tong Lu,Jie Zhou,Y. Qiao,Jifeng Dai",96,81,7
"06091944b864d6dc473cab63321a95fb9c4067cc","https://www.semanticscholar.org/paper/06091944b864d6dc473cab63321a95fb9c4067cc",5,"ChatCAD+: Towards a Universal and Reliable Interactive CAD using LLMs","ChatCAD+, which is designed to be universal and reliable, is introduced, capable of handling medical images from diverse domains and leveraging up-to-date information from reputable medical websites to provide reliable medical advice.","arXiv.org",2023,"Zihao Zhao,Sheng Wang,Jinchen Gu,Yitao Zhu,Lanzhuju Mei,Zixu Zhuang,Zhiming Cui,Qian Wang,Dinggang Shen",8,50,1
"833cdd713c27ab5899bb912a1d511c10af61cefb","https://www.semanticscholar.org/paper/833cdd713c27ab5899bb912a1d511c10af61cefb",5,"Making Multimodal Generation Easier: When Diffusion Models Meet LLMs","Efficient model designed to enhance multimodal understanding and generation by harnessing the capabilities of diffusion models and large language models, built upon a bidirectional conditional diffusion model named BiDiffuser, which promotes more efficient interactions between modalities.","arXiv.org",2023,"Xiangyu Zhao,Bo Liu,Qijiong Liu,Guangyuan Shi,Xiao-Ming Wu",3,58,0
"246017780386eba39d6cda760a1c2c70356baa50","https://www.semanticscholar.org/paper/246017780386eba39d6cda760a1c2c70356baa50",5,"VIoTGPT: Learning to Schedule Vision Tools towards Intelligent Video Internet of Things","VIoTGPT is built, the framework based on LLMs to correctly interact with humans, query knowledge videos, and invoke vision models to accomplish complicated tasks to address the challenges posed by the fine-grained and interrelated vision tool usage of VIoT.","arXiv.org",2023,"Yaoyao Zhong,Mengshi Qi,Rui Wang,Yuhan Qiu,Yang Zhang,Huadong Ma",0,78,0
"6bdfffbf92d01c8b543088d40d46233610e469a8","https://www.semanticscholar.org/paper/6bdfffbf92d01c8b543088d40d46233610e469a8",5,"CLIP in Medical Imaging: A Comprehensive Survey","This survey offers an in-depth exploration of the CLIP paradigm within the domain of medical imaging, regarding both refined CLIP pre-training and CLIP-driven applications, and investigates the adaptation of CLIP pre-training in the medical domain.","arXiv.org",2023,"Zihao Zhao,Yuxiao Liu,Han Wu,Yonghao Li,Sheng Wang,L. Teng,Disheng Liu,Xiang Li,Zhiming Cui,Qian Wang,Dinggang Shen",0,218,0
"31a7d8c4a5ab6bab522494b57270249105c8748e","https://www.semanticscholar.org/paper/31a7d8c4a5ab6bab522494b57270249105c8748e",5,"BiomedGPT: A Unified and Generalist Biomedical Generative Pre-trained Transformer for Vision, Language, and Multimodal Tasks","A unified and generalist BiomedGPT model, which leverages self-supervision on large and diverse datasets to accept multi-modal inputs and perform a range of downstream tasks, which presents a significant step forward in developing unified and generalist models for biomedicine.","arXiv.org",2023,"Kai Zhang,Jun Yu,Zhilin Yan,Yixin Liu,Eashan Adhikarla,S. Fu,Xun Chen,Chen Chen,Yuyin Zhou,Xiang Li,Lifang He,B. Davison,Quanzheng Li,Yong Chen,Hongfang Liu,Lichao Sun",28,147,2
"baa1dc079d98ca76b0173c8d653fed759fd0a371","https://www.semanticscholar.org/paper/baa1dc079d98ca76b0173c8d653fed759fd0a371",5,"A scoping review on multimodal deep learning in biomedical images and texts","This study reviewed the current uses of multimodal deep learning on five tasks: report generation, Visual question answering, Cross-modal retrieval, computer-aided diagnosis, and Semantic segmentation, and highlighted the diverse applications and potential of MDL.","Journal of Biomedical Informatics",2023,"Zhaoyi Sun,Mingquan Lin,Qingqing Zhu,Qianqian Xie,Fei Wang,Zhiyong Lu,Yifan Peng",2,148,0
"f0950a3f27c0fefffba60ae1c9a8ee360d5eb55f","https://www.semanticscholar.org/paper/f0950a3f27c0fefffba60ae1c9a8ee360d5eb55f",5,"Instruction Tuning for Large Language Models: A Survey","A systematic review of the literature, including the general methodology of IT, the construction of IT datasets, the training of IT models, and applications to different modalities, domains and applications, along with an analysis on aspects that influence the outcome of IT.","arXiv.org",2023,"Shengyu Zhang,Linfeng Dong,Xiaoya Li,Sen Zhang,Xiaofei Sun,Shuhe Wang,Jiwei Li,Runyi Hu,Tianwei Zhang,Fei Wu,Guoyin Wang",50,150,2
"2ddb6a253bc944c7a88e67747e44c34aafe734fc","https://www.semanticscholar.org/paper/2ddb6a253bc944c7a88e67747e44c34aafe734fc",5,"ViCrop: Perceiving Small Visual Details in Zero-shot Visual Question Answering with Multimodal Large Language Models","This work proposes ViCrop, a general framework that utilizes automatic visual cropping to enhance zero-shot VQA of MLLMs and improves MLLMs' zero-shot accuracy across different VQA datasets, for example, enhances BLIP2-T5's performance by $32.23\% on the TextVQA test set.","",2023,"Jiarui Zhang,Mahyar Khayatkhoei,P. Chhikara,Filip Ilievski",0,38,0
"da9134f694959b68027c33c8e998ffb3d41305da","https://www.semanticscholar.org/paper/da9134f694959b68027c33c8e998ffb3d41305da",5,"Exploring Question Decomposition for Zero-Shot VQA","A model-driven selective decomposition approach for second-guessing predictions and correcting errors is introduced, and its effectiveness on eight VQA tasks across three domains is validated, showing consistent improvements in accuracy.","arXiv.org",2023,"Zaid Khan,B. Vijaykumar,S. Schulter,Manmohan Chandraker,Yun Fu",0,60,0
"88bddfb7d1e0462be8fe99fdbd71c658140cb17b","https://www.semanticscholar.org/paper/88bddfb7d1e0462be8fe99fdbd71c658140cb17b",5,"From Image to Language: A Critical Analysis of Visual Question Answering (VQA) Approaches, Challenges, and Opportunities","This work presents a survey in the domain of VQA that delves into the intricacies of V QA datasets and methods over the field's history, introduces a detailed taxonomy to categorize the facets of VZA, and highlights the recent trends, challenges, and scopes for improvement.","arXiv.org",2023,"Md Farhan Ishmam,Md Sakib Hossain Shovon,M. F. Mridha,Nilanjan Dey",0,304,0
"d48fa3ed73817563130ef217d85011ce1fbe7470","https://www.semanticscholar.org/paper/d48fa3ed73817563130ef217d85011ce1fbe7470",5,"BESTMVQA: A Benchmark Evaluation System for Medical Visual Question Answering","A Benchmark Evaluation SysTem for Medical Visual Question Answering, denoted by BESTMVQA, is developed, which provides a useful tool for users to automatically build Med-V QA datasets, which helps overcoming the data insufficient problem.","arXiv.org",2023,"Xiaojie Hong,Zixin Song,Liangzhi Li,Xiaoli Wang,Feiyan Liu",0,23,0
"352252231462c24440bc0016638ea5fe8d4c6f7e","https://www.semanticscholar.org/paper/352252231462c24440bc0016638ea5fe8d4c6f7e",5,"UniDCP: Unifying Multiple Medical Vision-language Tasks via Dynamic Cross-modal Learnable Prompts","UniDCP is the first Med-VLP model capable of performing all 8 medical uni-modal and cross-modal tasks over 14 corresponding datasets, consistently yielding superior results over diverse state-of-the-art methods.","",2023,"Chenlu Zhan,Yufei Zhang,Yu Lin,Gaoang Wang,Hongwei Wang",0,67,0
"bca0bbd01ea917b7a9fe369288ea3ba03d3b1ff3","https://www.semanticscholar.org/paper/bca0bbd01ea917b7a9fe369288ea3ba03d3b1ff3",4,"A Survey of Large Language Models in Medicine: Progress, Application, and Challenge","This survey provides a comprehensive overview of the current progress, applications, and challenges faced by LLMs in medicine.","arXiv.org",2023,"Hongjian Zhou,Boyang Gu,Xinyu Zou,Yiru Li,Sam S. Chen,Peilin Zhou,Junling Liu,Y. Hua,Chengfeng Mao,Xian Wu,Zheng Li,Fenglin Liu",2,183,0
"094883e42bb9a41f602c0715c1059bc431e33fb2","https://www.semanticscholar.org/paper/094883e42bb9a41f602c0715c1059bc431e33fb2",4,"GPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest","Spatial instruction tuning is proposed, which introduces the reference to the region-of-interest (RoI) in the instruction, which achieves a remarkable accuracy of 81.6%, surpassing all existing models by a significant margin and almost reaching human-level performance of 85.0%.","arXiv.org",2023,"Shilong Zhang,Pei Sun,Shoufa Chen,Min Xiao,Wenqi Shao,Wenwei Zhang,Kai Chen,Ping Luo",53,96,5
"a5036f31f0e629dc661f120b8c3b1f374d479ab8","https://www.semanticscholar.org/paper/a5036f31f0e629dc661f120b8c3b1f374d479ab8",4,"Visual Instruction Tuning","This paper presents LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding and introduces GPT-4 generated visual instruction tuning data, the model and code base publicly available.","arXiv.org",2023,"Haotian Liu,Chunyuan Li,Qingyang Wu,Yong Jae Lee",575,63,192
"ca6a2bc279be5a3349a22bfd6866ed633d18734b","https://www.semanticscholar.org/paper/ca6a2bc279be5a3349a22bfd6866ed633d18734b",4,"MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models","MiniGPT-4 is presented, which aligns a frozen visual encoder with a frozen advanced LLM, Vicuna, using one projection layer to uncovers that properly aligning the visual features with an advanced large language model can possess numerous advanced multi-modal abilities demonstrated by G PT-4.","arXiv.org",2023,"Deyao Zhu,Jun Chen,Xiaoqian Shen,Xiang Li,Mohamed Elhoseiny",487,49,125
"7e32aac43e9f1df49e116add03327ee6f365dbf3","https://www.semanticscholar.org/paper/7e32aac43e9f1df49e116add03327ee6f365dbf3",4,"mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality",,"arXiv.org",2023,"Qinghao Ye,Haiyang Xu,Guohai Xu,Jiabo Ye,Ming Yan,Yi Zhou,Junyan Wang,Anwen Hu,Pengcheng Shi,Yaya Shi,Chenliang Li,Yuanhong Xu,Hehong Chen,Junfeng Tian,Qiang Qi,Ji Zhang,Feiyan Huang",249,36,34
"54a8b153ed04a872da878d695239bdc413dc782c","https://www.semanticscholar.org/paper/54a8b153ed04a872da878d695239bdc413dc782c",4,"InternGPT: Solving Vision-Centric Tasks by Interacting with Chatbots Beyond Language","By incorporating pointing instructions, the proposed iGPT significantly improves the efficiency of communication between users and chatbots, as well as the accuracy of chatbots in vision-centric tasks, especially in complicated visual scenarios where the number of objects is greater than 2.","arXiv.org",2023,"Zhaoyang Liu,Yinan He,Wenhai Wang,Weiyun Wang,Yi Wang,Shoufa Chen,Qing-Long Zhang,Yang Yang,Qingyun Li,Jiashuo Yu,Kunchang Li,Zhe Chen,Xuecheng Yang,Xizhou Zhu,Yali Wang,Limin Wang,Ping Luo,Jifeng Dai,Yu Qiao",35,83,0
"d48cb91b9e555194f7494c4d4bb9815021d3ee45","https://www.semanticscholar.org/paper/d48cb91b9e555194f7494c4d4bb9815021d3ee45",4,"VideoChat: Chat-Centric Video Understanding","An end-to-end chat-centric video understanding system that integrates video foundation models and large language models via a learnable neural interface, excelling in spatiotemporal reasoning, event localization, and causal relationship inference is initiated.","arXiv.org",2023,"Kunchang Li,Yinan He,Yi Wang,Yizhuo Li,Wen Wang,Ping Luo,Yali Wang,Limin Wang,Yu Qiao",107,65,19
"00cb69a9f280317d1c59ac5827551ee9b10642b8","https://www.semanticscholar.org/paper/00cb69a9f280317d1c59ac5827551ee9b10642b8",4,"EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought","This work introduces EmbodiedGPT, an end-to-end multi-modal foundation model for embodied AI, empowering embodied agents with multi- modal understanding and execution capabilities, and significantly enhances the success rate of the embodied control task by extracting more effective features.","arXiv.org",2023,"Yao Mu,Qinglong Zhang,Mengkang Hu,Wen Wang,Mingyu Ding,Jun Jin,Bin Wang,Jifeng Dai,Y. Qiao,Ping Luo",55,72,3
"9837349417e36ef5be06da0fd6c74042148bdaa2","https://www.semanticscholar.org/paper/9837349417e36ef5be06da0fd6c74042148bdaa2",4,"Visual Programming for Text-to-Image Generation and Evaluation","This work proposes two novel interpretable/explainable visual programming frameworks for text-to-image (T2I) generation and evaluation and introduces VPEval, an interpretable and explainable evaluation framework for T2I generation based on visual programming.","arXiv.org",2023,"Jaemin Cho,Abhaysinh Zala,Mohit Bansal",16,68,3
"9c3a9b4821daa03cb5369041d59d2714329a3811","https://www.semanticscholar.org/paper/9c3a9b4821daa03cb5369041d59d2714329a3811",4,"Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models","A novel and affordable solution for the effective VL adaption of LLMs, called Mixture-of-Modality Adaptation (MMA), which adopts lightweight modules, i.e., adapters, to bridge the gap between LLMs and VL tasks, which also enables the joint optimization of the image and language models","arXiv.org",2023,"Gen Luo,Yiyi Zhou,Tianhe Ren,Shen Chen,Xiaoshuai Sun,Rongrong Ji",29,53,5
"b458fc5261595f44b36325e5eaea1f874d65138f","https://www.semanticscholar.org/paper/b458fc5261595f44b36325e5eaea1f874d65138f",4,"GPT4Tools: Teaching Large Language Model to Use Tools via Self-instruction","The GPT4Tools based on self-instruct to enable open-source LLMs, such as LLaMA and OPT, to use tools, generates an instruction-following dataset by prompting an advanced teacher with various multi-modal contexts using the Low-Rank Adaptation (LoRA) optimization.","arXiv.org",2023,"Rui Yang,Lin Song,Yanwei Li,Sijie Zhao,Yixiao Ge,Xiu Li,Ying Shan",51,63,9
"d47524cd5c3c4b57af2e5a29f6f91c420310f236","https://www.semanticscholar.org/paper/d47524cd5c3c4b57af2e5a29f6f91c420310f236",4,"MIMIC-IT: Multi-Modal In-Context Instruction Tuning","MultI-Modal In-Context Instruction Tuning (MIMIC-IT), a dataset comprising 2.8 million multimodal instruction-response pairs, with 2.2 million unique instructions derived from images and videos, is presented and a large VLM named Otter is trained.","arXiv.org",2023,"Bo Li,Yuanhan Zhang,Liangyu Chen,Jinghao Wang,Fanyi Pu,Jingkang Yang,C. Li,Ziwei Liu",60,55,8
"4c4d176c6e28f48041f215d563f6ee8633534cff","https://www.semanticscholar.org/paper/4c4d176c6e28f48041f215d563f6ee8633534cff",4,"Valley: Video Assistant with Large Language model Enhanced abilitY","A novel multi-modal foundation model capable of comprehending video, image, and language within a general framework is developed, and Qualitative experiments demonstrate that Valley has the potential to function as a highly effective video assistant that can make complex video understanding scenarios easy.","arXiv.org",2023,"Ruipu Luo,Ziwang Zhao,Min Yang,Junwei Dong,Ming-Hui Qiu,Pengcheng Lu,Tao Wang,Zhongyu Wei",33,39,8
"d98536f24272e258b1d399074b64284d64786099","https://www.semanticscholar.org/paper/d98536f24272e258b1d399074b64284d64786099",4,"AVIS: Autonomous Visual Information Seeking with Large Language Models","An autonomous information seeking visual question answering framework that leverages a Large Language Model to dynamically strategize the utilization of external tools and to investigate their outputs, thereby acquiring the indispensable knowledge needed to provide answers to the posed questions is proposed.","arXiv.org",2023,"Ziniu Hu,Ahmet Iscen,Chen Sun,Kai-Wei Chang,Yizhou Sun,David A. Ross,C. Schmid,A. Fathi",9,133,1
"ebddfdc5d845a788e8062eddbbf7a335737cb99b","https://www.semanticscholar.org/paper/ebddfdc5d845a788e8062eddbbf7a335737cb99b",4,"What Matters in Training a GPT4-Style Language Model with Multimodal Inputs?","Lynx is presented, which performs the most accurate multi-modal understanding while keeping the best multi- modal generation ability compared to existing open-sourced GPT4-style models.","arXiv.org",2023,"Yan Zeng,Hanbo Zhang,Jiani Zheng,Jiangnan Xia,Guoqiang Wei,Yang Wei,Yuchen Zhang,Tao Kong",26,108,3
"2e52178416b93a0a7dc1cb7d21d04c7e72c2ce0f","https://www.semanticscholar.org/paper/2e52178416b93a0a7dc1cb7d21d04c7e72c2ce0f",4,"ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring Instruction Tuning","This study proposes ChatSpot, a unified end-to-end multimodal large language model that supports diverse forms of interactivity including mouse clicks, drag-and-drop, and drawing boxes, which provides a more flexible and seamless interactive experience.","arXiv.org",2023,"Liang Zhao,En Yu,Zheng Ge,Jinrong Yang,Hao-Ran Wei,Hongyu Zhou,Jian‐Yuan Sun,Yuang Peng,Runpei Dong,Chunrui Han,Xiangyu Zhang",12,39,2
"ad113d8b6f8f6dace7abd12dc88af520aaaf3fc7","https://www.semanticscholar.org/paper/ad113d8b6f8f6dace7abd12dc88af520aaaf3fc7",4,"LISA: Reasoning Segmentation via Large Language Model","This work proposes a new segmentation task -- reasoning segmentation, designed to output a segmentation mask given a complex and implicit query text, and presents LISA, which inherits the language generation capabilities of the multi-modal Large Language Model (LLM) while also possessing the ability to produce segmentation masks.","arXiv.org",2023,"Xin Lai,Zhuotao Tian,Yukang Chen,Yanwei Li,Yuhui Yuan,Shu Liu,Jiaya Jia",53,64,13
"d53945d4afb4528590d79e20de52883d29037e86","https://www.semanticscholar.org/paper/d53945d4afb4528590d79e20de52883d29037e86",4,"FashionLOGO: Prompting Multimodal Large Language Models for Fashion Logo Embeddings","This work explores how MLLMs can improve logo embedding by prompting them to generate explicit textual knowledge through three types of prompts, including image OCR, brief captions, and detailed descriptions prompts, in a zero-shot setting and adopt a cross-attention transformer to enable image embedding queries to learn supplementary knowledge from textual embeddings automatically.","arXiv.org",2023,"Yulin Su,Min Yang,Minghui Qiu,Jing Wang,Tao Wang",0,44,0
"eb5cf10406a8ad31e0ebe56b36571d5db4758a62","https://www.semanticscholar.org/paper/eb5cf10406a8ad31e0ebe56b36571d5db4758a62",4,"PUMGPT: A Large Vision-Language Model for Product Understanding","This paper presents PUMGPT, a large vision-language model that aims at unifying all product understanding tasks under a singular model structure, and proposes Layer-wise Adapters (LA), an approach that provides enhanced alignment with fewer visual tokens and enables parameter-efficient fine-tuning.","arXiv.org",2023,"Shuhui Wu,Zengming Tang,Zongyi Guo,Weiwei Zhang,Baoliang Cui,Haihong Tang,Weiming Lu",1,38,0
"6bcc6ab9c28805d4067e99b2cdc7524550fe80e1","https://www.semanticscholar.org/paper/6bcc6ab9c28805d4067e99b2cdc7524550fe80e1",4,"PointLLM: Empowering Large Language Models to Understand Point Clouds","Experimental results reveal PointLLM's superior performance over existing 2D and 3D baselines, with a notable achievement in human-evaluated object captioning tasks where it surpasses human annotators in over 50% of the samples.","arXiv.org",2023,"Runsen Xu,Xiaolong Wang,Tai Wang,Yilun Chen,Jiangmiao Pang,Dahua Lin",21,72,3
"3ec464696db25acc2c39a6d967ec3df09e06f633","https://www.semanticscholar.org/paper/3ec464696db25acc2c39a6d967ec3df09e06f633",4,"Multimodal Multi-Hop Question Answering Through a Conversation Between Tools and Efficiently Finetuned Large Language Models","A tool-interacting divide-and-conquer strategy enabling large language models (LLMs) to answer complex multimodal multi-hop questions, demonstrating the efficacy and generality of this approach.","arXiv.org",2023,"Hossein Rajabzadeh,Suyuchen Wang,Hyock Ju Kwon,Bang Liu",0,33,0
"bee68767debbdc96d6f75947e544a8be98b869e3","https://www.semanticscholar.org/paper/bee68767debbdc96d6f75947e544a8be98b869e3",4,"Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model: Explorations with GPT4-Vision and Beyond","This study introduces a new benchmark called PCA-EVAL, which evaluates embodied decision-making from the perspectives of Perception, Cognition, and Action and proposes HOLMES, a multi-agent cooperation framework that allows LLMs to leverage MLLMs and APIs to gather multimodal information for informed decision- making.","arXiv.org",2023,"Liang Chen,Yichi Zhang,Shuhuai Ren,Haozhe Zhao,Zefan Cai,Yuchi Wang,Tianyu Liu,Baobao Chang",7,60,2
"36b923d97d7cfaf73d11c55c15ea46605ba974a5","https://www.semanticscholar.org/paper/36b923d97d7cfaf73d11c55c15ea46605ba974a5",4,"BiLL-VTG: Bridging Large Language Models and Lightweight Visual Tools for Video-based Texts Generation","BiLL-VTG is introduced, a fast adaptive framework that leverages large language models (LLMs) to reasoning on videos based on essential lightweight visual tools and an Instruction-oriented Video Events Recognition (InsOVER) algorithm based on the efficient Hungarian matching to localize corresponding video events using linguistic instructions, enabling LLMs to interact with long videos.","arXiv.org",2023,"Ji Qi,Kaixuan Ji,Jifan Yu,Duokang Wang,Bin Xu,Lei Hou,Juanzi Li",0,68,0
"807f336176070bd3f95b82a16f125ee99b7d2c80","https://www.semanticscholar.org/paper/807f336176070bd3f95b82a16f125ee99b7d2c80",4,"Woodpecker: Hallucination Correction for Multimodal Large Language Models","Implemented in a post-remedy manner, Woodpecker can easily serve different MLLMs, while being interpretable by accessing intermediate outputs of the five stages, and shows the huge potential of this new paradigm.","arXiv.org",2023,"Shukang Yin,Chaoyou Fu,Sirui Zhao,Tong Xu,Hao Wang,Dianbo Sui,Yunhang Shen,Ke Li,Xingguo Sun,Enhong Chen",11,49,1
"ecfff30e570498b6c87c5d1319a0cbcdf7a8b86d","https://www.semanticscholar.org/paper/ecfff30e570498b6c87c5d1319a0cbcdf7a8b86d",4,"LLaVA-Plus: Learning to Use Tools for Creating Multimodal Agents","LLaVA-Plus is distinct in that the image query is directly grounded and actively engaged throughout the entire human-AI interaction sessions, significantly improving tool use performance and enabling new scenarios.","arXiv.org",2023,"Shilong Liu,Hao Cheng,Haotian Liu,Hao Zhang,Feng Li,Tianhe Ren,Xueyan Zou,Jianwei Yang,Hang Su,Jun-Juan Zhu,Lei Zhang,Jianfeng Gao,Chun-yue Li",8,52,1
"ef321c6f174ac59916ac54ec40ad18bca5b58e5c","https://www.semanticscholar.org/paper/ef321c6f174ac59916ac54ec40ad18bca5b58e5c",4,"PerceptionGPT: Effectively Fusing Visual Perception into LLM","A novel end-to-end framework named PerceptionGPT, which efficiently and effectively equips the VLLMs with visual perception abilities by leveraging the representation power of LLMs' token embedding and demonstrates significant improvements over previous methods with much fewer trainable parameters and GPU hours.","arXiv.org",2023,"Renjie Pi,Lewei Yao,Jiahui Gao,Jipeng Zhang,Tong Zhang",3,50,0
"52941cadbd340344f3e0a6f50719fe55b3de5088","https://www.semanticscholar.org/paper/52941cadbd340344f3e0a6f50719fe55b3de5088",4,"Multimodal Large Language Models: A Survey","A range of multimodal products are introduced and a compilation of the latest algorithms and commonly used datasets are presented, providing researchers with valuable resources for experimentation and evaluation.","arXiv.org",2023,"Jiayang Wu,Wensheng Gan,Zefeng Chen,Shicheng Wan,Philip S. Yu",4,74,0
"5eea245cc12c55905d4df827d0c9776c5ddfa743","https://www.semanticscholar.org/paper/5eea245cc12c55905d4df827d0c9776c5ddfa743",4,"Compositional Chain-of-Thought Prompting for Large Multimodal Models","The proposed Compositional Chain-of-Thought (CCoT) approach not only improves LMM performance on several vision and language VL compositional benchmarks but also improves the performance of several popular LMMs on general multimodal benchmarks, without the need for fine-tuning or annotated ground-truth SGs.","arXiv.org",2023,"Chancharik Mitra,Brandon Huang,Trevor Darrell,Roei Herzig",1,91,1
"f32ea390686b1eee3ba5b53c7a85e9e9385d4b94","https://www.semanticscholar.org/paper/f32ea390686b1eee3ba5b53c7a85e9e9385d4b94",4,"Visual Program Distillation: Distilling Tools and Programmatic Reasoning into Vision-Language Models","Visual Program Distillation (VPD) is proposed, an instruction tuning framework that produces a vision-language model (VLM) capable of solving complex visual tasks with a single forward pass and improves the VLM's ability to count, understand spatial relations, and reason compositionally.","arXiv.org",2023,"Yushi Hu,Otilia Stretcu,Chun-Ta Lu,Krishnamurthy Viswanathan,Kenji Hata,Enming Luo,Ranjay Krishna,Ariel Fuxman",0,59,0
"b240a1d8ec2860bdd7370daa3144268ce46ac018","https://www.semanticscholar.org/paper/b240a1d8ec2860bdd7370daa3144268ce46ac018",4,"Vary: Scaling up the Vision Vocabulary for Large Vision-Language Models","Compared to the popular BLIP-2, MiniGPT4, and LLaVA, Vary can maintain its vanilla capabilities while enjoying more excellent fine-grained perception and understanding ability and is competent in new document parsing features (OCR or markdown conversion).","arXiv.org",2023,"Haoran Wei,Lingyu Kong,Jinyue Chen,Liang Zhao,Zheng Ge,Jinrong Yang,Jian‐Yuan Sun,Chunrui Han,Xiangyu Zhang",0,52,0
"33c3dbf86dd6e338e2a49bba9c2e2193d1eca08a","https://www.semanticscholar.org/paper/33c3dbf86dd6e338e2a49bba9c2e2193d1eca08a",4,"Vista-LLaMA: Reliable Video Narrator via Equal Distance to Visual Tokens","This work proposes Vista-LLaMA, a novel framework that maintains the consistent distance between all visual tokens and any language tokens, irrespective of the generated text length, and presents a sequential visual projector that projects the current video frame into tokens of language space with the assistance of the previous frame.","arXiv.org",2023,"Fan Ma,Xiaojie Jin,Heng Wang,Yuchen Xian,Jiashi Feng,Yi Yang",0,39,0
"17a32c825bd746a2625eddc2728092171a9ef72a","https://www.semanticscholar.org/paper/17a32c825bd746a2625eddc2728092171a9ef72a",4,"Jack of All Tasks, Master of Many: Designing General-purpose Coarse-to-Fine Vision-Language Model","This work introduces VistaLLM, a powerful visual system that addresses coarse- and fine-grained VL tasks over single and multiple input images using a unified framework and addresses the lack of multi-image grounding datasets by introducing a novel task, AttCoSeg (Attribute-level Co-Segmentation), which boosts the model's reasoning and grounding capability over multiple input images.","",2023,"Shraman Pramanick,Guangxing Han,Rui Hou,Sayan Nag,Ser-Nam Lim,Nicolas Ballas,Qifan Wang,Rama Chellappa,Amjad Almahairi",0,126,0
"46f64681d7a0c7f80303bebb0d62a3b7acbcdcd9","https://www.semanticscholar.org/paper/46f64681d7a0c7f80303bebb0d62a3b7acbcdcd9",4,"An Improved Baseline for Reasoning Segmentation with Large Language Model","LISA++ is introduced, an update to the existing LISA model, focusing on improving core functionalities while keeping the base architecture intact, and its adaptability and improved features highlight the versatility of the mask-as-embedding paradigm proposed by LISA and the potential as a foundational model for diverse applications.","",2023,"Senqiao Yang,Tianyuan Qu,Xin Lai,Zhuotao Tian,Bohao Peng,Shu Liu,Jiaya Jia",0,42,0
"13b5b69355555e0c8b702261c5de3b4172ba653c","https://www.semanticscholar.org/paper/13b5b69355555e0c8b702261c5de3b4172ba653c",4,"The Art of SOCRATIC QUESTIONING: Zero-shot Multimodal Reasoning with Recursive Thinking and Self-Questioning","Qualitative analysis clearly demonstrates the intermediate thinking steps elicited by S OCRATIC Q UESTIONING are similar to the human’s recursively thinking process of a complex reasoning problem.","arXiv.org",2023,"Jingyuan Qi,Zhiyang Xu,Ying Shen,Minqian Liu,dingnan jin,Qifan Wang,Lifu Huang",8,27,0
"20fcc01d12a50f1da2af71d85f0a269b3ba48b77","https://www.semanticscholar.org/paper/20fcc01d12a50f1da2af71d85f0a269b3ba48b77",4,"LMEye: An Interactive Perception Network for Large Language Models","LMEye, a human-like eye with a play-and-plug interactive perception network, designed to enable dynamic interaction between LLMs and external vision information, is introduced, demonstrating that it significantly improves the zero-shot performance on various multimodal tasks compared to previous methods, with less parameters.","arXiv.org",2023,"Yunxin Li,Baotian Hu,Xinyu Chen,Lin Ma,M. Zhang",9,57,1
"8badb0587fef2ffc078b0cec549eb8ec96ed3ad4","https://www.semanticscholar.org/paper/8badb0587fef2ffc078b0cec549eb8ec96ed3ad4",4,"Self-Chained Image-Language Model for Video Localization and Question Answering","Self-Chained Video Localization-Answering (SeViLA), a novel framework that leverages a single image-language model (BLIP-2) to tackle both temporal keyframe localization and QA on videos, and achieves the state-of-the-art in both fine-tuning and zero-shot settings.","arXiv.org",2023,"Shoubin Yu,Jaemin Cho,Prateek Yadav,Mohit Bansal",19,96,6
"5d321194696f1f75cf9da045e6022b2f20ba5b9c","https://www.semanticscholar.org/paper/5d321194696f1f75cf9da045e6022b2f20ba5b9c",4,"Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding","Video-LLaMA shows the ability to perceive and comprehend video content and generate meaningful responses grounded in the visual and auditory information presented in the videos.","Conference on Empirical Methods in Natural Language Processing",2023,"Hang Zhang,Xin Li,Lidong Bing",127,42,21
"659a12d71d8709c132ccd9ccd235f0024cae0239","https://www.semanticscholar.org/paper/659a12d71d8709c132ccd9ccd235f0024cae0239",4,"The All-Seeing Project: Towards Panoptic Visual Recognition and Understanding of the Open World","The All-Seeing model (ASM), a unified framework for panoptic visual recognition and understanding, is developed with open-ended language prompts and locations, which allows it to generalize to various vision and language tasks with remarkable zero-shot performance.","arXiv.org",2023,"Weiyun Wang,Min Shi,Qingyun Li,Wen Wang,Zhenhang Huang,Linjie Xing,Zhe Chen,Hao Li,Xizhou Zhu,Zhiguo Cao,Yushi Chen,Tong Lu,Jifeng Dai,Y. Qiao",15,115,3
"2e3dcf5a5d58ac210d0d87e9f918540a8373211a","https://www.semanticscholar.org/paper/2e3dcf5a5d58ac210d0d87e9f918540a8373211a",4,"GIT-Mol: A Multi-modal Large Language Model for Molecular Science with Graph, Image, and Text","GIT-Mol is introduced, a multi-modal large language model that integrates the Graph, Image, and Text information and proposes GIT-Former, a novel architecture that is capable of aligning all modalities into a unified latent space.","arXiv.org",2023,"Peng Liu,Yiming Ren,Zhixiang Ren",5,63,1
"7f807249c0ef0fe07d5e9c810684cd5daba0edc5","https://www.semanticscholar.org/paper/7f807249c0ef0fe07d5e9c810684cd5daba0edc5",4,"De-fine: Decomposing and Refining Visual Programs with Auto-Feedback","This work introduces De-fine, a general framework that automatically decomposes complex tasks into simpler subtasks and refines programs through auto-feedback, which can improve logical reasoning performance by integrating the strengths of multiple models.","arXiv.org",2023,"Minghe Gao,Juncheng Li,Hao Fei,Liang Pang,Wei Ji,Guoming Wang,Wenqiao Zhang,Siliang Tang,Yueting Zhuang",0,40,0
"55c6d16b550c606d62dd85084f0d373d8f087966","https://www.semanticscholar.org/paper/55c6d16b550c606d62dd85084f0d373d8f087966",4,"VLAP: Efficient Video-Language Alignment via Frame Prompting and Distilling for Video Question Answering","This work proposes an efficient Video-Language Alignment via Frame-Prompting and Distilling (VLAP) network that addresses both efficient frame sampling and effective cross-modal alignment in a unified way and demonstrates the capability of selecting key frames with critical contents, thus improving the video-language alignment accuracy.","arXiv.org",2023,"Xijun Wang,Junbang Liang,Chun-Kai Wang,Kenan Deng,Yu Lou,Ming Lin,Shan Yang",0,65,0
"ea6982a936a2b263bbf46ff6eb27fc0b63fddaf7","https://www.semanticscholar.org/paper/ea6982a936a2b263bbf46ff6eb27fc0b63fddaf7",4,"VL-GPT: A Generative Pre-trained Transformer for Vision and Language Understanding and Generation",,"arXiv.org",2023,"Jinguo Zhu,Xiaohan Ding,Yixiao Ge,Yuying Ge,Sijie Zhao,Hengshuang Zhao,Xiaohua Wang,Ying Shan",1,55,0
"b1721374889899950994f67029fe899de257c140","https://www.semanticscholar.org/paper/b1721374889899950994f67029fe899de257c140",4,"A Foundational Multimodal Vision Language AI Assistant for Human Pathology","PathChat is presented, a vision-language generalist AI assistant for human pathology using an in-house developed foundational vision encoder pretrained on 100 million histology images from over 100,000 patient cases and 1.18 million pathology image-caption pairs.","arXiv.org",2023,"Ming Y. Lu,Bowen Chen,Drew F. K. Williamson,Richard J. Chen,Kenji Ikamura,Georg Gerber,Ivy Liang,L. Le,Tong Ding,Anil V. Parwani,Faisal Mahmood",1,126,0
"93886752191db25efd096a65af7b09df5c0a64e0","https://www.semanticscholar.org/paper/93886752191db25efd096a65af7b09df5c0a64e0",4,"Data-Centric Foundation Models in Computational Healthcare: A Survey","A wide range of data-centric approaches in the FM era (from model pre-training to inference) towards improving the healthcare workflow are investigated and a promising outlook of FM-based analytics to enhance the performance of patient outcome and clinical workflow in the evolving landscape of healthcare and medicine is offered.","",2024,"Yunkun Zhang,Jin Gao,Zheling Tan,Lingfeng Zhou,Kexin Ding,Mu Zhou,Shaoting Zhang,Dequan Wang",0,316,0
"8f3138f7ee5127faab265793be8ae278bc49d9b1","https://www.semanticscholar.org/paper/8f3138f7ee5127faab265793be8ae278bc49d9b1",4,"PMC-CLIP: Contrastive Language-Image Pre-training using Biomedical Documents","PMC-OA, a biomedical dataset with 1.6M image-caption pairs collected from PubMedCentral's OpenAccess subset, is built and released, which is 8 times larger than before and achieves state-of-the-art results on various downstream tasks.","International Conference on Medical Image Computing and Computer-Assisted Intervention",2023,"Weixiong Lin,Ziheng Zhao,Xiaoman Zhang,Chaoyi Wu,Ya Zhang,Yanfeng Wang,Weidi Xie",17,33,2
"f4793adffd6f67ffcb93ccfc5672ab301b8a2b96","https://www.semanticscholar.org/paper/f4793adffd6f67ffcb93ccfc5672ab301b8a2b96",4,"PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering","This paper proposes a generative-based model for medical visual understanding by aligning visual information from a pre-trained vision encoder with a large language model, and establishes a scalable pipeline to construct a large-scale medical visual question-answering dataset.","arXiv.org",2023,"Xiaoman Zhang,Chaoyi Wu,Ziheng Zhao,Weixiong Lin,Ya Zhang,Yanfeng Wang,Weidi Xie",31,38,2
"bf40c9e7832e1b2887cbf5798455f91705ea11ba","https://www.semanticscholar.org/paper/bf40c9e7832e1b2887cbf5798455f91705ea11ba",4,"Masked Vision and Language Pre-training with Unimodal and Multimodal Contrastive Losses for Medical Visual Question Answering","This paper presents a novel self-supervised approach that learns unimodal and multimodal feature representations of input images and text using medical image caption datasets, by leveraging both unimmodal and multi-modal contrastive losses, along with masked language modeling and image text matching as pretraining objectives.","International Conference on Medical Image Computing and Computer-Assisted Intervention",2023,"Pengfei Li,Gang Liu,Jinlong He,Zixu Zhao,Shenjun Zhong",3,26,0
"df0ddb588a200d095743e9d26fc4a9318619766e","https://www.semanticscholar.org/paper/df0ddb588a200d095743e9d26fc4a9318619766e",4,"Towards Generalist Foundation Model for Radiology","This study constructs a large-scale Medical Multi-modal Dataset, MedMD, which consists of 16M 2D and 3D medical scans with high-quality text descriptions or reports across various data formats, modalities, and tasks, covering over 5000 distinct diseases, and proposes a new evaluation benchmark, RadBench, aiming to comprehensively assess the capability of foundation models in handling practical clinical problems.","arXiv.org",2023,"Chaoyi Wu,Xiaoman Zhang,Ya Zhang,Yanfeng Wang,Weidi Xie",19,59,1
"1f5e1a036b24b9dd34c006ba3bb61119624f4fdb","https://www.semanticscholar.org/paper/1f5e1a036b24b9dd34c006ba3bb61119624f4fdb",4,"A Comprehensive Study of GPT-4V's Multimodal Capabilities in Medical Imaging","This paper presents a comprehensive evaluation of GPT-4V's capabilities across diverse medical imaging tasks, including Radiology Report Generation, Medical Visual Question Answering (VQA), and Visual Grounding, and finds the limitations of conventional evaluation metrics like the BLEU score.","medRxiv",2023,"Yingshu Li,Yunyi Liu,Zhanyu Wang,Xinyu Liang,Lingqiao Liu,Lei Wang,Leyang Cui,Zhaopeng Tu,Longyue Wang,Luping Zhou",5,67,1
"a3711dbf296b5ddd97ba93826660cd3995611625","https://www.semanticscholar.org/paper/a3711dbf296b5ddd97ba93826660cd3995611625",3,"Towards A Foundation Model for Generalist Robots: Diverse Skill Learning at Scale via Automated Task and Scene Generation",,"arXiv.org",2023,"Zhou Xian,Théophile Gervet,Zhenjia Xu,Yi-Ling Qiao,Tsun-Hsuan Wang",4,111,0
"692bc40edf4785d88c39e0c0fe9f270541fecf8a","https://www.semanticscholar.org/paper/692bc40edf4785d88c39e0c0fe9f270541fecf8a",3,"Towards Generalist Robots: A Promising Paradigm via Generative Simulation","This document presents a specific idea for mining knowledge in the latest large-scale foundation models for robotics research, which uses a fully automated generative pipeline which uses these models to generate diversified tasks, scenes and training supervisions at scale, thereby scaling up low-level skill learning and ultimately leading to a foundation model for robotics that empowers generalist robots.","",2023,"Zhou Xian,Théophile Gervet,Zhenjia Xu,Yi-Ling Qiao,Tsun-Hsuan Wang,Yian Wang",1,124,0
"8ec7d50250203543a0098d99f04957b22bbe2c77","https://www.semanticscholar.org/paper/8ec7d50250203543a0098d99f04957b22bbe2c77",3,"How to Bridge the Gap between Modalities: A Comprehensive Survey on Multimodal Large Language Model","This paper aims to explore modality alignment methods for LLMs and their existing capabilities, and surveys existing modal alignment methods in MLLMs into four groups: Multimodal Converters that change data into something LLMs can understand, and Data-Driven methods that teach LLMs to understand specific types of data in a dataset.","arXiv.org",2023,"Shezheng Song,Xiaopeng Li,Shasha Li",0,117,0
"44ccf252018f71898d52d89539f17d77a4f8d548","https://www.semanticscholar.org/paper/44ccf252018f71898d52d89539f17d77a4f8d548",3,"Chart Understanding with Large Language Model","A baseline multimodal model is introduced that integrates text and charts to enhance the chart comprehension capabilities of existing models, offering more pertinent insights and information related to the depicted charts.","",,"Yaser James,Will Li,John Feng",0,39,0
"5ce94181ea702f69c3651dce721d6bd8026b8106","https://www.semanticscholar.org/paper/5ce94181ea702f69c3651dce721d6bd8026b8106",3,"TPTU: Task Planning and Tool Usage of Large Language Model-based AI Agents","A structured framework tailored for LLM-based AI Agents is proposed and two distinct types of agents are designed to execute the inference process, which emphasizes the substantial potential of these models, while also identifying areas that need more investigation and improvement.","arXiv.org",2023,"Jingqing Ruan,Yihong Chen,Bin Zhang,Zhiwei Xu,Tianpeng Bao,Guoqing Du,Shiwei Shi,Hangyu Mao,Xingyu Zeng,Rui Zhao",22,91,3
"96a6df2b4aa50cfbd8984933e9c66b0763fc08a6","https://www.semanticscholar.org/paper/96a6df2b4aa50cfbd8984933e9c66b0763fc08a6",3,"Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V",,"",2023,"Jianwei Yang,Hao Zhang,Feng Li,Xueyan Zou,Chun-yue Li,Jianfeng Gao",17,88,0
"6845bea94b2fb17d4377b3bb2bd10f73a959f9cc","https://www.semanticscholar.org/paper/6845bea94b2fb17d4377b3bb2bd10f73a959f9cc",3,"Reasoning with Language Model Prompting: A Survey","This paper provides a comprehensive survey of cutting-edge research on reasoning with language model prompting with comparisons and summaries and provides systematic resources to help beginners.","Annual Meeting of the Association for Computational Linguistics",2022,"Shuofei Qiao,Yixin Ou,Ningyu Zhang,Xiang Chen,Yunzhi Yao,Shumin Deng,Chuanqi Tan,Fei Huang,Huajun Chen",101,219,3
"170c97c7215f42edfb20c2248f954879e91ef86e","https://www.semanticscholar.org/paper/170c97c7215f42edfb20c2248f954879e91ef86e",3,"Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models","This paper demonstrates the effectiveness of Chameleon on two multi-modal knowledge-intensive reasoning tasks: ScienceQA and TabMWP and shows that the GPT-4-powered planner exhibits more consistent and rational tool selection via inferring potential constraints from instructions, compared to a ChatGPT- powered planner.","arXiv.org",2023,"Pan Lu,Baolin Peng,Hao Cheng,Michel Galley,Kai-Wei Chang,Y. Wu,Song-Chun Zhu,Jianfeng Gao",113,72,15
"66d755730f5d08a6f4fcc5e81f24982ba389dca9","https://www.semanticscholar.org/paper/66d755730f5d08a6f4fcc5e81f24982ba389dca9",3,"LayoutGPT: Compositional Visual Planning and Generation with Large Language Models","This work proposes LayoutGPT, a method to compose in-context visual demonstrations in style sheet language to enhance the visual planning skills of LLMs, and shows superior performance in converting challenging language concepts like numerical and spatial relations to layout arrangements for faithful text-to-image generation.","arXiv.org",2023,"Weixi Feng,Wanrong Zhu,Tsu-Jui Fu,Varun Jampani,Arjun Reddy Akula,Xuehai He,Sugato Basu,X. Wang,William Yang Wang",23,65,4
"5ff2f5212713ec424662ac3c9e4aa5a8790d40cf","https://www.semanticscholar.org/paper/5ff2f5212713ec424662ac3c9e4aa5a8790d40cf",3,"ANPL: Towards Natural Programming with Interactive Decomposition","This paper introduces ANPL, an interactive programming system that ensures users can always refine the generated code towards their specific programmatic intents via structured decompositions, and deploys ANPL on the Abstraction and Reasoning Corpus, a set of unique tasks that are challenging for state-of-the-art AI systems.","",2023,"Di Huang,Ziyuan Nan,Xingui Hu,Pengwei Jin,Shaohui Peng,Yuanbo Wen,Rui Zhang,Zidong Du,Qi Guo,Yewen Pu,Yunji Chen",1,71,0
"af705d648b5b16daa3dcc593bc593f2574d76c07","https://www.semanticscholar.org/paper/af705d648b5b16daa3dcc593bc593f2574d76c07",3,"Grammar Prompting for Domain-Specific Language Generation with Large Language Models","Experiments demonstrate that grammar prompting can enable LLMs to perform competitively on a diverse set of DSL generation tasks, including semantic parsing, Overnight, GeoQuery, PDDL planning, and even molecule generation (SMILES).","arXiv.org",2023,"Bailin Wang,Zi Wang,Xuezhi Wang,Yuan Cao,R. Saurous,Yoon Kim",3,101,0
"ea566f87f6253bb2d32cf7b61cd3e2535a0c3f42","https://www.semanticscholar.org/paper/ea566f87f6253bb2d32cf7b61cd3e2535a0c3f42",3,"mPLUG-DocOwl: Modularized Multimodal Large Language Model for Document Understanding","Experimental results show that the proposed mPLUG-DocOwl model outperforms existing multi-modal models, demonstrating its strong ability of document understanding, and also generalizes well on various downstream tasks.","arXiv.org",2023,"Jiabo Ye,Anwen Hu,Haiyang Xu,Qinghao Ye,Mingshi Yan,Yuhao Dan,Chenlin Zhao,Guohai Xu,Chenliang Li,Junfeng Tian,Qiang Qi,Ji Zhang,Feiyan Huang",19,37,5
"446fb5dead075a1a08862662738f462e9a0e91c8","https://www.semanticscholar.org/paper/446fb5dead075a1a08862662738f462e9a0e91c8",3,"Tool Documentation Enables Zero-Shot Tool-Usage with Large Language Models","This work advocates the use of tool documentation, descriptions for the individual tool usage, over demonstrations, and shows that tool documentation is significantly more valuable than demonstrations, with zero-shot documentation significantly outperforming few-shot without documentation.","arXiv.org",2023,"Cheng-Yu Hsieh,Sibei Chen,Chun-Liang Li,Yasuhisa Fujii,Alexander J. Ratner,Chen-Yu Lee,Ranjay Krishna,Tomas Pfister",11,85,0
"dd0612ce863f64b0f69d0d9f708c52e829f6f859","https://www.semanticscholar.org/paper/dd0612ce863f64b0f69d0d9f708c52e829f6f859",3,"TPTU: Large Language Model-based AI Agents for Task Planning and Tool Usage","A structured framework tailored for LLM-based AI Agents is proposed and two distinct types of agents are designed to execute the inference process, which emphasizes the substantial potential of these models, while also identifying areas that need more investigation and improvement.","",2023,"Jingqing Ruan,Yihong Chen,Bin Zhang,Zhiwei Xu,Tianpeng Bao,Guoqing Du,Shiwei Shi,Hangyu Mao,Xingyu Zeng,Rui Zhao",3,95,0
"7f1ba5630c3baa09b11cc665b3f71cdb117e5ffb","https://www.semanticscholar.org/paper/7f1ba5630c3baa09b11cc665b3f71cdb117e5ffb",3,"OpenLEAF: Open-Domain Interleaved Image-Text Generation and Evaluation","A new interleaved generation framework based on prompting large-language models (LLMs) and pre-trained text-to-image (T2I) models, namely OpenLEAF is proposed, which can generate high-quality image-text content for various domains and applications.","arXiv.org",2023,"Jie An,Zhengyuan Yang,Linjie Li,Jianfeng Wang,K. Lin,Zicheng Liu,Lijuan Wang,Jiebo Luo",2,40,0
"1d14a708622917da4b9820ada6d32af24fc1651a","https://www.semanticscholar.org/paper/1d14a708622917da4b9820ada6d32af24fc1651a",3,"Idea2Img: Iterative Self-Refinement with GPT-4V(ision) for Automatic Image Design and Generation",,"arXiv.org",2023,"Zhengyuan Yang,Jianfeng Wang,Linjie Li,Kevin Lin,Chung-Ching Lin,Zicheng Liu,Lijuan Wang",4,56,0
"c020f15be1dee20f9e2e0c5a6f05f272b5508325","https://www.semanticscholar.org/paper/c020f15be1dee20f9e2e0c5a6f05f272b5508325",3,"LLaVA-Interactive: An All-in-One Demo for Image Chat, Segmentation, Generation and Editing","The development of LLaVA-Interactive is extremely cost-efficient as the system combines three multimodal skills of pre-built AI models without additional model training: visual chat of L LaVA, image segmentation from SEEM, as well as image generation and editing from GLIGEN.","arXiv.org",2023,"Wei-Ge Chen,Irina Spiridonova,Jianwei Yang,Jianfeng Gao,Chun-yue Li",3,36,0
"2fb605f67fee79cad94952ddfe0f686e926f49f5","https://www.semanticscholar.org/paper/2fb605f67fee79cad94952ddfe0f686e926f49f5",3,"GPT-4V in Wonderland: Large Multimodal Models for Zero-Shot Smartphone GUI Navigation","The findings demonstrate that large multimodal models, specifically GPT-4V, excel in zero-shot GUI navigation through its advanced screen interpretation, action reasoning, and precise action localization capabilities.","arXiv.org",2023,"An Yan,Zhengyuan Yang,Wanrong Zhu,K. Lin,Linjie Li,Jianfeng Wang,Jianwei Yang,Yiwu Zhong,Julian McAuley,Jianfeng Gao,Zicheng Liu,Lijuan Wang",5,58,0
"35a17f896847614a71df772bbe2b66ae231cabc7","https://www.semanticscholar.org/paper/35a17f896847614a71df772bbe2b66ae231cabc7",3,"CLOVA: A Closed-Loop Visual Assistant with Tool Usage and Update","Experiments show that CLOVA outperforms tool-usage methods by 5% in visual question answering and multiple-image reasoning tasks, by 10% in knowledge tagging tasks, and by 20% in image editing tasks, highlighting the significance of the learning capability for general visual assistants.","",2023,"Zhi Gao,Yuntao Du,Xintong Zhang,Xiaojian Ma,Wenjuan Han,Song-Chun Zhu,Qing Li",0,83,0
"24fc9ad715372358bd0108eeb7c944b915963293","https://www.semanticscholar.org/paper/24fc9ad715372358bd0108eeb7c944b915963293",3,"ASSISTGUI: Task-Oriented Desktop Graphical User Interface Automation","An advanced Actor-Critic Embodied Agent framework is proposed, which incorporates a sophisticated GUI parser driven by an LLM-agent and an enhanced reasoning mechanism adept at handling lengthy procedural tasks that outshine existing methods in performance.","",2023,"Difei Gao,Lei Ji,Zechen Bai,Mingyu Ouyang,Peiran Li,Dongxing Mao,Qinchen Wu,Weichen Zhang,Peiyi Wang,Xiangwu Guo,Hengxu Wang,Luowei Zhou,Mike Zheng Shou",0,54,0
"a06d3e9e90008c64c45a0029d580541d5f646771","https://www.semanticscholar.org/paper/a06d3e9e90008c64c45a0029d580541d5f646771",3,"If LLM Is the Wizard, Then Code Is the Wand: A Survey on How Code Empowers Large Language Models to Serve as Intelligent Agents","An overview of the various benefits of integrating code into LLMs' training data and how these profound capabilities of LLMs, brought by code, have led to their emergence as intelligent agents in situations where the ability to understand instructions, decompose goals, plan and execute actions, and refine from feedback are crucial to their success on downstream tasks are traced.","",2024,"Ke Yang,Jiateng Liu,John Wu,Chaoqi Yang,Y. Fung,Sha Li,Zixuan Huang,Xu Cao,Xingyao Wang,Yiquan Wang,Heng Ji,Chengxiang Zhai",1,168,0
"9eab4104973f5de650544729a4a69d84c594da92","https://www.semanticscholar.org/paper/9eab4104973f5de650544729a4a69d84c594da92",3,"A Vision Check-up for Language Models","Although LLM-generated images do not look like natural images, results on image generation and the ability of models to correct these generated images indicate that precise modeling of strings can teach language models about numerous aspects of the visual world.","",2024,"Pratyusha Sharma,Tamar Rott Shaham,Manel Baradad,Stephanie Fu,Adrian Rodriguez-Munoz,Shivam Duggal,Phillip Isola,Antonio Torralba",0,48,0
"d1120d67b700e4dfe8b39eb1e48fbdea4e1a0c43","https://www.semanticscholar.org/paper/d1120d67b700e4dfe8b39eb1e48fbdea4e1a0c43",3,"HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face","HuggingGPT is an LLM-powered agent that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities to solve AI tasks and can tackle a wide range of sophisticated AI tasks spanning different modalities and domains.","arXiv.org",2023,"Yongliang Shen,Kaitao Song,Xu Tan,D. Li,Weiming Lu,Y. Zhuang",349,42,46
"43e6e8d6663d83f1b74cf5a2be7b040b0928f867","https://www.semanticscholar.org/paper/43e6e8d6663d83f1b74cf5a2be7b040b0928f867",3,"X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages","X-LLM is proposed, which converts Multi-modalities into foreign languages using X2L interfaces and inputs them into a large Language model (ChatGLM), and demonstrates impressive multimodel chat abilities.","arXiv.org",2023,"Feilong Chen,Minglun Han,Haozhi Zhao,Qingyang Zhang,Jing Shi,Shuang Xu,Bo Xu",41,61,6
"6a5525c316b9be7909c433a79e090ed731425083","https://www.semanticscholar.org/paper/6a5525c316b9be7909c433a79e090ed731425083",3,"What Makes for Good Visual Tokenizers for Large Language Models?","A new MLLM equipped with a tailored Good Visual Tokenizer (GVT), which exhibits strong visual comprehension capability at multiple scales, without introducing extra parameters and task-specific fine-tuning.","arXiv.org",2023,"Guangzhi Wang,Yixiao Ge,Xiaohan Ding,Mohan S. Kankanhalli,Ying Shan",12,54,1
"ca055cfb9d4d47124cc035c346f38577825fcacf","https://www.semanticscholar.org/paper/ca055cfb9d4d47124cc035c346f38577825fcacf",3,"Enhance Reasoning Ability of Visual-Language Models via Large Language Models","A method called TReE is proposed, which transfers the reasoning ability of a large language model to a visual language model in zero-shot scenarios, and contains three stages: observation, thinking, and re-thinking.","arXiv.org",2023,"Yueting Yang,Xintong Zhang,Wenjuan Han",0,40,0
"c6ac708b65b24c20f80831d518c1795ce8133ad5","https://www.semanticscholar.org/paper/c6ac708b65b24c20f80831d518c1795ce8133ad5",3,"ChatBridge: Bridging Modalities with Large Language Model as a Language Catalyst","It is shown that only language-paired two-modality data is sufficient to connect all modalities and ChatBridge, a novel multimodal language model that leverages the expressive capabilities of language as the catalyst to bridge the gap between various modalities, is presented.","arXiv.org",2023,"Zijia Zhao,Longteng Guo,Tongtian Yue,Si-Qing Chen,Shuai Shao,Xinxin Zhu,Zehuan Yuan,Jing Liu",15,72,3
"50c1414fe41d0cb9db6f0933c9319aa124beac5d","https://www.semanticscholar.org/paper/50c1414fe41d0cb9db6f0933c9319aa124beac5d",3,"Contextual Object Detection with Multimodal Large Language Models","The ContextDET is presented, a unified multimodal model that is capable of end-to-end differentiable modeling of visual-language contexts, so as to locate, identify, and associate visual objects with language inputs for human-AI interaction.","arXiv.org",2023,"Yuhang Zang,Wei Li,Jun Han,Kaiyang Zhou,Chen Change Loy",14,87,0
"42ea55edb46395469aee1b760829657e65ab6577","https://www.semanticscholar.org/paper/42ea55edb46395469aee1b760829657e65ab6577",3,"Zero-Shot 3D Shape Correspondence","This work introduces a fully automatic method that exploits the exceptional reasoning capabilities of recent foundation models in language and vision to tackle difficult shape correspondence problems and produces highly plausible results in a zero-shot manner, especially between strongly non-isometric shapes.","ACM SIGGRAPH Conference and Exhibition on Computer Graphics and Interactive Techniques in Asia",2023,"Ahmed Abdelreheem,Abdelrahman Eldesokey,M. Ovsjanikov,Peter Wonka",6,67,0
"fed150a219f9c31bdb4920e615c7c9264c634736","https://www.semanticscholar.org/paper/fed150a219f9c31bdb4920e615c7c9264c634736",3,"On the Challenges and Perspectives of Foundation Models for Medical Image Analysis","The ""spectrum"" of medical foundation models, ranging from general imaging models, modality-specific models, to organ/task- specific models, are illustrated and highlighted, to highlight their challenges, opportunities and applications.","Medical Image Anal.",2023,"Shaoting Zhang,Dimitris N. Metaxas",13,88,0
"79150cb420d15830c8d36f0e91eea1b02e177f0f","https://www.semanticscholar.org/paper/79150cb420d15830c8d36f0e91eea1b02e177f0f",3,"Sticker820K: Empowering Interactive Retrieval with Stickers","The StickerCLIP is proposed as a benchmark model on the Sticker820K dataset, demonstrating strong superiority over the CLIP for the text-to-image retrieval task, and the recently popularized LLM is extended by means of prompt tuning, integrating its ability for sticker retrieval and allowing users to retrieve stickers through instructions.","arXiv.org",2023,"Sijie Zhao,Yixiao Ge,Zhongang Qi,Lin Song,Xiaohan Ding,Zehua Xie,Ying Shan",0,36,0
"697e0add95e880bd42e00bef838181e105f91981","https://www.semanticscholar.org/paper/697e0add95e880bd42e00bef838181e105f91981",3,"MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models","This paper presents the first comprehensive MLLM Evaluation benchmark MME, which measures both perception and cognition abilities on a total of 14 subtasks and suggests that existing MLLMs still have a large room for improvement, but also reveals the potential directions for the subsequent model optimization.","arXiv.org",2023,"Chaoyou Fu,Peixian Chen,Yunhang Shen,Yulei Qin,Mengdan Zhang,Xu Lin,Zhenyu Qiu,Wei Lin,Jinrui Yang,Xiawu Zheng,Ke Li,Xing Sun,Rongrong Ji",105,60,22
"efc694164312006c543ef745611348ef64e68dda","https://www.semanticscholar.org/paper/efc694164312006c543ef745611348ef64e68dda",3,"Towards Language Models That Can See: Computer Vision Through the LENS of Natural Language","This work proposes LENS, a modular approach for tackling computer vision problems by leveraging the power of large language models (LLMs) with a language model to reason over outputs from a set of independent and highly descriptive vision modules that provide exhaustive information about an image.","arXiv.org",2023,"William Berrios,Gautam Mittal,Tristan Thrush,Douwe Kiela,Amanpreet Singh",17,64,1
"451a3f03aca4aa87b93981364842137417549e58","https://www.semanticscholar.org/paper/451a3f03aca4aa87b93981364842137417549e58",3,"SVIT: Scaling up Visual Instruction Tuning","Scale up Visual Instruction Tuning (SVIT) by constructing a dataset of 4.2 million visual instruction tuning data and proposing a new data recipe to select subset with better diversity and balance, which evokes model's superior capabilities.","arXiv.org",2023,"Bo Zhao,Boya Wu,Tiejun Huang",25,55,2
"962ccf1fc49c83817fb031e5b24b81b19cdfb89d","https://www.semanticscholar.org/paper/962ccf1fc49c83817fb031e5b24b81b19cdfb89d",3,"BuboGPT: Enabling Visual Grounding in Multi-Modal LLMs","BozoGPT is a multi-modal LLM with visual grounding that can perform cross-modAL interaction between vision, audio and language, providing fine-grained understanding of visual objects and other given modalities and performs consistently well when provided by arbitrary modalities.","arXiv.org",2023,"Yang Zhao,Zhijie Lin,Daquan Zhou,Zilong Huang,Jiashi Feng,Bingyi Kang",27,33,1
"813ba033b8f593c98f9af44c5b4901408ba6f70a","https://www.semanticscholar.org/paper/813ba033b8f593c98f9af44c5b4901408ba6f70a",3,"Towards a Visual-Language Foundation Model for Computational Pathology","This work introduces CONtrastive learning from Captions for Histopathology (CONCH), a visual-language foundation model developed using diverse sources of histopathology images, biomedical text, and notably over 1.17 million image-caption pairs via task-agnostic pretraining, with the potential to directly facilitate a wide array of machine learning-based workflows requiring minimal or no further supervised fine-tuning.","arXiv.org",2023,"Ming Y. Lu,Bowen Chen,Drew F. K. Williamson,Richard J. Chen,Ivy Liang,Tong Ding,Guillaume Jaume,I. Odintsov,Andrew Zhang,L. Le,G. Gerber,A. Parwani,Faisal Mahmood",9,105,0
"94972e30504017156ef5b5debc419bf6edc67384","https://www.semanticscholar.org/paper/94972e30504017156ef5b5debc419bf6edc67384",3,"MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities","MM-Vet is proposed, an evaluation benchmark that examines large multimodal models (LMMs) on complicated multimodAL tasks and proposes an LLM-based evaluator for open-ended outputs that enables the evaluation across different question types and answer styles, resulting in a unified scoring metric.","arXiv.org",2023,"Weihao Yu,Zhengyuan Yang,Linjie Li,Jianfeng Wang,Kevin Lin,Zicheng Liu,Xinchao Wang,Lijuan Wang",56,91,5
"4f2be887e991efa85f7b874e7ab871080a745c39","https://www.semanticscholar.org/paper/4f2be887e991efa85f7b874e7ab871080a745c39",3,"CAESURA: Language Models as Multi-Modal Query Planners","This paper proposes Language-Model-Driven Query Planning, a new paradigm of query planning that uses Language Models to translate natural language queries into executable query plans that can contain complex operators that are able to process arbitrary modalities.","arXiv.org",2023,"Matthias Urban,Carsten Binnig",0,30,0
"1fd31b74f5e1eeb67341982fd35a613c6fad10e0","https://www.semanticscholar.org/paper/1fd31b74f5e1eeb67341982fd35a613c6fad10e0",3,"Link-Context Learning for Multimodal LLMs","This work proposes link-context learning (LCL), which emphasizes ""reasoning from cause and effect"" to augment the learning capabilities of MLLMs and introduces the ISEKAI dataset, comprising exclusively of unseen generated image-label pairs designed for link- context learning.","arXiv.org",2023,"Yan Tai,Weichen Fan,Zhao Zhang,Feng Zhu,Rui Zhao,Ziwei Liu",2,33,1
"da96ec9c32d63292e506ba8f8ea8e838df998c02","https://www.semanticscholar.org/paper/da96ec9c32d63292e506ba8f8ea8e838df998c02",3,"StableLLaVA: Enhanced Visual Instruction Tuning with Synthesized Image-Dialogue Data","This work proposes a novel data collection methodology that synchronously synthesizes images and dialogues for visual instruction tuning, marrying the abilities of ChatGPT and text-to-image generative models to yield a diverse and controllable dataset with varied image content.","arXiv.org",2023,"Yanda Li,Chi Zhang,Gang Yu,Zhibin Wang,Bin Fu,Guosheng Lin,Chunhua Shen,Ling Chen,Yunchao Wei",5,42,1
"ef1ebdb24ce45fb57e271783a0c9a877fe0e79c3","https://www.semanticscholar.org/paper/ef1ebdb24ce45fb57e271783a0c9a877fe0e79c3",3,"Position-Enhanced Visual Instruction Tuning for Multimodal Large Language Models","This paper proposes Position-enhanced Visual Instruction Tuning (PVIT), which extends the functionality of MLLMs by integrating an additional region-level vision encoder, which promotes a more detailed comprehension of images for the MLLM.","arXiv.org",2023,"Chi Chen,Ruoyu Qin,Fuwen Luo,Xiaoyue Mi,Peng Li,Maosong Sun,Yang Liu",16,34,1
"fa75a55760e6ea49b39b83cb85c99a22e1088254","https://www.semanticscholar.org/paper/fa75a55760e6ea49b39b83cb85c99a22e1088254",3,"NExT-GPT: Any-to-Any Multimodal LLM","This research showcases the promising possibility of building an AI agent capable of modeling universal modalities, paving the way for more human-like AI research in the community.","arXiv.org",2023,"Shengqiong Wu,Hao Fei,Leigang Qu,Wei Ji,Tat-Seng Chua",61,111,6
"d39182113cd4176ead48027b4fc05fe06ec6aaca","https://www.semanticscholar.org/paper/d39182113cd4176ead48027b4fc05fe06ec6aaca",3,"Language Models as Black-Box Optimizers for Vision-Language Models","This work proposes employing chat-based LLMs to search for the best text prompt for VLMs and highlights the advantage of conversational feedback that incorporates both positive and negative prompts, suggesting that LLMs can utilize the implicit gradient direction in textual feedback for a more efficient search.","arXiv.org",2023,"Samuel Yu,Shihong Liu,Zhiqiu Lin,Deepak Pathak,Deva Ramanan",1,107,0
"7a7128e9696dfedc24bf432c4b4c3aafa5e95a1a","https://www.semanticscholar.org/paper/7a7128e9696dfedc24bf432c4b4c3aafa5e95a1a",3,"An Empirical Study of Scaling Instruct-Tuned Large Multimodal Models","An empirical study of scaling LLaVA up to 33B and 65B/70B and performance of LoRA/QLoRA tuning of LMM are comparable to the performance of full-model fine-tuning, finding that scaling LMM consistently enhances model performance and improves language capabilities.","arXiv.org",2023,"Yadong Lu,Chunyuan Li,Haotian Liu,Jianwei Yang,Jianfeng Gao,Yelong Shen",6,22,0
"20ae101289965d36dbd93e9b8c47ec9deab03ed0","https://www.semanticscholar.org/paper/20ae101289965d36dbd93e9b8c47ec9deab03ed0",3,"What If the TV Was Off? Examining Counterfactual Reasoning Abilities of Multi-modal Language Models","This work benchmarking the counterfactual reasoning ability of multimodal large language models found that all models exhibit a large performance drop compared to the results tested on questions without counterfactual presupposition, indicating that there still exists space for developing vision language models.","2023 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)",2023,"Letian Zhang,Xiaotong Zhai,Zhongkai Zhao,Xin Wen,Yongshuo Zong,Bingchen Zhao",1,52,0
"e7d09b6f2bc878cf2c993acf675f409d0b55f35a","https://www.semanticscholar.org/paper/e7d09b6f2bc878cf2c993acf675f409d0b55f35a",3,"MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens","This work introduces an innovative interleaved vision-and-language generation technique anchored by the concept of ""generative vokens,"" which acts as the bridge for harmonized image-text outputs and exhibits substantial improvement over the baseline Divter model on the MMDialog dataset.","arXiv.org",2023,"Kaizhi Zheng,Xuehai He,Xin Eric Wang",15,38,0
"84f9bc5f89dac53662fb467b6af8ff26415ca3e7","https://www.semanticscholar.org/paper/84f9bc5f89dac53662fb467b6af8ff26415ca3e7",3,"InstructDET: Diversifying Referring Object Detection with Generalized Instructions","A data-centric method for referring object detection (ROD) that localizes target objects based on user instructions and shows that a conventional ROD model surpasses existing methods on standard REC datasets and the authors' InDET test set.","arXiv.org",2023,"Ronghao Dang,Jiangyan Feng,Haodong Zhang,Chongjian Ge,Lin Song,Lijun Gong,Chengju Liu,Qi Chen,Feng Zhu,Rui Zhao,Yibing Song",1,62,0
"33095b1334bed852e3652bd9d7da3f4df0cdf485","https://www.semanticscholar.org/paper/33095b1334bed852e3652bd9d7da3f4df0cdf485",3,"ViCor: Bridging Visual Understanding and Commonsense Reasoning with Large Language Models","This work explores the synergistic capabilities of pre-trained vision-and-language models (VLMs) and large language models (LLMs) for visual commonsense reasoning (VCR) and suggests a collaborative approach where LLMs, when uncertain about their reasoning, actively direct VLMs to concentrate on and gather relevant visual elements to support potential commonsense inferences.","arXiv.org",2023,"KAI-QING Zhou,Kwonjoon Lee,Teruhisa Misu,Xin Eric Wang",1,34,0
"b3e9f249dd2e09ec111496f6b533101e8217a5b0","https://www.semanticscholar.org/paper/b3e9f249dd2e09ec111496f6b533101e8217a5b0",3,"Multimodal Large Language Model for Visual Navigation","This work designs a model that combines a simple text prompt, current observations, and a history collector model that gathers information from previous observations as input and provides a probability distribution of possible actions that the agent can take during navigation.","arXiv.org",2023,"Yao-Hung Tsai,Vansh Dhar,Jialu Li,Bowen Zhang,Jian Zhang",1,46,0
"ac2e5bf716aed246ca8914a6816ef73e00286099","https://www.semanticscholar.org/paper/ac2e5bf716aed246ca8914a6816ef73e00286099",3,"Beyond Segmentation: Road Network Generation with Multi-Modal LLMs","An innovative approach to road network generation through the utilization of a multi-modal Large Language Model (LLM), specifically designed to process aerial images of road layouts and produce detailed, navigable road networks within the input images.","arXiv.org",2023,"Sumedh Rasal,S. Boddhu",2,34,0
"0b395ed1c8b284e551172b728e83cf257e33729a","https://www.semanticscholar.org/paper/0b395ed1c8b284e551172b728e83cf257e33729a",3,"HallusionBench: An Advanced Diagnostic Suite for Entangled Language Hallucination&Visual Illusion in Large Vision-Language Models","This work introduces HallusionBench, a comprehensive benchmark designed for the evaluation of image-context reasoning, and introduces a novel structure for these visual questions designed to establish control groups, which enables a quantitative analysis of the models' response tendencies, logical consistency, and various failure modes.","",2023,"Tianrui Guan,Fuxiao Liu,Xiyang Wu,Ruiqi Xian,Zongxia Li,Xiaoyu Liu,Xijun Wang,Lichang Chen,Furong Huang,Yaser Yacoob,Dinesh Manocha,Tianyi Zhou",1,57,0
"f8b8f926bbfa327c86c40796131fe2695db81126","https://www.semanticscholar.org/paper/f8b8f926bbfa327c86c40796131fe2695db81126",3,"DDCoT: Duty-Distinct Chain-of-Thought Prompting for Multimodal Reasoning in Language Models","This study proposes a novel DDCoT prompting that maintains a critical attitude through negative-space prompting and incorporates multimodality into reasoning by first dividing the reasoning responsibility of LLMs into reasoning and recognition and then integrating the visual recognition capability of visual models into the joint reasoning process.","arXiv.org",2023,"Ge Zheng,Bin Yang,Jiajin Tang,Hong-Yu Zhou,Sibei Yang",4,75,1
"8e5d42f5b98146d0784fe85e29c768a4989e1478","https://www.semanticscholar.org/paper/8e5d42f5b98146d0784fe85e29c768a4989e1478",3,"Foundational Models in Medical Imaging: A Comprehensive Survey and Future Vision","A methodical taxonomy of foundation models within the medical domain is offered, proposing a classification system primarily structured around training strategies, while also incorporating additional facets such as application domains, imaging modalities, specific organs of interest, and the algorithms integral to these models.","arXiv.org",2023,"Bobby Azad,Reza Azad,Sania Eskandari,Afshin Bozorgpour,A. Kazerouni,I. Rekik,D. Merhof",7,92,0
"c62711f6b5d8620ba36bc2c378ec6ab53f6e197c","https://www.semanticscholar.org/paper/c62711f6b5d8620ba36bc2c378ec6ab53f6e197c",3,"RoboGen: Towards Unleashing Infinite Data for Automated Robot Learning via Generative Simulation","RoboGen is presented, a generative robotic agent that automatically learns diverse robotic skills at scale via generative simulation and attempts to extract the extensive and versatile knowledge embedded in large-scale models and transfer them to the field of robotics.","arXiv.org",2023,"Yufei Wang,Zhou Xian,Feng Chen,Tsun-Hsuan Wang,Yian Wang,Zackory M. Erickson,David Held,Chuang Gan",5,127,0
"6ae4705139494fcb6b790b6dd6c4225b40ee40f8","https://www.semanticscholar.org/paper/6ae4705139494fcb6b790b6dd6c4225b40ee40f8",3,"GLaMM: Pixel Grounding Large Multimodal Model","This work presents Grounding LMM (GLaMM), the first model that can generate natural language responses seamlessly intertwined with corresponding object segmentation masks and is flexible enough to accept both textual and optional visual prompts (region of interest) as input.","arXiv.org",2023,"H. Rasheed,Muhammad Maaz,Sahal Shaji Mullappilly,Abdelrahman M. Shaker,Salman H. Khan,Hisham Cholakkal,R. Anwer,Erix Xing,Ming-Hsuan Yang,F. Khan",8,63,2
"76a3f4a79ae9a00db2f2b5f6877021d8deb96ada","https://www.semanticscholar.org/paper/76a3f4a79ae9a00db2f2b5f6877021d8deb96ada",3,"SPHINX: The Joint Mixing of Weights, Tasks, and Visual Embeddings for Multi-modal Large Language Models","SPHINX, a versatile multi-modal large language model (MLLM) with a joint mixing of model weights, tuning tasks, and visual embeddings is presented, and an efficient strategy aiming to better capture fine-grained appearances of high-resolution images is proposed.","arXiv.org",2023,"Ziyi Lin,Chris Liu,Renrui Zhang,Peng Gao,Longtian Qiu,Han Xiao,Han Qiu,Chen Lin,Wenqi Shao,Keqin Chen,Jiaming Han,Siyuan Huang,Yichi Zhang,Xuming He,Hongsheng Li,Y. Qiao",9,90,1
"5f370f52e24d185ae44bb0ea18cbd4be2aab0d15","https://www.semanticscholar.org/paper/5f370f52e24d185ae44bb0ea18cbd4be2aab0d15",3,"VLM-Eval: A General Evaluation on Video Large Language Models","This paper introduces a unified evaluation that encompasses multiple video tasks, including captioning, question and answering, retrieval, and action recognition, and proposes a simple baseline: Video-LLaVA, which uses a single linear projection and outperforms existing video LLMs.","arXiv.org",2023,"Shuailin Li,Yuang Zhang,Yucheng Zhao,Qiuyue Wang,Fan Jia,Ying-Hao Liu,Tiancai Wang",0,49,0
"451539c0d0f5f5785ff58d09ca5e67a5f129f9de","https://www.semanticscholar.org/paper/451539c0d0f5f5785ff58d09ca5e67a5f129f9de",3,"A Survey on Multimodal Large Language Models for Autonomous Driving","This paper introduces the background of Multimodal Large Language Models (MLLMs), the multimodal models development using LLMs, and the history of autonomous driving, and overviews existing MLLM tools for driving, transportation, and map systems together with existing datasets and benchmarks.","arXiv.org",2023,"Can Cui,Yunsheng Ma,Xu Cao,Wenqian Ye,Yang Zhou,Kaizhao Liang,Jintai Chen,Juanwu Lu,Zichong Yang,Kuei-Da Liao,Tianren Gao,Erlong Li,Kun Tang,Zhipeng Cao,Tongxi Zhou,Ao Liu,Xinrui Yan,Shuqi Mei,Jianguo Cao,Ziran Wang,Chao Zheng",8,203,0
"ee2c769943f9e46c3bbee117d1ecf14566b7bf1f","https://www.semanticscholar.org/paper/ee2c769943f9e46c3bbee117d1ecf14566b7bf1f",3,"Boosting the Power of Small Multimodal Reasoning Models to Match Larger Models with Self-Consistency Training","This work proposes MC-CoT, a self-consistency training strategy that generates multiple rationales and answers, subsequently selecting the most accurate through a voting process, and demonstrates that this approach significantly improves model performance across various benchmarks.","arXiv.org",2023,"Cheng Tan,Jingxuan Wei,Zhangyang Gao,Linzhuang Sun,Siyuan Li,Xihong Yang,Stan Z. Li",0,62,0
"7b0a186b0140ee91fb13991c9c7187f3dc3b0670","https://www.semanticscholar.org/paper/7b0a186b0140ee91fb13991c9c7187f3dc3b0670",3,"Visual Programming for Zero-shot Open-Vocabulary 3D Visual Grounding","This work proposes a novel visual programming approach for zero-shot open-vocabulary 3DVG, leveraging the capabilities of large language models (LLMs) and develops an innovative language-object correlation module to extend the scope of existing 3D object detectors into open- Vocabulary scenarios.","arXiv.org",2023,"Zhihao Yuan,Jinke Ren,Chun-Mei Feng,Hengshuang Zhao,Shuguang Cui,Zhen Li",0,67,0
"a756b584f8f8b4307e52895ae2120bc339580ad8","https://www.semanticscholar.org/paper/a756b584f8f8b4307e52895ae2120bc339580ad8",3,"See and Think: Embodied Agent in Virtual Environment","This paper proposes STEVE, a comprehensive and visionary embodied agent in the Minecraft virtual environment that consists of three key components: vision perception, language instruction, and code action.","arXiv.org",2023,"Zhonghan Zhao,Wenhao Chai,Xuan Wang,Li Boyi,Shengyu Hao,Shidong Cao,Tianbo Ye,Jenq-Neng Hwang,Gaoang Wang",0,66,0
"769a924d0af014acec326f50c15c5d70d258a969","https://www.semanticscholar.org/paper/769a924d0af014acec326f50c15c5d70d258a969",3,"LLMGA: Multimodal Large Language Model based Generation Assistant","This paper introduces a Multimodal Large Language Model-based Generation Assistant (LLMGA), leveraging the vast reservoir of knowledge and proficiency in reasoning, comprehension, and response inherent in Large Language Models to assist users in image generation and editing.","arXiv.org",2023,"Bin Xia,Shiyin Wang,Yingfan Tao,Yitong Wang,Jiaya Jia",0,65,0
"486c2df78cbb770a90a55f7fa3fe19102fba2c24","https://www.semanticscholar.org/paper/486c2df78cbb770a90a55f7fa3fe19102fba2c24",3,"LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models","A novel method to tackle the token generation challenge in Vision Language Models (VLMs) for video and image understanding, called LLaMA-VID, which empowers existing frameworks to support hour-long videos and pushes their upper limit with an extra context token.","arXiv.org",2023,"Yanwei Li,Chengyao Wang,Jiaya Jia",0,73,0
"9e2bac2777eebe603a39f69221689493609d4149","https://www.semanticscholar.org/paper/9e2bac2777eebe603a39f69221689493609d4149",3,"MLLMs-Augmented Visual-Language Representation Learning","This work demonstrates that multi-modal large language models (MLLMs) can enhance visual-language representation learning by improving data quality and proposes text shearing to maintain the same length for extended captions as that of the original captions.","arXiv.org",2023,"Yanqing Liu,Kai Wang,Wenqi Shao,Ping Luo,Yu Qiao,Mike Zheng Shou,Kaipeng Zhang,Yang You",0,69,0
"cc9627c4475b9ad1c7e4105f3bceb1e7b59b7cab","https://www.semanticscholar.org/paper/cc9627c4475b9ad1c7e4105f3bceb1e7b59b7cab",3,"Zero-Shot Video Question Answering with Procedural Programs","This work proposes to answer zero-shot questions about videos by generating short procedural programs that derive a final answer from solving a sequence of visual subtasks, using a large language model to generate such programs from an input question and an API of visual modules in the prompt, then executes them to obtain the output.","arXiv.org",2023,"Rohan Choudhury,Koichiro Niinuma,Kris M. Kitani,László A. Jeni",0,65,0
"ef4e4e4b52d4379ab5387d8dc53da87e561e78db","https://www.semanticscholar.org/paper/ef4e4e4b52d4379ab5387d8dc53da87e561e78db",3,"Good Questions Help Zero-Shot Image Reasoning","Question-Driven Visual Exploration (QVix) is introduced, a novel prompting strategy that enhances the exploratory capabilities of LVLMs in zero-shot reasoning tasks and significantly outperforms existing methods, highlighting its effectiveness in bridging the gap between complex visual data and LVL Ms' exploratory abilities.","arXiv.org",2023,"Kaiwen Yang,Tao Shen,Xinmei Tian,Xiubo Geng,Chongyang Tao,Dacheng Tao,Tianyi Zhou",0,36,0
"b92289123a94f6076505487adfb4513bd3495c1d","https://www.semanticscholar.org/paper/b92289123a94f6076505487adfb4513bd3495c1d",3,"LEGO: Learning EGOcentric Action Frame Generation via Visual Instruction Tuning","This paper finetune a visual large language model (VLLM) via visual instruction tuning for curating the enriched action descriptions to address a novel problem -- egocentric action frame generation.","arXiv.org",2023,"Bolin Lai,Xiaoliang Dai,Lawrence Chen,Guan Pang,James M. Rehg,Miao Liu",0,103,0
"369b34826e23cb43bea9a91395e9603eacfa7420","https://www.semanticscholar.org/paper/369b34826e23cb43bea9a91395e9603eacfa7420",3,"EgoPlan-Bench: Benchmarking Egocentric Embodied Planning with Multimodal Large Language Models","A benchmark with human annotations, EgoPlan-Bench, is introduced to quantitatively investigate the potential of MLLMs as embodied task planners in real-world scenarios and demonstrates that the model tuned on Ego plan-IT not only significantly improves performance on this benchmark, but also effectively acts as embodied planner in simulations.","arXiv.org",2023,"Yi Chen,Yuying Ge,Yixiao Ge,Mingyu Ding,Bohao Li,Rui Wang,Rui-Lan Xu,Ying Shan,Xihui Liu",0,49,0
"95d791ad14db2c779daa67ca7fdc3a75214c42eb","https://www.semanticscholar.org/paper/95d791ad14db2c779daa67ca7fdc3a75214c42eb",3,"3DAxiesPrompts: Unleashing the 3D Spatial Task Capabilities of GPT-4V","This work presents a new visual prompting method called 3DAxiesPrompts (3DAP) to unleash the capabilities of GPT-4V in performing 3D spatial tasks, and creates a 3D coordinate system tailored to 3D imagery, complete with annotated scale information.","arXiv.org",2023,"Dingning Liu,Xiaomeng Dong,Renrui Zhang,Xu Luo,Peng Gao,Xiaoshui Huang,Yongshun Gong,Zhihui Wang",0,85,0
"c672ec79f55cef8f7a32cd8dddfa981b893f1567","https://www.semanticscholar.org/paper/c672ec79f55cef8f7a32cd8dddfa981b893f1567",3,"V*: Guided Visual Search as a Core Mechanism in Multimodal LLMs","This work introduces V*, an LLM-guided visual search mechanism that employs the world knowledge in LLMs for efficient visual querying and results in a new MLLM meta-architecture, named Show, sEArch, and TelL (SEAL).","",2023,"Penghao Wu,Saining Xie",1,60,0
"82dc8edc49f9edf4a53056aedcdfb339be070166","https://www.semanticscholar.org/paper/82dc8edc49f9edf4a53056aedcdfb339be070166",3,"IQAGPT: Image Quality Assessment with Vision-language and ChatGPT Models","IQAGPT is introduced, an innovative image quality assessment system integrating an image quality captioning VLM with ChatGPT for generating quality scores and textual reports that outperforms GPT-4 and CLIP-IQA and multi-task classification and regression models that solely rely on images.","",2023,"Zhihao Chen,Bin Hu,Chuang Niu,Tao Chen,Yuxin Li,Hongming Shan,Ge Wang",0,46,0
"5f58863dd6474d6f127be995b5871e7c60f2792f","https://www.semanticscholar.org/paper/5f58863dd6474d6f127be995b5871e7c60f2792f",3,"Video Understanding with Large Language Models: A Survey","The emergent capabilities of Vid-LLMs are surprisingly advanced, particularly their ability for open-ended spatial-temporal reasoning combined with commonsense knowledge, suggesting a promising path for future video understanding.","",2023,"Yunlong Tang,Jing Bi,Siting Xu,Luchuan Song,Susan Liang,Teng Wang,Daoan Zhang,Jie An,Jingyang Lin,Rongyi Zhu,A. Vosoughi,Chao Huang,Zeliang Zhang,Feng Zheng,Jianguo Zhang,Ping Luo,Jiebo Luo,Chenliang Xu",0,218,0
"575f403261d5f99526f0b4dfc8644352d6c4467a","https://www.semanticscholar.org/paper/575f403261d5f99526f0b4dfc8644352d6c4467a",3,"DocLLM: A layout-aware generative language model for multimodal document understanding","DocLLM is presented, a lightweight extension to traditional large language models (LLMs) for reasoning over visual documents, taking into account both textual semantics and spatial layout, and outperforms SotA LLMs on 14 out of 16 datasets across all tasks, and generalizes well to 4 out of 5 previously unseen datasets.","",2023,"Dongsheng Wang,Natraj Raman,Mathieu Sibue,Zhiqiang Ma,Petr Babkin,Simerjot Kaur,Yulong Pei,Armineh Nourbakhsh,Xiaomo Liu",0,62,0
"002d2c4569d070a55fe69c25ebccad8e9ddae572","https://www.semanticscholar.org/paper/002d2c4569d070a55fe69c25ebccad8e9ddae572",3,"Incorporating Visual Experts to Resolve the Information Loss in Multimodal Large Language Models","A novel method is introduced that incorporates multi-task encoders and visual tools into the existing MLLMs training and inference pipeline, aiming to provide a more comprehensive and accurate summarization of visual inputs.","",2024,"Xin He,Longhui Wei,Lingxi Xie,Qi Tian",0,58,0
"23684a07517870cffd1f97fafbaae16ba22bd2b7","https://www.semanticscholar.org/paper/23684a07517870cffd1f97fafbaae16ba22bd2b7",3,"Large AI Models in Health Informatics: Applications, Challenges, and the Future","Seven key sectors in which large AI models are applicable and might have substantial influence, including: 1) bioinformatics; 2) medical diagnosis; 3) medical imaging; 4) medical informatics; 5) medical education; 6) public health; and 7) medical robotics are identified.","IEEE journal of biomedical and health informatics",2023,"Jianing Qiu,Lin Li,Jiankai Sun,Jiachuan Peng,Peilun Shi,Rui Zhang,Yinzhao Dong,Kyle Lam,F. P. Lo,Bo Xiao,Wu Yuan,Dong Xu,Benny P. L. Lo",29,348,0
"c9dbdae8146b9f97e254f5d26fd6efde96eaa703","https://www.semanticscholar.org/paper/c9dbdae8146b9f97e254f5d26fd6efde96eaa703",3,"Med-Flamingo: a Multimodal Medical Few-shot Learner","Med-Flamingo improves performance in generative medical VQA by up to 20\% in clinician's rating and firstly enables multimodal medical few-shot adaptations, such as rationale generation, as well as releasing the model, code, and evaluation app.","arXiv.org",2023,"Michael Moor,Qian Huang,Shirley Wu,Michihiro Yasunaga,C. Zakka,Yashodhara Dalmia,E. Reis,P. Rajpurkar,J. Leskovec",23,29,4
"5b038c1a93967072cc76689fd805e756f804cc42","https://www.semanticscholar.org/paper/5b038c1a93967072cc76689fd805e756f804cc42",3,"Large Models for Time Series and Spatio-Temporal Data: A Survey and Outlook","This survey coalesces the latest strides in large model-centric research on time series and spatio-temporal data, underscoring the solid foundations, current advances, practical applications, abundant resources, and future research opportunities.","arXiv.org",2023,"Ming Jin,Qingsong Wen,Yuxuan Liang,Chaoli Zhang,Siqiao Xue,Xue Wang,James Y. Zhang,Yi Wang,Haifeng Chen,Xiaoli Li,Shirui Pan,Vincent S. Tseng,Yu Zheng,Lei Chen,Hui Xiong",10,343,0
"2b3554a8fea6f123fc04bd3e120f2293f227e1b2","https://www.semanticscholar.org/paper/2b3554a8fea6f123fc04bd3e120f2293f227e1b2",3,"InstructMol: Multi-Modal Integration for Building a Versatile and Reliable Molecular Assistant in Drug Discovery","InstructMol, a multi-modal LLM, effectively aligns molecular structures with natural language via an instruction-tuning approach, utilizing a two-stage training strategy that adeptly combines limited domain-specific data with molecular and textual information.","arXiv.org",2023,"He Cao,Zijing Liu,Xingyu Lu,Yuan Yao,Yu Li",0,85,0
"28fbbf98bac1bb941162df553ca034d600cb59a6","https://www.semanticscholar.org/paper/28fbbf98bac1bb941162df553ca034d600cb59a6",3,"Rephrase, Augment, Reason: Visual Grounding of Questions for Vision-Language Models","Rephrase, Augment and Reason (RepARe), a gradient-free framework that extracts salient details about the image using the underlying LVLM as a captioner and reasoner, in order to propose modifications to the original question, is presented.","arXiv.org",2023,"Archiki Prasad,Elias Stengel-Eskin,Mohit Bansal",1,108,0
"beb3e8acd816bac1a5b7fccfd073f79048877e33","https://www.semanticscholar.org/paper/beb3e8acd816bac1a5b7fccfd073f79048877e33",3,"Frozen Transformers in Language Models Are Effective Visual Encoder Layers","This paper reveals that large language models (LLMs), despite being trained solely on textual data, are surprisingly strong encoders for purely visual tasks in the absence of language and proposes the information filtering hypothesis to explain the effectiveness of pre-trained LLMs in visual encoding.","arXiv.org",2023,"Ziqi Pang,Ziyang Xie,Yunze Man,Yu-Xiong Wang",0,83,0
"cf193b5b34178a444cb9bd9f51beb4124b753935","https://www.semanticscholar.org/paper/cf193b5b34178a444cb9bd9f51beb4124b753935",3,"HalluciDoctor: Mitigating Hallucinatory Toxicity in Visual Instruction Data","This work presents a novel hallucination detection and elimination framework, HalluciDoctor, based on the cross-checking paradigm, which is used to identify and eliminate hallucinations in the training data automatically and indicates that spurious correlations arising from long-tail object co-occurrences contribute to hallucinations.","arXiv.org",2023,"Qifan Yu,Juncheng Li,Longhui Wei,Liang Pang,Wentao Ye,Bosheng Qin,Siliang Tang,Qi Tian,Yueting Zhuang",0,38,0
"5ddb51ae85deca14dc7fc8adc07305c22a1ebe0a","https://www.semanticscholar.org/paper/5ddb51ae85deca14dc7fc8adc07305c22a1ebe0a",3,"Qwen-VL: A Frontier Large Vision-Language Model with Versatile Abilities","The Qwen-VL series is introduced, a set of large-scale vision-language models designed to perceive and understand both text and images that outperforms existing Large Vision Language Models (LVLMs).","arXiv.org",2023,"Jinze Bai,Shuai Bai,Shusheng Yang,Shijie Wang,Sinan Tan,Peng Wang,Junyang Lin,Chang Zhou,Jingren Zhou",88,80,20
"fc8988585c6846fdeee33b34779a6a87b92c3e86","https://www.semanticscholar.org/paper/fc8988585c6846fdeee33b34779a6a87b92c3e86",3,"Equivariant Similarity for Vision-Language Foundation Models","This study proposes EqSim, a regularization loss that can be efficiently calculated from any two matched training pairs and easily pluggable into existing image-text retrieval fine-tuning and presents a new challenging benchmark EqBen, the first to focus on ""visual-minimal change"".","arXiv.org",2023,"Tan Wang,Kevin Lin,Linjie Li,Chung-Ching Lin,Zhengyuan Yang,Hanwang Zhang,Zicheng Liu,Lijuan Wang",9,82,3
"0046306876ff2d5600699327e52bc29fa5e9ec91","https://www.semanticscholar.org/paper/0046306876ff2d5600699327e52bc29fa5e9ec91",3,"Transfer Visual Prompt Generator across LLMs","This work investigates the VPG transferability across LLMs, and develops a two-stage transfer framework named VPGTrans, which is simple yet highly effective and demonstrated to significantly speed up the transfer learning process without compromising performance.","arXiv.org",2023,"Ao Zhang,Hao Fei,Yuan Yao,Wei Ji,Li Li,Zhiyuan Liu,Tat-seng Chua",40,60,6
"3130643a5d02f0e849d83bb1f85577a924081f36","https://www.semanticscholar.org/paper/3130643a5d02f0e849d83bb1f85577a924081f36",3,"Paxion: Patching Action Knowledge in Video-Language Foundation Models","This work proposes a novel framework, Paxion, along with a new Discriminative Video Dynamics Modeling (DVDM) objective, and introduces the DVDM objective to train the Knowledge Patcher, which forces the model to encode the correlation between the action text and the correct ordering of video frames.","arXiv.org",2023,"Zhenhailong Wang,Ansel Blume,Sha Li,Genglin Liu,Jaemin Cho,Zineng Tang,Mohit Bansal,Heng Ji",7,63,1
"6fe2a4f7594abadafc9feb05e96ab7e9e634ce8f","https://www.semanticscholar.org/paper/6fe2a4f7594abadafc9feb05e96ab7e9e634ce8f",3,"Album Storytelling with Iterative Story-aware Captioning and Large Language Models","This work proposes a new iterative album storytelling pipeline, which starts with an initial story and builds a story-aware caption model to refine the captions using the whole story as guidance, then feeds into the LLMs to generate a new refined story.","arXiv.org",2023,"Munan Ning,Yujia Xie,Dongdong Chen,Zeyin Song,Lu Yuan,Yonghong Tian,Qixiang Ye,Liuliang Yuan",3,58,1
"c5b7f9e2af19db0c2bc9d57a113c17aa9d4d6fda","https://www.semanticscholar.org/paper/c5b7f9e2af19db0c2bc9d57a113c17aa9d4d6fda",3,"S-CLIP: Semi-supervised Vision-Language Learning using Few Specialist Captions","S-CLIP is proposed, a semi-supervised learning method for training CLIP that utilizes additional unpaired images and employs two pseudo-labeling strategies specifically designed for contrastive learning and the language modality.","",2023,"Sangwoo Mo,Min-Kyung Kim,Kyungmin Lee,Jinwoo Shin",0,100,0
"08b562aa8066c2342f0d03824221dea18f0a18d2","https://www.semanticscholar.org/paper/08b562aa8066c2342f0d03824221dea18f0a18d2",3,"Cream: Visually-Situated Natural Language Understanding with Contrastive Reading Model and Frozen Large Language Models","This paper introduces Contrastive Reading Model (Cream), a novel neural architecture designed to enhance the language-image understanding capability of LLMs by capturing intricate details that are often overlooked in existing methods.","Conference on Empirical Methods in Natural Language Processing",2023,"Geewook Kim,Hodong Lee,D. Kim,Haeji Jung,S. Park,Yoon Kim,Sangdoo Yun,T. Kil,Bado Lee,Seunghyun Park",2,78,0
"b634f9ba35123d40f0af8d96a9c154025cf2cf2a","https://www.semanticscholar.org/paper/b634f9ba35123d40f0af8d96a9c154025cf2cf2a",3,"Contrasting Intra-Modal and Ranking Cross-Modal Hard Negatives to Enhance Visio-Linguistic Compositional Understanding","A simple and effective method to improve compositional reasoning in VLMs by refining and expanding the standard image-text contrastive learning framework and does not require specific annotations and does not incur extra parameters.","",2023,"Le Zhang,Rabiul Awal,Aishwarya Agrawal",0,68,0
"0983883619a0ca597d055d0e58da2f514052913d","https://www.semanticscholar.org/paper/0983883619a0ca597d055d0e58da2f514052913d",3,"Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration","This work proposes Macaw-LLM, a novel multi-modal LLM that seamlessly integrates visual, audio, and textual information and builds on the work of previous work on instruction-tuned large language models to handle diverse data modalities and address complex real-world scenarios.","arXiv.org",2023,"Chenyang Lyu,Minghao Wu,Longyue Wang,Xinting Huang,Bingshuai Liu,Zefeng Du,Shuming Shi,Zhaopeng Tu",36,54,5
"8efc20988021ce3b4b05dd44b13e27260ee9b99b","https://www.semanticscholar.org/paper/8efc20988021ce3b4b05dd44b13e27260ee9b99b",3,"Investigating Prompting Techniques for Zero- and Few-Shot Visual Question Answering","Light is shed on the intricacies of prompting strategies in VLMs for VQA, emphasizing the synergistic use of captions, templates, and pre-processing to enhance model efficacy.","arXiv.org",2023,"Rabiul Awal,Le Zhang,Aishwarya Agrawal",1,59,0
"41c6028c620debae00ca5b30e2db5977225fec57","https://www.semanticscholar.org/paper/41c6028c620debae00ca5b30e2db5977225fec57",3,"mBLIP: Efficient Bootstrapping of Multilingual Vision-LLMs",,"arXiv.org",2023,"Gregor Geigle,Abhay Jain,R. Timofte,Goran Glavavs",0,84,0
"fc6a2f7478f68adefd69e2071f27e38aa1647f2f","https://www.semanticscholar.org/paper/fc6a2f7478f68adefd69e2071f27e38aa1647f2f",3,"Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond","The Qwen-VL series, a set of large-scale vision-language models (LVLMs) designed to perceive and understand both texts and images, set new records for generalist models under similar model scales on a broad range of visual-centric benchmarks.","",2023,"Jinze Bai,Shuai Bai,Shusheng Yang,Shijie Wang,Sinan Tan,Peng Wang,Junyang Lin,Chang Zhou,Jingren Zhou",35,85,8
"772724892819d7e6f15ce536753fdc32d022c0e0","https://www.semanticscholar.org/paper/772724892819d7e6f15ce536753fdc32d022c0e0",3,"A Survey on Image-text Multimodal Models","An exhaustive overview of the present research landscape of image-text multimodal models is offered, introducing a novel classification that segments their evolution into three distinct phases, based on their time of introduction and subsequent impact on the discipline.","arXiv.org",2023,"Ruifeng Guo,Jingxuan Wei,Linzhuang Sun,Bihui Yu,Guiyong Chang,Dawei Liu,Sibo Zhang,Zhengbing Yao,Mingjun Xu,Liping Bu",0,229,0
"96c43227831c4c3b12b7c64809e78674cea3a8a1","https://www.semanticscholar.org/paper/96c43227831c4c3b12b7c64809e78674cea3a8a1",3,"DeepSpeed-VisualChat: Multi-Round Multi-Image Interleave Chat via Multi-Modal Causal Attention","The DeepSpeed-VisualChat framework is presented, designed to optimize Large Language Models by incorporating multi-modal capabilities, with a focus on enhancing the proficiency of Large Vision and Language Models in handling interleaved inputs.","arXiv.org",2023,"Z. Yao,Xiaoxia Wu,Conglong Li,Minjia Zhang,Heyang Qi,Olatunji Ruwase,A. Awan,Samyam Rajbhandari,Yuxiong He",3,47,1
"11a4284e335ba39330b59d9f42ca3272a6166991","https://www.semanticscholar.org/paper/11a4284e335ba39330b59d9f42ca3272a6166991",3,"A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future","A thorough survey of the current research according to the taxonomies of methods within the domain of chain-of-thought reasoning, and describes XoT with frontier applications, covering planning, tool use, and distillation.","arXiv.org",2023,"Zheng Chu,Jingchang Chen,Qianglong Chen,Weijiang Yu,Tao He,Haotian Wang,Weihua Peng,Ming Liu,Bing Qin,Ting Liu",12,210,1
"a5d27bf7a2155d4ca016565a78b52ee90f81624c","https://www.semanticscholar.org/paper/a5d27bf7a2155d4ca016565a78b52ee90f81624c",3,"Beyond Task Performance: Evaluating and Reducing the Flaws of Large Multimodal Models with In-Context Learning","Despite their success, LMMs have flaws that remain unsolved with scaling alone and the effect of ICL on LMMs flaws is nuanced; despite its effectiveness for improved explainability, abstention, and instruction following, ICL does not improve compositional abilities, and actually even amplifies hallucinations.","arXiv.org",2023,"Mustafa Shukor,Alexandre Ramé,Corentin Dancette,M. Cord",2,95,0
"f220d3218f84340d1e06e01b89d9c3d64e61edd1","https://www.semanticscholar.org/paper/f220d3218f84340d1e06e01b89d9c3d64e61edd1",3,"SimVLG: Simple and Efficient Pretraining of Visual Language Generative Models","A one-stage, single-loss framework for the pre-training of computationally intensive vision-language generative models, leveraging frozen pre-trained large language models (LLMs), which effectively compacts the visual information while preserving the richness of semantic content, leading to fast convergence without sacrificing performance.","arXiv.org",2023,"Yiren Jian,Tingkai Liu,Yunzhe Tao,Soroush Vosoughi,HX Yang",1,59,0
"56025f5034f7aebe1b7292284d33d3d0e3317614","https://www.semanticscholar.org/paper/56025f5034f7aebe1b7292284d33d3d0e3317614",3,"Deep Tensor Network","The introduction of the Tensor Attention and Tensor Interaction Mechanism is introduced, a groundbreaking approach that leverages the tensor category to enhance the computational efficiency and the expressiveness of deep networks, and can even be generalized into the quantum realm.","arXiv.org",2023,"Yifan Zhang",0,90,0
"09157a8c0e7d7263ac035690118ddcbe295cee5c","https://www.semanticscholar.org/paper/09157a8c0e7d7263ac035690118ddcbe295cee5c",3,"ShapeGPT: 3D Shape Generation with A Unified Multi-modal Language Model","This work presents ShapeGPT, a shape-included multi-modal framework to leverage strong pre-trained language models to address multiple shape-relevant tasks, and achieves comparable performance across shape- relevant tasks, including text-to-shape, shape- to-text, shape completion, and shape editing.","arXiv.org",2023,"Fukun Yin,Xin Chen,C. Zhang,Biao Jiang,Zibo Zhao,Jiayuan Fan,Gang Yu,Taihao Li,Tao Chen",0,60,0
"769b794fe9f97268007676171f246d45e0631014","https://www.semanticscholar.org/paper/769b794fe9f97268007676171f246d45e0631014",3,"Towards More Unified In-context Visual Understanding","A new ICL framework for visual understanding with multi-modal output enabled is presented, capable of handling in-context vision understanding tasks with multimodal output in a unified pipeline and achieves competitive performance compared with specialized models and previous ICL baselines.","arXiv.org",2023,"Dianmo Sheng,Dongdong Chen,Zhentao Tan,Qiankun Liu,Qi Chu,Jianmin Bao,Tao Gong,Bin Liu,Shengwei Xu,Nenghai Yu",0,59,0
"6768a6aeb61ad8522795d92bb0ca44f87a327a59","https://www.semanticscholar.org/paper/6768a6aeb61ad8522795d92bb0ca44f87a327a59",3,"Pixel Aligned Language Models","This work aims to develop a vision-language model that can take locations, for example, a set of points or boxes, as either inputs or outputs, and performs dense word grounding and location-conditioned captioning.","arXiv.org",2023,"Jiarui Xu,Xingyi Zhou,Shen Yan,Xiuye Gu,Anurag Arnab,Chen Sun,Xiaolong Wang,Cordelia Schmid",0,58,0
"cb5f53cd7d43ac6f48abf4dde6fc58e53a083b0e","https://www.semanticscholar.org/paper/cb5f53cd7d43ac6f48abf4dde6fc58e53a083b0e",3,"Accelerating the integration of ChatGPT and other large‐scale AI models into biomedical research and healthcare","","MedComm – Future Medicine",2023,"Ding‐Qiao Wang,Long‐Yu Feng,Jin‐Guo Ye,Jin‐Gen Zou,Yingfeng Zheng",17,99,0
"d92c797f587ce7f1b001920ab9e6b7d31960bd77","https://www.semanticscholar.org/paper/d92c797f587ce7f1b001920ab9e6b7d31960bd77",3,"RemoteCLIP: A Vision Language Foundation Model for Remote Sensing","RemoteCLIP is proposed, the first vision-language foundation model for remote sensing that aims to learn robust visual features with rich semantics, as well as aligned text embeddings for seamless downstream application and consistently outperforms baseline foundation models across different model scales.","arXiv.org",2023,"F. Liu,Delong Chen,Zhan-Rong Guan,Xiaocong Zhou,Jiale Zhu,Jun Zhou",14,108,2
"e9f0223f8dce8b04d37d1f56e6c976b5d0cb5956","https://www.semanticscholar.org/paper/e9f0223f8dce8b04d37d1f56e6c976b5d0cb5956",3,"A Foundation LAnguage-Image model of the Retina (FLAIR): Encoding expert knowledge in text supervision","Interestingly, FLAIR outperforms by a large margin more generalist, larger-scale image-language models, which emphasizes the potential of embedding experts' domain knowledge and the limitations of generalist models in medical imaging.","arXiv.org",2023,"Julio Silva-Rodríguez,H. Chakor,R. Kobbi,J. Dolz,Ismail Ben Ayed",1,108,0
"785650a805851c7e945523e495c5a523c60f72a4","https://www.semanticscholar.org/paper/785650a805851c7e945523e495c5a523c60f72a4",3,"Open-Ended Medical Visual Question Answering Through Prefix Tuning of Language Models","This work focuses on open-ended VQA and motivated by the recent advances in language models consider it as a generative task, and introduces a novel method particularly suited for small, domain-specific, medical datasets.","International Conference on Medical Image Computing and Computer-Assisted Intervention",2023,"Tom van Sonsbeek,Mohammad Mahdi Derakhshani,Ivona Najdenkoska,Cees G. M. Snoek,M. Worring",11,35,3
"d77bc1a237b67c57b0c1b99b4802e703747a9688","https://www.semanticscholar.org/paper/d77bc1a237b67c57b0c1b99b4802e703747a9688",3,"BenchLMM: Benchmarking Cross-style Visual Capability of Large Multimodal Models","A new benchmark, BenchLMM, is proposed to assess the robustness of LMMs against three different styles: artistic image style, imaging sensor style, and application style, where each style has five sub-styles.","arXiv.org",2023,"Rizhao Cai,Zirui Song,Dayan Guan,Zhenhao Chen,Xing Luo,Chenyu Yi,Alex Kot",0,69,0
"b88f6aa65a4e1faf963494a76d28cc12112c9543","https://www.semanticscholar.org/paper/b88f6aa65a4e1faf963494a76d28cc12112c9543",3,"A Critical Analysis of Benchmarks, Techniques, and Models in Medical Visual Question Answering","The statistical analysis of medical VQA from 2018 to 2023 and individual yearly analyses reveals consistent preferences for LSTM and VGGNet, except in 2018 when ResNet was more commonly used.","IEEE Access",2023,"Suheer Al-Hadhrami,M. Menai,Saad Al-ahmadi,Ahmed Alnafessah",0,232,0
"90cd86b3c157e40cbaf1076f69cbd38d9c0781b9","https://www.semanticscholar.org/paper/90cd86b3c157e40cbaf1076f69cbd38d9c0781b9",3,"UnICLAM: Contrastive Representation Learning with Adversarial Masking for Unified and Interpretable Medical Vision Question Answering","Experimental results on VQA-RAD and SLAKE public benchmarks demonstrate that UnICLAM outperforms existing 11 state-of-the-art Medical-VQA models and makes an additional discussion about the performance of UnicLAM in diagnosing heart failure, verifying that it exhibits superior few-shot adaption performance in practical disease diagnosis.","arXiv.org",2022,"Chenlu Zhan,Peng Peng,Hongsen Wang,Tao Chen,Hongwei Wang",3,46,1
"f7ea746cd2cc25628a7a553ac27d228198be42cb","https://www.semanticscholar.org/paper/f7ea746cd2cc25628a7a553ac27d228198be42cb",3,"Pre-trained multilevel fuse network based on vision-conditioned reasoning and bilinear attentions for medical image visual question answering","This paper proposes a new pre-trained multilevel fusion network based on Vision-conditioned reasoning and Bilinear attentions for Med-VQA (VB-MVZA), which achieves more significant accuracy than the baseline models for open-ended questions and more powerful for language-bias Med- VQA datasets.","Journal of Supercomputing",2023,"Linqin Cai,Haodu Fang,Zhiqing Li",0,47,0
"ac4d13b6a4f9fb67337099f4602135a0351f5c99","https://www.semanticscholar.org/paper/ac4d13b6a4f9fb67337099f4602135a0351f5c99",3,"Towards Medical Artificial General Intelligence via Knowledge-Enhanced Multimodal Pretraining","The proposed MOTOR successfully mimics the human practice of fulfilling a""medical student"" to accelerate the process of becoming a""specialist"" and believes that this work makes a significant stride in realizing MAGI.","arXiv.org",2023,"Bingqian Lin,Zicong Chen,Mingjie Li,Haokun Lin,Hang Xu,Yi Zhu,Jian-zhuo Liu,Wenjia Cai,Lei Yang,Shen Zhao,Chenfei Wu,Ling Chen,Xiaojun Chang,Yi Yang,L. Xing,Xiaodan Liang",3,59,0
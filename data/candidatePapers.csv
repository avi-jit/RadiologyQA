"id","url","score","title","summary","venue","year","authors","citationCount","referenceCount","influentialCitationCount"
"d6d3604f369bb0415cbe814e43ca3131323b03e2","https://www.semanticscholar.org/paper/d6d3604f369bb0415cbe814e43ca3131323b03e2",6,"Otter: A Multi-Modal Model with In-Context Instruction Tuning","Otter, a multi-modal model based on OpenFlamingo (open-sourced version of DeepMind's Flamingo), trained on MIMIC-IT and showcasing improved instruction-following ability and in-context learning is introduced.","arXiv.org",2023,"Bo Li,Yuanhan Zhang,Liangyu Chen,Jinghao Wang,Jingkang Yang,Ziwei Liu",3,36,1
"86cbd30d1096b0c7e4ac6b03d97a8df12fd21457","https://www.semanticscholar.org/paper/86cbd30d1096b0c7e4ac6b03d97a8df12fd21457",6,"PathAsst: Redefining Pathology through Generative Foundation AI Assistant for Pathology","The PathAsst is presented, which is a generative foundation AI assistant to revolutionize diagnostic and predictive analytics in pathology, trained based on Vicuna-13B language model in coordination with the CLIP vision encoder.","arXiv.org",2023,"Yuxuan Sun,Chenglu Zhu,S. Zheng,Kai Zhang,Zhongyi Shui,Xiaoxuan Yu,Yi-Lei Zhao,Honglin Li,Yunlong Zhang,Ruojia Zhao,Xinheng Lyu,Lin Yang",0,47,0
"570079bbdd8758dfe865097e05719313c9c1301a","https://www.semanticscholar.org/paper/570079bbdd8758dfe865097e05719313c9c1301a",5,"LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model","This work augments LLaMA-Adapter by unlocking more learnable parameters and proposes an early fusion strategy to feed visual tokens only into the early LLM layers, contributing to better visual knowledge incorporation and achieves strong multi-modal reasoning with only a small-scale image-text and instruction dataset.","arXiv.org",2023,"Peng Gao,Jiaming Han,Renrui Zhang,Ziyi Lin,Shijie Geng,Aojun Zhou,W. Zhang,Pan Lu,Conghui He,Xiangyu Yue,Hongsheng Li,Y. Qiao",9,77,0
"7cf64070fd3d7e53d80f260c10e6bd7018d580e1","https://www.semanticscholar.org/paper/7cf64070fd3d7e53d80f260c10e6bd7018d580e1",5,"IdealGPT: Iteratively Decomposing Vision and Language Reasoning via Large Language Models","The IdealGPT framework is presented, a framework that iteratively decomposes VL reasoning using large language models (LLMs) and outperforms the best existing GPT-4-like models by an absolute 10% on VCR and 15% on SNLI-VE.","arXiv.org",2023,"Haoxuan You,Ruijun Sun,Zhecan Wang,Long Chen,Gengyu Wang,Hammad A. Ayyubi,Kai-Wei Chang,Shih-Fu Chang",0,46,0
"0ebc861f5478561f12941e6b48aad30574e996d8","https://www.semanticscholar.org/paper/0ebc861f5478561f12941e6b48aad30574e996d8",5,"Video ChatCaptioner: Towards Enriched Spatiotemporal Descriptions","This work introduces Video ChatCaptioner, an innovative approach for creating more comprehensive spatiotemporal video descriptions, specifically designed to select frames for posing video content-driven questions and shows promise as a method for enhancing video content.","arXiv.org",2023,"Jun Chen,Deyao Zhu,Kilichbek Haydarov,Xiang Li,Mohamed Elhoseiny",4,42,0
"e1ff32753e20e48b4b01e40b5e820254396e6e70","https://www.semanticscholar.org/paper/e1ff32753e20e48b4b01e40b5e820254396e6e70",5,"LMEye: An Interactive Perception Network for Large Language Models","Interactive Perception Network (IPN), aiming to achieve a LVLM by incorporating the image understanding capability into Large Language Models (LLMs), significantly improves the zero-shot performance of LVLMs on various multimodal tasks compared to previous methods.","arXiv.org",2023,"Yunxin Li,Baotian Hu,Xinyu Chen,Lin Ma,M. Zhang",0,48,0
"42a30dc5470f54ec249f25d3c31e05d7c376c8e3","https://www.semanticscholar.org/paper/42a30dc5470f54ec249f25d3c31e05d7c376c8e3",5,"VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks","This work presents an LLM-based framework for vision-centric tasks, termed VisionLLM, which provides a unified perspective for vision and language tasks by treating images as a foreign language and aligning vision-focused tasks with language tasks that can be flexibly defined and managed using language instructions.","arXiv.org",2023,"Wen Wang,Zhe Chen,Xiaokang Chen,Jiannan Wu,Xizhou Zhu,Gang Zeng,Ping Luo,Tong Lu,Jie Zhou,Y. Qiao,Jifeng Dai",5,78,0
"c3ac984267854c1701f0f8a7db8fbe503df8e820","https://www.semanticscholar.org/paper/c3ac984267854c1701f0f8a7db8fbe503df8e820",5,"ChatCAD+: Towards a Universal and Reliable Interactive CAD using LLMs","ChatCAD+ is presented, an interactive CAD system that is universal, reliable, and capable of handling medical images from diverse domains and incorporates a template retrieval system that emulates real-world diagnostic reporting, thereby improving its seamless integration into existing clinical workflows.","arXiv.org",2023,"Zihao Zhao,Sheng Wang,Jinchen Gu,Yitao Zhu,Lanzhuju Mei,Zixu Zhuang,Zhiming Cui,Qian Wang,Dinggang Shen",0,48,0
"31a7d8c4a5ab6bab522494b57270249105c8748e","https://www.semanticscholar.org/paper/31a7d8c4a5ab6bab522494b57270249105c8748e",5,"BiomedGPT: A Unified and Generalist Biomedical Generative Pre-trained Transformer for Vision, Language, and Multimodal Tasks","A unified and generalist Biomedical Generative Pre-trained Transformer model, which leverages self-supervision on large and diverse datasets to accept multi-modal inputs and perform a range of downstream tasks, with far-reaching implications for improving healthcare outcomes.","arXiv.org",2023,"Kaiyuan Zhang,Jun Yu,Zhilin Yan,Yixin Liu,Eashan Adhikarla,S. Fu,Xun Chen,Chen Chen,Yuyin Zhou,Xiang Li,Lifang He,B. Davison,Quanzheng Li,Yong Chen,Hongfang Liu,Lichao Sun",0,143,0
"7e32aac43e9f1df49e116add03327ee6f365dbf3","https://www.semanticscholar.org/paper/7e32aac43e9f1df49e116add03327ee6f365dbf3",4,"mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality",,"arXiv.org",2023,"Qinghao Ye,Haiyang Xu,Guohai Xu,Jiabo Ye,Ming Yan,Yi Zhou,Junyan Wang,Anwen Hu,Pengcheng Shi,Yaya Shi,Chenliang Li,Yuanhong Xu,Hehong Chen,Junfeng Tian,Qiang Qi,J. Zhang,Feiyan Huang",15,32,2
"54a8b153ed04a872da878d695239bdc413dc782c","https://www.semanticscholar.org/paper/54a8b153ed04a872da878d695239bdc413dc782c",4,"InternGPT: Solving Vision-Centric Tasks by Interacting with Chatbots Beyond Language","By incorporating pointing instructions, the proposed iGPT significantly improves the efficiency of communication between users and chatbots, as well as the accuracy of chatbots in vision-centric tasks, especially in complicated visual scenarios where the number of objects is greater than 2.","arXiv.org",2023,"Zhaoyang Liu,Yinan He,Wenhai Wang,Weiyun Wang,Yi Wang,Shoufa Chen,Qing-Long Zhang,Yang Yang,Qingyun Li,Jiashuo Yu,Kunchang Li,Zhe Chen,Xuecheng Yang,Xizhou Zhu,Yali Wang,Limin Wang,Ping Luo,Jifeng Dai,Yu Qiao",6,82,0
"d48cb91b9e555194f7494c4d4bb9815021d3ee45","https://www.semanticscholar.org/paper/d48cb91b9e555194f7494c4d4bb9815021d3ee45",4,"VideoChat: Chat-Centric Video Understanding","VideoChat is introduced, an end-to-end chat-centric video understanding system that integrates video foundation models and large language models via a learnable neural interface, excelling in spatiotemporal reasoning, event localization, and causal relationship inference.","arXiv.org",2023,"Kunchang Li,Yinan He,Yi Wang,Yizhuo Li,Wen Wang,Ping Luo,Yali Wang,Limin Wang,Yu Qiao",10,59,3
"9837349417e36ef5be06da0fd6c74042148bdaa2","https://www.semanticscholar.org/paper/9837349417e36ef5be06da0fd6c74042148bdaa2",4,"Visual Programming for Text-to-Image Generation and Evaluation","This work proposes two novel interpretable/explainable visual programming frameworks for text-to-image (T2I) generation and evaluation and introduces VPEval, an interpretable and explainable evaluation framework for T2I generation based on visual programming.","arXiv.org",2023,"Jaemin Cho,Abhaysinh Zala,Mohit Bansal",1,62,0
"6238fd213197ccf0d79191662828e38118a06d79","https://www.semanticscholar.org/paper/6238fd213197ccf0d79191662828e38118a06d79",4,"ECHo: Event Causality Inference via Human-centric Reasoning","A unified framework aligned with the Chain-of-Thought (CoT) paradigm is proposed to assess the reasoning capability of current AI systems and scrutinize the advanced large language and multimodal models via three complementary human-centric ECHo tasks.","arXiv.org",2023,"Yuxi Xie,Guanzhen Li,MingSung Kan",0,69,0
"00fcc983728346a5f3f8f005f1365be54456728e","https://www.semanticscholar.org/paper/00fcc983728346a5f3f8f005f1365be54456728e",4,"EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought","This work introduces EmbodiedGPT, an end-to-end multi-modal foundation model for embodied AI, empowering embodied agents with multi- modal understanding and execution capabilities, and significantly enhances the success rate of the embodied control task by extracting more effective features.","arXiv.org",2023,"Yao Mu,Qinglong Zhang,Mengkang Hu,Wen Wang,Mingyu Ding,Jun Jin,Bin Wang,Jifeng Dai,Y. Qiao,Ping Luo",1,67,0
"6822b29d86691669f79ec10d493606cf55e52c31","https://www.semanticscholar.org/paper/6822b29d86691669f79ec10d493606cf55e52c31",4,"Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models","A novel and affordable solution for the effective VL adaption of LLMs, called Mixture-of-Modality Adaptation (MMA), which adopts lightweight modules, i.e., adapters, to bridge the gap between LLMs and VL tasks, which also enables the joint optimization of the image and language models","arXiv.org",2023,"Gen Luo,Yiyi Zhou,Tianhe Ren,Shen Chen,Xiaoshuai Sun,Rongrong Ji",0,48,0
"f8f8267a2acd7598de6c15327f3953241901a62d","https://www.semanticscholar.org/paper/f8f8267a2acd7598de6c15327f3953241901a62d",4,"On Evaluating Adversarial Robustness of Large Vision-Language Models","Evaluating the robustness of open-source large VLMs in the most realistic and high-risk setting, where adversaries have only black-box system access and seek to deceive the model into returning the targeted responses is proposed.","arXiv.org",2023,"Yunqing Zhao,Tianyu Pang,Chao Du,Xiao Yang,Chongxuan Li,Ngai-Man Cheung,Min Lin",0,102,0
"b458fc5261595f44b36325e5eaea1f874d65138f","https://www.semanticscholar.org/paper/b458fc5261595f44b36325e5eaea1f874d65138f",4,"GPT4Tools: Teaching Large Language Model to Use Tools via Self-instruction","The GPT4Tools based on self-instruct to enable open-source LLMs, such as LLaMA and OPT, to use tools, generates an instruction-following dataset by prompting an advanced teacher with various multi-modal contexts using the Low-Rank Adaptation (LoRA) optimization.","arXiv.org",2023,"Rui Yang,Lin Song,Yanwei Li,Sijie Zhao,Yixiao Ge,Xiu Li,Ying Shan",1,63,0
"13b5b69355555e0c8b702261c5de3b4172ba653c","https://www.semanticscholar.org/paper/13b5b69355555e0c8b702261c5de3b4172ba653c",4,"The Art of SOCRATIC QUESTIONING: Zero-shot Multimodal Reasoning with Recursive Thinking and Self-Questioning","Qualitative analysis clearly demonstrates the intermediate thinking steps elicited by Socratic Questioning are similar to the human's recursively thinking process of a complex reasoning problem.","arXiv.org",2023,"Jingyuan Qi,Zhiyang Xu,Ying Shen,Minqian Liu,Di Jin,Qifan Wang,Lifu Huang",0,23,0
"20975c5c77286daf6ffe1956e2939cb34a5511a1","https://www.semanticscholar.org/paper/20975c5c77286daf6ffe1956e2939cb34a5511a1",4,"Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding","Video-LLaMA showcases the ability to perceive and comprehend video content, generating meaningful responses that are grounded in the visual and auditory information presented in the videos, highlighting the potential of Video- LLaMA as a promising prototype for audio-visual AI assistants.","",2023,"Han Zhang,Xin Li,Lidong Bing",1,30,0
"f4793adffd6f67ffcb93ccfc5672ab301b8a2b96","https://www.semanticscholar.org/paper/f4793adffd6f67ffcb93ccfc5672ab301b8a2b96",4,"PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering","This paper proposes a generative-based model for medical visual understanding by aligning visual information from a pre-trained vision encoder with a large language model, and establishes a scalable pipeline to construct a large-scale medical visual question-answering dataset.","arXiv.org",2023,"Xiaoman Zhang,Chaoyi Wu,Ziheng Zhao,Weixiong Lin,Ya Zhang,Yanfeng Wang,Weidi Xie",1,38,0
"a3711dbf296b5ddd97ba93826660cd3995611625","https://www.semanticscholar.org/paper/a3711dbf296b5ddd97ba93826660cd3995611625",3,"Towards A Foundation Model for Generalist Robots: Diverse Skill Learning at Scale via Automated Task and Scene Generation","This document presents a specific idea for mining knowledge in the latest large-scale foundation models for robotics research, and advocates for using them to generate diversified tasks and scenes at scale, thereby scaling up low-level skill learning and ultimately leading to a foundation model for robotics that empowers generalist robots.","arXiv.org",2023,"Zhou Xian,Théophile Gervet,Zhenjia Xu,Yi-Ling Qiao,Tsun-Hsuan Wang",0,111,0
"887138f4c365b9d1325de41a522d27bec34e0d7e","https://www.semanticscholar.org/paper/887138f4c365b9d1325de41a522d27bec34e0d7e",3,"Reasoning with Language Model Prompting: A Survey","This paper provides a comprehensive survey of cutting-edge research on reasoning with language model prompting with comparisons and summaries and provides systematic resources to help beginners.","arXiv.org",2022,"Shuofei Qiao,Yixin Ou,Ningyu Zhang,Xiang Chen,Yunzhi Yao,Shumin Deng,Chuanqi Tan,Fei Huang,Huajun Chen",34,204,1
"170c97c7215f42edfb20c2248f954879e91ef86e","https://www.semanticscholar.org/paper/170c97c7215f42edfb20c2248f954879e91ef86e",3,"Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models","This paper presents Chameleon, an AI system that mitigates LLM limitations by augmenting LLMs with plug-and-play modules for compositional reasoning, and shows that the GPT-4-powered planner exhibits more consistent and rational tool selection via inferring potential constraints from instructions, compared to a ChatGPT- powered planner.","arXiv.org",2023,"Pan Lu,Baolin Peng,Hao Cheng,Michel Galley,Kai-Wei Chang,Y. Wu,Song-Chun Zhu,Jianfeng Gao",24,65,3
"405bc18b9d2f783f22f50d5feb02c51b4b34655f","https://www.semanticscholar.org/paper/405bc18b9d2f783f22f50d5feb02c51b4b34655f",3,"LayoutGPT: Compositional Visual Planning and Generation with Large Language Models","This work proposes LayoutGPT, a method to compose in-context visual demonstrations in style sheet language to enhance the visual planning skills of LLMs, and shows superior performance in converting challenging language concepts like numerical and spatial relations to layout arrangements for faithful text-to-image generation.","arXiv.org",2023,"Weixi Feng,Wanrong Zhu,Tsu-Jui Fu,Varun Jampani,Arjun Reddy Akula,Xuehai He,Sugato Basu,X. Wang,William Yang Wang",1,59,0
"1a28e9c62eeb76a1a77dc152197027c15310927b","https://www.semanticscholar.org/paper/1a28e9c62eeb76a1a77dc152197027c15310927b",3,"ANPL: Compiling Natural Programs with Interactive Decomposition","ANPL, a programming system that allows users to decompose user-specific tasks, is introduced and deployed on the Abstraction and Reasoning Corpus (ARC), a set of unique tasks that are challenging for state-of-the-art AI systems.","arXiv.org",2023,"Di Huang,Ziyuan Nan,Xingui Hu,Pengwei Jin,Shaohui Peng,Yuanbo Wen,Rui Zhang,Zidong Du,Qi Guo,Yewen Pu,Yunji Chen",0,67,0
"af705d648b5b16daa3dcc593bc593f2574d76c07","https://www.semanticscholar.org/paper/af705d648b5b16daa3dcc593bc593f2574d76c07",3,"Grammar Prompting for Domain-Specific Language Generation with Large Language Models","Experiments demonstrate that grammar prompting can enable LLMs to perform competitively on a diverse set of DSL generation tasks, including semantic parsing, Overnight, GeoQuery, PDDL planning, and even molecule generation (SMILES).","arXiv.org",2023,"Bailin Wang,Zimu Wang,Xuezhi Wang,Yuan Cao,R. Saurous,Yoon Kim",1,92,0
"d1120d67b700e4dfe8b39eb1e48fbdea4e1a0c43","https://www.semanticscholar.org/paper/d1120d67b700e4dfe8b39eb1e48fbdea4e1a0c43",3,"HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face","HuggingGPT is a framework that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities to solve AI tasks and is able to cover numerous sophisticated AI tasks in different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks.","arXiv.org",2023,"Yongliang Shen,Kaitao Song,Xu Tan,D. Li,Weiming Lu,Y. Zhuang",86,39,10
"1a8eb2cae1833df3bf12fe3b41b03d60b4a4a98d","https://www.semanticscholar.org/paper/1a8eb2cae1833df3bf12fe3b41b03d60b4a4a98d",3,"Visual Instruction Tuning","This paper presents LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding and introduces GPT-4 generated visual instruction tuning data, the model and code base publicly available.","arXiv.org",2023,"Haotian Liu,Chunyuan Li,Qingyang Wu,Yong Jae Lee",48,55,23
"43e6e8d6663d83f1b74cf5a2be7b040b0928f867","https://www.semanticscholar.org/paper/43e6e8d6663d83f1b74cf5a2be7b040b0928f867",3,"X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages","X-LLM is proposed, which converts Multi-modalities into foreign languages using X2L interfaces and inputs them into a large Language model (ChatGLM), and demonstrates impressive multimodel chat abilities.","arXiv.org",2023,"Feilong Chen,Minglun Han,Haozhi Zhao,Qingyang Zhang,Jing Shi,Shuang Xu,Bo Xu",3,61,0
"6a5525c316b9be7909c433a79e090ed731425083","https://www.semanticscholar.org/paper/6a5525c316b9be7909c433a79e090ed731425083",3,"What Makes for Good Visual Tokenizers for Large Language Models?","A new MLLM equipped with a tailored Good Visual Tokenizer (GVT), which exhibits strong visual comprehension capability at multiple scales, without introducing extra parameters and task-specific fine-tuning.","arXiv.org",2023,"Guangzhi Wang,Yixiao Ge,Xiaohan Ding,Mohan S. Kankanhalli,Ying Shan",0,54,0
"ca055cfb9d4d47124cc035c346f38577825fcacf","https://www.semanticscholar.org/paper/ca055cfb9d4d47124cc035c346f38577825fcacf",3,"Enhance Reasoning Ability of Visual-Language Models via Large Language Models","A method called TReE is proposed, which transfers the reasoning ability of a large language model to a visual language model in zero-shot scenarios, and contains three stages: observation, thinking, and re-thinking.","arXiv.org",2023,"Yueting Yang,Xintong Zhang,Wenjuan Han",0,35,0
"c6ac708b65b24c20f80831d518c1795ce8133ad5","https://www.semanticscholar.org/paper/c6ac708b65b24c20f80831d518c1795ce8133ad5",3,"ChatBridge: Bridging Modalities with Large Language Model as a Language Catalyst","It is shown that only language-paired two-modality data is sufficient to connect all modalities and ChatBridge, a novel multimodal language model that leverages the expressive capabilities of language as the catalyst to bridge the gap between various modalities, is presented.","arXiv.org",2023,"Zijia Zhao,Longteng Guo,Tongtian Yue,Si-Qing Chen,Shuai Shao,Xinxin Zhu,Zehuan Yuan,Jing Liu",0,71,0
"50c1414fe41d0cb9db6f0933c9319aa124beac5d","https://www.semanticscholar.org/paper/50c1414fe41d0cb9db6f0933c9319aa124beac5d",3,"Contextual Object Detection with Multimodal Large Language Models","The ContextDET is presented, a unified multimodal model that is capable of end-to-end differentiable modeling of visual-language contexts, so as to locate, identify, and associate visual objects with language inputs for human-AI interaction.","arXiv.org",2023,"Yuhang Zang,Wei Li,Jun Han,Kaiyang Zhou,Chen Change Loy",0,84,0
"9a22b33b529484c912d1ea9f8698369d4546a1c1","https://www.semanticscholar.org/paper/9a22b33b529484c912d1ea9f8698369d4546a1c1",3,"Transfer Visual Prompt Generator across LLMs","This work investigates the VPG transferability across LLMs, and designs a two-stage transfer framework named VPGTrans, which is simple yet highly effective and demonstrated to significantly speed up the transfer learning process without compromising performance.","arXiv.org",2023,"Ao Zhang,Hao Fei,Yuan Yao,Wei Ji,Li Li,Zhiyuan Liu,Tat-Seng Chua",2,34,0
"d886fc1b43b1c14b1c82ce8e4eab7c48e2c6d7af","https://www.semanticscholar.org/paper/d886fc1b43b1c14b1c82ce8e4eab7c48e2c6d7af",3,"Paxion: Patching Action Knowledge in Video-Language Foundation Models","This work proposes a novel framework, Paxion, along with a new Discriminative Video Dynamics Modeling (DVDM) objective, which effectively fills the gap in action knowledge understanding, while maintaining or improving performance on a wide spectrum of both object- and action-centric downstream tasks.","arXiv.org",2023,"Zhenhailong Wang,Ansel Blume,Sha Li,Genglin Liu,Jaemin Cho,Zineng Tang,Mohit Bansal,Heng Ji",0,62,0
"bfa9df38bd8597d8cdf0c3497fe5e5a5e31f646f","https://www.semanticscholar.org/paper/bfa9df38bd8597d8cdf0c3497fe5e5a5e31f646f",3,"ONE-PEACE: Exploring One General Representation Model Toward Unlimited Modalities","This work releases ONE-PEACE, a highly extensible model with 4B parameters that can seamlessly align and integrate representations across vision, audio, and language modalities, and develops two modality-agnostic pretraining tasks, which align the semantic space of different modalities and capture fine-grained details within modalities concurrently.","arXiv.org",2023,"Peng Wang,Shijie Wang,Junyang Lin,Shuai Bai,Xiaohuan Zhou,Jingren Zhou,Xinggang Wang,Chang Zhou",1,169,0
"6fe2a4f7594abadafc9feb05e96ab7e9e634ce8f","https://www.semanticscholar.org/paper/6fe2a4f7594abadafc9feb05e96ab7e9e634ce8f",3,"Album Storytelling with Iterative Story-aware Captioning and Large Language Models","This work proposes a new iterative album storytelling pipeline, which starts with an initial story and builds a story-aware caption model to refine the captions using the whole story as guidance, then feeds into the LLMs to generate a new refined story.","arXiv.org",2023,"Munan Ning,Yujia Xie,Dongdong Chen,Zeyin Song,Lu Yuan,Yonghong Tian,Qixiang Ye,Liuliang Yuan",0,57,0
"065dcc6074ffc9e314799d97c1757e5d23e7e2b1","https://www.semanticscholar.org/paper/065dcc6074ffc9e314799d97c1757e5d23e7e2b1",3,"S-CLIP: Semi-supervised Vision-Language Pre-training using Few Specialist Captions","S-CLIP is proposed, a semi-supervised learning method for training CLIP that utilizes additional unpaired images and employs two pseudo-labeling strategies specifically designed for contrastive learning and the language modality.","arXiv.org",2023,"Sangwoo Mo,Min-Kyung Kim,Kyungmin Lee,Jinwoo Shin",0,100,0
"f45a3474bd38d65c1b2cc3342a64dacbf07f445a","https://www.semanticscholar.org/paper/f45a3474bd38d65c1b2cc3342a64dacbf07f445a",3,"Cream: Visually-Situated Natural Language Understanding with Contrastive Reading Model and Frozen Large Language Models","The Contrastive Reading Model (Cream) is proposed, a novel neural architecture designed to enhance the language-image understanding capability of LLMs by capturing intricate details typically overlooked by existing methods.","arXiv.org",2023,"Geewook Kim,Hodong Lee,Daehee Kim,Haeji Jung,S. Park,Yoon Kim,Sangdoo Yun,T. Kil,Bado Lee,Seunghyun Park",0,63,0
"cb5f53cd7d43ac6f48abf4dde6fc58e53a083b0e","https://www.semanticscholar.org/paper/cb5f53cd7d43ac6f48abf4dde6fc58e53a083b0e",3,"Accelerating the integration of ChatGPT and other large‐scale AI models into biomedical research and healthcare","","MedComm – Future Medicine",2023,"Ding‐Qiao Wang,Long‐Yu Feng,Jin‐Guo Ye,Jin‐Gen Zou,Yingfeng Zheng",1,99,0
"e9480d62e216f77d5556b7eda769daa4c92d004d","https://www.semanticscholar.org/paper/e9480d62e216f77d5556b7eda769daa4c92d004d",3,"VQAMix: Conditional Triplet Mixup for Medical Visual Question Answering","This paper proposes a simple yet effective data augmentation method, VQAMix, which generates more labeled training samples by linearly combining a pair of VQA samples, which can be easily embedded into any visual-language model to boost performance.","IEEE Transactions on Medical Imaging",2022,"Haifan Gong,Guanqi Chen,Mingzhi Mao,Z. Li,Guanbin Li",7,55,3
"ac4d13b6a4f9fb67337099f4602135a0351f5c99","https://www.semanticscholar.org/paper/ac4d13b6a4f9fb67337099f4602135a0351f5c99",3,"Towards Medical Artificial General Intelligence via Knowledge-Enhanced Multimodal Pretraining","The proposed MOTOR successfully mimics the human practice of fulfilling a""medical student"" to accelerate the process of becoming a""specialist"" and believes that this work makes a significant stride in realizing MAGI.","arXiv.org",2023,"Bingqian Lin,Zicong Chen,Mingjie Li,Haokun Lin,Hang Xu,Yi Zhu,Jian-zhuo Liu,Wenjia Cai,Lei Yang,Shen Zhao,Chenfei Wu,Ling Chen,Xiaojun Chang,Yi Yang,L. Xing,Xiaodan Liang",0,60,0
"b595b55ed27935d306b0a5e0b06a3b0a771275b1","https://www.semanticscholar.org/paper/b595b55ed27935d306b0a5e0b06a3b0a771275b1",2,"SheetCopilot: Bringing Software Productivity to the Next Level through Large Language Models","This work proposes a SheetCopilot agent which takes natural language task and control spreadsheet to fulfill the requirements, and proposes a set of atomic actions as an abstraction of spreadsheet software functionalities.","arXiv.org",2023,"Hongxin Li,Jingran Su,Yuntao Chen,Qing Li,Zhaoxiang Zhang",0,75,0
"ac7771c332da42b29a913b116bd6ef622cbf89cf","https://www.semanticscholar.org/paper/ac7771c332da42b29a913b116bd6ef622cbf89cf",2,"TaskMatrix.AI: Completing Tasks by Connecting Foundation Models with Millions of APIs","The vision of how to build such an ecosystem is presented, each key component is explained, and study cases are used to illustrate both the feasibility of this vision and the main challenges the authors need to address next.","arXiv.org",2023,"Yaobo Liang,Chenfei Wu,Ting Song,Wenshan Wu,Yan Xia,Yu Liu,Yangyiwen Ou,Shuai Lu,Lei Ji,Shaoguang Mao,Yun Wang,Linjun Shou,Ming Gong,Nan Duan",31,27,3
"01f9b773408115a16fe872147348db175789e82f","https://www.semanticscholar.org/paper/01f9b773408115a16fe872147348db175789e82f",2,"Tool Learning with Foundation Models","A systematic investigation of tool learning is presented, including its cognitive origins, the paradigm shift of foundation models, and the complementary roles of tools and models to inspire future research in integrating tools with foundation models.","arXiv.org",2023,"Yujia Qin,Shengding Hu,Yankai Lin,Weize Chen,Ning Ding,Ganqu Cui,Zheni Zeng,Yufei Huang,Chaojun Xiao,Chi Han,Y. Fung,Yusheng Su,Huadong Wang,Cheng Qian,Runchu Tian,Kunlun Zhu,Shi Liang,Xingyu Shen,Bokai Xu,Zhen Zhang,Yining Ye,Bo Li,Ziwei Tang,Jing Yi,Yu Zhu,Zhenning Dai,Lan Yan,Xin Cong,Ya-Ting Lu,Weilin Zhao,Yuxiang Huang,Jun-Han Yan,Xu Han,Xian Sun,Dahai Li,Jason Phang,Cheng Yang,Tongshuang Wu,Heng Ji,Zhiyuan Liu,Maosong Sun",13,224,2
"2195676f111ad492c50f4d4c96abb2bd3d72f7fc","https://www.semanticscholar.org/paper/2195676f111ad492c50f4d4c96abb2bd3d72f7fc",2,"Instruct2Act: Mapping Multi-modality Instructions to Robotic Actions with Large Language Model","This paper presents Instruct2Act, a framework that utilizes Large Language Models to map multi-modal instructions to sequential actions for robotic manipulation tasks, employing the LLM model to generate Python programs that constitute a comprehensive perception, planning, and action loop for robotic tasks.","arXiv.org",2023,"Siyuan Huang,Zhengkai Jiang,Hao-Wen Dong,Y. Qiao,Peng Gao,Hongsheng Li",1,45,1
"13a5140fc0b269c408ecfc666cb297410bc753c5","https://www.semanticscholar.org/paper/13a5140fc0b269c408ecfc666cb297410bc753c5",2,"Matcher: Segment Anything with One Shot Using All-Purpose Feature Matching","This work presents Matcher, which segments anything with one shot by integrating an all-purpose feature extraction model and a class-agnostic segmentation model, and proposes a novel instance-level matching strategy for controllable mask merging.","arXiv.org",2023,"Yang Liu,Muzhi Zhu,Hengtao Li,Hao Chen,Xinlong Wang,Chunhua Shen",1,44,0
"ba704774f194938b04b1e2be40b1d111a4ca08e1","https://www.semanticscholar.org/paper/ba704774f194938b04b1e2be40b1d111a4ca08e1",2,"CREATOR: Disentangling Abstract and Concrete Reasonings of Large Language Models through Tool Creation",,"arXiv.org",2023,"Cheng Qian,Chi Han,Y. Fung,Yujia Qin,Zhiyuan Liu,Heng Ji",1,30,0
"90027ca7802645671a69b00b65e1fa94e6b63544","https://www.semanticscholar.org/paper/90027ca7802645671a69b00b65e1fa94e6b63544",2,"ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models","This study proposes a modular paradigm ReWOO (Reasoning WithOut Observation) that detaches the reasoning process from external observations, thus significantly reducing token consumption and demonstrating robustness under tool-failure scenarios.","arXiv.org",2023,"Binfeng Xu,Zhiyuan Peng,Bowen Lei,Subhabrata Mukherjee,Yuchen Liu,Dongkuan Xu",1,41,0
"615962d8969c8e0ffe43319689dce6c50cbf1f29","https://www.semanticscholar.org/paper/615962d8969c8e0ffe43319689dce6c50cbf1f29",2,"Responsible Task Automation: Empowering Large Language Models as Responsible Task Automators","This paper presents Responsible Task Automation (ResponsibleTA) as a fundamental framework to facilitate responsible collaboration between LLM-based coordinators and executors for task automation with three empowered capabilities: predicting the feasibility of the commands for executors, verifying the completeness of executors and enhancing the security.","",2023,"Zhizheng Zhang,Xiaoyi Zhang,Wenxuan Xie,Yan Lu",0,44,0
"30c0cdc414f68211d5d0514df027cec22e005174","https://www.semanticscholar.org/paper/30c0cdc414f68211d5d0514df027cec22e005174",2,"A Survey for In-context Learning","This paper presents a formal definition of ICL and clarify its correlation to related studies, and organizes and discusses advanced techniques, including training strategies, demonstration designing strategies, as well as related analysis.","arXiv.org",2022,"Qingxiu Dong,Lei Li,Damai Dai,Ce Zheng,Zhiyong Wu,Baobao Chang,Xu Sun,Jingjing Xu,Zhifang Sui",2,125,0
"7562e25b666cba841b1dd5cf6e700978922beb04","https://www.semanticscholar.org/paper/7562e25b666cba841b1dd5cf6e700978922beb04",2,"SkinGPT: A Dermatology Diagnostic System with Vision Large Language Model","The ability to deploy it locally and protect user privacy makes SkinGPT an attractive option for patients seeking an accurate and reliable diagnosis of their skin conditions and the system can autonomously determine the characteristics and categories of skin conditions, perform analysis, and provide treatment recommendations.","arXiv.org",2023,"Juexiao Zhou,Xin Gao",1,35,1
"c56a51728678e5b2e3ff95e51caf21d267439c36","https://www.semanticscholar.org/paper/c56a51728678e5b2e3ff95e51caf21d267439c36",2,"ChatVideo: A Tracklet-centric Multimodal and Versatile Video Understanding System","The vision for multimodal and versatile video understanding is presented and a prototype system, built upon a tracklet-centric paradigm, which treats tracklets as the basic video unit and employs various Video Foundation Models to annotate their properties e.g., appearance, motion, etc.","arXiv.org",2023,"Junke Wang,Dongdong Chen,Chong Luo,Xiyang Dai,Lu Yuan,Zuxuan Wu,Yu-Gang Jiang",2,43,0
"8f95859cd6ccbe2c039fe8214f76c22382ebb9c3","https://www.semanticscholar.org/paper/8f95859cd6ccbe2c039fe8214f76c22382ebb9c3",2,"Caption Anything: Interactive Image Description with Diverse Multimodal Controls","Caption AnyThing (CAT) is presented, a foundation model augmented image captioning framework supporting a wide range of multimodel controls: 1) visual controls, including points, boxes, and trajectories; 2) language controls, such as sentiment, length, language, and factuality.","arXiv.org",2023,"Teng Wang,Jinrui Zhang,Junjie Fei,Yixiao Ge,Hao Zheng,Yun-Qiu Tang,Zhe Li,Mingqi Gao,Shanshan Zhao,Ying Shan,Feng Zheng",2,48,1
"80c44fab16852ea9599411da14de7079c4514172","https://www.semanticscholar.org/paper/80c44fab16852ea9599411da14de7079c4514172",2,"Vision-Language Models in Remote Sensing: Current Progress and Future Trends","A comprehensive review of the research on vision-language models in remote sensing, summarizing the latest progress, highlighting the current challenges, and identifying potential research opportunities is provided.","arXiv.org",2023,"Congcong Wen,Yuan Hu,Xiang Li,Zhenghang Yuan,Xiao Xiang Zhu",1,195,0
"affd5837492019cf252f4bf89afcda3708d2abac","https://www.semanticscholar.org/paper/affd5837492019cf252f4bf89afcda3708d2abac",2,"InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning","A systematic and comprehensive study on vision-language instruction tuning based on the pre-trained BLIP-2 models, which achieves state-of-the-art zero-shot performance across all 13 held-out datasets and qualitatively demonstrate the advantages of InstructBLIP over concurrent multimodal models.","arXiv.org",2023,"Wenliang Dai,Junnan Li,Dongxu Li,A. M. H. Tiong,Junqi Zhao,Weisheng Wang,Boyang Li,Pascale Fung,Steven Hoi",12,48,3
"abac9af9174e8be2f605453695d98e3686768a27","https://www.semanticscholar.org/paper/abac9af9174e8be2f605453695d98e3686768a27",2,"ArtGPT-4: Artistic Vision-Language Understanding with Adapter-enhanced MiniGPT-4","A novel multimodal model called ArtGPT-4 has been proposed to address some challenges in image understanding, particularly in artistic pictures, and novel benchmarks for evaluating the performance of vision-language models are proposed.","arXiv.org",2023,"Zheng Yuan,HU Xue,Xinyi Wang,Yongming Liu,Zhuanzhe Zhao,Kun Wang",0,42,0
"af0775ba00f359bc99c8be6f26506295d4089be2","https://www.semanticscholar.org/paper/af0775ba00f359bc99c8be6f26506295d4089be2",2,"On the Hidden Mystery of OCR in Large Multimodal Models","This study conducted a comprehensive study of existing publicly available multimodal models, evaluating their performance in text recognition, text-based visual question answering, key information extraction, and handwritten mathematical expression recognition.","arXiv.org",2023,"Yuliang Liu,Zhang Li,Hongliang Li,Wenwen Yu,Mingxin Huang,Dezhi Peng,Mingyu Liu,Mingrui Chen,Chunyuan Li,Lianwen Jin,Xiang Bai",1,61,0
"3f0c4d50050e8d74993b020897abaee8d1e8054d","https://www.semanticscholar.org/paper/3f0c4d50050e8d74993b020897abaee8d1e8054d",2,"Evaluating Object Hallucination in Large Vision-Language Models","This work presents the first systematic study on object hallucination of LVLMs, and designs an improved evaluation method by proposing a polling-based query method called POPE, which can evaluate the object hallucinated objects in a more stable and flexible way.","arXiv.org",2023,"Yifan Li,Yifan Du,Kun Zhou,Jinpeng Wang,Wayne Xin Zhao,Ji-rong Wen",2,41,0
"a979975d1a0aea0e01423f092249cc3de575b6cd","https://www.semanticscholar.org/paper/a979975d1a0aea0e01423f092249cc3de575b6cd",2,"X-IQE: eXplainable Image Quality Evaluation for Text-to-Image Generation with Visual Large Language Models","A novel explainable image quality evaluation approach called X-IQE, which leverages visual large language models (LLMs) to evaluate text-to-image generation methods by generating textual explanations by utilizing a hierarchical Chain of Thought to enable MiniGPT-4 to produce self-consistent, unbiased texts that are highly correlated with human evaluation.","arXiv.org",2023,"Yixiong Chen,Li Liu,C. Ding",1,56,0
"1bdd5fc17cc580efe998304692639c57c857cc84","https://www.semanticscholar.org/paper/1bdd5fc17cc580efe998304692639c57c857cc84",2,"Going Denser with Open-Vocabulary Part Segmentation","A detector with the ability to predict both open-vocabulary objects and their part segmentation and training on the joint of part-level, object-level and image-level data to build the multi-granularity alignment between language and image.","arXiv.org",2023,"Pei Sun,Shoufa Chen,Chenchen Zhu,Fanyi Xiao,Ping Luo,Saining Xie,Zhicheng Yan",0,94,0
"33f9ddca2469bf4831dcab085e1620792b1a6a80","https://www.semanticscholar.org/paper/33f9ddca2469bf4831dcab085e1620792b1a6a80",2,"LLM Itself Can Read and Generate CXR Images","This work presents a novel method to fine-tune a pre-trained LLM to read and generate images like text without any structural changes, extra training objectives, or the need for training an ad-hoc network while still preserving the of the instruction-following capability of the LLM.","arXiv.org",2023,"Suhyeon Lee,Won Jun Kim,Jong-Chul Ye",0,49,0
"f9bfc6d9ba1665b73af3323d46c7642b852759ef","https://www.semanticscholar.org/paper/f9bfc6d9ba1665b73af3323d46c7642b852759ef",2,"VideoLLM: Modeling Video Sequence with Large Language Models","This work proposes a novel framework called VideoLLM that leverages the sequence reasoning capabilities of pre-trained LLMs from natural language processing (NLP) for video sequence understanding and demonstrates that the understanding and Reasoning capabilities of LLMs can be effectively transferred to video understanding tasks.","arXiv.org",2023,"Guo Chen,Yin-Dong Zheng,Jiahao Wang,Jilan Xu,Yifei Huang,Junting Pan,Yi Wang,Yali Wang,Y. Qiao,Tong Lu,Limin Wang",0,101,0
"43a55dbd95c9d5cd82de8db276f41adeec4a937d","https://www.semanticscholar.org/paper/43a55dbd95c9d5cd82de8db276f41adeec4a937d",2,"Interactive Data Synthesis for Systematic Vision Adaptation via LLMs-AIGCs Collaboration","This work extensively study how LLMs communicate with AIGC model to achieve more controllable image generation and makes the first attempt to collaborate them for automatic data augmentation for a variety of downstream tasks.","arXiv.org",2023,"Qifan Yu,Juncheng Li,Wentao Ye,Siliang Tang,Yueting Zhuang",0,37,0
"e9a37d881abf7d94cb2c586f1cb26978343750ba","https://www.semanticscholar.org/paper/e9a37d881abf7d94cb2c586f1cb26978343750ba",2,"ReSee: Responding through Seeing Fine-grained Visual Knowledge in Open-domain Dialogue","Empirical, encouraging results not only demonstrate the effectiveness of introducing visual knowledge at both entity and turn level but also verify the proposed model ReSee outperforms several state-of-the-art methods on automatic and human evaluations.","arXiv.org",2023,"Haoqin Tu,Yitong Li,Fei Mi,Zhongliang Yang",0,55,0
"b82c1b0512d25307e3c81bb8d9df1607267a7a52","https://www.semanticscholar.org/paper/b82c1b0512d25307e3c81bb8d9df1607267a7a52",2,"MemeCap: A Dataset for Captioning and Interpreting Memes","The task of meme captioning is presented and a new dataset, MemeCap, containing 6.3K memes along with the title of the post containing the meme, the meme captions, the literal image caption, and the visual metaphors is released.","arXiv.org",2023,"EunJeong Hwang,V. Shwartz",1,38,0
"2ad8183c72a90511383a32ccaeea313eb85f4085","https://www.semanticscholar.org/paper/2ad8183c72a90511383a32ccaeea313eb85f4085",2,"DetGPT: Detect What You Need via Reasoning","This paper introduces a new paradigm for object detection that is called reasoning-based object detection, and leverages state-of-the-art multi-modal models and open-vocabulary object detectors to perform reasoning within the context of the user's instructions and the visual scene.","arXiv.org",2023,"Renjie Pi,Jiahui Gao,Shizhe Diao,Rui Pan,Hanze Dong,Jipeng Zhang,Lewei Yao,Jianhua Han,Hang Xu,Lingpeng Kong Tong Zhang",2,54,0
"d3f79210b54e168c76b8c311488f42d7d1048b81","https://www.semanticscholar.org/paper/d3f79210b54e168c76b8c311488f42d7d1048b81",2,"PandaGPT: One Model To Instruction-Follow Them All","PandaGPT is an approach to emPower large lANguage moDels with visual and Auditory instruction-following capabilities that can perform complex tasks such as detailed image description generation, writing stories inspired by videos, and answering questions about audios.","arXiv.org",2023,"Yixuan Su,Tian Lan,Huayang Li,Jialu Xu,Yan Wang,Deng Cai",2,29,0
"b13242323021bc1483a0d76e23428e324d409315","https://www.semanticscholar.org/paper/b13242323021bc1483a0d76e23428e324d409315",2,"NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models","The NavGPT is introduced, a purely LLM-based instruction-following navigation agent, to reveal the reasoning capability of GPT models in complex embodied scenes by performing zero-shot sequential action prediction for vision-and-language navigation (VLN).","arXiv.org",2023,"Gengze Zhou,Yicong Hong,Qi Wu",0,72,0
"7e3bbd7be60bb50a8093152795f269a69a4a0fd9","https://www.semanticscholar.org/paper/7e3bbd7be60bb50a8093152795f269a69a4a0fd9",2,"Chatting Makes Perfect - Chat-based Image Retrieval","This work introduces ChatIR: a chat-based image retrieval system that engages in a conversation with the user to elicit information, in addition to an initial query, in order to clarify the user's search intent.","arXiv.org",2023,"Matan Levy,Rami Ben-Ari,N. Darshan,D. Lischinski",0,55,0
"5fb7afae5fcacae1d40f109a348b43e00aa5d486","https://www.semanticscholar.org/paper/5fb7afae5fcacae1d40f109a348b43e00aa5d486",2,"Dense and Aligned Captions (DAC) Promote Compositional Reasoning in VL Models","A fine-tuning approach is proposed for automatically treating two factors limiting the VL models' compositional reasoning performance: the caption quality, or in other words `image-alignment', of the texts and the `density' of the captions in the sense of mentioning all the details appearing on the image.","arXiv.org",2023,"Sivan Doveh,Assaf Arbelle,Sivan Harary,Roei Herzig,Donghyun Kim,Paola Cascante-Bonilla,Amit Alfassy,R. Panda,R. Giryes,R. Feris,S. Ullman,Leonid Karlinsky",0,86,0
"0867f7029b3726740fb41ca8171833bf6f82e483","https://www.semanticscholar.org/paper/0867f7029b3726740fb41ca8171833bf6f82e483",2,"Exploring Open-Vocabulary Semantic Segmentation without Human Labels","This work presents ZeroSeg, a novel method that leverages the existing pretrained vision-language (VL) model to train open-vocabulary zero-shot semantic segmentation models, achieving state-of-the-art performance when compared to other zero- shot segmentation methods under the same training data, while also performing competitively compared to strongly supervised methods.","",2023,"Jun Chen,Deyao Zhu,Guocheng Qian,Bernard Ghanem,Zhicheng Yan,Chenchen Zhu,Fanyi Xiao,Mohamed Elhoseiny,S. Culatana",0,58,0
"31a68755ca6899e6c360ec8568704ae74f223a25","https://www.semanticscholar.org/paper/31a68755ca6899e6c360ec8568704ae74f223a25",2,"GPT4Image: Can Large Pre-trained Models Help Vision Models on Perception Tasks?","This paper presents a new learning paradigm in which the knowledge extracted from large pre-trained models are utilized to help models like CNN and ViT learn enhanced representations and achieve better performance.","",2023,"N. Ding,Yehui Tang,Zhongqian Fu,Chaoting Xu,Kai Han,Yunhe Wang",0,50,0
"de8121dc3d2c69bdab172f37e31168ddf2e6e62f","https://www.semanticscholar.org/paper/de8121dc3d2c69bdab172f37e31168ddf2e6e62f",2,"Segment Everything Everywhere All at Once","SEEM is a promptable, interactive model for Segmenting Everything Everywhere all at once in an image by incorporating learnable memory prompts to retain dialog history information via mask-guided cross-attention.","arXiv.org",2023,"Xueyan Zou,Jianwei Yang,Hao Zhang,Feng Li,Linjie Li,Jianfeng Gao,Yong Jae Lee",19,65,5
"508d9b43832790b4d35f4ae1fa76e9712859d6aa","https://www.semanticscholar.org/paper/508d9b43832790b4d35f4ae1fa76e9712859d6aa",2,"Vision and Structured-Language Pretraining for Cross-Modal Food Retrieval","VLPCook outperforms current SoTA by a significant margin on the task of Cross-Modal Food Retrieval on the large Recipe1M dataset and validates the generalization of the approach to other tasks and domains with structured text such as the Medical domain on the ROCO dataset.","",2022,"Mustafa Shukor,Nicolas Thome,M. Cord",0,85,0
"1329484d20c470b84e46ed7453786cee0acad2e0","https://www.semanticscholar.org/paper/1329484d20c470b84e46ed7453786cee0acad2e0",2,"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models","BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods, and is demonstrated's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.","arXiv.org",2023,"Junnan Li,Dongxu Li,S. Savarese,Steven Hoi",191,43,66
"fbfef4723d8c8467d7bd523e1d0b703cce0e0f9c","https://www.semanticscholar.org/paper/fbfef4723d8c8467d7bd523e1d0b703cce0e0f9c",2,"Language Is Not All You Need: Aligning Perception with Language Models","This work introduces Kosmos-1, a Multimodal Large Language Model (MLLM) that can perceive general modalities, learn in context, and follow instructions, and shows that MLLMs can benefit from cross-modal transfer, i.e., transfer knowledge from language to multimodals, and from multimodal to language.","arXiv.org",2023,"Shaohan Huang,Li Dong,Wenhui Wang,Y. Hao,Saksham Singhal,Shuming Ma,Tengchao Lv,Lei Cui,O. Mohammed,Qiang Liu,Kriti Aggarwal,Zewen Chi,Johan Bjorck,Vishrav Chaudhary,Subhojit Som,Xia Song,Furu Wei",55,67,8
"69cfdc8df16ae63b7acba4ac6f727f78b86893c3","https://www.semanticscholar.org/paper/69cfdc8df16ae63b7acba4ac6f727f78b86893c3",2,"ChatGPT Asks, BLIP-2 Answers: Automatic Questioning Towards Enriched Visual Descriptions","This paper introduces ChatCaptioner, a novel automatic-questioning method deployed in image captioning that identifies 53% more objects within the image than BLIP-2 alone measured by WordNet synset matching.","arXiv.org",2023,"Deyao Zhu,Jun Chen,Kilichbek Haydarov,Xiaoqian Shen,Wenxuan Zhang,Mohamed Elhoseiny",10,51,2
"9d12916dd46df7a6446cbec0bc4d054f7dafcdab","https://www.semanticscholar.org/paper/9d12916dd46df7a6446cbec0bc4d054f7dafcdab",2,"Scaling Vision-Language Models with Sparse Mixture of Experts","The effectiveness of MoE in scaling vision-language models is explored, demonstrating its potential to achieve state-of-the-art performance on a range of benchmarks over dense models of equivalent computational cost.","arXiv.org",2023,"Sheng Shen,Z. Yao,Chunyuan Li,Trevor Darrell,K. Keutzer,Yuxiong He",3,74,1
"4396e30f28eb49bb07c63cf62ca90415ebbe43d4","https://www.semanticscholar.org/paper/4396e30f28eb49bb07c63cf62ca90415ebbe43d4",2,"IRGen: Generative Modeling for Image Retrieval","The framework, IRGen, is a unified model that enables end-to-end differentiable search, thus achieving superior performance thanks to direct optimization and tackling the key technical challenge of converting an image into quite a short sequence of semantic units in order to enable efficient and effective retrieval.","arXiv.org",2023,"Yidan Zhang,Ting Zhang,Dong Chen,Yujing Wang,Qi Chen,Xingxu Xie,Hao Sun,Weiwei Deng,Qi Zhang,Fan Yang,Mao Yang,Q. Liao,B. Guo",1,110,0
"3049c992adbd56e29c4d957ee0c4e9d05fe3c6d1","https://www.semanticscholar.org/paper/3049c992adbd56e29c4d957ee0c4e9d05fe3c6d1",2,"EVA-02: A Visual Representation for Neon Genesis","EVA-02, a next-generation Transformer-based visual representation pre-trained to reconstruct strong and robust language-aligned vision features via masked image modeling, is launched, demonstrating superior performance compared to prior state-of-the-art approaches across various representative vision tasks, while utilizing significantly fewer parameters and compute budgets.","arXiv.org",2023,"Yuxin Fang,Quan Sun,Xinggang Wang,Tiejun Huang,Xinlong Wang,Yue Cao",6,146,2
"d064075c47e358f604034d06df4b985356757c71","https://www.semanticscholar.org/paper/d064075c47e358f604034d06df4b985356757c71",2,"Equivariant Similarity for Vision-Language Foundation Models","This study proposes EqSim, a regularization loss that can be efficiently calculated from any two matched training pairs and easily pluggable into existing image-text retrieval fine-tuning and presents a new challenging benchmark EqBen, the first to focus on ""visual-minimal change"".","arXiv.org",2023,"Tan Wang,Kevin Lin,Linjie Li,Chung-Ching Lin,Zhengyuan Yang,Hanwang Zhang,Zicheng Liu,Lijuan Wang",1,73,1
"b259d853b71a2d03cefa844bb9343b8e3ed816b1","https://www.semanticscholar.org/paper/b259d853b71a2d03cefa844bb9343b8e3ed816b1",2,"LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention","A zero-init attention mechanism with zero gating is proposed, which adaptively injects the new instructional cues into LLaMA, while effectively preserves its pre-trained knowledge.","arXiv.org",2023,"Renrui Zhang,Jiaming Han,Aojun Zhou,Xiangfei Hu,Shilin Yan,Pan Lu,Hongsheng Li,Peng Gao,Y. Qiao",30,63,2
"c84e2801512069acbc63f1a7f73273281939428c","https://www.semanticscholar.org/paper/c84e2801512069acbc63f1a7f73273281939428c",2,"A Study of Autoregressive Decoders for Multi-Tasking in Computer Vision","This work takes a close look at autoregressive decoders for multi-task learning in multimodal computer vision, including classification, captioning, visual question answering, and optical character recognition and compares these to well-tuned single-task baselines to highlight the cost incurred by multi-tasking.","arXiv.org",2023,"L. Beyer,Bo Wan,Gagan Madan,Filip Pavetic,A. Steiner,Alexander Kolesnikov,André Susano Pinto,Emanuele Bugliarello,Xiao Wang,Qihang Yu,Liang-Chieh Chen,Xiaohua Zhai",1,98,0
"db1c83ef73d2f7731b0dd255835f2f26db749e17","https://www.semanticscholar.org/paper/db1c83ef73d2f7731b0dd255835f2f26db749e17",2,"Not All Features Matter: Enhancing Few-shot CLIP with Adaptive Prior Refinement","This paper proposes APE, an Adaptive Prior rEfinement method for CLIP's pre-trained knowledge, which achieves superior accuracy with high computational efficiency and introduces two model variants, a training-free APE and aTraining-required APE-T.","arXiv.org",2023,"Xiang-yu Zhu,Renrui Zhang,Bowei He,A-Long Zhou,D. Wang,Bingyan Zhao,Peng Gao",2,100,0
"a43a3fadc9190e61b34f59a913f1716e443519e4","https://www.semanticscholar.org/paper/a43a3fadc9190e61b34f59a913f1716e443519e4",2,"On the Opportunities and Challenges of Foundation Models for Geospatial Artificial Intelligence","It is proposed that one of the major challenges of developing a FM for GeoAI is to address the multimodality nature of geospatial tasks and the possibility of a multimodal foundation model which can reason over various types ofGeoAI data through geosp spatial alignments is suggested.","arXiv.org",2023,"Gengchen Mai,Weiming Huang,Jin Sun,Suhang Song,Deepak Mishra,Ninghao Liu,Song Gao,Tianming Liu,G. Cong,Yingjie Hu,Chris Cundy,Ziyuan Li,Rui Zhu,Ni Lao",10,158,1
"b7d73f22d861f526541575a3b17449bd3c58ca74","https://www.semanticscholar.org/paper/b7d73f22d861f526541575a3b17449bd3c58ca74",2,"MVP-SEG: Multi-View Prompt Learning for Open-Vocabulary Semantic Segmentation","Experiments show that the multi-view prompts learned from seen categories have strong generalization to unseen categories, and MVP-SEG+ which combines the knowledge transfer stage significantly outperforms previous methods on several benchmarks, justifying that MVP- SEG does lead to better focus on different local parts.","arXiv.org",2023,"Jie Guo,Qimeng Wang,Yan Gao,Xiaolong Jiang,Xu Tang,Yao Hu,Baochang Zhang",1,56,0
"c3068e2a9f4cd374c7ff3be1b8f877b3d653e880","https://www.semanticscholar.org/paper/c3068e2a9f4cd374c7ff3be1b8f877b3d653e880",2,"Multimodal Neural Databases","This paper presents the first architecture able to answer complex database-like queries that involve reasoning over different input modalities, such as text and images, at scale, and presents the potential of these new techniques to process unstructured data coming from different modalities.","arXiv.org",2023,"Giovanni Trappolini,Andrea Santilli,E. Rodolà,A. Halevy,F. Silvestri",0,38,0
"0340c850e033abbf71c7214e403c8fe2be5ef91f","https://www.semanticscholar.org/paper/0340c850e033abbf71c7214e403c8fe2be5ef91f",2,"Visual Tuning","This survey characterizes a large and thoughtful selection of recent works, providing a systematic and comprehensive overview of existing work and models and categorizes recent visual tuning techniques into five groups: prompt tuning, adapter tuning, parameter tuning, and remapping tuning.","arXiv.org",2023,"Bruce X. B. Yu,Jianlong Chang,Haixin Wang,Lin Liu,Shijie Wang,Zhiyu Wang,Junfan Lin,Lingxi Xie,Haojie Li,Zhouchen Lin,Qi Tian,Chang Wen Chen",0,289,0
"b07fa63a6d2f39900f0f2cae8f58cd5507010aad","https://www.semanticscholar.org/paper/b07fa63a6d2f39900f0f2cae8f58cd5507010aad",2,"Multi-Prompt with Depth Partitioned Cross-Modal Learning","This study introduces the Partitioned Multi-modal Prompt (PMPO), a multi- modal prompting technique that extends the soft prompt from a single learnable prompt to multiple prompts, and incorporates prior information from manually designed templates and learnable multi-prompts, thus improving the generalization capabilities of the approach.","arXiv.org",2023,"Yiqi Wang,Xianda Guo,Zheng Hua Zhu,Yingjie Tian",0,43,0
"8badb0587fef2ffc078b0cec549eb8ec96ed3ad4","https://www.semanticscholar.org/paper/8badb0587fef2ffc078b0cec549eb8ec96ed3ad4",2,"Self-Chained Image-Language Model for Video Localization and Question Answering","Self-Chained Video Localization-Answering (SeViLA), a novel framework that leverages a single image-language model (BLIP-2) to tackle both temporal keyframe localization and QA on videos, outperforms several strong baselines/previous works on five video QA and event prediction tasks, and achieves the state-of-the-art in both fine-tuning and zero-shot settings.","arXiv.org",2023,"Shoubin Yu,Jaemin Cho,Prateek Yadav,Mohit Bansal",0,80,0
"38be7643bcad936739550a1802220eb53ca9b1df","https://www.semanticscholar.org/paper/38be7643bcad936739550a1802220eb53ca9b1df",2,"Simple Token-Level Confidence Improves Caption Correctness","This work explores Token-Level Confidence, or TLC, as a simple yet surprisingly effective method to assess caption correctness, and fine-tune a vision-language model on image captioning, input an image and proposed caption to the model, and aggregate either algebraic or learned token confidences over words or sequences to estimate image-caption consistency.","arXiv.org",2023,"Suzanne Petryk,Spencer Whitehead,Joseph Gonzalez,Trevor Darrell,Anna Rohrbach,Marcus Rohrbach",0,73,0
"8ae2e81495d426419e6fd96940b651002c046b61","https://www.semanticscholar.org/paper/8ae2e81495d426419e6fd96940b651002c046b61",2,"Enhancing Vision-Language Pre-Training with Jointly Learned Questioner and Dense Captioner","A novel method called Joint QA and DC GEneration (JADE), which utilizes a pre-trained multimodal model and easily-crawled image-text pairs to automatically generate and filter large-scale VQA and dense captioning datasets.","arXiv.org",2023,"Zikang Liu,Sihan Chen,Longteng Guo,Handong Li,Xingjian He,J. Liu",0,57,0
"6118eb18023429fa8bad64b7a1d95533127a62d7","https://www.semanticscholar.org/paper/6118eb18023429fa8bad64b7a1d95533127a62d7",2,"Prompt ChatGPT In MNER: Improved multimodal named entity recognition method based on auxiliary refining knowledge from ChatGPT","This paper uses ChatGPT as an implicit knowledge engine to acquire auxiliary refined knowledge, thereby bolstering the model's performance in MNER tasks and significantly outperforms all existing state-of-the-art methods on two classic MNER datasets.","arXiv.org",2023,"Jinyuan Li,Han Li,Zhufeng Pan,Gang Pan",0,48,0
"d57caa06a42baa90eb741a9afb10fe4fff8be82a","https://www.semanticscholar.org/paper/d57caa06a42baa90eb741a9afb10fe4fff8be82a",2,"SmartTrim: Adaptive Tokens and Parameters Pruning for Efficient Vision-Language Models","This work proposes an adaptive acceleration method SmartTrim for VLMs, which adjusts the inference overhead based on the complexity of instances, and incorporates lightweight trimming modules into the backbone to perform task-specific pruning on redundant inputs and parameters without the need for additional pre-training or data augmentation.","arXiv.org",2023,"Zekun Wang,Jingchang Chen,Wangchunshu Zhou,Ming Liu,Bing Qin",0,85,0
"c2c7ad3112c4b575e5d8163a0e574f9eb743cb52","https://www.semanticscholar.org/paper/c2c7ad3112c4b575e5d8163a0e574f9eb743cb52",2,"Zero-shot Visual Question Answering with Language Model Feedback","A novel language model guided captioning approach, LAMOC, for knowledge-based visual question answering (VQA), which outperforms several competitive zero-shot methods and even achieves comparable results to a fine-tuned VLP model.","arXiv.org",2023,"Yifan Du,Junyi Li,Tianyi Tang,Wayne Xin Zhao,Ji-rong Wen",0,40,0
"5c183d241fe54a6d67e21eea48fbd5ea6b31ec1c","https://www.semanticscholar.org/paper/5c183d241fe54a6d67e21eea48fbd5ea6b31ec1c",2,"VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset","Based on the proposed VAST-27M dataset, an omni-modality video-text foundational model named VAST is trained, which can perceive and process vision, audio, and subtitle modalities from video, and better support various tasks including vision- Text, audio-text, and multi-modal video- text tasks (retrieval, captioning and QA).","arXiv.org",2023,"Sihan Chen,Handong Li,Qunbo Wang,Zijia Zhao,Ming-Ting Sun,Xinxin Zhu,J. Liu",0,100,0
"fd928577d67dd01048d13f284a6256164bbcf2f0","https://www.semanticscholar.org/paper/fd928577d67dd01048d13f284a6256164bbcf2f0",2,"Learning without Forgetting for Vision-Language Models","ProjectiOn Fusion (PROOF) is proposed that enables VLMs to learn without forgetting and jointly adjusting visual and textual features, the model can capture semantic information with stronger representation ability.","arXiv.org",2023,"Da-Wei Zhou,Yuanhan Zhang,Jingyi Ning,Han-Jia Ye,De-chuan Zhan,Ziwei Liu",0,84,0
"f366bcf04d29c8c04c253bedbd4a94b0789c509b","https://www.semanticscholar.org/paper/f366bcf04d29c8c04c253bedbd4a94b0789c509b",2,"Benchmarking Robustness of Adaptation Methods on Pre-trained Vision-Language Models","The robustness of 11 widely-used adaptation methods across 4 vision-language datasets under multimodal corruptions is assessed and it is indicated that increasing the number of adaptation data and parameters does not guarantee enhanced robustness; instead it results in even lower robustness.","",2023,"Shuo Chen,Jindong Gu,Zhen Han,Yunpu Ma,Philip H. S. Torr,Volker Tresp",0,77,0
"d183cc170400e43535c5e2c37121c37ee0ba23dc","https://www.semanticscholar.org/paper/d183cc170400e43535c5e2c37121c37ee0ba23dc",2,"Open-Ended Medical Visual Question Answering Through Prefix Tuning of Language Models","This work focuses on open-ended VQA and motivated by the recent advances in language models consider it as a generative task, and introduces a novel method particularly suited for small, domain-specific, medical datasets.","arXiv.org",2023,"Tom van Sonsbeek,Mohammad Mahdi Derakhshani,Ivona Najdenkoska,Cees G. M. Snoek,M. Worring",3,33,2
"456a70485aafc12dfed4fb7354668d72aae9b658","https://www.semanticscholar.org/paper/456a70485aafc12dfed4fb7354668d72aae9b658",2,"SecureBERT: A Domain-Specific Language Model for Cybersecurity","This paper proposes SecureBERT, a cybersecurity language model capable of capturing text connotations in cybersecurity text and therefore successful in automation for many critical cybersecurity tasks that would otherwise rely on human expertise and time-consuming manual procedures.","",2022,"Ehsan Aghaei,Xi Niu,W. Shadid,E. Al-Shaer",1,33,0
"5d8fd04c436367b18b35e28332ee8e452a477f3f","https://www.semanticscholar.org/paper/5d8fd04c436367b18b35e28332ee8e452a477f3f",2,"Medical Image Understanding with Pretrained Vision Language Models: A Comprehensive Study","It is shown that well-designed medical prompts are the key to elicit knowledge from pre-trained VLMs, and by prompting with expressive attributes that are shared between domains, the VLM can carry the knowledge across domains and improve its generalization.","arXiv.org",2022,"Ziyuan Qin,Huahui Yi,Qicheng Lao,Kang Li",7,53,0
"131c6f328c11706de2c43cd16e0b7c5d5e610b6a","https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a",2,"Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond","A comprehensive and practical guide for practitioners and end-users working with Large Language Models in their downstream natural language processing (NLP) tasks, enabling the successful implementation of these models in a wide range of NLP tasks.","arXiv.org",2023,"Jingfeng Yang,Hongye Jin,Ruixiang Tang,Xiaotian Han,Qizhang Feng,Haoming Jiang,Bing Yin,Xia Hu",16,127,2
"51b169701290cd129e0781fc9f3a9918604c89b5","https://www.semanticscholar.org/paper/51b169701290cd129e0781fc9f3a9918604c89b5",2,"Improving accuracy of GPT-3/4 results on biomedical data using a retrieval-augmented language model","It is suggested that RetA models, supplemented with domain-specific corpora, may outperform general-purpose LLMs in accuracy and relevance within specific domains.","arXiv.org",2023,"D. Soong,S. Sridhar,Han Si,J. Wagner,Ana Caroline Costa S'a,Christina Y. Yu,Kubra Karagoz,Meijian Guan,Hisham K Hamadeh,Brandon Higgs",0,51,0
"362d4e00506f9bb39d42185a0b128f8602e139a8","https://www.semanticscholar.org/paper/362d4e00506f9bb39d42185a0b128f8602e139a8",2,"Utilizing ChatGPT to Enhance Clinical Trial Enrollment","","",2023,"Georgios Peikos,S. Symeonidis,Pranav Kasela,G. Pasi",0,105,0
"f646d3056ca02daa99820917b3ba48a43a0022e2","https://www.semanticscholar.org/paper/f646d3056ca02daa99820917b3ba48a43a0022e2",2,"SUR-adapter: Enhancing Text-to-Image Pre-trained Diffusion Models with Large Language Models","This work proposes a simple-yet-effective parameter-efficient fine-tuning approach called the Semantic Understanding and Reasoning adapter (SUR-adapter) for pre-trained diffusion models that can make text-to-image diffusion models easier to use with better user experience.","arXiv.org",2023,"Shan Zhong,Zhongzhan Huang,Wushao Wen,Jinghui Qin,Liang Lin",1,54,0
"052a5e2bcc999810ee6f1eedcf758c528e4f125f","https://www.semanticscholar.org/paper/052a5e2bcc999810ee6f1eedcf758c528e4f125f",2,"Retrieving Multimodal Information for Augmented Generation: A Survey","This survey provides an in-depth review of retrieval-augmented generation in different modalities and discusses potential future directions of this emerging field.","arXiv.org",2023,"Ruochen Zhao,Hailin Chen,Weishi Wang,Fangkai Jiao,Xuan Long Do,Chengwei Qin,Bosheng Ding,Xiaobao Guo,Minzhi Li,Xingxuan Li,Shafiq R. Joty",4,150,0
"357fc385caf2e5b9898c9140fa3ac9955e6bb3c6","https://www.semanticscholar.org/paper/357fc385caf2e5b9898c9140fa3ac9955e6bb3c6",2,"TeamS at VQA-Med 2021: BBN-Orchestra for Long-tailed Medical Visual Question Answering","The proposed BBN-Orchestra is an ensemble of bilateral-branch networks (BBN) and successfully reduces overfitting to train and validation data in addition to effectively modeling the imbalanced long-tailed image distribution.","Conference and Labs of the Evaluation Forum",2021,"Sedigheh Eslami,Gerard de Melo,C. Meinel",6,13,0
"3c83f80f06633ff4598d33c2959f8e4cdcad3e93","https://www.semanticscholar.org/paper/3c83f80f06633ff4598d33c2959f8e4cdcad3e93",2,"Cross-Modal Self-Attention with Multi-Task Pre-Training for Medical Visual Question Answering","This work reformulates image feature pre-training as a multi-task learning paradigm and witness its extraordinary superiority, forcing it to take into account the applicability of features for the specific image comprehension task.","International Conference on Multimedia Retrieval",2021,"Haifan Gong,Guanqi Chen,Sishuo Liu,Yizhou Yu,Guanbin Li",29,31,1
"e34b699cef0a711a8cb9c39ecea20ac2df1578f5","https://www.semanticscholar.org/paper/e34b699cef0a711a8cb9c39ecea20ac2df1578f5",2,"Medical Visual Question Answering: A Survey","This research presents a novel and scalable approaches to integrate 3D image recognition and 3D speech recognition into the clinical practice of ophthalmology.","arXiv.org",2021,"Zhihong Lin,Donghao Zhang,Qingyi Tao,Danli Shi,Gholamreza Haffari,Qi Wu,M. He,Z. Ge",14,109,1
"8c9a9a1bbba2a3e3bab34bce533b3b2acfda32b0","https://www.semanticscholar.org/paper/8c9a9a1bbba2a3e3bab34bce533b3b2acfda32b0",2,"Medical visual question answering based on question-type reasoning and semantic space constraint","A novel Med-VQA framework is proposed to alleviate the above-mentioned problems, which employed a question-type reasoning module severally to closed-ended and open-ended questions, thereby extracting the important information contained in the questions through an attention mechanism and filtering the noise to extract more valuable question features.","Artif. Intell. Medicine",2022,"Meiling Wang,Xiaohai He,Luping Liu,L. Qing,Honggang Chen,Yan Liu,Chao Ren",3,57,0
"2ac3bacbbee520b701707ebcf7b9ca7a3f233129","https://www.semanticscholar.org/paper/2ac3bacbbee520b701707ebcf7b9ca7a3f233129",2,"Medical visual question answering via corresponding feature fusion combined with semantic attention.","A corresponding feature fusion (CFF) method to strengthen the interactions of specific features from corresponding radiology images and questions and a semantic attention (SA) module for textual feature extraction is proposed.","Mathematical biosciences and engineering : MBE",2022,"Han Zhu,Xiaohai He,Meiling Wang,Mozhi Zhang,L. Qing",2,15,0
"ef2edea434e487f288d4eed6f9b1dc480b917211","https://www.semanticscholar.org/paper/ef2edea434e487f288d4eed6f9b1dc480b917211",2,"Adversarial Learning to Improve Question Image Embedding in Medical Visual Question Answering","A new method for training VQA models that utilizes adversarial learning to improve the question-image embedding and demonstrates how this embedding can be used as the ideal embedding for answer inference.","Moratuwa Engineering Research Conference",2022,"Kaveesha Silva,Thanuja Maheepala,Kasun Tharaka,Thanuja D. Ambegoda",0,20,0
"b047b3b7d76b79958e23b0fcab985be22b1ce42d","https://www.semanticscholar.org/paper/b047b3b7d76b79958e23b0fcab985be22b1ce42d",2,"Alternating Cross-attention Vision-Language Model for Efficient Learning with Medical Image and Report without Curation","It is experimentally demonstrated that the pre-trained MAX-VL model outperforms the current state-of-the-art vision language models in various vision-language tasks and suggested the clinical utility for the diagnosis of newly emerging diseases and human error detection as well as showed the widespread applicability of the model in different domain data.","arXiv.org",2022,"Sangjoon Park,Eunha Lee,Jeonghyeon Lee,Jong-Chul Ye",0,47,0
"79478a2ac67b9fdbeadcde13faa2d84eb239e080","https://www.semanticscholar.org/paper/79478a2ac67b9fdbeadcde13faa2d84eb239e080",2,"Vision-Language Pretraining Enables Radiographs and Reports to be Learned without Curation","It is experimentally demonstrated that the pre-trained medical X-VL model outperforms the current state-of-the-art models in various vision-language tasks in medical domains, which suggests the potential of the model for widespread applicability in different medical applications.","",2022,"Sangjoon Park,Eunha Lee,Jeonghyeon Lee,Jong-Chul Ye",0,50,0
"2441230bd2f3cca924d597b3044ad63aaff269ec","https://www.semanticscholar.org/paper/2441230bd2f3cca924d597b3044ad63aaff269ec",2,"Self-supervised Co-learning of Uncurated Images and Reports Enables Oversight AI in Radiology","A self-supervised model tailored for efficient vision-language pre-training that exploits cross attention in the radiological images and - trained medical X-VL model outperforms the current state-of-the-art models in various vision- language tasks in medical domains.","",2022,"Sangjoon Park,Eunha Lee,K. Shin,Jeonghyeon Lee,Jong-Chul Ye",4,49,0
"0cbd644254462341a897d4bfa0134637662c3ab5","https://www.semanticscholar.org/paper/0cbd644254462341a897d4bfa0134637662c3ab5",2,"A Transformer-based Medical Visual Question Answering Model","Experimental results demonstrate that the Transformer structure not only ensures the stability of the model performance, but also accelerates its convergence, and the MQAT model outperforms the existing state-of-the-art methods.","International Conference on Pattern Recognition",2022,"Lei Liu,Xiangdong Su,Hui Guo,Daobin Zhu",0,25,0
"56d8d9fff399f798da97a69e891de4eeb4568d4f","https://www.semanticscholar.org/paper/56d8d9fff399f798da97a69e891de4eeb4568d4f",2,"MHKD-MVQA: Multimodal Hierarchical Knowledge Distillation for Medical Visual Question Answering","This work proposes multimodal hierarchical knowledge distillation for medical VQA (MHKD-MVQA), which distill knowledge from not only the output but also the intermediate layers, which leverages the knowledge from limited samples to a greater extent and achieves state-of-the-art performance.","IEEE International Conference on Bioinformatics and Biomedicine",2022,"Jianfeng Wang,Shuokang Huang,Huifang Du,Yu Qin,Haofen Wang,Wenqiang Zhang",1,49,0
"5942335fdd35d1651aaabd7af4db129a29ed2a85","https://www.semanticscholar.org/paper/5942335fdd35d1651aaabd7af4db129a29ed2a85",2,"How Well Apply Multimodal Mixup and Simple MLPs Backbone to Medical Visual Question Answering?","This paper designs a Med-VQA model which employs multi-layer perceptrons (MLPs) as the backbone network for feature extraction and modal fusion and designs a multimodal mixup (M-Mixup) to augment images and questions separately, which effectively alleviates the problem of insufficient training samples in the Med- VQA task.","IEEE International Conference on Bioinformatics and Biomedicine",2022,"Lei Liu,Xiangdong Su",0,26,0
"a627232a97a7a63f8399d157f0b022eb1ccd547c","https://www.semanticscholar.org/paper/a627232a97a7a63f8399d157f0b022eb1ccd547c",2,"Biomedical Question Answering: A Survey of Approaches and Challenges","This survey identifies and characterize several key challenges in BQA that might lead to this issue, and discusses some potential future directions to explore.","ACM Computing Surveys",2021,"Qiao Jin,Zheng Yuan,Guangzhi Xiong,Qian Yu,Huaiyuan Ying,Chuanqi Tan,Mosha Chen,Songfang Huang,Xiaozhong Liu,Sheng Yu",35,246,1
"2580d3fc39fed3989f10665559a955b847b7eb7f","https://www.semanticscholar.org/paper/2580d3fc39fed3989f10665559a955b847b7eb7f",2,"Medical Visual Question Answering via Conditional Reasoning and Contrastive Learning","A novel conditional reasoning mechanism with a question- Conditioned reasoning component and a type-conditioned reasoning strategy to learn effective reasoning skills for different Med-VQA tasks adaptively and to pre-train a visual feature extractor via contrastive learning on large amounts of unlabeled radiology images.","IEEE Transactions on Medical Imaging",2022,"Bo Liu,Li-Ming Zhan,Li Xu,Xiao-Ming Wu",0,72,0
"940f303c2530a52c5fd3c52c9c64ceea4b53ab05","https://www.semanticscholar.org/paper/940f303c2530a52c5fd3c52c9c64ceea4b53ab05",2,"Diversity Learning Based on Multi-Latent Space for Medical Image Visual Question Generation","A diversity learning-based visual question generation model using a multi-latent space to generate informative question sets from medical images that works with an answering model for interactive automated clinical diagnosis and generates datasets to replace the process of annotation that incurs huge labor costs.","Italian National Conference on Sensors",2023,"He Zhu,Ren Togo,Takahiro Ogawa,M. Haseyama",0,59,0
"8200be2e8b9af243ee72a9d919a4f7fbe82a17d2","https://www.semanticscholar.org/paper/8200be2e8b9af243ee72a9d919a4f7fbe82a17d2",2,"Medical knowledge-based network for Patient-oriented Visual Question Answering","","Information Processing & Management",2023,"Jian Huang,Yihao Chen,Yong Li,Zhenguo Yang,Xuehao Gong,Fuhui Wang,Xiaohong Xu,Wenyin Liu",0,37,0
"17ca48ad1b944c897863f04ba9ffa72674dce1ce","https://www.semanticscholar.org/paper/17ca48ad1b944c897863f04ba9ffa72674dce1ce",2,"Parallel multi-head attention and term-weighted question embedding for medical visual question answering","The proposed MaMVQA model achieved significantly increased accuracy in predicting answers to both close-ended and open-ended questions and outperforms previous state-of-the-art methods in terms of accuracy while requiring no external data to train the model.","Multimedia tools and applications",2023,"Sruthy Manmadhan,Binsu C. Kovoor",0,56,0
"2587cb7b1c02fde76e3c23c13f1bd40d6a199c57","https://www.semanticscholar.org/paper/2587cb7b1c02fde76e3c23c13f1bd40d6a199c57",2,"Q2ATransformer: Improving Medical VQA via an Answer Querying Decoder","This paper proposes a new Transformer based framework for medical VQA (named as Q2ATransformer), which integrates the advantages of both the classification and the generation approaches and provides a unified treatment for the close-end and open-end questions.","arXiv.org",2023,"Yunyi Liu,Zhanyu Wang,Dong Xu,Luping Zhou",1,15,0
"c5bcc78ae708b29edb03481e12213eca53c28963","https://www.semanticscholar.org/paper/c5bcc78ae708b29edb03481e12213eca53c28963",2,"A multi-modal model based on transformers for medical visual question answering","The IIF module that can improve the model's ability to obtain visual feature is proposed and QAM is designed to help the model analyze the question better.","International Conference on Artificial Intelligence and Computer Engineering (ICAICE 2022)",2023,"Mingchun Huang,Ming Xu,Fuhuang Liu,Liyan Chen",0,12,0
"f22d71c7ce9720ba1f717a4f1181488200e78198","https://www.semanticscholar.org/paper/f22d71c7ce9720ba1f717a4f1181488200e78198",2,"LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day","This paper proposes a cost-efficient approach for training a vision-language conversational assistant that can answer open-ended research questions of biomedical images, and releases instruction-following data and the LLaVA-Med model, which exhibits excellent multimodal conversational capability.","",2023,"Chunyuan Li,Cliff Wong,Sheng Zhang,Naoto Usuyama,Haotian Liu,Jianwei Yang,Tristan Naumann,Hoifung Poon,Jianfeng Gao",1,45,0
"385376b8aa48c25403f17d6206db7c09b67e1314","https://www.semanticscholar.org/paper/385376b8aa48c25403f17d6206db7c09b67e1314",1,"Prompt Engineering for Healthcare: Methodologies and Applications","This review will introduce the latest advances in prompt engineering in the field of natural language processing (NLP) for the medical domain and highlight its significant contributions to healthcare NLP applications such as question-answering systems, text summarization, and machine translation.","arXiv.org",2023,"Jiaqi Wang,Enze Shi,Sigang Yu,Zihao Wu,Chong Ma,Haixing Dai,Qiushi Yang,Yanqing Kang,Jinru Wu,Huawen Hu,Chenxi Yue,Haiyang Zhang,Yi-Hsueh Liu,Xiang Li,Bao Ge,Dajiang Zhu,Yixuan Yuan,Dinggang Shen,Tianming Liu,Shu Zhang",1,105,0
"e62937949aa18caeebaf1263ef5d86d447acbc6e","https://www.semanticscholar.org/paper/e62937949aa18caeebaf1263ef5d86d447acbc6e",1,"Pre-trained Language Models in Biomedical Domain: A Systematic Survey","The recent progress of pre-trained language models in the biomedical domain and their applications in biomedical downstream tasks are summarized and a taxonomy of existing biomedical PLMs is proposed.","arXiv.org",2021,"Benyou Wang,Qianqian Xie,Jiahuan Pei,P. Tiwari,Zhao Li,Jie Fu",25,379,3
"10a8e7a7e07256178665f90074c5c41b071e73d3","https://www.semanticscholar.org/paper/10a8e7a7e07256178665f90074c5c41b071e73d3",1,"MDF-Net: Multimodal Dual-Fusion Network for Abnormality Detection using CXR Images and Clinical Data","This work proposes a novel architecture consisting of two fusion methods that enable the model to simultaneously process patients’ clinical data (structured data) and chest X-rays (image data).","arXiv.org",2023,"Chih-Jou Hsieh,Isabel Blanco Nobre,Sandra Costa Sousa,Chun Ouyang,Margot Brereton,J. Nascimento,Joaquim Jorge,Catarina Moreira",0,49,0
"e3c70b0b71b51872bbdaa0f4bf2b56908f97abec","https://www.semanticscholar.org/paper/e3c70b0b71b51872bbdaa0f4bf2b56908f97abec",1,"MedKLIP: Medical Knowledge Enhanced Language-Image Pre-Training","This paper adopts a novel report filter to extract the medical entities, and proposes a novel entity embedding module by querying an external knowledge description base to exploit the rich context of additional information that the medical domain affords, and implicitly build relationships between entities in the language embedding space.","medRxiv",2023,"Chaoyi Wu,Xiaoman Zhang,Ya Zhang,Yanfeng Wang,Weidi Xie",8,67,2
"a39adffc4e6de100a950f4476e113bfc402119f2","https://www.semanticscholar.org/paper/a39adffc4e6de100a950f4476e113bfc402119f2",1,"Knowledge-enhanced Visual-Language Pre-training on Chest Radiology Images","KAD is evaluated on four external X-ray datasets and it is demonstrated that its zero-shot performance is not only comparable to that of fully-supervised models, but also superior to the average of three expert radiologists for three pathologies with statistical significance.","",2023,"Xiaoman Zhang,Chaoyi Wu,Ya Zhang,Yanfeng Wang,Weidi Xie",0,73,0
"8700c5af25450bf8e84b94783344b054d268738b","https://www.semanticscholar.org/paper/8700c5af25450bf8e84b94783344b054d268738b",1,"Bi-VLGM : Bi-Level Class-Severity-Aware Vision-Language Graph Matching for Text Guided Medical Image Segmentation","A Bi-level class-severity-aware Vision-Language Graph Matching (Bi-VLGM) for text guided medical image segmentation and its superiority to existing methods is introduced.","",2023,"Chen Wenting,Liu Jie,Yuan Yixuan",0,48,0
"78b03df885cbb57361e8efc5ce5ad5eea211dda6","https://www.semanticscholar.org/paper/78b03df885cbb57361e8efc5ce5ad5eea211dda6",1,"Generalist Vision Foundation Models for Medical Imaging: A Case Study of Segment Anything Model on Zero-Shot Medical Segmentation","The study indicates the versatility of generalist vision foundation models on medical imaging, and their great potential to achieve desired performance through fine-turning and eventually address the challenges associated with accessing large and diverse medical datasets in support of clinical diagnostics.","Diagnostics",2023,"Peilun Shi,Jianing Qiu,Sai Mu Dalike Abaxi,Hao Wei,F. P. Lo,Wu Yuan",3,44,0
"80785017029cab501fcdb90b98985cd2b36e1fb8","https://www.semanticscholar.org/paper/80785017029cab501fcdb90b98985cd2b36e1fb8",1,"Evaluation of GPT-3.5 and GPT-4 for supporting real-world information needs in healthcare delivery","It is suggested that while general purpose LLMs are able to provide safe and credible responses, they often do not meet the specific information need of a given question.","arXiv.org",2023,"Debadutta Dash,Rahul Thapa,J. Banda,Akshay Swaminathan,Morgan Cheatham,M. Kashyap,N. Kotecha,Jonathan H. Chen,S. Gombar,L. Downing,Rachel A. Pedreira,Ethan Goh,A. Arnaout,Garret K. Morris,H. Magon,M. Lungren,E. Horvitz,N. Shah",4,38,1
"d0ee000f30420953f10dfcfd608a7f9ad40f1635","https://www.semanticscholar.org/paper/d0ee000f30420953f10dfcfd608a7f9ad40f1635",1,"Humans are Still Better than ChatGPT: Case of the IEEEXtreme Competition","This paper presents an instance where human performance excels in typical tasks suited for ChatGPT, specifically in the domain of computer programming, and provides evidence that contrary to popular belief, human programmers maintain a competitive edge overChatGPT in certain aspects of problem-solving within the programming context.","arXiv.org",2023,"A. Koubâa,B. Qureshi,Adel Ammar,Zahid Khan,W. Boulila,L. Ghouti",1,34,0
"e1a12117f15a6ee07133851a51439394bb9e7406","https://www.semanticscholar.org/paper/e1a12117f15a6ee07133851a51439394bb9e7406",1,"ChemCrow: Augmenting large-language models with chemistry tools","This study introduces ChemCrow, an LLM chemistry agent designed to accomplish tasks across organic synthesis, drug discovery, and materials design by integrating 13 expert-designed tools, which augments the LLM performance in chemistry, and new capabilities emerge.","",2023,"A. Bran,Sam Cox,Andrew D. White,P. Schwaller",9,78,1
"32dcd0887537cece54e214f531d2c384470b023f","https://www.semanticscholar.org/paper/32dcd0887537cece54e214f531d2c384470b023f",1,"Large Language Models as Tool Makers","A closed-loop framework, referred to as LLMs As Tool Makers (LATM), where LLMs create their own reusable tools for problem-solving, which can achieve performance that is on par with using GPT-4 for both tool making and tool using, while the inference cost is significantly reduced.","arXiv.org",2023,"Tianle Cai,Xuezhi Wang,Tengyu Ma,Xinyun Chen,Denny Zhou",2,37,0
"811115d36e1eabe2cef03b38a0809514e40b658e","https://www.semanticscholar.org/paper/811115d36e1eabe2cef03b38a0809514e40b658e",1,"Chain-Of-Thought Prompting Under Streaming Batch: A Case Study","This paper presents a case study on how to construct and optimize chain-of-thought prompting using batch data in streaming settings and concludes that all test data is visible before testing and only select a small subset to generate rationales, which is an unrealistic assumption.","",2023,"Yuxin Tang",0,16,0
"66d4631c28582b496d8e77653556893a9a6d8c3a","https://www.semanticscholar.org/paper/66d4631c28582b496d8e77653556893a9a6d8c3a",1,"MultiModal-GPT: A Vision and Language Model for Dialogue with Humans","A vision and language model named MultiModal-GPT to conduct multi-round dialogue with humans and finds the quality of training data is vital for the dialogue performance, where few data containing short answers can lead the model to respond shortly to any instructions.","arXiv.org",2023,"T. Gong,Chengqi Lyu,Shilong Zhang,Yudong Wang,Miao Zheng,Qianmengke Zhao,Kuikun Liu,Wenwei Zhang,Ping Luo,Kai Chen",6,17,0
"7bf902fb94a577d15293ac4f90d8967163850fb1","https://www.semanticscholar.org/paper/7bf902fb94a577d15293ac4f90d8967163850fb1",1,"Let's Think Frame by Frame: Evaluating Video Chain of Thought with Video Infilling and Prediction","A new research direction of VideoCOT on video keyframes is proposed, which leverages the multimodal generative abilities of vision-language models to enhance video reasoning while reducing the computational complexity of processing hundreds or thousands of frames.","arXiv.org",2023,"Vaishnavi Himakunthala,Andy Ouyang,Daniel Rose,Ryan He,Alex Mei,Yujie Lu,Chinmay Sonar,Michael Stephen Saxon,William Yang Wang",0,37,0
"5d44f16a36ba7ae6b3d9d7c98bbc1b877e598f35","https://www.semanticscholar.org/paper/5d44f16a36ba7ae6b3d9d7c98bbc1b877e598f35",1,"The False Promise of Imitating Proprietary LLMs","It is concluded that model imitation is a false promise: there exists a substantial capabilities gap between open and closed LMs that can be bridged using an unwieldy amount of imitation data or by using more capable base LMs.","arXiv.org",2023,"Arnav Gudibande,Eric Wallace,Charles Burton Snell,Xinyang Geng,Hao Liu,P. Abbeel,S. Levine,Dawn Song",4,45,1
"f6c23f3fe897b3836e29e91e7842c12f711c68fe","https://www.semanticscholar.org/paper/f6c23f3fe897b3836e29e91e7842c12f711c68fe",1,"Pre-trained transformer for adversarial purification","This work proposes a new scenario, RaPiD (Rapid Plug-in Defender), which is to rapidly defend against a certain attack for the frozen original service model with limitations of few clean and adversarial examples.","",2023,"K. Wu,Yujian Li,Xiaoyu Zhang,Handing Wang,J. Liu",0,34,0
"0244aeb7c6927e2fb0c2e668687e160a00737dbe","https://www.semanticscholar.org/paper/0244aeb7c6927e2fb0c2e668687e160a00737dbe",1,"Orca: Progressive Learning from Complex Explanation Traces of GPT-4","Orca is developed, a 13-billion parameter model that learns to imitate the reasoning process of LFMs, indicating that learning from step-by-step explanations, whether these are generated by humans or more advanced AI models, is a promising direction to improve model capabilities and skills.","",2023,"Subhabrata Mukherjee,Arindam Mitra,Ganesh Jawahar,Sahaj Agarwal,H. Palangi,A. Awadallah",0,32,0
"2d3905c1a92c28c056dff1225d89e4ca72ac4d8e","https://www.semanticscholar.org/paper/2d3905c1a92c28c056dff1225d89e4ca72ac4d8e",1,"Man vs the machine: The Struggle for Effective Text Anonymisation in the Age of Large Language Models","An experiment is conducted using GPT over anonymised texts of famous people to determine whether such trained networks can deanonymise them and introduces a novel methodology that employs Large Language Models to improve the anonymity of texts.","arXiv.org",2023,"C. Patsakis,Nikolaos Lykousas",1,38,0
"03b2dab693da1e8af9624750c0759fd1eb7b197e","https://www.semanticscholar.org/paper/03b2dab693da1e8af9624750c0759fd1eb7b197e",1,"Beyond One-Model-Fits-All: A Survey of Domain Specialization for Large Language Models","A systematic taxonomy is proposed that categorizes the LLM domain-specialization techniques based on the accessibility to LLMs and summarizes the framework for all the subcategories as well as their relations and differences to each other.","arXiv.org",2023,"Chen Ling,Xujiang Zhao,Jiaying Lu,Chengyuan Deng,Can Zheng,Junxiang Wang,Tanmoy Chowdhury,Yun-Qing Li,Hejie Cui,Tian-yu Zhao,Amit Panalkar,Wei Cheng,Haoyu Wang,Yanchi Liu,Zhengzhang Chen,Haifeng Chen,Chris White,Quanquan Gu,Carl Yang,Liang Zhao",0,286,0
"e81bfacbf3f03375860dbea28f9f12a3b0b2697a","https://www.semanticscholar.org/paper/e81bfacbf3f03375860dbea28f9f12a3b0b2697a",1,"Data Augmentation Approaches for Source Code Models: A Survey","This paper conducts a comprehensive and integrative survey of data augmentation for source code, wherein it systematically compile and encapsulate existing literature to provide a comprehensive overview of the field.","arXiv.org",2023,"Terry Yue Zhuo,Zhou Yang,Zhensu Sun,Yufei Wang,Li Li,Xiaoning Du,Zhenchang Xing,David Lo",0,118,0
"6139b6bc065b24562cb7f4f08227a42f5766138f","https://www.semanticscholar.org/paper/6139b6bc065b24562cb7f4f08227a42f5766138f",1,"Diffusion Models: A Comprehensive Survey of Methods and Applications","An overview of the rapidly expanding body of work on diffusion models is provided, categorizing the research into three key areas: efficient sampling, improved likelihood estimation, and handling data with special structures.","arXiv.org",2022,"Ling Yang,Zhilong Zhang,Shenda Hong,Runsheng Xu,Yue Zhao,Yingxia Shao,Wentao Zhang,Ming-Hsuan Yang,Bin Cui",133,364,8
"09ca5072a76796c65e5936b6fb4968afead61944","https://www.semanticscholar.org/paper/09ca5072a76796c65e5936b6fb4968afead61944",1,"Semantics-Empowered Communication: A Tutorial-cum-Survey","This work proposes to categorize the critical enabling techniques by explicit and implicit reasoning-based methods, and elaborate on how they evolve and contribute to modern content&channel semantics-empowered communications.","arXiv.org",2022,"Zhilin Lu,Rongpeng Li,Kun Lu,Xianfu Chen,E. Hossain,Zhifeng Zhao,Honggang Zhang",2,243,1
"419eb47fea3931c4098232f44ccbc216275d3f56","https://www.semanticscholar.org/paper/419eb47fea3931c4098232f44ccbc216275d3f56",1,"Decoding Visual Neural Representations by Multimodal Learning of Brain-Visual-Linguistic Features","The BraVL model can be trained under various semi-supervised scenarios to incorporate the visual and textual features obtained from the extra categories and constructed three trimodal matching datasets, leading to some interesting conclusions and cognitive insights.","IEEE Transactions on Pattern Analysis and Machine Intelligence",2022,"Changde Du,Kaicheng Fu,Jinpeng Li,Huiguang He",3,87,1
"9fe9af7cf3d54b707a7be3c53ce94b77dcc3bae5","https://www.semanticscholar.org/paper/9fe9af7cf3d54b707a7be3c53ce94b77dcc3bae5",1,"A Short Survey of Viewing Large Language Models in Legal Aspect","","arXiv.org",2023,"Zhongxiang Sun",7,25,0
"74e8ae03a385e72f5ae377667ba9858fb3e0bfa0","https://www.semanticscholar.org/paper/74e8ae03a385e72f5ae377667ba9858fb3e0bfa0",1,"Unleashing the Power of Edge-Cloud Generative AI in Mobile Networks: A Survey of AIGC Services","This survey paper focuses on the deployment of AIGC applications, e.g., ChatGPT and Dall-E, at mobile edge networks, that provide personalized and customized AigC services in real time while maintaining user privacy.","arXiv.org",2023,"Minrui Xu,Hongyang Du,D. Niyato,Jiawen Kang,Zehui Xiong,Shiwen Mao,Zhu Han,A. Jamalipour,Dong In Kim,X. Shen,Victor C. M. Leung,H. V. Poor",6,232,2
"1d29334cfbe9a1a943082058876f0c22d44c62fd","https://www.semanticscholar.org/paper/1d29334cfbe9a1a943082058876f0c22d44c62fd",1,"A Survey of Large Language Models","A review of the recent advances of large language models by introducing the background, key findings, and mainstream techniques, and focusing on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation.","arXiv.org",2023,"Wayne Xin Zhao,Kun Zhou,Junyi Li,Tianyi Tang,Xiaolei Wang,Yupeng Hou,Yingqian Min,Beichen Zhang,Junjie Zhang,Zican Dong,Yifan Du,Chen Yang,Yushuo Chen,Z. Chen,Jinhao Jiang,Ruiyang Ren,Yifan Li,Xinyu Tang,Zikang Liu,Peiyu Liu,J. Nie,Ji-rong Wen",87,415,8
"26ccfbd8bfad44aeed695c12579ff7126adbfae9","https://www.semanticscholar.org/paper/26ccfbd8bfad44aeed695c12579ff7126adbfae9",1,"Summary of ChatGPT/GPT-4 Research and Perspective Towards the Future of Large Language Models","A comprehensive survey of ChatGPT and GPT-4, state-of-the-art large language models from the GPT series, and their prospective applications across diverse domains, encompassing trend analysis, word cloud representation, and distribution analysis across various application domains is presented.","arXiv.org",2023,"Yi-Hsien Liu,Tianle Han,Siyuan Ma,Jia-Yu Zhang,Yuanyu Yang,Jiaming Tian,Haoyang He,Antong Li,Mengshen He,Zheng Liu,Zihao Wu,Dajiang Zhu,Xiang Li,Ning Qiang,Dingang Shen,Tianming Liu,Bao Ge",26,106,3
"9fd980237e7fdfa4c103a2dc08657e73adf847c4","https://www.semanticscholar.org/paper/9fd980237e7fdfa4c103a2dc08657e73adf847c4",1,"OpenAGI: When LLM Meets Domain Experts","OpenAGI is developed, an open-source AGI research platform, specifically designed to offer complex, multi-step tasks and accompanied by task-specific datasets, evaluation metrics, and a diverse range of extensible models.","arXiv.org",2023,"Yingqiang Ge,Wenyue Hua,Jianchao Ji,Juntao Tan,Shuyuan Xu,Yongfeng Zhang",6,42,0
"6316cbb4f1e7dba5806a3310ec7f89f3571bc3db","https://www.semanticscholar.org/paper/6316cbb4f1e7dba5806a3310ec7f89f3571bc3db",1,"Boosting Cross-task Transferability of Adversarial Patches with Visual Relations","A novel Visual Relation-based cross-task Adversarial Patch generation method called VRAP is proposed, which aims to evaluate the robustness of various visual tasks, especially those involving visual reasoning, such as Visual Question Answering and Image Captioning.","arXiv.org",2023,"Tony Ma,Songze Li,Yisong Xiao,Shunchang Liu",0,22,0
"ba2f935d2578fbf77ec1aa79e26e3db396771e38","https://www.semanticscholar.org/paper/ba2f935d2578fbf77ec1aa79e26e3db396771e38",1,"Self-collaboration Code Generation via ChatGPT","It is showcased that self-collaboration could potentially enable LLMs to efficiently handle complex real-world tasks that are not readily solved by direct code generation, as evidenced in case study.","arXiv.org",2023,"Yihong Dong,Xue Jiang,Zhi Jin,Ge Li",4,47,0
"be2b0396de9431bae931642516a1d3e4906329f5","https://www.semanticscholar.org/paper/be2b0396de9431bae931642516a1d3e4906329f5",1,"Low-code LLM: Visual Programming over LLMs","A novel human-LLM interaction framework that incorporates six types of simple low-code visual programming interactions, all supported by clicking, dragging, or text editing, to achieve more controllable and stable responses.","arXiv.org",2023,"Yuzhe Cai,Shaoguang Mao,Wenshan Wu,Zehua Wang,Yaobo Liang,Tao Ge,Chenfei Wu,Wang You,Ting Song,Yan Xia,Jonathan Tien,Nan Duan",3,27,0
"e36e5df3dde73c4e3606cdd4498cfc304a29bf5c","https://www.semanticscholar.org/paper/e36e5df3dde73c4e3606cdd4498cfc304a29bf5c",1,"Learning to Program with Natural Language","This work proposes using natural language as a new programming language to describe task procedures, making them easily understandable to both humans and LLMs and shows that the learned program can be directly used to guide another LLM to improve its performance, which reveals a new transfer learning paradigm.","arXiv.org",2023,"Yiduo Guo,Yaobo Liang,Chenfei Wu,Wenshan Wu,Dongyan Zhao,Nan Duan",2,37,0
"4c8ef2db0c77aba453783f5211ebafc6695d3835","https://www.semanticscholar.org/paper/4c8ef2db0c77aba453783f5211ebafc6695d3835",1,"ChatABL: Abductive Learning via Natural Language Interaction with ChatGPT","This paper presents a novel method (ChatABL) for integrating LLMs into the ABL framework, aiming at unifying the three abilities in a more user-friendly and understandable manner, and is the first attempt to explore a new pattern for further approaching human-level cognitive ability via natural language interaction with ChatGPT.","arXiv.org",2023,"Tianyang Zhong,Yaonai Wei,Li Yang,Zihao Wu,Zheng Liu,Xiaozheng Wei,WenJu Sun,Junjie Yao,Chongfei Ma,Xiang Li,Dajiang Zhu,Xi Jiang,Jun-Feng Han,Dinggang Shen,Tianming Liu,Tuo Zhang",1,69,0
"7a5c31341e7ec22409e175542368eb76e08900aa","https://www.semanticscholar.org/paper/7a5c31341e7ec22409e175542368eb76e08900aa",1,"The Potential of Visual ChatGPT For Remote Sensing","","arXiv.org",2023,"L. Osco,Eduardo Lopes de Lemos,W. Gonçalves,A. P. Ramos,J. M. Junior",0,36,0
"8bc617c9139648d7a92991d70c671230bac7b2e2","https://www.semanticscholar.org/paper/8bc617c9139648d7a92991d70c671230bac7b2e2",1,"AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head","A multi-modal AI system named AudioGPT is proposed, which complements LLMs with 1) foundation models to process complex audio information and solve numerous understanding and generation tasks; and 2) the input/output interface (ASR, TTS) to support spoken dialogue.","arXiv.org",2023,"Rongjie Huang,Mingze Li,Dongchao Yang,Jiatong Shi,Xuankai Chang,Zhenhui Ye,Yuning Wu,Zhiqing Hong,Jia-Bin Huang,Jinglin Liu,Yixiang Ren,Zhou Zhao,Shinji Watanabe",12,46,1
"37ba1833e844f5178f91f50d82bfff616551e6ad","https://www.semanticscholar.org/paper/37ba1833e844f5178f91f50d82bfff616551e6ad",1,"The Role of Summarization in Generative Agents: A Preliminary Perspective","","arXiv.org",2023,"Xiachong Feng,Xiaocheng Feng,Bing Qin",0,11,0
"c77d908ba29567445a9a4ad1bd4461d441cce174","https://www.semanticscholar.org/paper/c77d908ba29567445a9a4ad1bd4461d441cce174",1,"AutoML-GPT: Automatic Machine Learning with GPT","The AutoML-GPT is presented, which employs GPT as the bridge to diverse AI models and dynamically trains models with optimized hyperparameters and achieves remarkable results in computer vision, natural language processing, and other challenging areas.","arXiv.org",2023,"Shujian Zhang,Chengyue Gong,Lemeng Wu,Xingchao Liu,Mi Zhou",0,33,0
"d473847dff63e3f5d238251cb23597f8205f72f2","https://www.semanticscholar.org/paper/d473847dff63e3f5d238251cb23597f8205f72f2",1,"Generating Virtual On-body Accelerometer Data from Virtual Textual Descriptions for Human Activity Recognition","This work introduces an automated pipeline that first uses ChatGPT to generate diverse textual descriptions of activities that are later converted to streams of virtual IMU data and demonstrates how HAR models can be improved through the generation of virtual training data that do not require any manual effort.","arXiv.org",2023,"Zi-Jian Leng,HyeokHyen Kwon,T. Plotz",0,33,0
"e0dc8e113dbdd2896fb6420ac93e0b976c47f2a2","https://www.semanticscholar.org/paper/e0dc8e113dbdd2896fb6420ac93e0b976c47f2a2",1,"Augmented Large Language Models with Parametric Knowledge Guiding","The novel Parametric Knowledge Guiding (PKG) framework is proposed, which equips LLMs with a knowledge-guiding module to access relevant knowledge without altering the LLMs' parameters, and is based on open-source language models, allowing offline memory of any knowledge that LLMs require.","arXiv.org",2023,"Ziyang Luo,Can Xu,Pu Zhao,Xiubo Geng,Chongyang Tao,Jing Ma,Qingwei Lin,Daxin Jiang",0,57,0
"7a6dc7071891cb3d658c93418801942a4c6ed373","https://www.semanticscholar.org/paper/7a6dc7071891cb3d658c93418801942a4c6ed373",1,"Autonomous GIS: the next-generation AI-powered GIS","Although still in its infancy and lacking several important modules such as logging and code testing, LLM-Geo demonstrates a potential path toward the next-generation AI-powered GIS, and advocates for the GIScience community to dedicate more effort to the research and development of autonomous GIS.","arXiv.org",2023,"Zhenlong Li,H. Ning",1,64,0
"8dbb29f93292d8b1b861c322d232fe087b2ef7b1","https://www.semanticscholar.org/paper/8dbb29f93292d8b1b861c322d232fe087b2ef7b1",1,"Small Models are Valuable Plug-ins for Large Language Models","The experiments demonstrate that SuperICL can improve performance beyond state-of-the-art fine-tuned models while addressing the instability problem of in-context learning, and can enhance the capabilities of smaller models, such as multilinguality and interpretability.","arXiv.org",2023,"Canwen Xu,Yichong Xu,Shuo Wang,Yang Liu,Chenguang Zhu,Julian McAuley",5,35,1
"7787efaf502421eac9b6b0fd946a82e1ecf4c8c9","https://www.semanticscholar.org/paper/7787efaf502421eac9b6b0fd946a82e1ecf4c8c9",1,"Generating coherent comic with rich story using ChatGPT and Stable Diffusion","A novel way to evaluate AI-generated stories is introduced, and SOTA performance on character fidelity and art style is achieved by fine-tuning stable diffusion using LoRA, ControlNet, etc.","arXiv.org",2023,"Ze Jin,Zorina Song",0,17,0
"38aa4d2f58fbb951f885df1582195c262d976322","https://www.semanticscholar.org/paper/38aa4d2f58fbb951f885df1582195c262d976322",1,"Chain-of-Symbol Prompting Elicits Planning in Large Langauge Models","A novel method called CoS (Chain-of-Symbol Prompting) that represents the complex environments with condensed symbolic spatial representations during the chained intermediate thinking steps and is easy to use and does not need additional training on LLMs.","arXiv.org",2023,"Hanxu Hu,Hongyuan Lu,Huajian Zhang,Wai Lam,Yue Zhang",1,27,0
"49bb6f42879031e2374235715c942006af5c88d9","https://www.semanticscholar.org/paper/49bb6f42879031e2374235715c942006af5c88d9",1,"Empower Large Language Model to Perform Better on Industrial Domain-Specific Question Answering","This paper provides a benchmark Question Answering (QA) dataset named MSQA, which is about Microsoft products and IT technical problems encountered by customers, and proposes a new model interaction paradigm that can empower LLM to achieve better performance on domain-specific tasks where it is not proficient.","arXiv.org",2023,"Zezhong Wang,Fan Yang,Pu Zhao,Lu Wang,Jue Zhang,Mohit Garg,Qingwei Lin,Dongmei Zhang",0,41,0
"fbd4a876cee20eaf98f344aca597a55338f663f5","https://www.semanticscholar.org/paper/fbd4a876cee20eaf98f344aca597a55338f663f5",1,"Examining the Inter-Consistency of Large Language Models: An In-depth Analysis via Debate","Through extensive experiments on the commonsense reasoning task, LLMs not only become more inter-consistent but also achieve higher performance and the importance of a competent judge, such as GPT-4, is highlighted.","arXiv.org",2023,"Kai Xiong,Xiao Ding,Yixin Cao,Ting Liu,Bing Qin",0,40,0
"eb291a2e237774b162d9c51c21c4868795589e94","https://www.semanticscholar.org/paper/eb291a2e237774b162d9c51c21c4868795589e94",1,"Diving into the Inter-Consistency of Large Language Models: An Insightful Analysis through Debate","A formal debate framework is designed to delve into the inter-consistency problem among LLMs with three-stage debate: fair debate, mismatched debate, and roundtable debate and shows how a much stronger LLM would be dominant in mismatched debates, while it will be easily misled by relatively weaker LLMs in a more complex debate scenario such as round table debate.","",2023,"Kai Xiong,Xiao Ding,Yixin Cao,Ting Liu,Bing Qin",1,42,0
"205d2ed0906440f07a0275d7d6a63bced60951fc","https://www.semanticscholar.org/paper/205d2ed0906440f07a0275d7d6a63bced60951fc",1,"InstructVid2Vid: Controllable Video Editing with Natural Language Instructions","Experiments demonstrate that InstructVid2Vid is able to generate high-quality, temporally coherent videos and perform diverse edits, including attribute editing, change of background, and style transfer, which highlight the versatility and effectiveness of the proposed method.","arXiv.org",2023,"Bosheng Qin,Juncheng Li,Siliang Tang,Tat-Seng Chua,Yueting Zhuang",0,61,0
"a4ce9552b6427beaae2bcadfe5f460024858dd25","https://www.semanticscholar.org/paper/a4ce9552b6427beaae2bcadfe5f460024858dd25",1,"ChipGPT: How far are we from natural language hardware design","This work attempts to demonstrate an automated design environment that explores LLMs to generate hardware logic designs from natural language specifications without retraining or finetuning, and shows broader design optimization space compared to prior work and native LLMs alone.","arXiv.org",2023,"Kaiyan Chang,Y. Wang,Haimeng Ren,Mengdi Wang,Shengwen Liang,Yinhe Han,Huawei Li,Xiaowei Li",0,27,0
"abab79d1135684d039cfbebd0097e48ef4c1940c","https://www.semanticscholar.org/paper/abab79d1135684d039cfbebd0097e48ef4c1940c",1,"Vision + Language Applications: A Survey","A relevant research track within multimodal applications, including text, vision, audio, and others, is explored, includingText-to-image generation within multi-modal applications.","arXiv.org",2023,"Yutong Zhou,N. Shimada",0,220,0
"7c4f6fd4c7eadcc7189a6797db215895340f93c7","https://www.semanticscholar.org/paper/7c4f6fd4c7eadcc7189a6797db215895340f93c7",1,"ChatFace: Chat-Guided Real Face Editing via Diffusion Latent Space Manipulation","A novel approach that conduct text-driven image editing in the semantic latent space of diffusion model by aligning the temporal feature of the diffusion model with the semantic condition at generative process is proposed and an interactive system named ChatFace is developed, which combines the zero-shot reasoning ability of large language models to perform efficient manipulations in diffusion semantic latentspace.","arXiv.org",2023,"Dongxu Yue,Qin Guo,Munan Ning,Jiaxi Cui,Yuesheng Zhu,Liuliang Yuan",0,54,0
"ef8c21e1f574495f0c80b8c1037dbdb886f0808d","https://www.semanticscholar.org/paper/ef8c21e1f574495f0c80b8c1037dbdb886f0808d",1,"Towards Language-guided Interactive 3D Generation: LLMs as Layout Interpreter with Generative Feedback","A novel language-guided interactive 3D generation system that integrates LLMs as a 3D layout interpreter into the off-the-shelf layout-to-3D generative models, allowing users to flexibly and interactively generate visual content.","arXiv.org",2023,"Yiqi Lin,Hao Wu,Ruichen Wang,H. Lu,Xiaodong Lin,Hui Xiong,Lin Wang",0,39,0
"f0888b9c0ef63e68c7758e6aec2370961c0eede9","https://www.semanticscholar.org/paper/f0888b9c0ef63e68c7758e6aec2370961c0eede9",1,"On the Tool Manipulation Capability of Open-source Large Language Models","It is demonstrated that classical methods in LLM literature can adapt as model alignment with programmatic data generation, system prompts and in-context demonstration retrievers to enhance open-source LLMs for tool manipulation, and it is shown that such enhancement typically requires about one developer day to curate data for each tool, rendering a recipe with practical amount of human supervision.","arXiv.org",2023,"Qiantong Xu,Fenglu Hong,B. Li,Changran Hu,Zhe Chen,Jian Zhang",1,66,0
"7e72eb196b7c90b3a5d6385af536fe8e5934fb82","https://www.semanticscholar.org/paper/7e72eb196b7c90b3a5d6385af536fe8e5934fb82",1,"ConvGenVisMo: Evaluation of Conversational Generative Vision Models","This paper presents ConvGenVisMo, a framework for the novel task of evaluating CGVMs, which introduces a new benchmark evaluation dataset for this task, and also provides a suite of existing and new automated evaluation metrics to evaluate the outputs.","arXiv.org",2023,"Narjes Nikzad Khasmakhi,M. Asgari-Chenaghlu,Nabiha Asghar,Philipp Schaer,Dietlind Zuhlke",0,15,0
"702e3b669912cbe9df70c7fb731842bb9801f25f","https://www.semanticscholar.org/paper/702e3b669912cbe9df70c7fb731842bb9801f25f",1,"Diffusion Model is an Effective Planner and Data Synthesizer for Multi-Task Reinforcement Learning","This paper proposes Multi-Task Diffusion Model, a diffusion-based method that incorporates Transformer backbones and prompt learning for generative planning and data synthesis in multi-task offline settings and finds it outperforms state-of-the-art algorithms across 50 tasks on Meta-World and 8 maps on Maze2D.","arXiv.org",2023,"Haoran He,Chenjia Bai,Kang Xu,Zhuoran Yang,Weinan Zhang,Dong Wang,Bingyan Zhao,Xuelong Li",0,76,0
"c85c90ef9e9a71efe031c3f7d6e34561f91168fe","https://www.semanticscholar.org/paper/c85c90ef9e9a71efe031c3f7d6e34561f91168fe",1,"Deliberate then Generate: Enhanced Prompting Framework for Text Generation","This paper proposes a novel Deliberate then Generate (DTG) prompting framework, which consists of error detection instructions and candidates that may contain errors, and shows that DTG consistently outperforms existing prompting methods and achieves state-of-the-art performance on multiple text generation tasks.","arXiv.org",2023,"Bei Li,Rui Wang,Junliang Guo,Kaitao Song,Xuejiao Tan,Hany Hassan,Arul Menezes,Tong Xiao,Jiang Bian,Jingbo Zhu",0,48,0
"486a8c8655b81c7f87ff257141466ec1186d4aea","https://www.semanticscholar.org/paper/486a8c8655b81c7f87ff257141466ec1186d4aea",1,"Prompt Sapper: LLM-Empowered Software Engineering Infrastructure for AI-Native Services","The R\&D motivation behind Prompt Sapper is introduced, along with its corresponding AI chain engineering methodology and technical practices, which create a large language model (LLM) empowered software engineering infrastructure for authoring AI chains through human-AI collaborative intelligence.","",2023,"Zhenchang Xing,Qing Huang,Yu Cheng,Liming Zhu,Qinghua Lu,Xiwei Xu",0,33,0
"0f19e94f30b99d6c4b349900057cdae9262034f9","https://www.semanticscholar.org/paper/0f19e94f30b99d6c4b349900057cdae9262034f9",1,"The Contribution of Knowledge in Visiolinguistic Learning: A Survey on Tasks and Challenges","This survey analyzes tasks that have benefited from hybrid approaches to visiolinguistic learning, and categorizes existing knowledge sources and types, proceeding to discussion regarding the KG vs LLM dilemma and its potential impact to future hybrid approaches.","arXiv.org",2023,"Maria Lymperaiou,G. Stamou",0,140,0
"09840a5c151f858ed0eaf1db2a4d3741516f693b","https://www.semanticscholar.org/paper/09840a5c151f858ed0eaf1db2a4d3741516f693b",1,"ICL-D3IE: In-Context Learning with Diverse Demonstrations Updating for Document Information Extraction","A simple but effective in-context learning framework called ICL-D3IE, which enables LLMs to perform DIE with different types of demonstration examples and enables GPT-3/ChatGPT to achieve superior performance when compared to previous pre-trained methods fine-tuned with full training in both the in-dist distribution (ID) setting and in the out-of-distribution (OOD) setting.","arXiv.org",2023,"Jiabang He,Lei Wang,Yingpeng Hu,Ning Liu,Hui-juan Liu,Xingdong Xu,Hengtao Shen",7,56,0
"5dea6facab090a070be1444920230689e7189599","https://www.semanticscholar.org/paper/5dea6facab090a070be1444920230689e7189599",1,"SurgicalGPT: End-to-End Language-Vision GPT for Visual Question Answering in Surgery","An end-to-end trainable Language-Vision GPT model that expands the GPT2 model to include vision input (image) and extensively study and present the effects of token sequencing, token type and pose embedding for vision tokens in the LV-GPT model.","arXiv.org",2023,"L. Seenivasan,Mobarakol Islam,Gokul Kannan,Hongliang Ren",1,25,0
"d00ca5c49415d3a45bfcf3fabaf0a60a1c52a6ff","https://www.semanticscholar.org/paper/d00ca5c49415d3a45bfcf3fabaf0a60a1c52a6ff",1,"PromptCap: Prompt-Guided Task-Aware Image Captioning","PromptCap (Prompt-guided image Captioning), a captioning model designed to serve as a better connector between images and black-box LMs, achieves state-of-the-art accuracy on knowledge-based VQA tasks and generalizes well to unseen domains.","arXiv.org",2022,"Yushi Hu,Hang Hua,Zhengyuan Yang,Weijia Shi,Noah A. Smith,Jiebo Luo",16,100,0
"1367dcff4ccb927a5e95c452041288b3f0dd0eff","https://www.semanticscholar.org/paper/1367dcff4ccb927a5e95c452041288b3f0dd0eff",1,"Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation","A new T2V generation setting, where only one text-video pair is presented, and Tune-A-Video, which involves a tailored spatio-temporal attention mechanism and an efficient one-shot tuning strategy, is introduced.","arXiv.org",2022,"Jay Zhangjie Wu,Yixiao Ge,Xintao Wang,Weixian Lei,Yuchao Gu,W. Hsu,Ying Shan,Xiaohu Qie,Mike Zheng Shou",50,67,14
"00c1ff63468305ea3fa430c2b3aef156d580c4ff","https://www.semanticscholar.org/paper/00c1ff63468305ea3fa430c2b3aef156d580c4ff",1,"P ROMPT C AP : Prompt-Guided Image Captioning for VQA with GPT-3","P ROMPT C AP is a captioning model designed to serve as a better connector between images and black-box LMs that outperforms generic captions by a large margin and achieves state-of-the-art accuracy on knowledge-based VQA tasks.","",2023,"Yushi Hu,Hang Hua,Zhengyuan Yang,Weijia Shi,Noah A. Smith,Jiebo Luo",1,72,0
"3062bb79d12ff55c29c8731211a84e8cf344e235","https://www.semanticscholar.org/paper/3062bb79d12ff55c29c8731211a84e8cf344e235",1,"Vision Learners Meet Web Image-Text Pairs","A new visual representation pre-training method, MUlti-modal Generator~(MUG), that learns from scalable web sourced image-text data that achieves state-of-the-art transfer performance on a variety of tasks and demonstrates promising scaling properties.","arXiv.org",2023,"Bingchen Zhao,Quan Cui,Hao Wu,O. Yoshie,Cheng Yang",0,88,0
"a082b61a7d9d6c890861661be919fd9190893b38","https://www.semanticscholar.org/paper/a082b61a7d9d6c890861661be919fd9190893b38",1,"Bias-to-Text: Debiasing Unknown Visual Biases through Language Interpretation","The bias-to-text (B2T) framework is introduced, which uses language interpretation to identify and mitigate biases in vision models, such as image classifiers and text- to-image generative models, and its effectiveness on various image classification and generation tasks is demonstrated.","",2023,"Younghyun Kim,Sangwoo Mo,Min-Kyung Kim,Kyungmin Lee,Jaeho Lee,Jinwoo Shin",0,102,0
"fccada3fc530ea98d612126399f13ecb0844fc21","https://www.semanticscholar.org/paper/fccada3fc530ea98d612126399f13ecb0844fc21",1,"Contrast with Reconstruct: Contrastive 3D Representation Learning Guided by Generative Pretraining","ReCon is trained to learn from both generative modeling teachers and single/cross-modal contrastive teachers through ensemble distillation, where the generative student guides the contrastive student.","arXiv.org",2023,"Zekun Qi,Runpei Dong,Guo Fan,Zheng Ge,Xiangyu Zhang,Kaisheng Ma,Li Yi",6,103,1
"a3ff4df653b6970898c04e6b768e58b99786d073","https://www.semanticscholar.org/paper/a3ff4df653b6970898c04e6b768e58b99786d073",1,"Learning gain differences between ChatGPT and human tutor generated algebra hints","This paper conducts the first learning gain evaluation of ChatGPT by comparing the efficacy of its hints with hints authored by human tutors with 77 participants across two algebra topic areas, Elementary Algebra and Intermediate Algebra.","arXiv.org",2023,"Z. Pardos,Shreya Bhandari",11,30,2
"c8f98f28f1f28a9f5db7c4b4d9a6b7853a100214","https://www.semanticscholar.org/paper/c8f98f28f1f28a9f5db7c4b4d9a6b7853a100214",1,"Can Pre-trained Vision and Language Models Answer Visual Information-Seeking Questions?","The analysis shows that it is challenging for the state-of-the-art multi-modal pre-trained models to answer visual information seeking questions, but this capability is improved through fine-tuning on the automated InfoSeek dataset.","arXiv.org",2023,"Yang Chen,Hexiang Hu,Yi Luan,Haitian Sun,Soravit Changpinyo,Alan Ritter,Ming-Wei Chang",2,58,0
"467b839cb8a2475477ca004df94b797d967ad057","https://www.semanticscholar.org/paper/467b839cb8a2475477ca004df94b797d967ad057",1,"Human-Art: A Versatile Human-Centric Dataset Bridging Natural and Artificial Scenes","The Human-Art dataset is introduced and contains 50k high-quality images with over 123k person instances from 5 natural and 15 artificial scenarios, which are annotated with bounding boxes, keypoints, self-contact points, and text information for humans represented in both 2D and 3D.","arXiv.org",2023,"Xu Ju,Ailing Zeng,Jianan Wang,Qian Xu,Lei Zhang",2,88,0
"a7b3a868a80dbe97689135c99b1a6b6e10dcdfe5","https://www.semanticscholar.org/paper/a7b3a868a80dbe97689135c99b1a6b6e10dcdfe5",1,"A Comprehensive Survey of AI-Generated Content (AIGC): A History of Generative AI from GAN to ChatGPT","This survey provides a comprehensive review on the history of generative models, and basic components, recent advances in AIGC from unimmodal interaction and multimodal interaction, and introduces the generation tasks and relative models of text and image.","arXiv.org",2023,"Yihan Cao,Siyu Li,Yixin Liu,Zhiling Yan,Yutong Dai,Philip S. Yu,Lichao Sun",25,280,2
"6a4ef6c4799dc871a4253c0536126d397ca3ec1e","https://www.semanticscholar.org/paper/6a4ef6c4799dc871a4253c0536126d397ca3ec1e",1,"Interpretable Visual Question Answering Referring to Outside Knowledge","A novel multimodal interpretable VQA model that can answer the question more accurately and generate diverse explanations and can outperform state-of-the-art methods regarding answer accuracy and explanation rationality.","arXiv.org",2023,"He Zhu,Ren Togo,Takahiro Ogawa,M. Haseyama",0,29,0
"e5a7be5b9e6c368a1839455bfbb51bc07ed161f1","https://www.semanticscholar.org/paper/e5a7be5b9e6c368a1839455bfbb51bc07ed161f1",1,"ODIN: On-demand Data Formulation to Mitigate Dataset Lock-in","This work evaluated ODIN on various datasets in terms of model accuracy and data diversity to demonstrate its potential, and conducted post-experiments for further investigation.","arXiv.org",2023,"SP Choi,Jihun Lee,HyeongSeok Ahn,S. Jung,Bumsoo Kang",0,38,0
"aa75ec0ee1aa18d3b0603d5a425e92eabcb7ac02","https://www.semanticscholar.org/paper/aa75ec0ee1aa18d3b0603d5a425e92eabcb7ac02",1,"Breaking Common Sense: WHOOPS! A Vision-and-Language Benchmark of Synthetic and Compositional Images","This work introduces WHOOPS!, a new dataset and benchmark for visual commonsense, comprised of purposefully commonsense-defying images created by designers using publicly-available image generation tools like Midjourney and introduces a difficult explanation generation task, where models must identify and explain why a given image is unusual.","arXiv.org",2023,"Nitzan Bitton-Guetta,Yonatan Bitton,Jack Hessel,Ludwig Schmidt,Y. Elovici,G. Stanovsky,Roy Schwartz",4,44,1
"id","url","score","title","summary","venue","year","authors","citationCount","referenceCount","influentialCitationCount"
"584ca135b61482fd89247113da87d784f738dbfa","https://www.semanticscholar.org/paper/584ca135b61482fd89247113da87d784f738dbfa",6,"Foundational Models Defining a New Era in Vision: A Survey and Outlook","A comprehensive review of emerging foundational models in computer vision, including typical architecture designs to combine different modalities, training objectives, pre-training datasets, fine-tuning mechanisms, and the common prompting patterns; textual, visual, and heterogeneous.","arXiv.org",2023,"Muhammad Awais,Muzammal Naseer,Salman Siddique Khan,R. Anwer,Hisham Cholakkal,M. Shah,Ming Yang,F. Khan",10,365,3
"ebedc4d7a2356090904baba4104ef0832bc236df","https://www.semanticscholar.org/paper/ebedc4d7a2356090904baba4104ef0832bc236df",6,"A Survey on Multimodal Large Language Models","This paper presents the formulation of MLLM and delineate its related concepts, and discusses the key techniques and applications, including Multimodal Instruction Tuning (M-IT), Multimmodal In-Context Learning (M -ICL), MultIModal Chain of Thought (m-CoT), and LLM-Aided Visual Reasoning (LAVR).","arXiv.org",2023,"Shukang Yin,Chaoyou Fu,Sirui Zhao,Ke Li,Xing Sun,Tong Xu,Enhong Chen",53,108,1
"c7492913370b5726eaa6ced163a60de6c9d4bb7f","https://www.semanticscholar.org/paper/c7492913370b5726eaa6ced163a60de6c9d4bb7f",6,"A Survey of Large Language Models for Healthcare: from Data, Technology, and Applications to Accountability and Ethics","It is contended that a significant paradigm shift is underway, transitioning from PLMs to LLMs, which encompasses a move from discriminative AI approaches to generativeAI approaches, as well as a shift from model-centered methodologies to datacentered methodologies.","arXiv.org",2023,"Kai He,Rui Mao,Qika Lin,Yucheng Ruan,Xiang Lan,Mengling Feng,Erik Cambria",3,377,1
"8d2709ed1788a67e64425fb410bb49f3ee49e088","https://www.semanticscholar.org/paper/8d2709ed1788a67e64425fb410bb49f3ee49e088",6,"Large Language Models Illuminate a Progressive Pathway to Artificial Healthcare Assistant: A Review","This review offers an extensive analysis on the transformative potential of LLMs in modern medicine and highlights the pivotal need for continuous optimizations and ethical oversight before these models can be effectively integrated into clinical practice.","arXiv.org",2023,"Mingze Yuan,Peng Bao,J. Yuan,Yunhao Shen,Zi Chen,Yi Xie,Jie Zhao,Yang Chen,Li Zhang,Lin Shen,Bin Dong",0,186,0
"570079bbdd8758dfe865097e05719313c9c1301a","https://www.semanticscholar.org/paper/570079bbdd8758dfe865097e05719313c9c1301a",5,"LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model","This work augments LLaMA-Adapter by unlocking more learnable parameters and proposes an early fusion strategy to feed visual tokens only into the early LLM layers, contributing to better visual knowledge incorporation and achieves strong multi-modal reasoning with only a small-scale image-text and instruction dataset.","arXiv.org",2023,"Peng Gao,Jiaming Han,Renrui Zhang,Ziyi Lin,Shijie Geng,Aojun Zhou,W. Zhang,Pan Lu,Conghui He,Xiangyu Yue,Hongsheng Li,Y. Qiao",116,79,14
"d6d3604f369bb0415cbe814e43ca3131323b03e2","https://www.semanticscholar.org/paper/d6d3604f369bb0415cbe814e43ca3131323b03e2",5,"Otter: A Multi-Modal Model with In-Context Instruction Tuning","Otter, a multi-modal model based on OpenFlamingo (open-sourced version of DeepMind's Flamingo), trained on MIMIC-IT and showcasing improved instruction-following ability and in-context learning is introduced.","arXiv.org",2023,"Bo Li,Yuanhan Zhang,Liangyu Chen,Jinghao Wang,Jingkang Yang,Ziwei Liu",111,38,18
"86cbd30d1096b0c7e4ac6b03d97a8df12fd21457","https://www.semanticscholar.org/paper/86cbd30d1096b0c7e4ac6b03d97a8df12fd21457",5,"PathAsst: Redefining Pathology through Generative Foundation AI Assistant for Pathology","The PathAsst is presented, which is a generative foundation AI assistant to revolutionize diagnostic and predictive analytics in pathology, trained based on Vicuna-13B language model in coordination with the CLIP vision encoder.","arXiv.org",2023,"Yuxuan Sun,Chenglu Zhu,S. Zheng,Kai Zhang,Zhongyi Shui,Xiaoxuan Yu,Yi-Lei Zhao,Honglin Li,Yunlong Zhang,Ruojia Zhao,Xinheng Lyu,Lin Yang",7,47,2
"7cf64070fd3d7e53d80f260c10e6bd7018d580e1","https://www.semanticscholar.org/paper/7cf64070fd3d7e53d80f260c10e6bd7018d580e1",5,"IdealGPT: Iteratively Decomposing Vision and Language Reasoning via Large Language Models","The IdealGPT framework is presented, a framework that iteratively decomposes VL reasoning using large language models (LLMs) and outperforms the best existing GPT-4-like models by an absolute 10% on VCR and 15% on SNLI-VE.","arXiv.org",2023,"Haoxuan You,Rui Sun,Zhecan Wang,Long Chen,Gengyu Wang,Hammad A. Ayyubi,Kai-Wei Chang,Shih-Fu Chang",10,46,2
"8ecdbfe011b7189fa0ee49ffc4e42a93d728a371","https://www.semanticscholar.org/paper/8ecdbfe011b7189fa0ee49ffc4e42a93d728a371",5,"On Evaluating Adversarial Robustness of Large Vision-Language Models","Evaluating the robustness of open-source large VLMs in the most realistic and high-risk setting, where adversaries have only black-box system access and seek to deceive the model into returning the targeted responses is proposed.","arXiv.org",2023,"Yunqing Zhao,Tianyu Pang,Chao Du,Xiao Yang,Chongxuan Li,Ngai-Man Cheung,Min Lin",19,108,1
"fd755dc7b5b206c17fd953db04e1c888d45b6e4e","https://www.semanticscholar.org/paper/fd755dc7b5b206c17fd953db04e1c888d45b6e4e",5,"LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark","This work extends the research of MLLMs to point clouds and presents the LAMM-Dataset and LAMm-Benchmark for 2D image and 3D point cloud understanding and establishes an extensible framework to facilitate the extension of M LLMs to additional modalities.","arXiv.org",2023,"Zhen-fei Yin,Jiong Wang,Jianjian Cao,Zhelun Shi,Dingning Liu,Mukai Li,Lu Sheng,Lei Bai,Xiaoshui Huang,Zhiyong Wang,Wanli Ouyang,Jing Shao",21,99,6
"051549d8ef56937b2f4d113afdcf8c7586d3770b","https://www.semanticscholar.org/paper/051549d8ef56937b2f4d113afdcf8c7586d3770b",5,"Towards AGI in Computer Vision: Lessons Learned from GPT and Large Language Models","It is pointed out that the essential weakness of CV lies in lacking a paradigm to learn from environments, yet NLP has accomplished the task in the text world and is still far from a system like GPT that naturally integrates all tasks.","arXiv.org",2023,"Lingxi Xie,Longhui Wei,Xiaopeng Zhang,Kaifeng Bi,Xiaotao Gu,Jianlong Chang,Qi Tian",2,169,0
"966852963a88a28786b798c91b6662d6e501e590","https://www.semanticscholar.org/paper/966852963a88a28786b798c91b6662d6e501e590",5,"AssistGPT: A General Multi-modal Assistant that can Plan, Execute, Inspect, and Learn","A multi-modal AI assistant with an interleaved code and language reasoning approach called Plan, Execute, Inspect, and Learn (PEIL) to integrate LLMs with various tools, and a Learner is designed to enable the model to autonomously explore and discover the optimal solution.","arXiv.org",2023,"Difei Gao,Lei Ji,Luowei Zhou,Kevin Lin,Joya Chen,Zihan Fan,Mike Zheng Shou",11,73,0
"ca31b8584b6c022ef15ddfe994fe361e002b7729","https://www.semanticscholar.org/paper/ca31b8584b6c022ef15ddfe994fe361e002b7729",5,"A Comprehensive Overview of Large Language Models","This review article is intended to provide a systematic survey, but also a quick comprehensive reference for the researchers and practitioners to draw insights from extensive informative summaries of the existing works to advance the LLM research direction.","arXiv.org",2023,"Humza Naveed,Asad Ullah Khan,Shi Qiu,Muhammad Saqib,Saeed Anwar,Muhammad Usman,N. Barnes,A. Mian",15,443,1
"d6c2523ab97416c2692cbbeab082ed1790e8e55e","https://www.semanticscholar.org/paper/d6c2523ab97416c2692cbbeab082ed1790e8e55e",5,"VisIT-Bench: A Benchmark for Vision-Language Instruction Following Inspired by Real-World Use","This work introduces VisIT-Bench (Visual InsTruction Benchmark), a benchmark for evaluation of instruction-following vision-language models for real-world use, and curates 70 'instruction families' that it envision instruction tuned vision- language models should be able to address.","arXiv.org",2023,"Yonatan Bitton,Hritik Bansal,Jack Hessel,Rulin Shao,Wanrong Zhu,Anas Awadalla,Josh Gardner,Rohan Taori,L. Schimdt",6,99,0
"894ed1aba8e42a4ec27ba53ecde383b14c5128ca","https://www.semanticscholar.org/paper/894ed1aba8e42a4ec27ba53ecde383b14c5128ca",5,"Examining User-Friendly and Open-Sourced Large GPT Models: A Survey on Language, Multimodal, and Scientific GPT Models","This survey paper provides an examination of alternative open-sourced models of large GPTs, focusing on user-friendly and relatively small models that facilitate easier deployment and accessibility, and aims to equip researchers, practitioners, and enthusiasts with a thorough understanding of these models.","arXiv.org",2023,"Kaiyuan Gao,Su He,Zhenyu He,Jiacheng Lin,Qizhi Pei,Jie Shao,Wei Zhang",0,178,0
"4eb87eaa193929dbef93fa2db9419245a8e8916f","https://www.semanticscholar.org/paper/4eb87eaa193929dbef93fa2db9419245a8e8916f",5,"TextBind: Multi-turn Interleaved Multimodal Instruction-following in the Wild","This work introduces TextBind, an almost annotation-free framework for empowering larger language models with the multi-turn interleaved multimodal instruction-following capabilities, and devise MIM, a language model-centric architecture that seamlessly integrates image encoder and decoder models.","arXiv.org",2023,"Huayang Li,Siheng Li,Deng Cai,Longyue Wang,Lemao Liu,Taro Watanabe,Yujiu Yang,Shuming Shi",1,59,1
"f34b6b4f75fb4e6f2c5654ee13fb2479c6170b3a","https://www.semanticscholar.org/paper/f34b6b4f75fb4e6f2c5654ee13fb2479c6170b3a",5,"Kosmos-2.5: A Multimodal Literate Model","Kosmos-2.5 can be readily adapted for any text-intensive image understanding task with different prompts through supervised fine-tuning, making it a general-purpose tool for real-world applications involving text-rich images and paves the way for the future scaling of multimodal large language models.","arXiv.org",2023,"Tengchao Lv,Yupan Huang,Jingye Chen,Lei Cui,Shuming Ma,Ya-Chi Chang,Shaohan Huang,Wenhui Wang,Li Dong,Weiyao Luo,Shaoxiang Wu,Guoxin Wang,Cha Zhang,Furu Wei",0,85,0
"7b689adb8c156d6158660f90d1c86888ee281f63","https://www.semanticscholar.org/paper/7b689adb8c156d6158660f90d1c86888ee281f63",5,"DreamLLM: Synergistic Multimodal Comprehension and Creation","A learning framework that first achieves versatile Multimodal Large Language Models (MLLMs) empowered with frequently overlooked synergy between multimodal comprehension and creation, reaping from the enhanced learning synergy.","arXiv.org",2023,"Runpei Dong,Chunrui Han,Yuang Peng,Zekun Qi,Zheng Ge,Jinrong Yang,Liang Zhao,Jian‐Yuan Sun,Hongyu Zhou,Hao-Ran Wei,Xiangwen Kong,Xiangyu Zhang,Kaisheng Ma,Li Yi",10,169,1
"092245d86b77181c36f972b1b7a17a59cd989c4a","https://www.semanticscholar.org/paper/092245d86b77181c36f972b1b7a17a59cd989c4a",5,"Guiding Instruction-based Image Editing via Multimodal Large Language Models","This work investigates how MLLMs facilitate edit instructions and presents MLLM-Guided Image Editing (MGIE), which learns to derive expressive instructions and provides explicit guidance and can lead to a notable improvement in automatic metrics and human evaluation while maintaining competitive inference efficiency.","arXiv.org",2023,"Tsu-Jui Fu,Wenze Hu,Xianzhi Du,William Yang Wang,Yinfei Yang,Zhe Gan",0,63,0
"a710efa9247207a72f06e0c9db302fd3ecab5fbb","https://www.semanticscholar.org/paper/a710efa9247207a72f06e0c9db302fd3ecab5fbb",5,"Towards Robust Multi-Modal Reasoning via Model Selection","This framework improves model selection and bolsters the robustness of multi-modal agents in multi-step reasoning, and enables dynamic model selection, considering both user inputs and subtask dependencies, thereby robustifying the overall reasoning process.","arXiv.org",2023,"Xiangyan Liu,Rongxue Li,Wei Ji,Tao Lin",0,57,0
"0ebc861f5478561f12941e6b48aad30574e996d8","https://www.semanticscholar.org/paper/0ebc861f5478561f12941e6b48aad30574e996d8",5,"Video ChatCaptioner: Towards Enriched Spatiotemporal Descriptions","This work introduces Video ChatCaptioner, an innovative approach for creating more comprehensive spatiotemporal video descriptions, specifically designed to select frames for posing video content-driven questions and shows promise as a method for enhancing video content.","arXiv.org",2023,"Jun Chen,Deyao Zhu,Kilichbek Haydarov,Xiang Li,Mohamed Elhoseiny",9,44,0
"42a30dc5470f54ec249f25d3c31e05d7c376c8e3","https://www.semanticscholar.org/paper/42a30dc5470f54ec249f25d3c31e05d7c376c8e3",5,"VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks","This work presents an LLM-based framework for vision-centric tasks, termed VisionLLM, which provides a unified perspective for vision and language tasks by treating images as a foreign language and aligning vision-focused tasks with language tasks that can be flexibly defined and managed using language instructions.","arXiv.org",2023,"Wen Wang,Zhe Chen,Xiaokang Chen,Jiannan Wu,Xizhou Zhu,Gang Zeng,Ping Luo,Tong Lu,Jie Zhou,Y. Qiao,Jifeng Dai",55,78,7
"833cdd713c27ab5899bb912a1d511c10af61cefb","https://www.semanticscholar.org/paper/833cdd713c27ab5899bb912a1d511c10af61cefb",5,"Making Multimodal Generation Easier: When Diffusion Models Meet LLMs","Efficient model designed to enhance multimodal understanding and generation by harnessing the capabilities of diffusion models and large language models, built upon a bidirectional conditional diffusion model named BiDiffuser, which promotes more efficient interactions between modalities.","arXiv.org",2023,"Xiangyu Zhao,Bo Liu,Qijiong Liu,Guangyuan Shi,Xiao-Ming Wu",0,58,0
"06091944b864d6dc473cab63321a95fb9c4067cc","https://www.semanticscholar.org/paper/06091944b864d6dc473cab63321a95fb9c4067cc",5,"ChatCAD+: Towards a Universal and Reliable Interactive CAD using LLMs","ChatCAD+, which is designed to be universal and reliable, is introduced, capable of handling medical images from diverse domains and leveraging up-to-date information from reputable medical websites to provide reliable medical advice.","arXiv.org",2023,"Zihao Zhao,Sheng Wang,Jinchen Gu,Yitao Zhu,Lanzhuju Mei,Zixu Zhuang,Zhiming Cui,Qian Wang,Dinggang Shen",6,51,1
"bca0bbd01ea917b7a9fe369288ea3ba03d3b1ff3","https://www.semanticscholar.org/paper/bca0bbd01ea917b7a9fe369288ea3ba03d3b1ff3",5,"A Survey of Large Language Models in Medicine: Progress, Application, and Challenge","This survey provides a comprehensive overview of the current progress, applications, and challenges faced by LLMs in medicine and serves as a valuable resource for constructing practical and effective medical LLMs.","arXiv.org",2023,"Hongjian Zhou,Boyang Gu,Xinyu Zou,Yiru Li,Sam S. Chen,Peilin Zhou,Junling Liu,Y. Hua,Chengfeng Mao,Xian Wu,Zheng Li,Fenglin Liu",0,183,0
"88bddfb7d1e0462be8fe99fdbd71c658140cb17b","https://www.semanticscholar.org/paper/88bddfb7d1e0462be8fe99fdbd71c658140cb17b",5,"From Image to Language: A Critical Analysis of Visual Question Answering (VQA) Approaches, Challenges, and Opportunities","This work presents a survey in the domain of VQA that delves into the intricacies of V QA datasets and methods over the field's history, introduces a detailed taxonomy to categorize the facets of VZA, and highlights the recent trends, challenges, and scopes for improvement.","arXiv.org",2023,"Md Farhan Ishmam,Md Sakib Hossain Shovon,M. F. Mridha,Nilanjan Dey",0,303,0
"31a7d8c4a5ab6bab522494b57270249105c8748e","https://www.semanticscholar.org/paper/31a7d8c4a5ab6bab522494b57270249105c8748e",5,"BiomedGPT: A Unified and Generalist Biomedical Generative Pre-trained Transformer for Vision, Language, and Multimodal Tasks","A unified and generalist Biomedical Generative Pre-trained Transformer model, which leverages self-supervision on large and diverse datasets to accept multi-modal inputs and perform a range of downstream tasks, with far-reaching implications for improving healthcare outcomes.","arXiv.org",2023,"Kai Zhang,Jun Yu,Zhilin Yan,Yixin Liu,Eashan Adhikarla,S. Fu,Xun Chen,Chen Chen,Yuyin Zhou,Xiang Li,Lifang He,B. Davison,Quanzheng Li,Yong Chen,Hongfang Liu,Lichao Sun",18,147,3
"baa1dc079d98ca76b0173c8d653fed759fd0a371","https://www.semanticscholar.org/paper/baa1dc079d98ca76b0173c8d653fed759fd0a371",5,"A scoping review on multimodal deep learning in biomedical images and texts","This study reviewed the current uses of multimodal deep learning on five tasks: report generation, Visual question answering, Cross-modal retrieval, computer-aided diagnosis, and Semantic segmentation, and highlighted the diverse applications and potential of MDL.","Journal of Biomedical Informatics",2023,"Zhaoyi Sun,Mingquan Lin,Qingqing Zhu,Qianqian Xie,Fei Wang,Zhiyong Lu,Yifan Peng",1,148,0
"f0950a3f27c0fefffba60ae1c9a8ee360d5eb55f","https://www.semanticscholar.org/paper/f0950a3f27c0fefffba60ae1c9a8ee360d5eb55f",5,"Instruction Tuning for Large Language Models: A Survey","A systematic review of the literature, including the general methodology of IT, the construction of IT datasets, the training of IT models, and applications to different modalities, domains and applications, along with an analysis on aspects that influence the outcome of IT.","arXiv.org",2023,"Shengyu Zhang,Linfeng Dong,Xiaoya Li,Sen Zhang,Xiaofei Sun,Shuhe Wang,Jiwei Li,Runyi Hu,Tianwei Zhang,Fei Wu,Guoyin Wang",27,150,2
"da9134f694959b68027c33c8e998ffb3d41305da","https://www.semanticscholar.org/paper/da9134f694959b68027c33c8e998ffb3d41305da",5,"Exploring Question Decomposition for Zero-Shot VQA","A model-driven selective decomposition approach for second-guessing predictions and correcting errors is introduced, and its effectiveness on eight VQA tasks across three domains is validated, showing consistent improvements in accuracy.","arXiv.org",2023,"Zaid Khan,B. Vijaykumar,S. Schulter,M. Chandraker,Yun Fu",0,60,0
"094883e42bb9a41f602c0715c1059bc431e33fb2","https://www.semanticscholar.org/paper/094883e42bb9a41f602c0715c1059bc431e33fb2",4,"GPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest","Spatial instruction tuning is proposed, which introduces the reference to the region-of-interest (RoI) in the instruction, which achieves a remarkable accuracy of 81.6%, surpassing all existing models by a significant margin and almost reaching human-level performance of 85.0%.","arXiv.org",2023,"Shilong Zhang,Pei Sun,Shoufa Chen,Min Xiao,Wenqi Shao,Wenwei Zhang,Kai Chen,Ping Luo",21,98,2
"ca6a2bc279be5a3349a22bfd6866ed633d18734b","https://www.semanticscholar.org/paper/ca6a2bc279be5a3349a22bfd6866ed633d18734b",4,"MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models","MiniGPT-4 is presented, which aligns a frozen visual encoder with a frozen advanced LLM, Vicuna, using one projection layer to uncovers that properly aligning the visual features with an advanced large language model can possess numerous advanced multi-modal abilities demonstrated by G PT-4.","arXiv.org",2023,"Deyao Zhu,Jun Chen,Xiaoqian Shen,Xiang Li,Mohamed Elhoseiny",303,49,89
"7e32aac43e9f1df49e116add03327ee6f365dbf3","https://www.semanticscholar.org/paper/7e32aac43e9f1df49e116add03327ee6f365dbf3",4,"mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality",,"arXiv.org",2023,"Qinghao Ye,Haiyang Xu,Guohai Xu,Jiabo Ye,Ming Yan,Yi Zhou,Junyan Wang,Anwen Hu,Pengcheng Shi,Yaya Shi,Chenliang Li,Yuanhong Xu,Hehong Chen,Junfeng Tian,Qiang Qi,Ji Zhang,Feiyan Huang",156,36,22
"54a8b153ed04a872da878d695239bdc413dc782c","https://www.semanticscholar.org/paper/54a8b153ed04a872da878d695239bdc413dc782c",4,"InternGPT: Solving Vision-Centric Tasks by Interacting with Chatbots Beyond Language","By incorporating pointing instructions, the proposed iGPT significantly improves the efficiency of communication between users and chatbots, as well as the accuracy of chatbots in vision-centric tasks, especially in complicated visual scenarios where the number of objects is greater than 2.","arXiv.org",2023,"Zhaoyang Liu,Yinan He,Wenhai Wang,Weiyun Wang,Yi Wang,Shoufa Chen,Qing-Long Zhang,Yang Yang,Qingyun Li,Jiashuo Yu,Kunchang Li,Zhe Chen,Xuecheng Yang,Xizhou Zhu,Yali Wang,Limin Wang,Ping Luo,Jifeng Dai,Yu Qiao",22,83,1
"d48cb91b9e555194f7494c4d4bb9815021d3ee45","https://www.semanticscholar.org/paper/d48cb91b9e555194f7494c4d4bb9815021d3ee45",4,"VideoChat: Chat-Centric Video Understanding","VideoChat is introduced, an end-to-end chat-centric video understanding system that integrates video foundation models and large language models via a learnable neural interface, excelling in spatiotemporal reasoning, event localization, and causal relationship inference.","arXiv.org",2023,"Kunchang Li,Yinan He,Yi Wang,Yizhuo Li,Wen Wang,Ping Luo,Yali Wang,Limin Wang,Yu Qiao",59,59,10
"9837349417e36ef5be06da0fd6c74042148bdaa2","https://www.semanticscholar.org/paper/9837349417e36ef5be06da0fd6c74042148bdaa2",4,"Visual Programming for Text-to-Image Generation and Evaluation","This work proposes two novel interpretable/explainable visual programming frameworks for text-to-image (T2I) generation and evaluation and introduces VPEval, an interpretable and explainable evaluation framework for T2I generation based on visual programming.","arXiv.org",2023,"Jaemin Cho,Abhaysinh Zala,Mohit Bansal",6,62,2
"00cb69a9f280317d1c59ac5827551ee9b10642b8","https://www.semanticscholar.org/paper/00cb69a9f280317d1c59ac5827551ee9b10642b8",4,"EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought","This work introduces EmbodiedGPT, an end-to-end multi-modal foundation model for embodied AI, empowering embodied agents with multi- modal understanding and execution capabilities, and significantly enhances the success rate of the embodied control task by extracting more effective features.","arXiv.org",2023,"Yao Mu,Qinglong Zhang,Mengkang Hu,Wen Wang,Mingyu Ding,Jun Jin,Bin Wang,Jifeng Dai,Y. Qiao,Ping Luo",32,73,2
"9c3a9b4821daa03cb5369041d59d2714329a3811","https://www.semanticscholar.org/paper/9c3a9b4821daa03cb5369041d59d2714329a3811",4,"Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models","A novel and affordable solution for the effective VL adaption of LLMs, called Mixture-of-Modality Adaptation (MMA), which adopts lightweight modules, i.e., adapters, to bridge the gap between LLMs and VL tasks, which also enables the joint optimization of the image and language models","arXiv.org",2023,"Gen Luo,Yiyi Zhou,Tianhe Ren,Shen Chen,Xiaoshuai Sun,Rongrong Ji",21,53,3
"b458fc5261595f44b36325e5eaea1f874d65138f","https://www.semanticscholar.org/paper/b458fc5261595f44b36325e5eaea1f874d65138f",4,"GPT4Tools: Teaching Large Language Model to Use Tools via Self-instruction","The GPT4Tools based on self-instruct to enable open-source LLMs, such as LLaMA and OPT, to use tools, generates an instruction-following dataset by prompting an advanced teacher with various multi-modal contexts using the Low-Rank Adaptation (LoRA) optimization.","arXiv.org",2023,"Rui Yang,Lin Song,Yanwei Li,Sijie Zhao,Yixiao Ge,Xiu Li,Ying Shan",30,63,5
"d47524cd5c3c4b57af2e5a29f6f91c420310f236","https://www.semanticscholar.org/paper/d47524cd5c3c4b57af2e5a29f6f91c420310f236",4,"MIMIC-IT: Multi-Modal In-Context Instruction Tuning","MultI-Modal In-Context Instruction Tuning (MIMIC-IT), a dataset comprising 2.8 million multimodal instruction-response pairs, with 2.2 million unique instructions derived from images and videos, is presented and a large VLM named Otter is trained.","arXiv.org",2023,"Bo Li,Yuanhan Zhang,Liangyu Chen,Jinghao Wang,Fanyi Pu,Jingkang Yang,C. Li,Ziwei Liu",40,55,6
"4c4d176c6e28f48041f215d563f6ee8633534cff","https://www.semanticscholar.org/paper/4c4d176c6e28f48041f215d563f6ee8633534cff",4,"Valley: Video Assistant with Large Language model Enhanced abilitY","A novel multi-modal foundation model capable of comprehending video, image, and language within a general framework is developed, and Qualitative experiments demonstrate that Valley has the potential to function as a highly effective video assistant that can make complex video understanding scenarios easy.","arXiv.org",2023,"Ruipu Luo,Ziwang Zhao,Min Yang,Junwei Dong,Ming-Hui Qiu,Pengcheng Lu,Tao Wang,Zhongyu Wei",15,39,4
"d98536f24272e258b1d399074b64284d64786099","https://www.semanticscholar.org/paper/d98536f24272e258b1d399074b64284d64786099",4,"AVIS: Autonomous Visual Information Seeking with Large Language Models","An autonomous information seeking visual question answering framework that leverages a Large Language Model to dynamically strategize the utilization of external tools and to investigate their outputs, thereby acquiring the indispensable knowledge needed to provide answers to the posed questions is proposed.","arXiv.org",2023,"Ziniu Hu,Ahmet Iscen,Chen Sun,Kai-Wei Chang,Yizhou Sun,David A. Ross,C. Schmid,A. Fathi",4,133,0
"efc694164312006c543ef745611348ef64e68dda","https://www.semanticscholar.org/paper/efc694164312006c543ef745611348ef64e68dda",4,"Towards Language Models That Can See: Computer Vision Through the LENS of Natural Language","This work proposes LENS, a modular approach for tackling computer vision problems by leveraging the power of large language models (LLMs) with a language model to reason over outputs from a set of independent and highly descriptive vision modules that provide exhaustive information about an image.","arXiv.org",2023,"William M. Berrios,Gautam Mittal,Tristan Thrush,Douwe Kiela,Amanpreet Singh",13,64,1
"4ad771a10145e5e3b7e74bf6e98b165d7258889f","https://www.semanticscholar.org/paper/4ad771a10145e5e3b7e74bf6e98b165d7258889f",4,"What Matters in Training a GPT4-Style Language Model with Multimodal Inputs?","Lynx is presented, which performs the most accurate multi-modal understanding while keeping the best multi- modal generation ability compared to existing open-sourced GPT4-style models.","arXiv.org",2023,"Yan Zeng,Hanbo Zhang,Jiani Zheng,Jiangnan Xia,Guoqiang Wei,Yang Wei,Yuchen Zhang,Tao Kong",17,107,1
"2e52178416b93a0a7dc1cb7d21d04c7e72c2ce0f","https://www.semanticscholar.org/paper/2e52178416b93a0a7dc1cb7d21d04c7e72c2ce0f",4,"ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring Instruction Tuning","This study proposes ChatSpot, a unified end-to-end multimodal large language model that supports diverse forms of interactivity including mouse clicks, drag-and-drop, and drawing boxes, which provides a more flexible and seamless interactive experience.","arXiv.org",2023,"Liang Zhao,En Yu,Zheng Ge,Jinrong Yang,Hao-Ran Wei,Hongyu Zhou,Jian‐Yuan Sun,Yuang Peng,Runpei Dong,Chunrui Han,Xiangyu Zhang",6,39,2
"ad113d8b6f8f6dace7abd12dc88af520aaaf3fc7","https://www.semanticscholar.org/paper/ad113d8b6f8f6dace7abd12dc88af520aaaf3fc7",4,"LISA: Reasoning Segmentation via Large Language Model","This work proposes a new segmentation task -- reasoning segmentation, designed to output a segmentation mask given a complex and implicit query text, and presents LISA, which inherits the language generation capabilities of the multi-modal Large Language Model (LLM) while also possessing the ability to produce segmentation masks.","arXiv.org",2023,"Xin Lai,Zhuotao Tian,Yukang Chen,Yanwei Li,Yuhui Yuan,Shu Liu,Jiaya Jia",14,64,3
"d53945d4afb4528590d79e20de52883d29037e86","https://www.semanticscholar.org/paper/d53945d4afb4528590d79e20de52883d29037e86",4,"FashionLOGO: Prompting Multimodal Large Language Models for Fashion Logo Embeddings","This work explores how MLLMs can improve logo embedding by prompting them to generate explicit textual knowledge through three types of prompts, including image OCR, brief captions, and detailed descriptions prompts, in a zero-shot setting and adopt a cross-attention transformer to enable image embedding queries to learn supplementary knowledge from textual embeddings automatically.","arXiv.org",2023,"Yulin Su,Min Yang,Minghui Qiu,Jing Wang,Tao Wang",0,44,0
"eb5cf10406a8ad31e0ebe56b36571d5db4758a62","https://www.semanticscholar.org/paper/eb5cf10406a8ad31e0ebe56b36571d5db4758a62",4,"PUMGPT: A Large Vision-Language Model for Product Understanding","This paper presents PUMGPT, a large vision-language model that aims at unifying all product understanding tasks under a singular model structure, and proposes Layer-wise Adapters (LA), an approach that provides enhanced alignment with fewer visual tokens and enables parameter-efficient fine-tuning.","arXiv.org",2023,"Shuhui Wu,Zengming Tang,Zongyi Guo,Weiwei Zhang,Baoliang Cui,Haihong Tang,Weiming Lu",0,38,0
"25f7401d87f9b3cf8a2511a76b181a3559a1833f","https://www.semanticscholar.org/paper/25f7401d87f9b3cf8a2511a76b181a3559a1833f",4,"PointLLM: Empowering Large Language Models to Understand Point Clouds","PointLLM leverages a point cloud encoder with a powerful LLM to effectively fuse geometric, appearance, and linguistic information, thereby enabling LLMs to understand point clouds and offering a new avenue beyond 2D visual data.","arXiv.org",2023,"Runsen Xu,Xiaolong Wang,Tai Wang,Yilun Chen,Jiangmiao Pang,Dahua Lin",6,56,0
"3ec464696db25acc2c39a6d967ec3df09e06f633","https://www.semanticscholar.org/paper/3ec464696db25acc2c39a6d967ec3df09e06f633",4,"Multimodal Multi-Hop Question Answering Through a Conversation Between Tools and Efficiently Finetuned Large Language Models","A tool-interacting divide-and-conquer strategy enabling large language models (LLMs) to answer complex multimodal multi-hop questions, demonstrating the efficacy and generality of this approach.","arXiv.org",2023,"Hossein Rajabzadeh,Suyuchen Wang,Hyock Ju Kwon,Bang Liu",0,33,0
"bee68767debbdc96d6f75947e544a8be98b869e3","https://www.semanticscholar.org/paper/bee68767debbdc96d6f75947e544a8be98b869e3",4,"Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model: Explorations with GPT4-Vision and Beyond","This study introduces a new benchmark called PCA-EVAL, which evaluates embodied decision-making from the perspectives of Perception, Cognition, and Action and proposes HOLMES, a multi-agent cooperation framework that allows LLMs to leverage MLLMs and APIs to gather multimodal information for informed decision- making.","arXiv.org",2023,"Liang Chen,Yichi Zhang,Shuhuai Ren,Haozhe Zhao,Zefan Cai,Yuchi Wang,Tianyu Liu,Baobao Chang",0,59,0
"36b923d97d7cfaf73d11c55c15ea46605ba974a5","https://www.semanticscholar.org/paper/36b923d97d7cfaf73d11c55c15ea46605ba974a5",4,"BiLL-VTG: Bridging Large Language Models and Lightweight Visual Tools for Video-based Texts Generation","BiLL-VTG is introduced, a fast adaptive framework that leverages large language models (LLMs) to reasoning on videos based on essential lightweight visual tools and an Instruction-oriented Video Events Recognition (InsOVER) algorithm based on the efficient Hungarian matching to localize corresponding video events using linguistic instructions, enabling LLMs to interact with long videos.","arXiv.org",2023,"Ji Qi,Kaixuan Ji,Jifan Yu,Duokang Wang,Bin Xu,Lei Hou,Juanzi Li",0,68,0
"807f336176070bd3f95b82a16f125ee99b7d2c80","https://www.semanticscholar.org/paper/807f336176070bd3f95b82a16f125ee99b7d2c80",4,"Woodpecker: Hallucination Correction for Multimodal Large Language Models","Implemented in a post-remedy manner, Woodpecker can easily serve different MLLMs, while being interpretable by accessing intermediate outputs of the five stages, and shows the huge potential of this new paradigm.","arXiv.org",2023,"Shukang Yin,Chaoyou Fu,Sirui Zhao,Tong Xu,Hao Wang,Dianbo Sui,Yunhang Shen,Ke Li,Xingguo Sun,Enhong Chen",0,49,0
"ecfff30e570498b6c87c5d1319a0cbcdf7a8b86d","https://www.semanticscholar.org/paper/ecfff30e570498b6c87c5d1319a0cbcdf7a8b86d",4,"LLaVA-Plus: Learning to Use Tools for Creating Multimodal Agents","LLaVA-Plus is distinct in that the image query is directly grounded and actively engaged throughout the entire human-AI interaction sessions, significantly improving tool use performance and enabling new scenarios.","arXiv.org",2023,"Shilong Liu,Hao Cheng,Haotian Liu,Hao Zhang,Feng Li,Tianhe Ren,Xueyan Zou,Jianwei Yang,Hang Su,Jun-Juan Zhu,Lei Zhang,Jianfeng Gao,Chun-yue Li",0,52,0
"ef321c6f174ac59916ac54ec40ad18bca5b58e5c","https://www.semanticscholar.org/paper/ef321c6f174ac59916ac54ec40ad18bca5b58e5c",4,"PerceptionGPT: Effectively Fusing Visual Perception into LLM","A novel end-to-end framework named PerceptionGPT, which efficiently and effectively equips the VLLMs with visual perception abilities by leveraging the representation power of LLMs' token embedding and demonstrates significant improvements over previous methods with much fewer trainable parameters and GPU hours.","arXiv.org",2023,"Renjie Pi,Lewei Yao,Jiahui Gao,Jipeng Zhang,Tong Zhang",0,50,0
"13b5b69355555e0c8b702261c5de3b4172ba653c","https://www.semanticscholar.org/paper/13b5b69355555e0c8b702261c5de3b4172ba653c",4,"The Art of SOCRATIC QUESTIONING: Zero-shot Multimodal Reasoning with Recursive Thinking and Self-Questioning","Qualitative analysis clearly demonstrates the intermediate thinking steps elicited by S OCRATIC Q UESTIONING are similar to the human’s recursively thinking process of a complex reasoning problem.","arXiv.org",2023,"Jingyuan Qi,Zhiyang Xu,Ying Shen,Minqian Liu,dingnan jin,Qifan Wang,Lifu Huang",5,27,0
"20fcc01d12a50f1da2af71d85f0a269b3ba48b77","https://www.semanticscholar.org/paper/20fcc01d12a50f1da2af71d85f0a269b3ba48b77",4,"LMEye: An Interactive Perception Network for Large Language Models","LMEye, a human-like eye with a play-and-plug interactive perception network, designed to enable dynamic interaction between LLMs and external vision information, is introduced, demonstrating that it significantly improves the zero-shot performance on various multimodal tasks compared to previous methods, with less parameters.","arXiv.org",2023,"Yunxin Li,Baotian Hu,Xinyu Chen,Lin Ma,M. Zhang",6,58,1
"5d321194696f1f75cf9da045e6022b2f20ba5b9c","https://www.semanticscholar.org/paper/5d321194696f1f75cf9da045e6022b2f20ba5b9c",4,"Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding","Video-LLaMA shows the ability to perceive and comprehend video content and generate meaningful responses grounded in the visual and auditory information presented in the videos.","arXiv.org",2023,"Hang Zhang,Xin Li,Lidong Bing",66,42,8
"659a12d71d8709c132ccd9ccd235f0024cae0239","https://www.semanticscholar.org/paper/659a12d71d8709c132ccd9ccd235f0024cae0239",4,"The All-Seeing Project: Towards Panoptic Visual Recognition and Understanding of the Open World","The All-Seeing model (ASM), a unified framework for panoptic visual recognition and understanding, is developed with open-ended language prompts and locations, which allows it to generalize to various vision and language tasks with remarkable zero-shot performance.","arXiv.org",2023,"Weiyun Wang,Min Shi,Qingyun Li,Wen Wang,Zhenhang Huang,Linjie Xing,Zhe Chen,Hao Li,Xizhou Zhu,Zhiguo Cao,Yushi Chen,Tong Lu,Jifeng Dai,Y. Qiao",4,113,1
"f4793adffd6f67ffcb93ccfc5672ab301b8a2b96","https://www.semanticscholar.org/paper/f4793adffd6f67ffcb93ccfc5672ab301b8a2b96",4,"PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering","This paper proposes a generative-based model for medical visual understanding by aligning visual information from a pre-trained vision encoder with a large language model, and establishes a scalable pipeline to construct a large-scale medical visual question-answering dataset.","arXiv.org",2023,"Xiaoman Zhang,Chaoyi Wu,Ziheng Zhao,Weixiong Lin,Ya Zhang,Yanfeng Wang,Weidi Xie",19,38,2
"1f5e1a036b24b9dd34c006ba3bb61119624f4fdb","https://www.semanticscholar.org/paper/1f5e1a036b24b9dd34c006ba3bb61119624f4fdb",4,"A Comprehensive Study of GPT-4V's Multimodal Capabilities in Medical Imaging","This paper presents a comprehensive evaluation of GPT-4V's capabilities across diverse medical imaging tasks, including Radiology Report Generation, Medical Visual Question Answering (VQA), and Visual Grounding, and finds the limitations of conventional evaluation metrics like the BLEU score.","medRxiv",2023,"Yingshu Li,Yunyi Liu,Zhanyu Wang,Xinyu Liang,Lingqiao Liu,Lei Wang,Leyang Cui,Zhaopeng Tu,Longyue Wang,Luping Zhou",0,67,0
"bf40c9e7832e1b2887cbf5798455f91705ea11ba","https://www.semanticscholar.org/paper/bf40c9e7832e1b2887cbf5798455f91705ea11ba",4,"Masked Vision and Language Pre-training with Unimodal and Multimodal Contrastive Losses for Medical Visual Question Answering","This paper presents a novel self-supervised approach that learns unimodal and multimodal feature representations of input images and text using medical image caption datasets, by leveraging both unimmodal and multi-modal contrastive losses, along with masked language modeling and image text matching as pretraining objectives.","International Conference on Medical Image Computing and Computer-Assisted Intervention",2023,"Pengfei Li,Gang Liu,Jinlong He,Zixu Zhao,Shenjun Zhong",0,27,0
"df0ddb588a200d095743e9d26fc4a9318619766e","https://www.semanticscholar.org/paper/df0ddb588a200d095743e9d26fc4a9318619766e",4,"Towards Generalist Foundation Model for Radiology","This study constructs a large-scale Medical Multi-modal Dataset, MedMD, which consists of 16M 2D and 3D medical scans with high-quality text descriptions or reports across various data formats, modalities, and tasks, covering over 5000 distinct diseases, and proposes a new evaluation benchmark, RadBench, aiming to comprehensively assess the capability of foundation models in handling practical clinical problems.","arXiv.org",2023,"Chaoyi Wu,Xiaoman Zhang,Ya Zhang,Yanfeng Wang,Weidi Xie",7,59,1
"a3711dbf296b5ddd97ba93826660cd3995611625","https://www.semanticscholar.org/paper/a3711dbf296b5ddd97ba93826660cd3995611625",3,"Towards A Foundation Model for Generalist Robots: Diverse Skill Learning at Scale via Automated Task and Scene Generation",,"arXiv.org",2023,"Zhou Xian,Théophile Gervet,Zhenjia Xu,Yi-Ling Qiao,Tsun-Hsuan Wang",3,111,0
"692bc40edf4785d88c39e0c0fe9f270541fecf8a","https://www.semanticscholar.org/paper/692bc40edf4785d88c39e0c0fe9f270541fecf8a",3,"Towards Generalist Robots: A Promising Paradigm via Generative Simulation","This document presents a specific idea for mining knowledge in the latest large-scale foundation models for robotics research, which uses a fully automated generative pipeline which uses these models to generate diversified tasks, scenes and training supervisions at scale, thereby scaling up low-level skill learning and ultimately leading to a foundation model for robotics that empowers generalist robots.","",2023,"Zhou Xian,Théophile Gervet,Zhenjia Xu,Yi-Ling Qiao,Tsun-Hsuan Wang,Yian Wang",0,124,0
"6845bea94b2fb17d4377b3bb2bd10f73a959f9cc","https://www.semanticscholar.org/paper/6845bea94b2fb17d4377b3bb2bd10f73a959f9cc",3,"Reasoning with Language Model Prompting: A Survey","This paper provides a comprehensive survey of cutting-edge research on reasoning with language model prompting with comparisons and summaries and provides systematic resources to help beginners.","Annual Meeting of the Association for Computational Linguistics",2022,"Shuofei Qiao,Yixin Ou,Ningyu Zhang,Xiang Chen,Yunzhi Yao,Shumin Deng,Chuanqi Tan,Fei Huang,Huajun Chen",82,218,3
"170c97c7215f42edfb20c2248f954879e91ef86e","https://www.semanticscholar.org/paper/170c97c7215f42edfb20c2248f954879e91ef86e",3,"Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models","This paper presents Chameleon, an AI system that mitigates LLM limitations by augmenting LLMs with plug-and-play modules for compositional reasoning, and shows that the GPT-4-powered planner exhibits more consistent and rational tool selection via inferring potential constraints from instructions, compared to a ChatGPT- powered planner.","arXiv.org",2023,"Pan Lu,Baolin Peng,Hao Cheng,Michel Galley,Kai-Wei Chang,Y. Wu,Song-Chun Zhu,Jianfeng Gao",85,68,12
"1a28e9c62eeb76a1a77dc152197027c15310927b","https://www.semanticscholar.org/paper/1a28e9c62eeb76a1a77dc152197027c15310927b",3,"ANPL: Compiling Natural Programs with Interactive Decomposition","ANPL, a programming system that allows users to decompose user-specific tasks, is introduced and deployed on the Abstraction and Reasoning Corpus (ARC), a set of unique tasks that are challenging for state-of-the-art AI systems.","arXiv.org",2023,"Di Huang,Ziyuan Nan,Xingui Hu,Pengwei Jin,Shaohui Peng,Yuanbo Wen,Rui Zhang,Zidong Du,Qi Guo,Yewen Pu,Yunji Chen",3,67,1
"af705d648b5b16daa3dcc593bc593f2574d76c07","https://www.semanticscholar.org/paper/af705d648b5b16daa3dcc593bc593f2574d76c07",3,"Grammar Prompting for Domain-Specific Language Generation with Large Language Models","Experiments demonstrate that grammar prompting can enable LLMs to perform competitively on a diverse set of DSL generation tasks, including semantic parsing, Overnight, GeoQuery, PDDL planning, and even molecule generation (SMILES).","arXiv.org",2023,"Bailin Wang,Zi Wang,Xuezhi Wang,Yuan Cao,R. Saurous,Yoon Kim",2,100,0
"ea566f87f6253bb2d32cf7b61cd3e2535a0c3f42","https://www.semanticscholar.org/paper/ea566f87f6253bb2d32cf7b61cd3e2535a0c3f42",3,"mPLUG-DocOwl: Modularized Multimodal Large Language Model for Document Understanding","Experimental results show that the proposed mPLUG-DocOwl model outperforms existing multi-modal models, demonstrating its strong ability of document understanding, and also generalizes well on various downstream tasks.","arXiv.org",2023,"Jiabo Ye,Anwen Hu,Haiyang Xu,Qinghao Ye,Mingshi Yan,Yuhao Dan,Chenlin Zhao,Guohai Xu,Chenliang Li,Junfeng Tian,Qiang Qi,Ji Zhang,Feiyan Huang",7,37,0
"446fb5dead075a1a08862662738f462e9a0e91c8","https://www.semanticscholar.org/paper/446fb5dead075a1a08862662738f462e9a0e91c8",3,"Tool Documentation Enables Zero-Shot Tool-Usage with Large Language Models","This work advocates the use of tool documentation, descriptions for the individual tool usage, over demonstrations, and shows that tool documentation is significantly more valuable than demonstrations, with zero-shot documentation significantly outperforming few-shot without documentation.","arXiv.org",2023,"Cheng-Yu Hsieh,Sibei Chen,Chun-Liang Li,Yasuhisa Fujii,Alexander J. Ratner,Chen-Yu Lee,Ranjay Krishna,Tomas Pfister",8,84,0
"dd0612ce863f64b0f69d0d9f708c52e829f6f859","https://www.semanticscholar.org/paper/dd0612ce863f64b0f69d0d9f708c52e829f6f859",3,"TPTU: Large Language Model-based AI Agents for Task Planning and Tool Usage","A structured framework tailored for LLM-based AI Agents is proposed and two distinct types of agents are designed to execute the inference process, which emphasizes the substantial potential of these models, while also identifying areas that need more investigation and improvement.","",2023,"Jingqing Ruan,Yihong Chen,Bin Zhang,Zhiwei Xu,Tianpeng Bao,Guoqing Du,Shiwei Shi,Hangyu Mao,Xingyu Zeng,Rui Zhao",5,94,0
"7f1ba5630c3baa09b11cc665b3f71cdb117e5ffb","https://www.semanticscholar.org/paper/7f1ba5630c3baa09b11cc665b3f71cdb117e5ffb",3,"OpenLEAF: Open-Domain Interleaved Image-Text Generation and Evaluation","A new interleaved generation framework based on prompting large-language models (LLMs) and pre-trained text-to-image (T2I) models, namely OpenLEAF is proposed, which can generate high-quality image-text content for various domains and applications.","arXiv.org",2023,"Jie An,Zhengyuan Yang,Linjie Li,Jianfeng Wang,Kevin Lin,Zicheng Liu,Lijuan Wang,Jiebo Luo",0,40,0
"c020f15be1dee20f9e2e0c5a6f05f272b5508325","https://www.semanticscholar.org/paper/c020f15be1dee20f9e2e0c5a6f05f272b5508325",3,"LLaVA-Interactive: An All-in-One Demo for Image Chat, Segmentation, Generation and Editing","The development of LLaVA-Interactive is extremely cost-efficient as the system combines three multimodal skills of pre-built AI models without additional model training: visual chat of L LaVA, image segmentation from SEEM, as well as image generation and editing from GLIGEN.","arXiv.org",2023,"Wei-Ge Chen,Irina Spiridonova,Jianwei Yang,Jianfeng Gao,Chun-yue Li",1,36,0
"2fb605f67fee79cad94952ddfe0f686e926f49f5","https://www.semanticscholar.org/paper/2fb605f67fee79cad94952ddfe0f686e926f49f5",3,"GPT-4V in Wonderland: Large Multimodal Models for Zero-Shot Smartphone GUI Navigation","The findings demonstrate that large multimodal models, specifically GPT-4V, excel in zero-shot GUI navigation through its advanced screen interpretation, action reasoning, and precise action localization capabilities.","arXiv.org",2023,"An Yan,Zhengyuan Yang,Wanrong Zhu,Kevin Lin,Linjie Li,Jianfeng Wang,Jianwei Yang,Yiwu Zhong,Julian McAuley,Jianfeng Gao,Zicheng Liu,Lijuan Wang",0,58,0
"d1120d67b700e4dfe8b39eb1e48fbdea4e1a0c43","https://www.semanticscholar.org/paper/d1120d67b700e4dfe8b39eb1e48fbdea4e1a0c43",3,"HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face","HuggingGPT is a framework that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities to solve AI tasks and is able to cover numerous sophisticated AI tasks in different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks.","arXiv.org",2023,"Yongliang Shen,Kaitao Song,Xu Tan,D. Li,Weiming Lu,Y. Zhuang",282,43,38
"1a8eb2cae1833df3bf12fe3b41b03d60b4a4a98d","https://www.semanticscholar.org/paper/1a8eb2cae1833df3bf12fe3b41b03d60b4a4a98d",3,"Visual Instruction Tuning","This paper presents LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding and introduces GPT-4 generated visual instruction tuning data, the model and code base publicly available.","arXiv.org",2023,"Haotian Liu,Chunyuan Li,Qingyang Wu,Yong Jae Lee",340,58,104
"43e6e8d6663d83f1b74cf5a2be7b040b0928f867","https://www.semanticscholar.org/paper/43e6e8d6663d83f1b74cf5a2be7b040b0928f867",3,"X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages","X-LLM is proposed, which converts Multi-modalities into foreign languages using X2L interfaces and inputs them into a large Language model (ChatGLM), and demonstrates impressive multimodel chat abilities.","arXiv.org",2023,"Feilong Chen,Minglun Han,Haozhi Zhao,Qingyang Zhang,Jing Shi,Shuang Xu,Bo Xu",34,62,7
"6a5525c316b9be7909c433a79e090ed731425083","https://www.semanticscholar.org/paper/6a5525c316b9be7909c433a79e090ed731425083",3,"What Makes for Good Visual Tokenizers for Large Language Models?","A new MLLM equipped with a tailored Good Visual Tokenizer (GVT), which exhibits strong visual comprehension capability at multiple scales, without introducing extra parameters and task-specific fine-tuning.","arXiv.org",2023,"Guangzhi Wang,Yixiao Ge,Xiaohan Ding,Mohan S. Kankanhalli,Ying Shan",8,55,0
"ca055cfb9d4d47124cc035c346f38577825fcacf","https://www.semanticscholar.org/paper/ca055cfb9d4d47124cc035c346f38577825fcacf",3,"Enhance Reasoning Ability of Visual-Language Models via Large Language Models","A method called TReE is proposed, which transfers the reasoning ability of a large language model to a visual language model in zero-shot scenarios, and contains three stages: observation, thinking, and re-thinking.","arXiv.org",2023,"Yueting Yang,Xintong Zhang,Wenjuan Han",0,40,0
"c6ac708b65b24c20f80831d518c1795ce8133ad5","https://www.semanticscholar.org/paper/c6ac708b65b24c20f80831d518c1795ce8133ad5",3,"ChatBridge: Bridging Modalities with Large Language Model as a Language Catalyst","It is shown that only language-paired two-modality data is sufficient to connect all modalities and ChatBridge, a novel multimodal language model that leverages the expressive capabilities of language as the catalyst to bridge the gap between various modalities, is presented.","arXiv.org",2023,"Zijia Zhao,Longteng Guo,Tongtian Yue,Si-Qing Chen,Shuai Shao,Xinxin Zhu,Zehuan Yuan,Jing Liu",12,72,1
"50c1414fe41d0cb9db6f0933c9319aa124beac5d","https://www.semanticscholar.org/paper/50c1414fe41d0cb9db6f0933c9319aa124beac5d",3,"Contextual Object Detection with Multimodal Large Language Models","The ContextDET is presented, a unified multimodal model that is capable of end-to-end differentiable modeling of visual-language contexts, so as to locate, identify, and associate visual objects with language inputs for human-AI interaction.","arXiv.org",2023,"Yuhang Zang,Wei Li,Jun Han,Kaiyang Zhou,Chen Change Loy",9,88,0
"42ea55edb46395469aee1b760829657e65ab6577","https://www.semanticscholar.org/paper/42ea55edb46395469aee1b760829657e65ab6577",3,"Zero-Shot 3D Shape Correspondence","This work introduces a fully automatic method that exploits the exceptional reasoning capabilities of recent foundation models in language and vision to tackle difficult shape correspondence problems and produces highly plausible results in a zero-shot manner, especially between strongly non-isometric shapes.","arXiv.org",2023,"Ahmed Abdelreheem,A. Eldesokey,M. Ovsjanikov,Peter Wonka",2,66,0
"fed150a219f9c31bdb4920e615c7c9264c634736","https://www.semanticscholar.org/paper/fed150a219f9c31bdb4920e615c7c9264c634736",3,"On the Challenges and Perspectives of Foundation Models for Medical Image Analysis","The ""spectrum"" of medical foundation models, ranging from general imaging models, modality-specific models, to organ/task- specific models, are illustrated and highlighted, to highlight their challenges, opportunities and applications.","Medical Image Analysis",2023,"Shaoting Zhang,Dimitris N. Metaxas",7,65,0
"79150cb420d15830c8d36f0e91eea1b02e177f0f","https://www.semanticscholar.org/paper/79150cb420d15830c8d36f0e91eea1b02e177f0f",3,"Sticker820K: Empowering Interactive Retrieval with Stickers","The StickerCLIP is proposed as a benchmark model on the Sticker820K dataset, demonstrating strong superiority over the CLIP for the text-to-image retrieval task, and the recently popularized LLM is extended by means of prompt tuning, integrating its ability for sticker retrieval and allowing users to retrieve stickers through instructions.","arXiv.org",2023,"Sijie Zhao,Yixiao Ge,Zhongang Qi,Lin Song,Xiaohan Ding,Zehua Xie,Ying Shan",0,36,0
"697e0add95e880bd42e00bef838181e105f91981","https://www.semanticscholar.org/paper/697e0add95e880bd42e00bef838181e105f91981",3,"MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models","The first MLLM Evaluation benchmark MME is presented, which suggests that existing MLLMs still have a large room for improvement, but also reveals the potential directions for the subsequent model optimization.","arXiv.org",2023,"Chaoyou Fu,Peixian Chen,Yunhang Shen,Yulei Qin,Mengdan Zhang,Xu Lin,Zhenyu Qiu,Wei Lin,Jinrui Yang,Xiawu Zheng,Ke Li,Xing Sun,Rongrong Ji",54,43,18
"451a3f03aca4aa87b93981364842137417549e58","https://www.semanticscholar.org/paper/451a3f03aca4aa87b93981364842137417549e58",3,"SVIT: Scaling up Visual Instruction Tuning","It is empirically verified that training multimodal models on SVIT can significantly improve the multi-modal performance in terms of visual perception, reasoning and planing.","arXiv.org",2023,"Bo Zhao,Boya Wu,Tiejun Huang",11,40,1
"85d9151aa2efd0cbe822e403138cfe49f9536703","https://www.semanticscholar.org/paper/85d9151aa2efd0cbe822e403138cfe49f9536703",3,"SITTA: A Semantic Image-Text Alignment for Image Captioning","This work introduces two novel ways of constructing a linear mapping that successfully transfers semantics between the embedding spaces of the two pretrained models, and makes image captioning more accessible for institutions with restricted computational resources.","arXiv.org",2023,"Fabian Paischer,Thomas Adler,M. Hofmarcher,S. Hochreiter",0,116,0
"962ccf1fc49c83817fb031e5b24b81b19cdfb89d","https://www.semanticscholar.org/paper/962ccf1fc49c83817fb031e5b24b81b19cdfb89d",3,"BuboGPT: Enabling Visual Grounding in Multi-Modal LLMs","BozoGPT is a multi-modal LLM with visual grounding that can perform cross-modAL interaction between vision, audio and language, providing fine-grained understanding of visual objects and other given modalities and performs consistently well when provided by arbitrary modalities.","arXiv.org",2023,"Yang Zhao,Zhijie Lin,Daquan Zhou,Zilong Huang,Jiashi Feng,Bingyi Kang",15,33,0
"813ba033b8f593c98f9af44c5b4901408ba6f70a","https://www.semanticscholar.org/paper/813ba033b8f593c98f9af44c5b4901408ba6f70a",3,"Towards a Visual-Language Foundation Model for Computational Pathology","This work introduces CONtrastive learning from Captions for Histopathology (CONCH), a visual-language foundation model developed using diverse sources of histopathology images, biomedical text, and notably over 1.17 million image-caption pairs via task-agnostic pretraining, with the potential to directly facilitate a wide array of machine learning-based workflows requiring minimal or no further supervised fine-tuning.","arXiv.org",2023,"Ming Y. Lu,Bowen Chen,Drew F. K. Williamson,Richard J. Chen,Ivy Liang,Tong Ding,Guillaume Jaume,I. Odintsov,Andrew Zhang,L. Le,G. Gerber,A. Parwani,Faisal Mahmood",4,106,0
"94972e30504017156ef5b5debc419bf6edc67384","https://www.semanticscholar.org/paper/94972e30504017156ef5b5debc419bf6edc67384",3,"MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities","MM-Vet is proposed, an evaluation benchmark that examines large multimodal models (LMMs) on complicated multimodAL tasks and proposes an LLM-based evaluator for open-ended outputs that enables the evaluation across different question types and answer styles, resulting in a unified scoring metric.","arXiv.org",2023,"Weihao Yu,Zhengyuan Yang,Linjie Li,Jianfeng Wang,Kevin Lin,Zicheng Liu,Xinchao Wang,Lijuan Wang",23,91,4
"4f2be887e991efa85f7b874e7ab871080a745c39","https://www.semanticscholar.org/paper/4f2be887e991efa85f7b874e7ab871080a745c39",3,"CAESURA: Language Models as Multi-Modal Query Planners","This paper proposes Language-Model-Driven Query Planning, a new paradigm of query planning that uses Language Models to translate natural language queries into executable query plans that can contain complex operators that are able to process arbitrary modalities.","arXiv.org",2023,"Matthias Urban,Carsten Binnig",0,19,0
"1fd31b74f5e1eeb67341982fd35a613c6fad10e0","https://www.semanticscholar.org/paper/1fd31b74f5e1eeb67341982fd35a613c6fad10e0",3,"Link-Context Learning for Multimodal LLMs","This work proposes link-context learning (LCL), which emphasizes ""reasoning from cause and effect"" to augment the learning capabilities of MLLMs and introduces the ISEKAI dataset, comprising exclusively of unseen generated image-label pairs designed for link- context learning.","arXiv.org",2023,"Yan Tai,Weichen Fan,Zhao Zhang,Feng Zhu,Rui Zhao,Ziwei Liu",0,33,0
"ef1ebdb24ce45fb57e271783a0c9a877fe0e79c3","https://www.semanticscholar.org/paper/ef1ebdb24ce45fb57e271783a0c9a877fe0e79c3",3,"Position-Enhanced Visual Instruction Tuning for Multimodal Large Language Models","This paper proposes Position-enhanced Visual Instruction Tuning (PVIT), which extends the functionality of MLLMs by integrating an additional region-level vision encoder, which promotes a more detailed comprehension of images for the MLLM.","arXiv.org",2023,"Chi Chen,Ruoyu Qin,Fuwen Luo,Xiaoyue Mi,Peng Li,Maosong Sun,Yang Liu",5,34,1
"fa75a55760e6ea49b39b83cb85c99a22e1088254","https://www.semanticscholar.org/paper/fa75a55760e6ea49b39b83cb85c99a22e1088254",3,"NExT-GPT: Any-to-Any Multimodal LLM","This research showcases the promising possibility of building an AI agent capable of modeling universal modalities, paving the way for more human-like AI research in the community.","arXiv.org",2023,"Shengqiong Wu,Hao Fei,Leigang Qu,Wei Ji,Tat-Seng Chua",25,112,2
"d39182113cd4176ead48027b4fc05fe06ec6aaca","https://www.semanticscholar.org/paper/d39182113cd4176ead48027b4fc05fe06ec6aaca",3,"Language Models as Black-Box Optimizers for Vision-Language Models","This work aims to develop a novel fine-tuning approach for VLMs through natural language prompts, thereby avoiding the need to access model parameters, feature embeddings, or output logits, and finds that the text prompts generated through this strategy are not only more interpretable but also transfer well across different CLIP architectures in a black-box manner.","arXiv.org",2023,"Samuel Yu,Shihong Liu,Zhiqiu Lin,Deepak Pathak,Deva Ramanan",1,93,0
"7a7128e9696dfedc24bf432c4b4c3aafa5e95a1a","https://www.semanticscholar.org/paper/7a7128e9696dfedc24bf432c4b4c3aafa5e95a1a",3,"An Empirical Study of Scaling Instruct-Tuned Large Multimodal Models","An empirical study of scaling LLaVA up to 33B and 65B/70B and performance of LoRA/QLoRA tuning of LMM are comparable to the performance of full-model fine-tuning, finding that scaling LMM consistently enhances model performance and improves language capabilities.","arXiv.org",2023,"Yadong Lu,Chunyuan Li,Haotian Liu,Jianwei Yang,Jianfeng Gao,Yelong Shen",3,22,0
"e7d09b6f2bc878cf2c993acf675f409d0b55f35a","https://www.semanticscholar.org/paper/e7d09b6f2bc878cf2c993acf675f409d0b55f35a",3,"MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens","This work introduces an innovative interleaved vision-and-language generation technique anchored by the concept of ""generative vokens,"" which acts as the bridge for harmonized image-text outputs and exhibits substantial improvement over the baseline Divter model on the MMDialog dataset.","arXiv.org",2023,"Kaizhi Zheng,Xuehai He,Xin Eric Wang",6,38,0
"33095b1334bed852e3652bd9d7da3f4df0cdf485","https://www.semanticscholar.org/paper/33095b1334bed852e3652bd9d7da3f4df0cdf485",3,"ViCor: Bridging Visual Understanding and Commonsense Reasoning with Large Language Models","This work explores the synergistic capabilities of pre-trained vision-and-language models (VLMs) and large language models (LLMs) for visual commonsense reasoning (VCR) and suggests a collaborative approach where LLMs, when uncertain about their reasoning, actively direct VLMs to concentrate on and gather relevant visual elements to support potential commonsense inferences.","arXiv.org",2023,"KAI-QING Zhou,Kwonjoon Lee,Teruhisa Misu,Xin Eric Wang",0,34,0
"b3e9f249dd2e09ec111496f6b533101e8217a5b0","https://www.semanticscholar.org/paper/b3e9f249dd2e09ec111496f6b533101e8217a5b0",3,"Multimodal Large Language Model for Visual Navigation","This work designs a model that combines a simple text prompt, current observations, and a history collector model that gathers information from previous observations as input and provides a probability distribution of possible actions that the agent can take during navigation.","arXiv.org",2023,"Yao-Hung Tsai,Vansh Dhar,Jialu Li,Bowen Zhang,Jian Zhang",0,47,0
"ac2e5bf716aed246ca8914a6816ef73e00286099","https://www.semanticscholar.org/paper/ac2e5bf716aed246ca8914a6816ef73e00286099",3,"Beyond Segmentation: Road Network Generation with Multi-Modal LLMs","An innovative approach to road network generation through the utilization of a multi-modal Large Language Model (LLM), specifically designed to process aerial images of road layouts and produce detailed, navigable road networks within the input images.","arXiv.org",2023,"Sumedh Rasal,S. Boddhu",0,35,0
"beb3e8acd816bac1a5b7fccfd073f79048877e33","https://www.semanticscholar.org/paper/beb3e8acd816bac1a5b7fccfd073f79048877e33",3,"Frozen Transformers in Language Models Are Effective Visual Encoder Layers","This paper reveals that large language models (LLMs), despite being trained solely on textual data, are surprisingly strong encoders for purely visual tasks in the absence of language and proposes the information filtering hypothesis to explain the effectiveness of pre-trained LLMs in visual encoding.","arXiv.org",2023,"Ziqi Pang,Ziyang Xie,Yunze Man,Yu-Xiong Wang",0,83,0
"65d5728ea17f016382870aa27aac1e78d590b50c","https://www.semanticscholar.org/paper/65d5728ea17f016382870aa27aac1e78d590b50c",3,"HallusionBench: You See What You Think? Or You Think What You See? An Image-Context Reasoning Benchmark Challenging for GPT-4V(ision), LLaVA-1.5, and Other Multi-modality Models","To study these two types of VLM mistakes, i.e., language hallucination and visual illusion, the authors curated HallusionBench, an image-context reasoning benchmark that is still challenging to even GPT-4V and LLaVA-1.5.","arXiv.org",2023,"Fuxiao Liu,Tianrui Guan,Zongxia Li,Lichang Chen,Yaser Yacoob,Dinesh Manocha,Tianyi Zhou",3,30,0
"d4ed52f6dd71573c86b65d9f9f171a52e88fb728","https://www.semanticscholar.org/paper/d4ed52f6dd71573c86b65d9f9f171a52e88fb728",3,"DDCoT: Duty-Distinct Chain-of-Thought Prompting for Multimodal Reasoning in Language Models","This study proposes a novel DDCoT prompting that maintains a critical attitude through negative-space prompting and incorporates multimodality into reasoning by first dividing the reasoning responsibility of LLMs into reasoning and recognition and then integrating the visual recognition capability of visual models into the joint reasoning process.","arXiv.org",2023,"Ge Zheng,Bin Yang,Jiajin Tang,Hong-Yu Zhou,Sibei Yang",1,75,0
"11194f500407de81dc8c7468eee5ce64030b800e","https://www.semanticscholar.org/paper/11194f500407de81dc8c7468eee5ce64030b800e",3,"Foundational Models in Medical Imaging: A Comprehensive Survey and Future Vision","A methodical taxonomy of foundation models within the medical domain is offered, proposing a classification system primarily structured around training strategies, while also incorporating additional facets such as application domains, imaging modalities, specific organs of interest, and the algorithms integral to these models.","arXiv.org",2023,"Bobby Azad,Reza Azad,Sania Eskandari,Afshin Bozorgpour,A. Kazerouni,I. Rekik,D. Merhof",0,92,0
"c62711f6b5d8620ba36bc2c378ec6ab53f6e197c","https://www.semanticscholar.org/paper/c62711f6b5d8620ba36bc2c378ec6ab53f6e197c",3,"RoboGen: Towards Unleashing Infinite Data for Automated Robot Learning via Generative Simulation","RoboGen is presented, a generative robotic agent that automatically learns diverse robotic skills at scale via generative simulation and attempts to extract the extensive and versatile knowledge embedded in large-scale models and transfer them to the field of robotics.","arXiv.org",2023,"Yufei Wang,Zhou Xian,Feng Chen,Tsun-Hsuan Wang,Yian Wang,Katerina Fragkiadaki,Zackory M. Erickson,David Held,Chuang Gan",0,127,0
"5ddb51ae85deca14dc7fc8adc07305c22a1ebe0a","https://www.semanticscholar.org/paper/5ddb51ae85deca14dc7fc8adc07305c22a1ebe0a",3,"Qwen-VL: A Frontier Large Vision-Language Model with Versatile Abilities","The Qwen-VL series is introduced, a set of large-scale vision-language models designed to perceive and understand both text and images that outperforms existing Large Vision Language Models (LVLMs).","arXiv.org",2023,"Jinze Bai,Shuai Bai,Shusheng Yang,Shijie Wang,Sinan Tan,Peng Wang,Junyang Lin,Chang Zhou,Jingren Zhou",23,81,7
"fc8988585c6846fdeee33b34779a6a87b92c3e86","https://www.semanticscholar.org/paper/fc8988585c6846fdeee33b34779a6a87b92c3e86",3,"Equivariant Similarity for Vision-Language Foundation Models","This study proposes EqSim, a regularization loss that can be efficiently calculated from any two matched training pairs and easily pluggable into existing image-text retrieval fine-tuning and presents a new challenging benchmark EqBen, the first to focus on ""visual-minimal change"".","arXiv.org",2023,"Tan Wang,Kevin Lin,Linjie Li,Chung-Ching Lin,Zhengyuan Yang,Hanwang Zhang,Zicheng Liu,Lijuan Wang",6,82,2
"0046306876ff2d5600699327e52bc29fa5e9ec91","https://www.semanticscholar.org/paper/0046306876ff2d5600699327e52bc29fa5e9ec91",3,"Transfer Visual Prompt Generator across LLMs","This work investigates the VPG transferability across LLMs, and develops a two-stage transfer framework named VPGTrans, which is simple yet highly effective and demonstrated to significantly speed up the transfer learning process without compromising performance.","arXiv.org",2023,"Ao Zhang,Hao Fei,Yuan Yao,Wei Ji,Li Li,Zhiyuan Liu,Tat-seng Chua",32,61,6
"3130643a5d02f0e849d83bb1f85577a924081f36","https://www.semanticscholar.org/paper/3130643a5d02f0e849d83bb1f85577a924081f36",3,"Paxion: Patching Action Knowledge in Video-Language Foundation Models","This work proposes a novel framework, Paxion, along with a new Discriminative Video Dynamics Modeling (DVDM) objective, and introduces the DVDM objective to train the Knowledge Patcher, which forces the model to encode the correlation between the action text and the correct ordering of video frames.","arXiv.org",2023,"Zhenhailong Wang,Ansel Blume,Sha Li,Genglin Liu,Jaemin Cho,Zineng Tang,Mohit Bansal,Heng Ji",6,63,1
"bfa9df38bd8597d8cdf0c3497fe5e5a5e31f646f","https://www.semanticscholar.org/paper/bfa9df38bd8597d8cdf0c3497fe5e5a5e31f646f",3,"ONE-PEACE: Exploring One General Representation Model Toward Unlimited Modalities","This work releases ONE-PEACE, a highly extensible model with 4B parameters that can seamlessly align and integrate representations across vision, audio, and language modalities, and develops two modality-agnostic pretraining tasks, which align the semantic space of different modalities and capture fine-grained details within modalities concurrently.","arXiv.org",2023,"Peng Wang,Shijie Wang,Junyang Lin,Shuai Bai,Xiaohuan Zhou,Jingren Zhou,Xinggang Wang,Chang Zhou",16,172,4
"6fe2a4f7594abadafc9feb05e96ab7e9e634ce8f","https://www.semanticscholar.org/paper/6fe2a4f7594abadafc9feb05e96ab7e9e634ce8f",3,"Album Storytelling with Iterative Story-aware Captioning and Large Language Models","This work proposes a new iterative album storytelling pipeline, which starts with an initial story and builds a story-aware caption model to refine the captions using the whole story as guidance, then feeds into the LLMs to generate a new refined story.","arXiv.org",2023,"Munan Ning,Yujia Xie,Dongdong Chen,Zeyin Song,Lu Yuan,Yonghong Tian,Qixiang Ye,Liuliang Yuan",3,58,1
"065dcc6074ffc9e314799d97c1757e5d23e7e2b1","https://www.semanticscholar.org/paper/065dcc6074ffc9e314799d97c1757e5d23e7e2b1",3,"S-CLIP: Semi-supervised Vision-Language Learning using Few Specialist Captions","S-CLIP is proposed, a semi-supervised learning method for training CLIP that utilizes additional unpaired images and employs two pseudo-labeling strategies specifically designed for contrastive learning and the language modality.","",2023,"Sangwoo Mo,Min-Kyung Kim,Kyungmin Lee,Jinwoo Shin",0,100,0
"08b562aa8066c2342f0d03824221dea18f0a18d2","https://www.semanticscholar.org/paper/08b562aa8066c2342f0d03824221dea18f0a18d2",3,"Cream: Visually-Situated Natural Language Understanding with Contrastive Reading Model and Frozen Large Language Models","This paper introduces Contrastive Reading Model (Cream), a novel neural architecture designed to enhance the language-image understanding capability of LLMs by capturing intricate details that are often overlooked in existing methods.","arXiv.org",2023,"Geewook Kim,Hodong Lee,D. Kim,Haeji Jung,S. Park,Yoon Kim,Sangdoo Yun,T. Kil,Bado Lee,Seunghyun Park",2,78,0
"0983883619a0ca597d055d0e58da2f514052913d","https://www.semanticscholar.org/paper/0983883619a0ca597d055d0e58da2f514052913d",3,"Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration","This work proposes Macaw-LLM, a novel multi-modal LLM that seamlessly integrates visual, audio, and textual information and builds on the work of previous work on instruction-tuned large language models to handle diverse data modalities and address complex real-world scenarios.","arXiv.org",2023,"Chenyang Lyu,Minghao Wu,Longyue Wang,Xinting Huang,Bingshuai Liu,Zefeng Du,Shuming Shi,Zhaopeng Tu",23,54,6
"8efc20988021ce3b4b05dd44b13e27260ee9b99b","https://www.semanticscholar.org/paper/8efc20988021ce3b4b05dd44b13e27260ee9b99b",3,"Investigating Prompting Techniques for Zero- and Few-Shot Visual Question Answering","It is demonstrated that carefully designed question templates and the integration of additional visual cues, like image captions, can contribute to improved VQA performance, especially when used in conjunction with few-shot examples, however, a limitation in the use of chain-of-thought rationalization is identified, which negatively affects V QA accuracy.","arXiv.org",2023,"Rabiul Awal,Le Zhang,Aishwarya Agrawal",1,45,0
"94053805cd59f2e9a47fe3f080c7e7afefb337cc","https://www.semanticscholar.org/paper/94053805cd59f2e9a47fe3f080c7e7afefb337cc",3,"Generative Pretraining in Multimodality","Emu, a Transformer-based multimodal foundation model, is presented, which can seamlessly generate images and texts in multi-modality context through a one-model-for-all autoregressive training process and demonstrates superb performance compared to state-of-the-art large multimodAL models.","arXiv.org",2023,"Quan Sun,Qiying Yu,Yufeng Cui,Fan Zhang,Xiaosong Zhang,Yueze Wang,Hongcheng Gao,Jingjing Liu,Tiejun Huang,Xinlong Wang",24,74,5
"41c6028c620debae00ca5b30e2db5977225fec57","https://www.semanticscholar.org/paper/41c6028c620debae00ca5b30e2db5977225fec57",3,"mBLIP: Efficient Bootstrapping of Multilingual Vision-LLMs",,"arXiv.org",2023,"Gregor Geigle,Abhay Jain,R. Timofte,Goran Glavavs",0,83,0
"2e3dcf5a5d58ac210d0d87e9f918540a8373211a","https://www.semanticscholar.org/paper/2e3dcf5a5d58ac210d0d87e9f918540a8373211a",3,"GIT-Mol: A Multi-modal Large Language Model for Molecular Science with Graph, Image, and Text","GIT-Mol is introduced, a multi-modal large language model that integrates the structure Graph, Image, and Text information, including the Simplified Molecular Input Line Entry System (SMILES) and molecular captions, and proposes GIT-Former, a novel architecture capable of mapping all modalities into a unified latent space.","arXiv.org",2023,"Peng Liu,Yiming Ren,Zhixiang Ren",0,59,0
"431a2dfd60225e615cf27d044ad00a6ac147501f","https://www.semanticscholar.org/paper/431a2dfd60225e615cf27d044ad00a6ac147501f",3,"Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond","The Qwen-VL series, a set of large-scale vision-language models (LVLMs) designed to perceive and understand both texts and images, set new records for generalist models under similar model scales on a broad range of visual-centric benchmarks.","",2023,"Jinze Bai,Shuai Bai,Shusheng Yang,Shijie Wang,Sinan Tan,Peng Wang,Junyang Lin,Chang Zhou,Jingren Zhou",8,86,1
"772724892819d7e6f15ce536753fdc32d022c0e0","https://www.semanticscholar.org/paper/772724892819d7e6f15ce536753fdc32d022c0e0",3,"A Survey on Image-text Multimodal Models","An exhaustive overview of the present research landscape of image-text multimodal models is offered, introducing a novel classification that segments their evolution into three distinct phases, based on their time of introduction and subsequent impact on the discipline.","arXiv.org",2023,"Ruifeng Guo,Jingxuan Wei,Linzhuang Sun,Bihui Yu,Guiyong Chang,Dawei Liu,Sibo Zhang,Zhengbing Yao,Mingjun Xu,Liping Bu",0,231,0
"96c43227831c4c3b12b7c64809e78674cea3a8a1","https://www.semanticscholar.org/paper/96c43227831c4c3b12b7c64809e78674cea3a8a1",3,"DeepSpeed-VisualChat: Multi-Round Multi-Image Interleave Chat via Multi-Modal Causal Attention","The DeepSpeed-VisualChat framework is presented, designed to optimize Large Language Models by incorporating multi-modal capabilities, with a focus on enhancing the proficiency of Large Vision and Language Models in handling interleaved inputs.","arXiv.org",2023,"Z. Yao,Xiaoxia Wu,Conglong Li,Minjia Zhang,Heyang Qi,Olatunji Ruwase,A. Awan,Samyam Rajbhandari,Yuxiong He",1,47,0
"11a4284e335ba39330b59d9f42ca3272a6166991","https://www.semanticscholar.org/paper/11a4284e335ba39330b59d9f42ca3272a6166991",3,"A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future","A thorough survey of the current research according to the taxonomies of methods within the domain of chain-of-thought reasoning, and describes XoT with frontier applications, covering planning, tool use, and distillation.","arXiv.org",2023,"Zheng Chu,Jingchang Chen,Qianglong Chen,Weijiang Yu,Tao He,Haotian Wang,Weihua Peng,Ming Liu,Bing Qin,Ting Liu",3,211,0
"a5d27bf7a2155d4ca016565a78b52ee90f81624c","https://www.semanticscholar.org/paper/a5d27bf7a2155d4ca016565a78b52ee90f81624c",3,"Beyond Task Performance: Evaluating and Reducing the Flaws of Large Multimodal Models with In-Context Learning","Despite their success, LMMs have flaws that remain unsolved with scaling alone and the effect of ICL on LMMs flaws is nuanced; despite its effectiveness for improved explainability, abstention, and instruction following, ICL does not improve compositional abilities, and actually even amplifies hallucinations.","arXiv.org",2023,"Mustafa Shukor,Alexandre Ramé,Corentin Dancette,M. Cord",0,95,0
"f220d3218f84340d1e06e01b89d9c3d64e61edd1","https://www.semanticscholar.org/paper/f220d3218f84340d1e06e01b89d9c3d64e61edd1",3,"SimVLG: Simple and Efficient Pretraining of Visual Language Generative Models","A one-stage, single-loss framework for the pre-training of computationally intensive vision-language generative models, leveraging frozen pre-trained large language models (LLMs), which effectively compacts the visual information while preserving the richness of semantic content, leading to fast convergence without sacrificing performance.","arXiv.org",2023,"Yiren Jian,Tingkai Liu,Yunzhe Tao,Soroush Vosoughi,HX Yang",0,59,0
"23684a07517870cffd1f97fafbaae16ba22bd2b7","https://www.semanticscholar.org/paper/23684a07517870cffd1f97fafbaae16ba22bd2b7",3,"Large AI Models in Health Informatics: Applications, Challenges, and the Future","Seven key sectors in which large AI models are applicable and might have substantial influence, including 1) bioinformatics; 2) medical diagnosis; 3) medical imaging; 4) medical informatics; 5) medical education; 6) public health; and 7) medical robotics are identified.","IEEE journal of biomedical and health informatics",2023,"Jianing Qiu,Lin Li,Jiankai Sun,Jiachuan Peng,Peilun Shi,Rui Zhang,Yinzhao Dong,Kyle Lam,F. P. Lo,Bo Xiao,Wu Yuan,Dong Xu,Benny P. L. Lo",23,345,0
"c9dbdae8146b9f97e254f5d26fd6efde96eaa703","https://www.semanticscholar.org/paper/c9dbdae8146b9f97e254f5d26fd6efde96eaa703",3,"Med-Flamingo: a Multimodal Medical Few-shot Learner","Med-Flamingo improves performance in generative medical VQA by up to 20\% in clinician's rating and firstly enables multimodal medical few-shot adaptations, such as rationale generation, as well as releasing the model, code, and evaluation app.","arXiv.org",2023,"Michael Moor,Qian Huang,Shirley Wu,Michihiro Yasunaga,C. Zakka,Yashodhara Dalmia,E. Reis,P. Rajpurkar,J. Leskovec",9,30,3
"5b038c1a93967072cc76689fd805e756f804cc42","https://www.semanticscholar.org/paper/5b038c1a93967072cc76689fd805e756f804cc42",3,"Large Models for Time Series and Spatio-Temporal Data: A Survey and Outlook","This survey coalesces the latest strides in large model-centric research on time series and spatio-temporal data, underscoring the solid foundations, current advances, practical applications, abundant resources, and future research opportunities.","arXiv.org",2023,"Ming Jin,Qingsong Wen,Yuxuan Liang,Chaoli Zhang,Siqiao Xue,Xue Wang,James Y. Zhang,Yi Wang,Haifeng Chen,Xiaoli Li,Shirui Pan,Vincent S. Tseng,Yu Zheng,Lei Chen,Hui Xiong",4,343,0
"cb5f53cd7d43ac6f48abf4dde6fc58e53a083b0e","https://www.semanticscholar.org/paper/cb5f53cd7d43ac6f48abf4dde6fc58e53a083b0e",3,"Accelerating the integration of ChatGPT and other large‐scale AI models into biomedical research and healthcare","","MedComm – Future Medicine",2023,"Ding‐Qiao Wang,Long‐Yu Feng,Jin‐Guo Ye,Jin‐Gen Zou,Yingfeng Zheng",14,99,0
"d92c797f587ce7f1b001920ab9e6b7d31960bd77","https://www.semanticscholar.org/paper/d92c797f587ce7f1b001920ab9e6b7d31960bd77",3,"RemoteCLIP: A Vision Language Foundation Model for Remote Sensing","RemoteCLIP is proposed, the first vision-language foundation model for remote sensing that aims to learn robust visual features with rich semantics, as well as aligned text embeddings for seamless downstream application and consistently outperforms baseline foundation models across different model scales.","arXiv.org",2023,"F. Liu,Delong Chen,Zhan-Rong Guan,Xiaocong Zhou,Jiale Zhu,Jun Zhou",5,109,0
"4df485f8ad1c55f8a3fd3f06b5cc46bcb4f0ddc1","https://www.semanticscholar.org/paper/4df485f8ad1c55f8a3fd3f06b5cc46bcb4f0ddc1",3,"A Foundation LAnguage-Image model of the Retina (FLAIR): Encoding expert knowledge in text supervision","Interestingly, FLAIR outperforms by a large margin more generalist, larger-scale image-language models, which emphasizes the potential of embedding experts' domain knowledge and the limitations of generalist models in medical imaging.","arXiv.org",2023,"Julio Silva-Rodríguez,H. Chakor,R. Kobbi,J. Dolz,Ismail Ben Ayed",0,108,0
"36a584b56b146cb61949e923d4177e9a5ac26be9","https://www.semanticscholar.org/paper/36a584b56b146cb61949e923d4177e9a5ac26be9",3,"Visual Cropping Improves Zero-Shot Question Answering of Multimodal Large Language Models","This work shows that multimodal LLMs zero-shot accuracy in answering visual questions is very sensitive to the size of the visual subject of the question, declining up to $46\%$ with size, and proposes three automatic visual cropping methods as inference time mechanisms to improve their zero- shot performance.","arXiv.org",2023,"Jiarui Zhang,Mahyar Khayatkhoei,P. Chhikara,Filip Ilievski",0,27,0
"90cd86b3c157e40cbaf1076f69cbd38d9c0781b9","https://www.semanticscholar.org/paper/90cd86b3c157e40cbaf1076f69cbd38d9c0781b9",3,"UnICLAM: Contrastive Representation Learning with Adversarial Masking for Unified and Interpretable Medical Vision Question Answering","Experimental results on VQA-RAD and SLAKE public benchmarks demonstrate that UnICLAM outperforms existing 11 state-of-the-art Medical-VQA models and makes an additional discussion about the performance of UnicLAM in diagnosing heart failure, verifying that it exhibits superior few-shot adaption performance in practical disease diagnosis.","arXiv.org",2022,"Chenlu Zhan,Peng Peng,Hongsen Wang,Tao Chen,Hongwei Wang",2,46,1
"785650a805851c7e945523e495c5a523c60f72a4","https://www.semanticscholar.org/paper/785650a805851c7e945523e495c5a523c60f72a4",3,"Open-Ended Medical Visual Question Answering Through Prefix Tuning of Language Models","This work focuses on open-ended VQA and motivated by the recent advances in language models consider it as a generative task, and introduces a novel method particularly suited for small, domain-specific, medical datasets.","International Conference on Medical Image Computing and Computer-Assisted Intervention",2023,"Tom van Sonsbeek,Mohammad Mahdi Derakhshani,Ivona Najdenkoska,Cees G. M. Snoek,M. Worring",7,35,3
"f7ea746cd2cc25628a7a553ac27d228198be42cb","https://www.semanticscholar.org/paper/f7ea746cd2cc25628a7a553ac27d228198be42cb",3,"Pre-trained multilevel fuse network based on vision-conditioned reasoning and bilinear attentions for medical image visual question answering","This paper proposes a new pre-trained multilevel fusion network based on Vision-conditioned reasoning and Bilinear attentions for Med-VQA (VB-MVZA), which achieves more significant accuracy than the baseline models for open-ended questions and more powerful for language-bias Med- VQA datasets.","Journal of Supercomputing",2023,"Linqin Cai,Haodu Fang,Zhiqing Li",0,47,0
"ac4d13b6a4f9fb67337099f4602135a0351f5c99","https://www.semanticscholar.org/paper/ac4d13b6a4f9fb67337099f4602135a0351f5c99",3,"Towards Medical Artificial General Intelligence via Knowledge-Enhanced Multimodal Pretraining","The proposed MOTOR successfully mimics the human practice of fulfilling a""medical student"" to accelerate the process of becoming a""specialist"" and believes that this work makes a significant stride in realizing MAGI.","arXiv.org",2023,"Bingqian Lin,Zicong Chen,Mingjie Li,Haokun Lin,Hang Xu,Yi Zhu,Jian-zhuo Liu,Wenjia Cai,Lei Yang,Shen Zhao,Chenfei Wu,Ling Chen,Xiaojun Chang,Yi Yang,L. Xing,Xiaodan Liang",0,60,0
"64fa56962dd0f4bbe206be6142fbe0315c4e7c2f","https://www.semanticscholar.org/paper/64fa56962dd0f4bbe206be6142fbe0315c4e7c2f",3,"Multi-modal Pre-training for Medical Vision-language Understanding and Generation: An Empirical Study with A New Benchmark","RadioGraphy Captions (RGC), a high-quality, multi-modality radiographic dataset containing 18,434 image-caption pairs collected from an open-access online database MedPix, is proposed, which can be used as a pre-training dataset or a new benchmark for medical report generation and medical image-text retrieval.","arXiv.org",2023,"Li Xu,Bo Liu,Ameer Hamza Khan,Lu Fan,Xiao-Ming Wu",0,71,0
"534675abb9d72fc0c08d080d4f73335ceb75902c","https://www.semanticscholar.org/paper/534675abb9d72fc0c08d080d4f73335ceb75902c",3,"Multimodal Prompt Retrieval for Generative Visual Question Answering","A novel generative model enhanced by multi-modal prompt retrieval (MPR) that integrates retrieved prompts and multimodal features to generate answers in free text that enables rapid zero-shot dataset adaptation to unseen data distributions and open-set answer labels across datasets.","arXiv.org",2023,"Timothy Ossowski,Junjie Hu",0,46,0
"e0e9ba0c01d441e1fdcb8628d3f743d387b0b017","https://www.semanticscholar.org/paper/e0e9ba0c01d441e1fdcb8628d3f743d387b0b017",3,"UIT-Saviors at MEDVQA-GI 2023: Improving Multimodal Learning with Image Enhancement for Gastrointestinal Visual Question Answering","This study highlights the dominance of Transformer-based vision models over the CNNs and demonstrates the effectiveness of the image enhancement process, with six out of the eight vision models achieving better F1-Score.","Conference and Labs of the Evaluation Forum",2023,"T. M. Thai,A. T. Vo,Hao K. Tieu,Linh Bui,T. Nguyen",1,43,0
"304f8b4edea01fdb5a2f7f8b998c83188deeccff","https://www.semanticscholar.org/paper/304f8b4edea01fdb5a2f7f8b998c83188deeccff",3,"Towards Generalist Biomedical AI","Med-PaLM M is a large multimodal generative model that flexibly encodes and interprets biomedical data including clinical language, imaging, and genomics with the same set of model weights and reaches performance competitive with or exceeding the state of the art on all MultiMedBench tasks, often surpassing specialist models by a wide margin.","arXiv.org",2023,"Tao Tu,Shekoofeh Azizi,Danny Driess,M. Schaekermann,Mohamed Amin,Pi-Chuan Chang,Andrew Carroll,Chuck Lau,Ryutaro Tanno,Ira Ktena,B. Mustafa,Aakanksha Chowdhery,Yun Liu,Simon Kornblith,David J. Fleet,P. A. Mansfield,Sushant Prakash,Renee C Wong,S. Virmani,Christopher Semturs,S. S. Mahdavi,Bradley Green,Ewa Dominowska,B. A. Y. Arcas,J. Barral,D. Webster,G. Corrado,Yossi Matias,K. Singhal,Peter R. Florence,A. Karthikesalingam,Vivek Natarajan",17,122,1
"a0476578761e983d5ab2083abab07b81236c1d58","https://www.semanticscholar.org/paper/a0476578761e983d5ab2083abab07b81236c1d58",3,"Asymmetric cross-modal attention network with multimodal augmented mixup for medical visual question answering","A new Asymmetric Cross Modal Attention network called ACMA is proposed, which constructs an image- guided attention and a question-guided attention to improve multimodal interactions from insufficient data.","Artif. Intell. Medicine",2023,"Yong Li,Qihao Yang,Fu Lee Wang,Lap-Kei Lee,Yingying Qu,Tianyong Hao",0,59,0
"749104d1a207f5bc192c7d95a12856b5e7f84d1f","https://www.semanticscholar.org/paper/749104d1a207f5bc192c7d95a12856b5e7f84d1f",3,"Mapping medical image-text to a joint space via masked modeling.","A self-supervised learning paradigm, multi-modal masked autoencoders (M3AE) is introduced, which learns to map medical images and texts to a joint space by reconstructing pixels and tokens from randomly masked images andtext.","Medical Image Analysis",2023,"Zhihong Chen,Yuhao Du,Jinpeng Hu,Yang Liu,Guanbin Li,Xiang Wan,Tsung-Hui Chang",0,43,0
"72b06aef94f798ad9035b5775c186fd2fd6a8f38","https://www.semanticscholar.org/paper/72b06aef94f798ad9035b5775c186fd2fd6a8f38",3,"Multi-domain improves out-of-distribution and data-limited scenarios for medical image analysis","The incorporation of diverse medical image domains, including different imaging modalities like X-ray, MRI, CT, and ultrasound images, as well as various viewpoints such as axial, coronal, and sagittal views, are introduced to underscore the superior generalization capabilities of multi-domain models, particularly in scenarios characterized by limited data availability and out-of-distribution.","arXiv.org",2023,"Ece Ozkan,Xavier Boix",0,64,0
"82a9b8984e26fdf234431459bdb445fbcfc3cb76","https://www.semanticscholar.org/paper/82a9b8984e26fdf234431459bdb445fbcfc3cb76",2,"Visual Instruction Tuning with Polite Flamingo","Polite Flamingo is introduced, a multi-modal response rewriter that transforms raw annotations into a more appealing,""polite""format and demonstrates its advantages in both multi- modal understanding and response politeness according to automated and human evaluations.","arXiv.org",2023,"Delong Chen,Jianfeng Liu,Wenliang Dai,Baoyuan Wang",9,78,1
"bb1083425517bdac8d9a6438fcf5032543acb20e","https://www.semanticscholar.org/paper/bb1083425517bdac8d9a6438fcf5032543acb20e",2,"Evaluation and Analysis of Hallucination in Large Vision-Language Models","This paper proposes Hallucination Evaluation based on Large Language Models (HaELM), an LLM-based hallucination evaluation framework that achieves an approximate 95% performance comparable to ChatGPT and has additional advantages including low cost, reproducibility, privacy preservation and local deployment.","arXiv.org",2023,"Junyan Wang,Yi Zhou,Guohai Xu,Pengcheng Shi,Chenlin Zhao,Haiyang Xu,Qinghao Ye,Mingshi Yan,Ji Zhang,Jihua Zhu,J. Sang,Haoyu Tang",5,35,1
"0f8d12775a4685575f1489796b5dee9e11fbdfb5","https://www.semanticscholar.org/paper/0f8d12775a4685575f1489796b5dee9e11fbdfb5",2,"OphGLM: Training an Ophthalmology Large Language-and-Vision Assistant based on Instructions and Dialogue","This paper establishes a new ophthalmic multimodal instruction-following and dialogue fine-tuning dataset based on disease-related knowledge data and publicly available real-world medical dialogue, and introduces visual ability into the large language model to complete the OphGLM.","arXiv.org",2023,"Weihao Gao,Zhuo Deng,Zhiyuan Niu,Fuju Rong,Chucheng Chen,Zheng Gong,Wenze Zhang,Daimin Xiao,Fangjun Li,Zhenjie Cao,Zhaoyi Ma,Wenbin Wei,Lan Ma",4,25,1
"b18daa14486920016c4664c3ed1759f2de1ba854","https://www.semanticscholar.org/paper/b18daa14486920016c4664c3ed1759f2de1ba854",2,"Exploring Transfer Learning in Medical Image Segmentation using Vision-Language Models","The results indicate that VLSMs trained in natural image-text pairs transfer reasonably to the medical domain in zero-shot settings when prompted appropriately for non-radiology photographic modalities; when finetuned, they obtain comparable performance to conventional architectures, even in X-rays and ultrasound modalities.","arXiv.org",2023,"K. Poudel,Manish Dhakal,Prasiddha Bhandari,Rabin Adhikari,Safal Thapaliya,Bishesh Khanal",1,80,0
"59ba440bdce4b9b963124a46ee87b63b67c4c58c","https://www.semanticscholar.org/paper/59ba440bdce4b9b963124a46ee87b63b67c4c58c",2,"Training CLIP models on Data from Scientific Papers","Experiments on small-scale CLIP models (ViT B/32) show that model performance increases on average, but only moderately, indicating that using the data sources considered in the paper to train large-scale ClIP models is a worthwile research direction.","arXiv.org",2023,"Calvin Metzger",0,23,0
"e45036dddb5f27d3e87d2f14a2d9e6a402e7b5b7","https://www.semanticscholar.org/paper/e45036dddb5f27d3e87d2f14a2d9e6a402e7b5b7",2,"SheetCopilot: Bringing Software Productivity to the Next Level through Large Language Models","This work proposes a SheetCopilot agent that takes natural language task and control spreadsheet to fulfill the requirements, and proposes a set of atomic actions as an abstraction of spreadsheet software functionalities.","arXiv.org",2023,"Hongxin Li,Jingran Su,Yuntao Chen,Qing Li,Zhaoxiang Zhang",3,92,1
"28c6ac721f54544162865f41c5692e70d61bccab","https://www.semanticscholar.org/paper/28c6ac721f54544162865f41c5692e70d61bccab",2,"A Survey on Large Language Model based Autonomous Agents","A systematic review of the field of LLM-based autonomous agents from a holistic perspective, and proposes a unified framework that encompasses a majority of the previous work.","arXiv.org",2023,"Lei Wang,Chengbang Ma,Xueyang Feng,Zeyu Zhang,Hao-ran Yang,Jingsen Zhang,Zhi-Yang Chen,Jiakai Tang,Xu Chen,Yankai Lin,Wayne Xin Zhao,Zhewei Wei,Ji-rong Wen",53,187,5
"4f63c5a89c7299a864c6c48aa1844fb0fe8c9437","https://www.semanticscholar.org/paper/4f63c5a89c7299a864c6c48aa1844fb0fe8c9437",2,"Survey of Vulnerabilities in Large Language Models Revealed by Adversarial Attacks","Research in the emerging interdisciplinary field of adversarial attacks on LLMs, a subfield of trustworthy ML, is surveyed, combining the perspectives of Natural Language Processing and Security.","arXiv.org",2023,"Erfan Shayegani,Md. Abdullah Al Mamun,Yu Fu,Pedram Zaree,Yue Dong,Nael B. Abu-Ghazaleh",1,182,0
"cf7d69709bdeddd561c183178bbc1f0c2e156a08","https://www.semanticscholar.org/paper/cf7d69709bdeddd561c183178bbc1f0c2e156a08",2,"Analyzing Modular Approaches for Visual Question Decomposition","ViperGPT's reported gains over BLIP-2 can be attributed to its selection of task-specific modules, and when it is run using a more task-agnostic selection of modules, these gains go away, but on some benchmarks, modular approaches significantly benefit by representing subtasks with natural language, instead of code.","arXiv.org",2023,"Apoorv Khandelwal,Ellie Pavlick,Chen Sun",0,39,0
"ed9943d73eb42116fe33564b5065c78b5ca0b16e","https://www.semanticscholar.org/paper/ed9943d73eb42116fe33564b5065c78b5ca0b16e",2,"RestGPT: Connecting Large Language Models with Real-World Applications via RESTful APIs","This paper introduces RestGPT, which leverages LLMs to solve user requests by connecting with RESTful APIs and proposes a coarse-to-fine online planning mechanism to enhance the ability of planning and API selection.","arXiv.org",2023,"Yifan Song,Weimin Xiong,Dawei Zhu,Chengzu Li,Ke Wang,Ye Tian,Sujian Li",10,20,0
"53df959bcf6499c45e316086a96a624389a39a52","https://www.semanticscholar.org/paper/53df959bcf6499c45e316086a96a624389a39a52",2,"Accountable Textual-Visual Chat Learns to Reject Human Instructions in Image Re-creation","This paper constructs two new multimodal datasets and proposes a two-state training procedure to train the image auto-encoder and auto-regressive transformer from scratch, and provides comprehensive analyses of experimental results in terms of re-created image quality, answer accuracy, and the model behavior when faced with uncertainty and imperfect user queries.","",2023,"Zhiwei Zhang,Yuliang Liu",0,122,0
"ac7771c332da42b29a913b116bd6ef622cbf89cf","https://www.semanticscholar.org/paper/ac7771c332da42b29a913b116bd6ef622cbf89cf",2,"TaskMatrix.AI: Completing Tasks by Connecting Foundation Models with Millions of APIs","The vision of how to build such an ecosystem is presented, each key component is explained, and study cases are used to illustrate both the feasibility of this vision and the main challenges the authors need to address next.","Intelligent Computing",2023,"Yaobo Liang,Chenfei Wu,Ting Song,Wenshan Wu,Yan Xia,Yu Liu,Yangyiwen Ou,Shuai Lu,Lei Ji,Shaoguang Mao,Yun Wang,Linjun Shou,Ming Gong,Nan Duan",68,30,2
"352420ee61a8da783ca7750170793613b18b8d9c","https://www.semanticscholar.org/paper/352420ee61a8da783ca7750170793613b18b8d9c",2,"Tool Learning with Foundation Models","A systematic investigation of tool learning is presented, including its cognitive origins, the paradigm shift of foundation models, and the complementary roles of tools and models to inspire future research in integrating tools with foundation models.","arXiv.org",2023,"Yujia Qin,Shengding Hu,Yankai Lin,Weize Chen,Ning Ding,Ganqu Cui,Zheni Zeng,Yufei Huang,Chaojun Xiao,Chi Han,Y. Fung,Yusheng Su,Huadong Wang,Cheng Qian,Runchu Tian,Kunlun Zhu,Shi Liang,Xingyu Shen,Bokai Xu,Zhen Zhang,Yining Ye,Bo Li,Ziwei Tang,Jing Yi,Yu Zhu,Zhenning Dai,Lan Yan,Xin Cong,Ya-Ting Lu,Weilin Zhao,Yuxiang Huang,Jun-Han Yan,Xu Han,Xian Sun,Dahai Li,Jason Phang,Cheng Yang,Tongshuang Wu,Heng Ji,Zhiyuan Liu,Maosong Sun",61,237,4
"2195676f111ad492c50f4d4c96abb2bd3d72f7fc","https://www.semanticscholar.org/paper/2195676f111ad492c50f4d4c96abb2bd3d72f7fc",2,"Instruct2Act: Mapping Multi-modality Instructions to Robotic Actions with Large Language Model","This paper presents Instruct2Act, a framework that utilizes Large Language Models to map multi-modal instructions to sequential actions for robotic manipulation tasks, employing the LLM model to generate Python programs that constitute a comprehensive perception, planning, and action loop for robotic tasks.","arXiv.org",2023,"Siyuan Huang,Zhengkai Jiang,Hao-Wen Dong,Y. Qiao,Peng Gao,Hongsheng Li",22,55,3
"13a5140fc0b269c408ecfc666cb297410bc753c5","https://www.semanticscholar.org/paper/13a5140fc0b269c408ecfc666cb297410bc753c5",2,"Matcher: Segment Anything with One Shot Using All-Purpose Feature Matching","This work presents Matcher, which segments anything with one shot by integrating an all-purpose feature extraction model and a class-agnostic segmentation model, and proposes a novel instance-level matching strategy for controllable mask merging.","arXiv.org",2023,"Yang Liu,Muzhi Zhu,Hengtao Li,Hao Chen,Xinlong Wang,Chunhua Shen",7,44,1
"8da9b1436212b233fc49c7daf1ba15c22874ff5a","https://www.semanticscholar.org/paper/8da9b1436212b233fc49c7daf1ba15c22874ff5a",2,"CREATOR: Tool Creation for Disentangling Abstract and Concrete Reasoning of Large Language Models","The proposed CREATOR, a novel framework that enables LLMs to create their own tools using documentation and code realization, disentangles abstract tool creation and concrete decision execution, resulting in improved performance.","",2023,"Cheng Qian,Chi Han,Y. Fung,Yujia Qin,Zhiyuan Liu,Heng Ji",7,35,1
"90027ca7802645671a69b00b65e1fa94e6b63544","https://www.semanticscholar.org/paper/90027ca7802645671a69b00b65e1fa94e6b63544",2,"ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models","This study proposes a modular paradigm ReWOO (Reasoning WithOut Observation) that detaches the reasoning process from external observations, thus significantly reducing token consumption and demonstrating robustness under tool-failure scenarios.","arXiv.org",2023,"Binfeng Xu,Zhiyuan Peng,Bowen Lei,Subhabrata Mukherjee,Yuchen Liu,Dongkuan Xu",17,41,1
"69335077fcacbff7a7cf25697da1949e6bdfa968","https://www.semanticscholar.org/paper/69335077fcacbff7a7cf25697da1949e6bdfa968",2,"The Art of SOCRATIC QUESTIONING: Recursive Thinking with Large Language Models","The qualitative analysis clearly shows that the intermediate reasoning steps elicited by SOCRATIC QUESTIONING are similar to humans' recursively thinking process of complex reasoning problems.","",2023,"Jingyuan Qi,Zhiyang Xu,Ying Shen,Minqian Liu,dingnan jin,Qifan Wang,Lifu Huang",0,43,0
"615962d8969c8e0ffe43319689dce6c50cbf1f29","https://www.semanticscholar.org/paper/615962d8969c8e0ffe43319689dce6c50cbf1f29",2,"Responsible Task Automation: Empowering Large Language Models as Responsible Task Automators","This paper presents Responsible Task Automation (ResponsibleTA) as a fundamental framework to facilitate responsible collaboration between LLM-based coordinators and executors for task automation with three empowered capabilities: predicting the feasibility of the commands for executors, verifying the completeness of executors and enhancing the security.","arXiv.org",2023,"Zhizheng Zhang,Xiaoyi Zhang,Wenxuan Xie,Yan Lu",2,46,1
"adae495ceae5399f581aa7755142786369abac17","https://www.semanticscholar.org/paper/adae495ceae5399f581aa7755142786369abac17",2,"RestGPT: Connecting Large Language Models with Real-World RESTful APIs","This paper proposes RestGPT, which exploits the power of LLMs and conducts a coarse-to-fine online planning mechanism to enhance the abilities of task decomposition and API selection and paves a new way towards AGI.","",2023,"Yifan Song,Weimin Xiong,Dawei Zhu,Wenhao Wu,Han Qian,Mingbo Song,Hailiang Huang,Chengzu Li,Ke Wang,Rong Yao,Ye Tian,Sujian Li",3,25,1
"473eb062612a17c965eaa62136322f0dec6b1f8e","https://www.semanticscholar.org/paper/473eb062612a17c965eaa62136322f0dec6b1f8e",2,"Data-Copilot: Bridging Billions of Data and Humans with Autonomous Workflow","This work proposes Data-Copilot, an LLM-based system that connects numerous data sources on one end and caters to diverse human demands on the other end, and autonomously transforms raw data into visualization results that best match the user's intent.","arXiv.org",2023,"Wenqi Zhang,Yongliang Shen,Weiming Lu,Y. Zhuang",9,37,1
"9dbb39eccbcd31b8f6b4ff0a2c96f61a7c34e54b","https://www.semanticscholar.org/paper/9dbb39eccbcd31b8f6b4ff0a2c96f61a7c34e54b",2,"RM-PRT: Realistic Robotic Manipulation Simulator and Benchmark with Progressive Reasoning Tasks","A realistic robotic manipulation simulator is introduced and a Robotic Manipulation with Progressive Reasoning Tasks (RM-PRT) benchmark is built on this basis, which evaluates the robot's ability to understand natural language instructions in two modes of adsorption and grasping.","arXiv.org",2023,"Pengzhen Ren,Kaiwen Zhang,Hetao Zheng,Zixuan Li,Yuhang Wen,Fengda Zhu,Mas Ma,Xiaodan Liang",1,63,0
"bbcd5cc4bf6c77282e88cae07f7f2adb1da818ca","https://www.semanticscholar.org/paper/bbcd5cc4bf6c77282e88cae07f7f2adb1da818ca",2,"Testing the Depth of ChatGPT's Comprehension via Cross-Modal Tasks Based on ASCII-Art: GPT3.5's Abilities in Regard to Recognizing and Generating ASCII-Art Are Not Totally Lacking","This work examines GPT3.5's aptitude for visual tasks, where the inputs feature content provided as ASCII-art without overt distillation into a lingual summary, and conducts experiments analyzing the model's performance on image recognition tasks after various transforms typical in visual settings.","arXiv.org",2023,"David Bayani",0,85,0
"da96ec9c32d63292e506ba8f8ea8e838df998c02","https://www.semanticscholar.org/paper/da96ec9c32d63292e506ba8f8ea8e838df998c02",2,"StableLLaVA: Enhanced Visual Instruction Tuning with Synthesized Image-Dialogue Data","This work proposes a novel data collection methodology that synchronously synthesizes images and dialogues for visual instruction tuning, marrying the abilities of ChatGPT and text-to-image generative models to yield a diverse and controllable dataset with varied image content.","arXiv.org",2023,"Yanda Li,Chi Zhang,Gang Yu,Zhibin Wang,Bin Fu,Guosheng Lin,Chunhua Shen,Ling Chen,Yunchao Wei",2,29,1
"3b36d16985286b03e06e8404a7be49a9713d37b9","https://www.semanticscholar.org/paper/3b36d16985286b03e06e8404a7be49a9713d37b9",2,"Confucius: Iterative Tool Learning from Introspection Feedback by Easy-to-Difficult Curriculum","The Confucius is proposed, a novel tool learning framework to train large language models to use complicated tools in real-world scenarios, which contains two main phases: a multi-stage learning method to teach the LLM to use various tools from an easy-to-difficult curriculum and the Iterative Self-instruct from Introspective Feedback to dynamically construct the dataset to improve the ability to use the complicated tool.","arXiv.org",2023,"Shen Gao,Zhengliang Shi,Minghang Zhu,Bowen Fang,Xin Xin,Pengjie Ren,Zhumin Chen,Jun Ma",3,42,0
"c237a22698223e4060d83027f399f4fb2aa24291","https://www.semanticscholar.org/paper/c237a22698223e4060d83027f399f4fb2aa24291",2,"Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations","InteRecAgent enables traditional recommender systems, such as those ID-based matrix factorization models, to become interactive systems with a natural language interface through the integration of LLMs, and achieves satisfying performance as a conversational recommender system.","arXiv.org",2023,"Xu Huang,Jianxun Lian,Yuxuan Lei,Jing Yao,Defu Lian,Xing Xie",5,51,0
"1e7a2f9f9441462e92ee349f00414aff49617caa","https://www.semanticscholar.org/paper/1e7a2f9f9441462e92ee349f00414aff49617caa",2,"Enhancing Subtask Performance of Multi-modal Large Language Model","This study first selects multiple pre-trained models focused on the same subtask based on distinct evaluation approaches, and then invokes these models in parallel to process input data and generate corresponding subtask results.","arXiv.org",2023,"Yongqiang Zhao,Zhenyu Li,Feng Zhang,Xinhai Xu,Donghong Liu",0,22,0
"a1426b13b74dbad17b34606d25aabe1d61f6e11a","https://www.semanticscholar.org/paper/a1426b13b74dbad17b34606d25aabe1d61f6e11a",2,"CRAFT: Customizing LLMs by Creating and Retrieving from Specialized Toolsets","CRAFT is designed to be flexible and offers a plug-and-play approach to adapt off-the-shelf LLMs to unseen domains and modalities, without any finetuning, and achieves substantial improvements compared to strong baselines.","arXiv.org",2023,"Lifan Yuan,Yangyi Chen,Xingyao Wang,Y. Fung,Hao Peng,Heng Ji",2,61,0
"8918e3cc21ecaf81532e452d3b9518360d14860e","https://www.semanticscholar.org/paper/8918e3cc21ecaf81532e452d3b9518360d14860e",2,"Reinforced UI Instruction Grounding: Towards a Generic UI Task Automation API","This work builds a multimodal model to ground natural language instructions in given UI screenshots as a generic UI task automation executor and proposes an innovative Reinforcement Learning (RL) based algorithm to supervise the tokens in such sequence jointly with visually semantic metrics, which effectively strengthens the spatial decoding capability of the pixel-to-sequence paradigm.","arXiv.org",2023,"Zhizheng Zhang,Wenxuan Xie,Xiaoyi Zhang,Yan Lu",0,48,0
"112597088d25b21b3ad2d77f107b816e6b5bb36c","https://www.semanticscholar.org/paper/112597088d25b21b3ad2d77f107b816e6b5bb36c",2,"In-Context Learning with Iterative Demonstration Selection","Using zero-shot chain-of-thought reasoning (Zero-shot-CoT), Iterative Demonstration Selection (IDS) iteratively selects examples that are diverse but still strongly correlated with the test sample as ICL demonstrations.","arXiv.org",2023,"Chengwei Qin,Aston Zhang,Anirudh Dagar,Wenming Ye",0,70,0
"96a6df2b4aa50cfbd8984933e9c66b0763fc08a6","https://www.semanticscholar.org/paper/96a6df2b4aa50cfbd8984933e9c66b0763fc08a6",2,"Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V","The experiments show that GPT-4V with SoM outperforms the state-of-the-art fully-finetuned referring segmentation model on RefCOCOg in a zero-shot setting and the effectiveness of SoM on a wide range of fine-grained vision and multimodal tasks is validated.","arXiv.org",2023,"Jianwei Yang,Hao Zhang,Feng Li,Xueyan Zou,Chun-yue Li,Jianfeng Gao",7,51,0
"06083548a24fb6de86abdfb0369df9f355913f7b","https://www.semanticscholar.org/paper/06083548a24fb6de86abdfb0369df9f355913f7b",2,"ToolChain*: Efficient Action Space Navigation in Large Language Models with A* Search","This work proposes ToolChain*, an efficient tree search-based planning algorithm for LLM-based agents that formulates the entire action space as a decision tree, where each node represents a possible API function call involved in a solution plan.","arXiv.org",2023,"Yuchen Zhuang,Xiang Chen,Tong Yu,Saayan Mitra,Victor S. Bursztyn,Ryan A. Rossi,Somdeb Sarkhel,Chao Zhang",0,60,0
"688b4650e4e2ab1d6a80177e8c7260b1be22cfd7","https://www.semanticscholar.org/paper/688b4650e4e2ab1d6a80177e8c7260b1be22cfd7",2,"MoqaGPT : Zero-Shot Multi-modal Open-domain Question Answering with Large Language Model","MoqaGPT, a straightforward and flexible framework built upon LLMs, retrieves and extracts answers from each modality separately, then fuses this multi-modal information using LLMs to produce a final answer.","arXiv.org",2023,"Le Zhang,Yihong Wu,Fengran Mo,Jian-Yun Nie,Aishwarya Agrawal",0,30,0
"8e3e7deb95d2a984cba615ec847e64f354626cdf","https://www.semanticscholar.org/paper/8e3e7deb95d2a984cba615ec847e64f354626cdf",2,"WebWISE: Web Interface Control and Sequential Exploration with Large Language Models","This paper investigates using a Large Language Model (LLM) to automatically perform web software tasks using click, scroll, and text input operations using filtered Document Object Model elements as observations and performs tasks step-by-step, sequentially generating small programs based on the current observations.","arXiv.org",2023,"Heyi Tao,TV Sethuraman,Michal Shlapentokh-Rothman,Derek Hoiem",0,35,0
"0212dca18cd0765deed0b6ba80a796f0ad46e066","https://www.semanticscholar.org/paper/0212dca18cd0765deed0b6ba80a796f0ad46e066",2,"mPLUG-Octopus: The Versatile Assistant Empowered by A Modularized End-to-End Multimodal LLM",,"ACM Multimedia",2023,"Qinghao Ye,Haiyang Xu,Mingshi Yan,Chenlin Zhao,Junyang Wang,Xiaoshan Yang,Ji Zhang,Fei Huang,J. Sang,Changsheng Xu",0,10,0
"1df754afa6d27b630ecf91d15bb1ae4ed12a194e","https://www.semanticscholar.org/paper/1df754afa6d27b630ecf91d15bb1ae4ed12a194e",2,"A Survey on Interpretable Cross-modal Reasoning","This survey delves into the realm of interpretable cross-modal reasoning (I-CMR), where the objective is not only to achieve high predictive performance but also to provide human-understandable explanations for the results.","arXiv.org",2023,"Dizhan Xue,Shengsheng Qian,Zuyi Zhou,Changsheng Xu",0,116,0
"6f2f20c78d311c4ce8be0bb6855177c5169bb6cd","https://www.semanticscholar.org/paper/6f2f20c78d311c4ce8be0bb6855177c5169bb6cd",2,"MuseChat: A Conversational Music Recommendation System for Videos","The evaluations show that MuseChat surpasses existing state-of-the-art models in music retrieval tasks and pioneers the integration of the recommendation process within a natural language framework.","arXiv.org",2023,"Zhikang Dong,Bin Chen,Xiulong Liu,Pawel Polak,Peng Zhang",0,61,0
"8892b0937b1e6f3892647a812841e9dccacd7a34","https://www.semanticscholar.org/paper/8892b0937b1e6f3892647a812841e9dccacd7a34",2,"Dynamic Texts From UAV Perspective Natural Images","A camera-based visibility and weather condition estimation approach using complementary multiple Deep Learning and Vision Language Models (VLM) under adversarial conditions is proposed and experimental results show the superiority of enhanced 2D/3D captions with physical scales over SOTA VLMs.","",,"Hidetomo Sakaino",0,60,0
"47c6fb7e512a337227f578f690ce956b1293f31e","https://www.semanticscholar.org/paper/47c6fb7e512a337227f578f690ce956b1293f31e",2,"PCLmed at ImageCLEFmedical 2023: Customizing General-Purpose Foundation Models for Medical Report Generation","This work proposes customizing off-the-shelf general-purpose large-scale pre-trained models, i.e., foundation models (FMs), in computer vision and natural language processing with a specific focus on medical report generation and demonstrates that unfreezing EVA-ViT-g to learn medical image representations, followed by parameter-efficient training of ChatGLM-6B to capture the writing styles of medical reports, is essential for achieving optimal results.","Conference and Labs of the Evaluation Forum",2023,"Bang Yang,Asif Raza,Yuexian Zou,Tong Zhang",0,65,0
"e3fe61ce7a3c013de8770f7a43f3666f9a8e2ce5","https://www.semanticscholar.org/paper/e3fe61ce7a3c013de8770f7a43f3666f9a8e2ce5",2,"Empowering Vision-Language Models to Follow Interleaved Vision-Language Instructions",,"arXiv.org",2023,"Juncheng Li,Kaihang Pan,Zhiqi Ge,Minghe Gao,Hanwang Zhang,Wei Ji,Wenqiao Zhang,Tat-Seng Chua,Siliang Tang,Yueting Zhuang",4,64,2
"1e7115da4db1622c01ec8e268e622fa8f254599c","https://www.semanticscholar.org/paper/1e7115da4db1622c01ec8e268e622fa8f254599c",2,"L ARGE C ONTENT AND B EHAVIOR M ODELS TO U N - DERSTAND , S IMULATE , AND O PTIMIZE C ONTENT AND B EHAVIOR","The trained models, other than showing similar performance to LLMs on content understanding tasks, show generalization capabilities on behavior simulation, content simulation, behavior understanding, and behavior domain adaptation.","",2023,"Aditya Agrawal,Aanisha Bhattacharyya,Yaman Kumar Singla,Somesh Singh,Uttaran Bhattacharya,Ishita Dasgupta,Stefano Petrangeli,R. Shah,Changan Chen,Balaji Krishnamurthy",0,45,0
"bd80a6b09bfea8c9a681ed796a73c76bccbdb6ad","https://www.semanticscholar.org/paper/bd80a6b09bfea8c9a681ed796a73c76bccbdb6ad",2,"L ARGE C ONTENT AND B EHAVIOR M ODELS TO U N - DERSTAND , S IMULATE , AND O PTIMIZE C ONTENT AND B EHAVIOR","The trained models, other than showing similar performance to LLMs on content understanding tasks, show generalization capabilities on behavior simulation, content simulation, behavior understanding, and behavior domain adaptation.","",2023,"Ashmit Khandelwal,Aditya Agrawal,Aanisha Bhattacharyya,Yaman Kumar Singla,Somesh Singh,Uttaran Bhattacharya,Ishita Dasgupta,Stefano Petrangeli,R. Shah,Changan Chen,Balaji Krishnamurthy",0,46,0
"6f425ba8c1fe3139fcb886d9dda30cd6520517ac","https://www.semanticscholar.org/paper/6f425ba8c1fe3139fcb886d9dda30cd6520517ac",2,"More Context, Less Distraction: Visual Classification by Inferring and Conditioning on Contextual Attributes","PerceptionCLIP is proposed, a training-free, two-step zero-shot classification method that first infers contextual attributes and then performs object classification conditioning on them and achieves better generalization, group robustness, and better interpretability.","arXiv.org",2023,"Bang An,Sicheng Zhu,Michael Panaitescu-Liess,Chaithanya Kumar Mummadi,Furong Huang",1,59,0
"7562e25b666cba841b1dd5cf6e700978922beb04","https://www.semanticscholar.org/paper/7562e25b666cba841b1dd5cf6e700978922beb04",2,"SkinGPT: A Dermatology Diagnostic System with Vision Large Language Model","The ability to deploy it locally and protect user privacy makes SkinGPT an attractive option for patients seeking an accurate and reliable diagnosis of their skin conditions and the system can autonomously determine the characteristics and categories of skin conditions, perform analysis, and provide treatment recommendations.","arXiv.org",2023,"Juexiao Zhou,Xin Gao",1,35,1
"738852940591ecf864abf402878ecf66e2945267","https://www.semanticscholar.org/paper/738852940591ecf864abf402878ecf66e2945267",2,"Visual Adversarial Examples Jailbreak Large Language Models",,"arXiv.org",2023,"Xiangyu Qi,Kaixuan Huang,Ashwinee Panda,Mengdi Wang,Prateek Mittal",5,51,1
"7e3bbd7be60bb50a8093152795f269a69a4a0fd9","https://www.semanticscholar.org/paper/7e3bbd7be60bb50a8093152795f269a69a4a0fd9",2,"Chatting Makes Perfect - Chat-based Image Retrieval","This work introduces ChatIR: a chat-based image retrieval system that engages in a conversation with the user to elicit information, in addition to an initial query, in order to clarify the user’s search intent.","arXiv.org",2023,"Matan Levy,Rami Ben-Ari,N. Darshan,Dani Lischinski",0,58,0
"84dc889beff9d51fe429cff8c92735e7410ee3c2","https://www.semanticscholar.org/paper/84dc889beff9d51fe429cff8c92735e7410ee3c2",2,"Aligning Large Multi-Modal Model with Robust Instruction Tuning","This paper introduces the first large and diverse visual instruction tuning dataset, named Large-scale Robust Visual (LRV)-Instruction, and proposes GPT4-Assisted Visual Instruction Evaluation (GAVIE), a novel approach to evaluate visual instruction tuned without the need for human-annotated groundtruth answers and can adapt to diverse instruction formats.","arXiv.org",2023,"Fuxiao Liu,Kevin Lin,Linjie Li,Jianfeng Wang,Yaser Yacoob,Lijuan Wang",26,39,5
"06d8562831c32844285a691c5250d04726df3c61","https://www.semanticscholar.org/paper/06d8562831c32844285a691c5250d04726df3c61",2,"A Systematic Survey of Prompt Engineering on Vision-Language Foundation Models","This paper aims to provide a comprehensive survey of cutting-edge research in prompt engineering on three types of vision-language models: multimodal-to-text generation models, image-text matching models, and text- to-image generation models.","arXiv.org",2023,"Jindong Gu,Zhen Han,Shuo Chen,Ahmad Beirami,Bailan He,Gengyuan Zhang,Ruotong Liao,Yao Qin,Volker Tresp,Philip H. S. Torr",7,216,0
"d0c87ca688547f5e63fd4900300474980d900b57","https://www.semanticscholar.org/paper/d0c87ca688547f5e63fd4900300474980d900b57",2,"Towards Inadequately Pre-trained Models in Transfer Learning","It is found that during the same pre-training process, models at middle epochs, which is inadequately pre-trained, can outperform fully trained models when used as feature extractors (FE), while the fine-tuning (FT) performance still grows with the source performance.","",2022,"Andong Deng,Xingjian Li,Di Hu,Tianyang Wang,H. Xiong,Chengzhong Xu",0,86,0
"30c0cdc414f68211d5d0514df027cec22e005174","https://www.semanticscholar.org/paper/30c0cdc414f68211d5d0514df027cec22e005174",2,"A Survey on In-context Learning","This paper presents a formal definition of ICL and clarify its correlation to related studies, and organizes and discusses advanced techniques, including training strategies, demonstration designing strategies, as well as related analysis.","",2022,"Qingxiu Dong,Lei Li,Damai Dai,Ce Zheng,Zhiyong Wu,Baobao Chang,Xu Sun,Jingjing Xu,Zhifang Sui",39,128,1
"f890b4dfe915174b23db909b07c515d465eaeff2","https://www.semanticscholar.org/paper/f890b4dfe915174b23db909b07c515d465eaeff2",2,"Can Pre-trained Vision and Language Models Answer Visual Information-Seeking Questions?","The findings reveal that state-of-the-art pre-trained multi-modal models face challenges in answering visual information-seeking questions, but fine-tuning on the InfoSeek dataset elicits models to use fine-grained knowledge that was learned during their pre-training.","arXiv.org",2023,"Yang Chen,Hexiang Hu,Yi Luan,Haitian Sun,Soravit Changpinyo,Alan Ritter,Ming-Wei Chang",8,88,3
"27d0d2923a42bd2bced1b100844e232ff87368e3","https://www.semanticscholar.org/paper/27d0d2923a42bd2bced1b100844e232ff87368e3",2,"SkinGPT-4: An Interactive Dermatology Diagnostic System with Visual Large Language Model","Though SkinGPT-4 is not a substitute for doctors, it could enhance users' comprehension of their medical conditions, facilitate improve communication between patients and doctors, expedite the diagnostic process for dermatologists, and potentially promote human-centred care and healthcare equity in underdeveloped areas.","medRxiv",2023,"Juexiao Zhou,Xiao-Zhen He,Liyuan Sun,Jian-Hui Xu,Xiuying Chen,Yuetan Chu,Longxi Zhou,Xingyu Liao,Bin Zhang,Xin Gao",6,71,0
"c56a51728678e5b2e3ff95e51caf21d267439c36","https://www.semanticscholar.org/paper/c56a51728678e5b2e3ff95e51caf21d267439c36",2,"ChatVideo: A Tracklet-centric Multimodal and Versatile Video Understanding System","The vision for multimodal and versatile video understanding is presented and a prototype system, built upon a tracklet-centric paradigm, which treats tracklets as the basic video unit and employs various Video Foundation Models to annotate their properties e.g., appearance, motion, etc.","arXiv.org",2023,"Junke Wang,Dongdong Chen,Chong Luo,Xiyang Dai,Lu Yuan,Zuxuan Wu,Yu-Gang Jiang",8,43,0
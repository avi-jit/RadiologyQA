"id","url","title","summary","venue","year","authors","citationCount","referenceCount","influentialCitationCount"
"18f9a6045ba01cb079c4fa49a630d71bbd27cd92","https://www.semanticscholar.org/paper/18f9a6045ba01cb079c4fa49a630d71bbd27cd92","A dataset of clinically generated visual questions and answers about radiology images","This work introduces VQA-RAD, the first manually constructed dataset where clinicians asked naturally occurring questions about radiology images and provided reference answers and demonstrates the rich quality of this dataset over other automatically constructed ones.","Scientific Data",2018,"J. Lau,Soumya Gayen,Asma Ben Abacha,Dina Demner-Fushman",86,23,31
"4d4a96708fc67403176bb2b891b564af7a20c148","https://www.semanticscholar.org/paper/4d4a96708fc67403176bb2b891b564af7a20c148","RAMM: Retrieval-augmented Biomedical Visual Question Answering with Multi-modal Pre-training","This paper collects a new biomedical dataset named PMCPM which offers patient-based image-text pairs containing diverse patient situations from PubMed and proposes a retrieval-augmented pretrain-and-finetune paradigm named RAMM for biomedical VQA to overcome the data limitation issue.","arXiv.org",2023,"Zheng Yuan,Qiao Jin,Chuanqi Tan,Zhengyun Zhao,Hongyi Yuan,Fei Huang,Songfang Huang",2,46,0
"93b6b79b4ef6c345f31722ce7c829385c6dce0d6","https://www.semanticscholar.org/paper/93b6b79b4ef6c345f31722ce7c829385c6dce0d6","Slake: A Semantically-Labeled Knowledge-Enhanced Dataset For Medical Visual Question Answering","A large bilingual dataset, SLAKE, with comprehensive semantic labels annotated by experienced physicians and a new structural medical knowledge base for Med-VQA is presented, which includes richer modalities and covers more human body parts than the currently available dataset.","IEEE International Symposium on Biomedical Imaging",2021,"Bo Liu,Li-Ming Zhan,Li Xu,Lin Ma,Y. Yang,Xiao-Ming Wu",23,15,8
"1e43c7084bdcb6b3102afaf301cce10faead2702","https://www.semanticscholar.org/paper/1e43c7084bdcb6b3102afaf301cce10faead2702","BioBERT: a pre-trained biomedical language representation model for biomedical text mining","This article introduces BioBERT (Bidirectional Encoder Representations from Transformers for Biomedical Text Mining), which is a domain-specific language representation model pre-trained on large-scale biomedical corpora that largely outperforms BERT and previous state-of-the-art models in a variety of biomedical text mining tasks when pre- trained on biomedical Corpora.","Bioinform.",2019,"Jinhyuk Lee,Wonjin Yoon,Sungdong Kim,Donghyeon Kim,Sunkyu Kim,Chan Ho So,Jaewoo Kang",2887,45,536
"4ef3d9e492479e28fa57d107e52acc6a0c803de2","https://www.semanticscholar.org/paper/4ef3d9e492479e28fa57d107e52acc6a0c803de2","Does CLIP Benefit Visual Question Answering in the Medical Domain as Much as it Does in the General Domain?","A fine-tuned version of CLIP for the medical domain based on PubMed articles is presented, which leads to noticeable improvements for MedVQA and fundamental performance differences of VQA in general versus medical domains are witnessed.","arXiv.org",2021,"Sedigheh Eslami,Gerard de Melo,C. Meinel",23,30,7
"02251886950770e82b3d68564d60cdfe15e73199","https://www.semanticscholar.org/paper/02251886950770e82b3d68564d60cdfe15e73199","Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks","This work introduces a general-purpose multimodal foundation model BEiT-3, which achieves state-of-the-art transfer performance on both vision and vision-language tasks and introduces Multiway Transformers for general- Purpose modeling, where the modular architecture enables both deep fusion and modality-specific encoding.","arXiv.org",2022,"Wenhui Wang,Hangbo Bao,Li Dong,Johan Bjorck,Zhiliang Peng,Qiang Liu,Kriti Aggarwal,O. Mohammed,Saksham Singhal,S. Som,Furu Wei",164,74,25
"967907503b24423b9b74621051811fcf684e3957","https://www.semanticscholar.org/paper/967907503b24423b9b74621051811fcf684e3957","Generalized Decoding for Pixel, Image, and Language","X-Decoder is the first work that provides a unified way to support all types of image segmentation and a variety of vision-language (VL) tasks and achieves state-of-the-art results on open-vocabulary segmentsation and referring segmentation on eight datasets.","arXiv.org",2022,"Xueyan Zou,Zi-Yi Dou,Jianwei Yang,Zhe Gan,Linjie Li,Chunyuan Li,Xiyang Dai,Harkirat Singh Behl,Jianfeng Wang,Lu Yuan,Nanyun Peng,Lijuan Wang,Yong Jae Lee,Jianfeng Gao",7,99,0
"336ce63b472a65f053f854d45851d6f0e896f05e","https://www.semanticscholar.org/paper/336ce63b472a65f053f854d45851d6f0e896f05e","BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models","BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods, and is demonstrated's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.","arXiv.org",2023,"Junnan Li,Dongxu Li,S. Savarese,Steven Hoi",49,40,10
"6edcb09a09c8df43cb62119133df9bb2eb75e5cf","https://www.semanticscholar.org/paper/6edcb09a09c8df43cb62119133df9bb2eb75e5cf","From Images to Textual Prompts: Zero-shot VQA with Frozen Large Language Models","Img2Prompt is a plug-and-play module that provides the prompts that can bridge the aforementioned modality and task disconnections, so that LLMs can perform zero-shot VQA tasks without end-to-end training.","arXiv.org",2022,"Jiaxian Guo,Junnan Li,Dongxu Li,A. M. H. Tiong,Boyang Li,Dacheng Tao,Steven Hoi",3,69,0
"af997821231898a5f8d0fd78dad4eec526acabe5","https://www.semanticscholar.org/paper/af997821231898a5f8d0fd78dad4eec526acabe5","Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models","A system to enable the user to interact with ChatGPT by sending and receiving not only languages but also images and providing complex visual questions or visual editing instructions that require the collaboration of multiple AI models with multi-steps, and opens the door to investigating the visual roles ofChatGPT with the help of Visual Foundation Models.","arXiv.org",2023,"Chenfei Wu,Sheng-Kai Yin,Weizhen Qi,Xiaodong Wang,Zecheng Tang,Nan Duan",26,57,6
"6e754273d54a91371efbc928cd6b156364d517da","https://www.semanticscholar.org/paper/6e754273d54a91371efbc928cd6b156364d517da","ViperGPT: Visual Inference via Python Execution for Reasoning","ViperGPT is introduced, a framework that leverages code-generation models to compose vision-and-language models into subroutines to produce a result for any query and achieves state-of-the-art results across various complex visual tasks.","arXiv.org",2023,"D'idac Sur'is,Sachit Menon,Carl Vondrick",5,67,1
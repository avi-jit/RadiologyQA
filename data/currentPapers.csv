"id","title","summary","venue","year","authors","citationCount","referenceCount","influentialCitationCount","url"
"4d4a96708fc67403176bb2b891b564af7a20c148","RAMM: Retrieval-augmented Biomedical Visual Question Answering with Multi-modal Pre-training","This paper collects a new biomedical dataset named PMCPM which offers patient-based image-text pairs containing diverse patient situations from PubMed and proposes a retrieval-augmented pretrain-and-finetune paradigm named RAMM for biomedical VQA to overcome the data limitation issue.","arXiv.org",2023,"Zheng Yuan,Qiao Jin,Chuanqi Tan,Zhengyun Zhao,Hongyi Yuan,Fei Huang,Songfang Huang",3,49,0,"https://www.semanticscholar.org/paper/4d4a96708fc67403176bb2b891b564af7a20c148"
"93b6b79b4ef6c345f31722ce7c829385c6dce0d6","Slake: A Semantically-Labeled Knowledge-Enhanced Dataset For Medical Visual Question Answering","A large bilingual dataset, SLAKE, with comprehensive semantic labels annotated by experienced physicians and a new structural medical knowledge base for Med-VQA is presented, which includes richer modalities and covers more human body parts than the currently available dataset.","IEEE International Symposium on Biomedical Imaging",2021,"Bo Liu,Li-Ming Zhan,Li Xu,Lin Ma,Y. Yang,Xiao-Ming Wu",55,15,8,"https://www.semanticscholar.org/paper/93b6b79b4ef6c345f31722ce7c829385c6dce0d6"
"18f9a6045ba01cb079c4fa49a630d71bbd27cd92","A dataset of clinically generated visual questions and answers about radiology images","This work introduces VQA-RAD, the first manually constructed dataset where clinicians asked naturally occurring questions about radiology images and provided reference answers and demonstrates the rich quality of this dataset over other automatically constructed ones.","Scientific Data",2018,"J. Lau,Soumya Gayen,Asma Ben Abacha,Dina Demner-Fushman",133,14,31,"https://www.semanticscholar.org/paper/18f9a6045ba01cb079c4fa49a630d71bbd27cd92"
"4ef3d9e492479e28fa57d107e52acc6a0c803de2","Does CLIP Benefit Visual Question Answering in the Medical Domain as Much as it Does in the General Domain?","A fine-tuned version of CLIP for the medical domain based on PubMed articles is presented, which leads to noticeable improvements for MedVQA and fundamental performance differences of VQA in general versus medical domains are witnessed.","arXiv.org",2021,"Sedigheh Eslami,Gerard de Melo,C. Meinel",43,31,7,"https://www.semanticscholar.org/paper/4ef3d9e492479e28fa57d107e52acc6a0c803de2"
"1e43c7084bdcb6b3102afaf301cce10faead2702","BioBERT: a pre-trained biomedical language representation model for biomedical text mining","This article introduces BioBERT (Bidirectional Encoder Representations from Transformers for Biomedical Text Mining), which is a domain-specific language representation model pre-trained on large-scale biomedical corpora that largely outperforms BERT and previous state-of-the-art models in a variety of biomedical text mining tasks when pre- trained on biomedical Corpora.","Bioinform.",2019,"Jinhyuk Lee,Wonjin Yoon,Sungdong Kim,Donghyeon Kim,Sunkyu Kim,Chan Ho So,Jaewoo Kang",3597,45,536,"https://www.semanticscholar.org/paper/1e43c7084bdcb6b3102afaf301cce10faead2702"
"02251886950770e82b3d68564d60cdfe15e73199","Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks","This work introduces a general-purpose multimodal foundation model BEiT-3, which achieves state-of-the-art transfer performance on both vision and vision-language tasks and introduces Multiway Transformers for general- Purpose modeling, where the modular architecture enables both deep fusion and modality-specific encoding.","arXiv.org",2022,"Wenhui Wang,Hangbo Bao,Li Dong,Johan Bjorck,Zhiliang Peng,Qiang Liu,Kriti Aggarwal,O. Mohammed,Saksham Singhal,S. Som,Furu Wei",375,75,25,"https://www.semanticscholar.org/paper/02251886950770e82b3d68564d60cdfe15e73199"
"336ce63b472a65f053f854d45851d6f0e896f05e","BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models","BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods, and is demonstrated's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.","arXiv.org",2023,"Junnan Li,Dongxu Li,S. Savarese,Steven Hoi",712,46,10,"https://www.semanticscholar.org/paper/336ce63b472a65f053f854d45851d6f0e896f05e"
"6edcb09a09c8df43cb62119133df9bb2eb75e5cf","From Images to Textual Prompts: Zero-shot VQA with Frozen Large Language Models","Img2Prompt is a plug-and-play module that provides the prompts that can bridge the aforementioned modality and task disconnections, so that LLMs can perform zero-shot VQA tasks without end-to-end training.","arXiv.org",2022,"Jiaxian Guo,Junnan Li,Dongxu Li,A. M. H. Tiong,Boyang Li,Dacheng Tao,Steven Hoi",14,72,0,"https://www.semanticscholar.org/paper/6edcb09a09c8df43cb62119133df9bb2eb75e5cf"
"af997821231898a5f8d0fd78dad4eec526acabe5","Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models","A system to enable the user to interact with ChatGPT by sending and receiving not only languages but also images and providing complex visual questions or visual editing instructions that require the collaboration of multiple AI models with multi-steps, and opens the door to investigating the visual roles ofChatGPT with the help of Visual Foundation Models.","arXiv.org",2023,"Chenfei Wu,Sheng-Kai Yin,Weizhen Qi,Xiaodong Wang,Zecheng Tang,Nan Duan",229,62,6,"https://www.semanticscholar.org/paper/af997821231898a5f8d0fd78dad4eec526acabe5"
"6e754273d54a91371efbc928cd6b156364d517da","ViperGPT: Visual Inference via Python Execution for Reasoning","ViperGPT is introduced, a framework that leverages code-generation models to compose vision-and-language models into subroutines to produce a result for any query and achieves state-of-the-art results across various complex visual tasks.","arXiv.org",2023,"D'idac Sur'is,Sachit Menon,Carl Vondrick",101,78,1,"https://www.semanticscholar.org/paper/6e754273d54a91371efbc928cd6b156364d517da"
"d8da72e7857cc1a0d3505e6c8a746eac815901b2","Large-Scale Domain-Specific Pretraining for Biomedical Vision-Language Processing","This paper conducted by far the largest study on biomedical VLP, using 15 million figure-caption pairs extracted from biomedical research articles in PubMed Central, and established new state of the art in a wide range of standard datasets, substantially outperformed prior VLP approaches.","arXiv.org",2023,"Shenmin Zhang,Yanbo Xu,Naoto Usuyama,J. Bagga,Robert Tinn,Sam Preston,Rajesh N. Rao,Mu-Hsin Wei,Naveen Valluri,Cliff Wong,M. Lungren,Tristan Naumann,Hoifung Poon",34,76,0,"https://www.semanticscholar.org/paper/d8da72e7857cc1a0d3505e6c8a746eac815901b2"
"994e08ac813028601907516aee9c4699234a6b4d","Large AI Models in Health Informatics: Applications, Challenges, and the Future","An up-to-date comprehensive review of large AI models, from background to their applications, is presented, including seven key sectors that largeAI models are applicable and might have substantial influence, including 1) molecular biology and drug discovery; 2) medical diagnosis and decision-making; 3) medical imaging and vision; 4) medical informatics; 5) medical education; 6) public health; and 7) medical robotics.","arXiv.org",2023,"Jianing Qiu,Lin Li,Jiankai Sun,Jiachuan Peng,Peilun Shi,Rui Zhang,Yinzhao Dong,Kyle Lam,F. P. Lo,Bo Xiao,Wu Yuan,Dong Xu,Benny P. L. Lo",23,347,0,"https://www.semanticscholar.org/paper/994e08ac813028601907516aee9c4699234a6b4d"
"f44ad7ad67ddd5fe74598fe491ca75c5221380df","MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models","MiniGPT-4 is presented, which aligns a frozen visual encoder with a frozen LLM, Vicuna, using just one projection layer and possesses many capabilities similar to those exhibited by G PT-4 like detailed image description generation and website creation from hand-written drafts.","",2023,"Deyao Zhu,Jun Chen,Xiaoqian Shen,Xiang Li,Mohamed Elhoseiny",309,49,1,"https://www.semanticscholar.org/paper/f44ad7ad67ddd5fe74598fe491ca75c5221380df"
"6ae700c89a9a9a3da7e55dd51c4710b5ed8c8d4e","Caption-Aware Medical VQA via Semantic Focusing and Progressive Cross-Modality Comprehension","A caption-aware VQA method that can read the summary information of image content and clinic diagnoses from plenty of medical images and answer the medical question with richer multimodality features is proposed.","ACM Multimedia",2022,"Fu'ze Cong,Shibiao Xu,Li Guo,Yinbing Tian",3,45,0,"https://www.semanticscholar.org/paper/6ae700c89a9a9a3da7e55dd51c4710b5ed8c8d4e"
"28ff0816f19a5e3e37eac5569de41872fd262f0a","Align, Reason and Learn: Enhancing Medical Vision-and-Language Pre-training with Knowledge","A systematic and effective approach to enhance Med-VLP by structured medical knowledge from three perspectives is proposed, which align the representations of the vision encoder and the language encoder through knowledge.","ACM Multimedia",2022,"Zhihong Chen,Guanbin Li,Xiang Wan",20,61,1,"https://www.semanticscholar.org/paper/28ff0816f19a5e3e37eac5569de41872fd262f0a"
"c7a9c7302a72301ed79a7c0696d5af2e03ad3ac4","MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action","This paper defines and explores a comprehensive list of advanced vision tasks that are intriguing to solve, but may exceed the capabilities of existing vision and vision-language models and proposes MM-REACT, a system paradigm that integrates ChatGPT with a pool of vision experts to achieve multimodal reasoning and action.","arXiv.org",2023,"Zhengyuan Yang,Linjie Li,Jianfeng Wang,Kevin Lin,E. Azarnasab,Faisal Ahmed,Zicheng Liu,Ce Liu,Michael Zeng,Lijuan Wang",119,45,3,"https://www.semanticscholar.org/paper/c7a9c7302a72301ed79a7c0696d5af2e03ad3ac4"
"da9579539385daedd33a0de0f814e2977ad0d1f5","Towards Unifying Medical Vision-and-Language Pre-training via Soft Prompts","Experimental results show that the proposed PTUnifier approach achieves state-of-the-art results on a broad range of tasks, spanning uni-modal tasks (\textit{i.e.}, image-to-text generation and image-text/text-image retrieval), and multi- modal tasks, demonstrating the effectiveness of the approach.","arXiv.org",2023,"Zhihong Chen,Shizhe Diao,Benyou Wang,Guanbin Li,Xiang Wan",4,69,0,"https://www.semanticscholar.org/paper/da9579539385daedd33a0de0f814e2977ad0d1f5"
"2ea26b243171e37ef20af269942ffde414f9f8cc","UnICLAM: Contrastive Representation Learning with Adversarial Masking for Unified and Interpretable Medical Vision Question Answering","Experimental results on VQA-RAD and SLAKE public benchmarks demonstrate that UnICLAM outperforms existing 11 state-of-the-art Medical-VQA models and makes an additional discussion about the performance of UnicLAM in diagnosing heart failure, verifying that it exhibits superior few-shot adaption performance in practical disease diagnosis.","arXiv.org",2022,"Chenlu Zhan,Peng Peng,Hongsen Wang,Tao Chen,Hongwei Wang",2,46,0,"https://www.semanticscholar.org/paper/2ea26b243171e37ef20af269942ffde414f9f8cc"
"170667a96f04adf3b3b83526f75fe8d1063e0f7a","Self-supervised vision-language pretraining for Medical visual question answering","A self-supervised method that applies Masked image modeling, Masked language modeling, Image text matching and Image text alignment via contrastive learning (M2I2) for pretraining on medical image caption dataset, and finetunes to downstream medical VQA tasks is proposed.","arXiv.org",2022,"Pengfei Li,Gang Liu,Lin Tan,Jinying Liao,Shenjun Zhong",2,27,0,"https://www.semanticscholar.org/paper/170667a96f04adf3b3b83526f75fe8d1063e0f7a"
"3d45e69557f0c6a54ec698304c2e27ec29bc1c2b","PMC-CLIP: Contrastive Language-Image Pre-training using Biomedical Documents","PMC-OA, a biomedical dataset with 1.6M image-caption pairs collected from PubMedCentral's OpenAccess subset, is built and released, which is 8 times larger than before and achieves state-of-the-art results on various downstream tasks.","arXiv.org",2023,"Weixiong Lin,Ziheng Zhao,Xiaoman Zhang,Chaoyi Wu,Ya Zhang,Yanfeng Wang,Weidi Xie",12,31,0,"https://www.semanticscholar.org/paper/3d45e69557f0c6a54ec698304c2e27ec29bc1c2b"
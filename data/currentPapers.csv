"id","title","summary","venue","year","authors","citationCount","referenceCount","influentialCitationCount","url"
"93b6b79b4ef6c345f31722ce7c829385c6dce0d6","Slake: A Semantically-Labeled Knowledge-Enhanced Dataset For Medical Visual Question Answering","A large bilingual dataset, SLAKE, with comprehensive semantic labels annotated by experienced physicians and a new structural medical knowledge base for Med-VQA is presented, which includes richer modalities and covers more human body parts than the currently available dataset.","IEEE International Symposium on Biomedical Imaging",2021,"Bo Liu,Li-Ming Zhan,Li Xu,Lin Ma,Y. Yang,Xiao-Ming Wu",73,15,8,"https://www.semanticscholar.org/paper/93b6b79b4ef6c345f31722ce7c829385c6dce0d6"
"18f9a6045ba01cb079c4fa49a630d71bbd27cd92","A dataset of clinically generated visual questions and answers about radiology images","This work introduces VQA-RAD, the first manually constructed dataset where clinicians asked naturally occurring questions about radiology images and provided reference answers and demonstrates the rich quality of this dataset over other automatically constructed ones.","Scientific Data",2018,"J. Lau,Soumya Gayen,Asma Ben Abacha,Dina Demner-Fushman",151,14,31,"https://www.semanticscholar.org/paper/18f9a6045ba01cb079c4fa49a630d71bbd27cd92"
"6edcb09a09c8df43cb62119133df9bb2eb75e5cf","From Images to Textual Prompts: Zero-shot VQA with Frozen Large Language Models","Img2Prompt is a plug-and-play module that provides the prompts that can bridge the aforementioned modality and task disconnections, so that LLMs can perform zero-shot VQA tasks without end-to-end training.","arXiv.org",2022,"Jiaxian Guo,Junnan Li,Dongxu Li,A. M. H. Tiong,Boyang Li,Dacheng Tao,Steven Hoi",56,73,0,"https://www.semanticscholar.org/paper/6edcb09a09c8df43cb62119133df9bb2eb75e5cf"
"1e43c7084bdcb6b3102afaf301cce10faead2702","BioBERT: a pre-trained biomedical language representation model for biomedical text mining","This article introduces BioBERT (Bidirectional Encoder Representations from Transformers for Biomedical Text Mining), which is a domain-specific language representation model pre-trained on large-scale biomedical corpora that largely outperforms BERT and previous state-of-the-art models in a variety of biomedical text mining tasks when pre- trained on biomedical Corpora.","Bioinform.",2019,"Jinhyuk Lee,Wonjin Yoon,Sungdong Kim,Donghyeon Kim,Sunkyu Kim,Chan Ho So,Jaewoo Kang",3974,45,536,"https://www.semanticscholar.org/paper/1e43c7084bdcb6b3102afaf301cce10faead2702"
"af997821231898a5f8d0fd78dad4eec526acabe5","Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models","A system to enable the user to interact with ChatGPT by sending and receiving not only languages but also images and providing complex visual questions or visual editing instructions that require the collaboration of multiple AI models with multi-steps, and opens the door to investigating the visual roles ofChatGPT with the help of Visual Foundation Models.","arXiv.org",2023,"Chenfei Wu,Sheng-Kai Yin,Weizhen Qi,Xiaodong Wang,Zecheng Tang,Nan Duan",330,61,6,"https://www.semanticscholar.org/paper/af997821231898a5f8d0fd78dad4eec526acabe5"
"6e754273d54a91371efbc928cd6b156364d517da","ViperGPT: Visual Inference via Python Execution for Reasoning","ViperGPT is introduced, a framework that leverages code-generation models to compose vision-and-language models into subroutines to produce a result for any query and achieves state-of-the-art results across various complex visual tasks.","arXiv.org",2023,"D'idac Sur'is,Sachit Menon,Carl Vondrick",168,79,1,"https://www.semanticscholar.org/paper/6e754273d54a91371efbc928cd6b156364d517da"
"28ff0816f19a5e3e37eac5569de41872fd262f0a","Align, Reason and Learn: Enhancing Medical Vision-and-Language Pre-training with Knowledge","A systematic and effective approach to enhance Med-VLP by structured medical knowledge from three perspectives is proposed, which align the representations of the vision encoder and the language encoder through knowledge.","ACM Multimedia",2022,"Zhihong Chen,Guanbin Li,Xiang Wan",24,61,1,"https://www.semanticscholar.org/paper/28ff0816f19a5e3e37eac5569de41872fd262f0a"
"c7a9c7302a72301ed79a7c0696d5af2e03ad3ac4","MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action","This paper defines and explores a comprehensive list of advanced vision tasks that are intriguing to solve, but may exceed the capabilities of existing vision and vision-language models and proposes MM-REACT, a system paradigm that integrates ChatGPT with a pool of vision experts to achieve multimodal reasoning and action.","arXiv.org",2023,"Zhengyuan Yang,Linjie Li,Jianfeng Wang,Kevin Lin,E. Azarnasab,Faisal Ahmed,Zicheng Liu,Ce Liu,Michael Zeng,Lijuan Wang",178,45,3,"https://www.semanticscholar.org/paper/c7a9c7302a72301ed79a7c0696d5af2e03ad3ac4"
"3d45e69557f0c6a54ec698304c2e27ec29bc1c2b","PMC-CLIP: Contrastive Language-Image Pre-training using Biomedical Documents","PMC-OA, a biomedical dataset with 1.6M image-caption pairs collected from PubMedCentral's OpenAccess subset, is built and released, which is 8 times larger than before and achieves state-of-the-art results on various downstream tasks.","arXiv.org",2023,"Weixiong Lin,Ziheng Zhao,Xiaoman Zhang,Chaoyi Wu,Ya Zhang,Yanfeng Wang,Weidi Xie",24,33,0,"https://www.semanticscholar.org/paper/3d45e69557f0c6a54ec698304c2e27ec29bc1c2b"
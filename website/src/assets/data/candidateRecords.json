{"papers":[{"url":"https://www.semanticscholar.org/paper/2c7e346aa311fec4dda04bdf3a214ce2026d8807","title":"Medical Vision Language Pretraining: A survey","venue":"arXiv.org","year":2023,"referenceCount":161,"citationCount":1,"influentialCitationCount":0,"publicationDate":"11/12/2023","authors":"Prashant Shrestha,Sanskar Amgain,Bidur Khanal,C. Linte,Binod Bhattarai","id":"2c7e346aa311fec4dda04bdf3a214ce2026d8807","summary":"This paper reviews existing works through the lens of different pretraining objectives, architectures, downstream evaluation tasks, and datasets utilized for pretraining and downstream tasks, then dives into current challenges in medical VLP, discussing existing and potential solutions, and concludes by highlighting future directions.","score":8},{"url":"https://www.semanticscholar.org/paper/584ca135b61482fd89247113da87d784f738dbfa","title":"Foundational Models Defining a New Era in Vision: A Survey and Outlook","venue":"arXiv.org","year":2023,"referenceCount":366,"citationCount":21,"influentialCitationCount":4,"publicationDate":"25/07/2023","authors":"Muhammad Awais,Muzammal Naseer,Salman Siddique Khan,R. Anwer,Hisham Cholakkal,M. Shah,Ming Yang,F. Khan","id":"584ca135b61482fd89247113da87d784f738dbfa","summary":"A comprehensive review of emerging foundational models in computer vision, including typical architecture designs to combine different modalities, training objectives, pre-training datasets, fine-tuning mechanisms, and the common prompting patterns; textual, visual, and heterogeneous.","score":6},{"url":"https://www.semanticscholar.org/paper/420087f314633a381e61e6c5cd73ccc2070a749e","title":"PeFoMed: Parameter Efficient Fine-tuning on Multimodal Large Language Models for Medical Visual Question Answering","venue":"arXiv.org","year":2024,"referenceCount":43,"citationCount":0,"influentialCitationCount":0,"publicationDate":"05/01/2024","authors":"Jinlong He,Pengfei Li,Gang Liu,Zixu Zhao,Shenjun Zhong","id":"420087f314633a381e61e6c5cd73ccc2070a749e","summary":"A parameter efficient framework for fine-tuning MLLM specifically tailored to Med-VQA applications is proposed and empirically validate it on a public benchmark dataset, revealing that it outperforms the GPT-4v model by a significant margin of 26% absolute accuracy on closed-ended questions.","score":6},{"url":"https://www.semanticscholar.org/paper/ebedc4d7a2356090904baba4104ef0832bc236df","title":"A Survey on Multimodal Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":108,"citationCount":84,"influentialCitationCount":2,"publicationDate":"23/06/2023","authors":"Shukang Yin,Chaoyou Fu,Sirui Zhao,Ke Li,Xing Sun,Tong Xu,Enhong Chen","id":"ebedc4d7a2356090904baba4104ef0832bc236df","summary":"This paper presents the formulation of MLLM and delineate its related concepts, and discusses the key techniques and applications, including Multimodal Instruction Tuning (M-IT), Multimmodal In-Context Learning (M -ICL), MultIModal Chain of Thought (m-CoT), and LLM-Aided Visual Reasoning (LAVR).","score":6},{"url":"https://www.semanticscholar.org/paper/c7492913370b5726eaa6ced163a60de6c9d4bb7f","title":"A Survey of Large Language Models for Healthcare: from Data, Technology, and Applications to Accountability and Ethics","venue":"arXiv.org","year":2023,"referenceCount":377,"citationCount":13,"influentialCitationCount":2,"publicationDate":"09/10/2023","authors":"Kai He,Rui Mao,Qika Lin,Yucheng Ruan,Xiang Lan,Mengling Feng,Erik Cambria","id":"c7492913370b5726eaa6ced163a60de6c9d4bb7f","summary":"It is contended that a significant paradigm shift is underway, transitioning from PLMs to LLMs, which encompasses a move from discriminative AI approaches to generativeAI approaches, as well as a shift from model-centered methodologies to datacentered methodologies.","score":6},{"url":"https://www.semanticscholar.org/paper/8d2709ed1788a67e64425fb410bb49f3ee49e088","title":"Large Language Models Illuminate a Progressive Pathway to Artificial Healthcare Assistant: A Review","venue":"arXiv.org","year":2023,"referenceCount":186,"citationCount":1,"influentialCitationCount":0,"publicationDate":"03/11/2023","authors":"Mingze Yuan,Peng Bao,J. Yuan,Yunhao Shen,Zi Chen,Yi Xie,Jie Zhao,Yang Chen,Li Zhang,Lin Shen,Bin Dong","id":"8d2709ed1788a67e64425fb410bb49f3ee49e088","summary":"This review offers an extensive analysis on the transformative potential of LLMs in modern medicine and highlights the pivotal need for continuous optimizations and ethical oversight before these models can be effectively integrated into clinical practice.","score":6},{"url":"https://www.semanticscholar.org/paper/570079bbdd8758dfe865097e05719313c9c1301a","title":"LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model","venue":"arXiv.org","year":2023,"referenceCount":79,"citationCount":179,"influentialCitationCount":25,"publicationDate":"28/04/2023","authors":"Peng Gao,Jiaming Han,Renrui Zhang,Ziyi Lin,Shijie Geng,Aojun Zhou,W. Zhang,Pan Lu,Conghui He,Xiangyu Yue,Hongsheng Li,Y. Qiao","id":"570079bbdd8758dfe865097e05719313c9c1301a","summary":"This work augments LLaMA-Adapter by unlocking more learnable parameters and proposes an early fusion strategy to feed visual tokens only into the early LLM layers, contributing to better visual knowledge incorporation and achieves strong multi-modal reasoning with only a small-scale image-text and instruction dataset.","score":5},{"url":"https://www.semanticscholar.org/paper/d6d3604f369bb0415cbe814e43ca3131323b03e2","title":"Otter: A Multi-Modal Model with In-Context Instruction Tuning","venue":"arXiv.org","year":2023,"referenceCount":38,"citationCount":189,"influentialCitationCount":32,"publicationDate":"05/05/2023","authors":"Bo Li,Yuanhan Zhang,Liangyu Chen,Jinghao Wang,Jingkang Yang,Ziwei Liu","id":"d6d3604f369bb0415cbe814e43ca3131323b03e2","summary":"Otter, a multi-modal model based on OpenFlamingo (open-sourced version of DeepMind's Flamingo), trained on MIMIC-IT and showcasing improved instruction-following ability and in-context learning is introduced.","score":5},{"url":"https://www.semanticscholar.org/paper/7cf64070fd3d7e53d80f260c10e6bd7018d580e1","title":"IdealGPT: Iteratively Decomposing Vision and Language Reasoning via Large Language Models","venue":"Conference on Empirical Methods in Natural Language Processing","year":2023,"referenceCount":51,"citationCount":14,"influentialCitationCount":3,"publicationDate":"24/05/2023","authors":"Haoxuan You,Rui Sun,Zhecan Wang,Long Chen,Gengyu Wang,Hammad A. Ayyubi,Kai-Wei Chang,Shih-Fu Chang","id":"7cf64070fd3d7e53d80f260c10e6bd7018d580e1","summary":"The IdealGPT framework is presented, a framework that iteratively decomposes VL reasoning using large language models (LLMs) and outperforms the best existing GPT-4-like models by an absolute 10% on VCR and 15% on SNLI-VE.","score":5},{"url":"https://www.semanticscholar.org/paper/86cbd30d1096b0c7e4ac6b03d97a8df12fd21457","title":"PathAsst: Redefining Pathology through Generative Foundation AI Assistant for Pathology","venue":"arXiv.org","year":2023,"referenceCount":47,"citationCount":10,"influentialCitationCount":2,"publicationDate":"24/05/2023","authors":"Yuxuan Sun,Chenglu Zhu,S. Zheng,Kai Zhang,Zhongyi Shui,Xiaoxuan Yu,Yi-Lei Zhao,Honglin Li,Yunlong Zhang,Ruojia Zhao,Xinheng Lyu,Lin Yang","id":"86cbd30d1096b0c7e4ac6b03d97a8df12fd21457","summary":"The PathAsst is presented, which is a generative foundation AI assistant to revolutionize diagnostic and predictive analytics in pathology, trained based on Vicuna-13B language model in coordination with the CLIP vision encoder.","score":5},{"url":"https://www.semanticscholar.org/paper/fd755dc7b5b206c17fd953db04e1c888d45b6e4e","title":"LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark","venue":"arXiv.org","year":2023,"referenceCount":98,"citationCount":38,"influentialCitationCount":8,"publicationDate":"11/06/2023","authors":"Zhen-fei Yin,Jiong Wang,Jianjian Cao,Zhelun Shi,Dingning Liu,Mukai Li,Lu Sheng,Lei Bai,Xiaoshui Huang,Zhiyong Wang,Wanli Ouyang,Jing Shao","id":"fd755dc7b5b206c17fd953db04e1c888d45b6e4e","summary":"This work extends the research of MLLMs to point clouds and presents the LAMM-Dataset and LAMm-Benchmark for 2D image and 3D point cloud understanding and establishes an extensible framework to facilitate the extension of M LLMs to additional modalities.","score":5},{"url":"https://www.semanticscholar.org/paper/051549d8ef56937b2f4d113afdcf8c7586d3770b","title":"Towards AGI in Computer Vision: Lessons Learned from GPT and Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":169,"citationCount":2,"influentialCitationCount":0,"publicationDate":"14/06/2023","authors":"Lingxi Xie,Longhui Wei,Xiaopeng Zhang,Kaifeng Bi,Xiaotao Gu,Jianlong Chang,Qi Tian","id":"051549d8ef56937b2f4d113afdcf8c7586d3770b","summary":"It is pointed out that the essential weakness of CV lies in lacking a paradigm to learn from environments, yet NLP has accomplished the task in the text world and is still far from a system like GPT that naturally integrates all tasks.","score":5},{"url":"https://www.semanticscholar.org/paper/966852963a88a28786b798c91b6662d6e501e590","title":"AssistGPT: A General Multi-modal Assistant that can Plan, Execute, Inspect, and Learn","venue":"arXiv.org","year":2023,"referenceCount":73,"citationCount":23,"influentialCitationCount":2,"publicationDate":"14/06/2023","authors":"Difei Gao,Lei Ji,Luowei Zhou,Kevin Lin,Joya Chen,Zihan Fan,Mike Zheng Shou","id":"966852963a88a28786b798c91b6662d6e501e590","summary":"A multi-modal AI assistant with an interleaved code and language reasoning approach called Plan, Execute, Inspect, and Learn (PEIL) to integrate LLMs with various tools, and a Learner is designed to enable the model to autonomously explore and discover the optimal solution.","score":5},{"url":"https://www.semanticscholar.org/paper/ca31b8584b6c022ef15ddfe994fe361e002b7729","title":"A Comprehensive Overview of Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":447,"citationCount":36,"influentialCitationCount":1,"publicationDate":"12/07/2023","authors":"Humza Naveed,Asad Ullah Khan,Shi Qiu,Muhammad Saqib,Saeed Anwar,Muhammad Usman,N. Barnes,A. Mian","id":"ca31b8584b6c022ef15ddfe994fe361e002b7729","summary":"A self-contained comprehensive overview of the existing literature on a broad range of LLM-related concepts discusses relevant background concepts along with covering the advanced topics at the frontier of research in LLMs.","score":5},{"url":"https://www.semanticscholar.org/paper/d6c2523ab97416c2692cbbeab082ed1790e8e55e","title":"VisIT-Bench: A Benchmark for Vision-Language Instruction Following Inspired by Real-World Use","venue":"arXiv.org","year":2023,"referenceCount":99,"citationCount":18,"influentialCitationCount":1,"publicationDate":"12/08/2023","authors":"Yonatan Bitton,Hritik Bansal,Jack Hessel,Rulin Shao,Wanrong Zhu,Anas Awadalla,Josh Gardner,Rohan Taori,L. Schimdt","id":"d6c2523ab97416c2692cbbeab082ed1790e8e55e","summary":"This work introduces VisIT-Bench (Visual InsTruction Benchmark), a benchmark for evaluation of instruction-following vision-language models for real-world use, and curates 70 'instruction families' that it envision instruction tuned vision- language models should be able to address.","score":5},{"url":"https://www.semanticscholar.org/paper/894ed1aba8e42a4ec27ba53ecde383b14c5128ca","title":"Examining User-Friendly and Open-Sourced Large GPT Models: A Survey on Language, Multimodal, and Scientific GPT Models","venue":"arXiv.org","year":2023,"referenceCount":178,"citationCount":1,"influentialCitationCount":0,"publicationDate":"27/08/2023","authors":"Kaiyuan Gao,Su He,Zhenyu He,Jiacheng Lin,Qizhi Pei,Jie Shao,Wei Zhang","id":"894ed1aba8e42a4ec27ba53ecde383b14c5128ca","summary":"This survey paper provides an examination of alternative open-sourced models of large GPTs, focusing on user-friendly and relatively small models that facilitate easier deployment and accessibility, and aims to equip researchers, practitioners, and enthusiasts with a thorough understanding of these models.","score":5},{"url":"https://www.semanticscholar.org/paper/4eb87eaa193929dbef93fa2db9419245a8e8916f","title":"TextBind: Multi-turn Interleaved Multimodal Instruction-following in the Wild","venue":"arXiv.org","year":2023,"referenceCount":68,"citationCount":4,"influentialCitationCount":1,"publicationDate":"14/09/2023","authors":"Huayang Li,Siheng Li,Deng Cai,Longyue Wang,Lemao Liu,Taro Watanabe,Yujiu Yang,Shuming Shi","id":"4eb87eaa193929dbef93fa2db9419245a8e8916f","summary":"This work introduces TextBind, an almost annotation-free framework for empowering larger language models with the multi-turn interleaved multimodal instruction-following capabilities, and devise MIM, a language model-centric architecture that seamlessly integrates image encoder and decoder models.","score":5},{"url":"https://www.semanticscholar.org/paper/f34b6b4f75fb4e6f2c5654ee13fb2479c6170b3a","title":"Kosmos-2.5: A Multimodal Literate Model","venue":"arXiv.org","year":2023,"referenceCount":84,"citationCount":8,"influentialCitationCount":0,"publicationDate":"20/09/2023","authors":"Tengchao Lv,Yupan Huang,Jingye Chen,Lei Cui,Shuming Ma,Ya-Chi Chang,Shaohan Huang,Wenhui Wang,Li Dong,Weiyao Luo,Shaoxiang Wu,Guoxin Wang,Cha Zhang,Furu Wei","id":"f34b6b4f75fb4e6f2c5654ee13fb2479c6170b3a","summary":"Kosmos-2.5 can be readily adapted for any text-intensive image understanding task with different prompts through supervised fine-tuning, making it a general-purpose tool for real-world applications involving text-rich images and paves the way for the future scaling of multimodal large language models.","score":5},{"url":"https://www.semanticscholar.org/paper/7b689adb8c156d6158660f90d1c86888ee281f63","title":"DreamLLM: Synergistic Multimodal Comprehension and Creation","venue":"arXiv.org","year":2023,"referenceCount":169,"citationCount":27,"influentialCitationCount":2,"publicationDate":"20/09/2023","authors":"Runpei Dong,Chunrui Han,Yuang Peng,Zekun Qi,Zheng Ge,Jinrong Yang,Liang Zhao,Jian‐Yuan Sun,Hongyu Zhou,Hao-Ran Wei,Xiangwen Kong,Xiangyu Zhang,Kaisheng Ma,Li Yi","id":"7b689adb8c156d6158660f90d1c86888ee281f63","summary":"A learning framework that first achieves versatile Multimodal Large Language Models (MLLMs) empowered with frequently overlooked synergy between multimodal comprehension and creation, reaping from the enhanced learning synergy.","score":5},{"url":"https://www.semanticscholar.org/paper/092245d86b77181c36f972b1b7a17a59cd989c4a","title":"Guiding Instruction-based Image Editing via Multimodal Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":63,"citationCount":7,"influentialCitationCount":0,"publicationDate":"29/09/2023","authors":"Tsu-Jui Fu,Wenze Hu,Xianzhi Du,William Yang Wang,Yinfei Yang,Zhe Gan","id":"092245d86b77181c36f972b1b7a17a59cd989c4a","summary":"This work investigates how MLLMs facilitate edit instructions and presents MLLM-Guided Image Editing (MGIE), which learns to derive expressive instructions and provides explicit guidance and can lead to a notable improvement in automatic metrics and human evaluation while maintaining competitive inference efficiency.","score":5},{"url":"https://www.semanticscholar.org/paper/a710efa9247207a72f06e0c9db302fd3ecab5fbb","title":"Towards Robust Multi-Modal Reasoning via Model Selection","venue":"arXiv.org","year":2023,"referenceCount":57,"citationCount":0,"influentialCitationCount":0,"publicationDate":"12/10/2023","authors":"Xiangyan Liu,Rongxue Li,Wei Ji,Tao Lin","id":"a710efa9247207a72f06e0c9db302fd3ecab5fbb","summary":"This framework improves model selection and bolsters the robustness of multi-modal agents in multi-step reasoning, and enables dynamic model selection, considering both user inputs and subtask dependencies, thereby robustifying the overall reasoning process.","score":5},{"url":"https://www.semanticscholar.org/paper/aad3d2e690f6c73f04a14622ceff51464bbc560e","title":"Chat-UniVi: Unified Visual Representation Empowers Large Language Models with Image and Video Understanding","venue":"arXiv.org","year":2023,"referenceCount":74,"citationCount":6,"influentialCitationCount":1,"publicationDate":"14/11/2023","authors":"Peng Jin,Ryuichi Takanobu,Caiwan Zhang,Xiaochun Cao,Li Yuan","id":"aad3d2e690f6c73f04a14622ceff51464bbc560e","summary":"This work introduces Chat-UniVi, a unified vision-language model capable of comprehending and engaging in conversations involving images and videos through a unified visual representation that consistently outperforms even existing methods exclusively designed for either images or videos.","score":5},{"url":"https://www.semanticscholar.org/paper/107fb6eec2febbae12db29bf3e311aaf5680027c","title":"Video-LLaVA: Learning United Visual Representation by Alignment Before Projection","venue":"arXiv.org","year":2023,"referenceCount":54,"citationCount":2,"influentialCitationCount":0,"publicationDate":"16/11/2023","authors":"Bin Lin,Bin Zhu,Yang Ye,Munan Ning,Peng Jin,Li Yuan","id":"107fb6eec2febbae12db29bf3e311aaf5680027c","summary":"This work unify visual representation into the language feature space to advance the foundational LLM towards a unified LVLM, and establishes a simple but robust LVLM baseline, Video-LLaVA, which learns from a mixed dataset of images and videos, mutually enhancing each other.","score":5},{"url":"https://www.semanticscholar.org/paper/6d2ab31aa75468f5458b9d96192c3f4a28f55d73","title":"DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving","venue":"arXiv.org","year":2023,"referenceCount":80,"citationCount":2,"influentialCitationCount":0,"publicationDate":"14/12/2023","authors":"Wenhai Wang,Jiangwei Xie,ChuanYang Hu,Haoming Zou,Jianan Fan,Wenwen Tong,Yang Wen,Silei Wu,Hanming Deng,Zhiqi Li,Hao Tian,Lewei Lu,Xizhou Zhu,Xiaogang Wang,Yu Qiao,Jifeng Dai","id":"6d2ab31aa75468f5458b9d96192c3f4a28f55d73","summary":"DriveMLM is introduced, an LLM-based AD framework that can perform close-loop autonomous driving in realistic simulators and can plug-and-play in existing AD systems such as Apollo for close-loop driving.","score":5},{"url":"https://www.semanticscholar.org/paper/6a33e58ef961a3a0a5657518b2be86395eb7c8d0","title":"InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks","venue":"arXiv.org","year":2023,"referenceCount":185,"citationCount":1,"influentialCitationCount":0,"publicationDate":"21/12/2023","authors":"Zhe Chen,Jiannan Wu,Wenhai Wang,Weijie Su,Guo Chen,Sen Xing,Zhong Muyan,Qinglong Zhang,Xizhou Zhu,Lewei Lu,Bin Li,Ping Luo,Tong Lu,Yu Qiao,Jifeng Dai","id":"6a33e58ef961a3a0a5657518b2be86395eb7c8d0","summary":"A large-scale vision-language foundation model (InternVL), which scales up the vision foundation model to 6 billion parameters and progressively aligns it with the LLM, using web-scale image-text data from various sources is designed.","score":5},{"url":"https://www.semanticscholar.org/paper/8ecdbfe011b7189fa0ee49ffc4e42a93d728a371","title":"On Evaluating Adversarial Robustness of Large Vision-Language Models","venue":"arXiv.org","year":2023,"referenceCount":108,"citationCount":25,"influentialCitationCount":2,"publicationDate":"26/05/2023","authors":"Yunqing Zhao,Tianyu Pang,Chao Du,Xiao Yang,Chongxuan Li,Ngai-Man Cheung,Min Lin","id":"8ecdbfe011b7189fa0ee49ffc4e42a93d728a371","summary":"Evaluating the robustness of open-source large VLMs in the most realistic and high-risk setting, where adversaries have only black-box system access and seek to deceive the model into returning the targeted responses is proposed.","score":5},{"url":"https://www.semanticscholar.org/paper/6bdfffbf92d01c8b543088d40d46233610e469a8","title":"CLIP in Medical Imaging: A Comprehensive Survey","venue":"arXiv.org","year":2023,"referenceCount":218,"citationCount":1,"influentialCitationCount":0,"publicationDate":"12/12/2023","authors":"Zihao Zhao,Yuxiao Liu,Han Wu,Yonghao Li,Sheng Wang,L. Teng,Disheng Liu,Xiang Li,Zhiming Cui,Qian Wang,Dinggang Shen","id":"6bdfffbf92d01c8b543088d40d46233610e469a8","summary":"This survey offers an in-depth exploration of the CLIP paradigm within the domain of medical imaging, regarding both refined CLIP pre-training and CLIP-driven applications, and investigates the adaptation of CLIP pre-training in the medical domain.","score":5},{"url":"https://www.semanticscholar.org/paper/0ebc861f5478561f12941e6b48aad30574e996d8","title":"Video ChatCaptioner: Towards Enriched Spatiotemporal Descriptions","venue":"arXiv.org","year":2023,"referenceCount":44,"citationCount":11,"influentialCitationCount":0,"publicationDate":"09/04/2023","authors":"Jun Chen,Deyao Zhu,Kilichbek Haydarov,Xiang Li,Mohamed Elhoseiny","id":"0ebc861f5478561f12941e6b48aad30574e996d8","summary":"This work introduces Video ChatCaptioner, an innovative approach for creating more comprehensive spatiotemporal video descriptions, specifically designed to select frames for posing video content-driven questions and shows promise as a method for enhancing video content.","score":5},{"url":"https://www.semanticscholar.org/paper/42a30dc5470f54ec249f25d3c31e05d7c376c8e3","title":"VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks","venue":"arXiv.org","year":2023,"referenceCount":81,"citationCount":103,"influentialCitationCount":8,"publicationDate":"18/05/2023","authors":"Wen Wang,Zhe Chen,Xiaokang Chen,Jiannan Wu,Xizhou Zhu,Gang Zeng,Ping Luo,Tong Lu,Jie Zhou,Y. Qiao,Jifeng Dai","id":"42a30dc5470f54ec249f25d3c31e05d7c376c8e3","summary":"This work presents an LLM-based framework for vision-centric tasks, termed VisionLLM, which provides a unified perspective for vision and language tasks by treating images as a foreign language and aligning vision-focused tasks with language tasks that can be flexibly defined and managed using language instructions.","score":5},{"url":"https://www.semanticscholar.org/paper/06091944b864d6dc473cab63321a95fb9c4067cc","title":"ChatCAD+: Towards a Universal and Reliable Interactive CAD using LLMs","venue":"arXiv.org","year":2023,"referenceCount":50,"citationCount":9,"influentialCitationCount":1,"publicationDate":"25/05/2023","authors":"Zihao Zhao,Sheng Wang,Jinchen Gu,Yitao Zhu,Lanzhuju Mei,Zixu Zhuang,Zhiming Cui,Qian Wang,Dinggang Shen","id":"06091944b864d6dc473cab63321a95fb9c4067cc","summary":"ChatCAD+, which is designed to be universal and reliable, is introduced, capable of handling medical images from diverse domains and leveraging up-to-date information from reputable medical websites to provide reliable medical advice.","score":5},{"url":"https://www.semanticscholar.org/paper/833cdd713c27ab5899bb912a1d511c10af61cefb","title":"Making Multimodal Generation Easier: When Diffusion Models Meet LLMs","venue":"arXiv.org","year":2023,"referenceCount":58,"citationCount":3,"influentialCitationCount":0,"publicationDate":"13/10/2023","authors":"Xiangyu Zhao,Bo Liu,Qijiong Liu,Guangyuan Shi,Xiao-Ming Wu","id":"833cdd713c27ab5899bb912a1d511c10af61cefb","summary":"Efficient model designed to enhance multimodal understanding and generation by harnessing the capabilities of diffusion models and large language models, built upon a bidirectional conditional diffusion model named BiDiffuser, which promotes more efficient interactions between modalities.","score":5},{"url":"https://www.semanticscholar.org/paper/246017780386eba39d6cda760a1c2c70356baa50","title":"VIoTGPT: Learning to Schedule Vision Tools towards Intelligent Video Internet of Things","venue":"arXiv.org","year":2023,"referenceCount":78,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/12/2023","authors":"Yaoyao Zhong,Mengshi Qi,Rui Wang,Yuhan Qiu,Yang Zhang,Huadong Ma","id":"246017780386eba39d6cda760a1c2c70356baa50","summary":"VIoTGPT is built, the framework based on LLMs to correctly interact with humans, query knowledge videos, and invoke vision models to accomplish complicated tasks to address the challenges posed by the fine-grained and interrelated vision tool usage of VIoT.","score":5},{"url":"https://www.semanticscholar.org/paper/2ddb6a253bc944c7a88e67747e44c34aafe734fc","title":"ViCrop: Perceiving Small Visual Details in Zero-shot Visual Question Answering with Multimodal Large Language Models","venue":"","year":2023,"referenceCount":38,"citationCount":0,"influentialCitationCount":0,"publicationDate":"24/10/2023","authors":"Jiarui Zhang,Mahyar Khayatkhoei,P. Chhikara,Filip Ilievski","id":"2ddb6a253bc944c7a88e67747e44c34aafe734fc","summary":"This work proposes ViCrop, a general framework that utilizes automatic visual cropping to enhance zero-shot VQA of MLLMs and improves MLLMs' zero-shot accuracy across different VQA datasets, for example, enhances BLIP2-T5's performance by $32.23\\% on the TextVQA test set.","score":5},{"url":"https://www.semanticscholar.org/paper/88bddfb7d1e0462be8fe99fdbd71c658140cb17b","title":"From Image to Language: A Critical Analysis of Visual Question Answering (VQA) Approaches, Challenges, and Opportunities","venue":"arXiv.org","year":2023,"referenceCount":304,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/11/2023","authors":"Md Farhan Ishmam,Md Sakib Hossain Shovon,M. F. Mridha,Nilanjan Dey","id":"88bddfb7d1e0462be8fe99fdbd71c658140cb17b","summary":"This work presents a survey in the domain of VQA that delves into the intricacies of V QA datasets and methods over the field's history, introduces a detailed taxonomy to categorize the facets of VZA, and highlights the recent trends, challenges, and scopes for improvement.","score":5},{"url":"https://www.semanticscholar.org/paper/f4793adffd6f67ffcb93ccfc5672ab301b8a2b96","title":"PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering","venue":"arXiv.org","year":2023,"referenceCount":40,"citationCount":32,"influentialCitationCount":3,"publicationDate":"17/05/2023","authors":"Xiaoman Zhang,Chaoyi Wu,Ziheng Zhao,Weixiong Lin,Ya Zhang,Yanfeng Wang,Weidi Xie","id":"f4793adffd6f67ffcb93ccfc5672ab301b8a2b96","summary":"This paper proposes a generative-based model for medical visual understanding by aligning visual information from a pre-trained vision encoder with a large language model, and establishes a scalable pipeline to construct a large-scale medical visual question-answering dataset.","score":5},{"url":"https://www.semanticscholar.org/paper/07d45ce7de598ef03b400f8ddba7d2e055e77a08","title":"BiomedGPT: A Unified and Generalist Biomedical Generative Pre-trained Transformer for Vision, Language, and Multimodal Tasks","venue":"","year":2023,"referenceCount":131,"citationCount":0,"influentialCitationCount":0,"publicationDate":"26/05/2023","authors":"Kai Zhang,Jun Yu,Eashan Adhikarla,Rong Zhou,Zhiling Yan,Yixin Liu,Zheng Liu,Lifang He,Brian D. Davison,Xiang Li,Hui Ren,S. Fu,James Zou,Wei Liu,Jing Huang,Chen Chen,Yuyin Zhou,Tianming Liu,Xun Chen,Yong Chen,Quanzheng Li,Hongfang Liu,Lichao Sun","id":"07d45ce7de598ef03b400f8ddba7d2e055e77a08","summary":"BiomedGPT is proposed, the first open-source and generalist visual language AI for diverse biomedical tasks and facilitates zero-shot transfer learning, greatly enhancing its utility as a biomedical assistant, similar to ChatGPT.","score":5},{"url":"https://www.semanticscholar.org/paper/baa1dc079d98ca76b0173c8d653fed759fd0a371","title":"A scoping review on multimodal deep learning in biomedical images and texts","venue":"Journal of Biomedical Informatics","year":2023,"referenceCount":148,"citationCount":4,"influentialCitationCount":0,"publicationDate":"14/07/2023","authors":"Zhaoyi Sun,Mingquan Lin,Qingqing Zhu,Qianqian Xie,Fei Wang,Zhiyong Lu,Yifan Peng","id":"baa1dc079d98ca76b0173c8d653fed759fd0a371","summary":"This study reviewed the current uses of multimodal deep learning on five tasks: report generation, Visual question answering, Cross-modal retrieval, computer-aided diagnosis, and Semantic segmentation, and highlighted the diverse applications and potential of MDL.","score":5},{"url":"https://www.semanticscholar.org/paper/f0950a3f27c0fefffba60ae1c9a8ee360d5eb55f","title":"Instruction Tuning for Large Language Models: A Survey","venue":"arXiv.org","year":2023,"referenceCount":150,"citationCount":58,"influentialCitationCount":2,"publicationDate":"21/08/2023","authors":"Shengyu Zhang,Linfeng Dong,Xiaoya Li,Sen Zhang,Xiaofei Sun,Shuhe Wang,Jiwei Li,Runyi Hu,Tianwei Zhang,Fei Wu,Guoyin Wang","id":"f0950a3f27c0fefffba60ae1c9a8ee360d5eb55f","summary":"A systematic review of the literature, including the general methodology of IT, the construction of IT datasets, the training of IT models, and applications to different modalities, domains and applications, along with an analysis on aspects that influence the outcome of IT.","score":5},{"url":"https://www.semanticscholar.org/paper/da9134f694959b68027c33c8e998ffb3d41305da","title":"Exploring Question Decomposition for Zero-Shot VQA","venue":"arXiv.org","year":2023,"referenceCount":60,"citationCount":0,"influentialCitationCount":0,"publicationDate":"25/10/2023","authors":"Zaid Khan,B. Vijaykumar,S. Schulter,Manmohan Chandraker,Yun Fu","id":"da9134f694959b68027c33c8e998ffb3d41305da","summary":"A model-driven selective decomposition approach for second-guessing predictions and correcting errors is introduced, and its effectiveness on eight VQA tasks across three domains is validated, showing consistent improvements in accuracy.","score":5},{"url":"https://www.semanticscholar.org/paper/d48fa3ed73817563130ef217d85011ce1fbe7470","title":"BESTMVQA: A Benchmark Evaluation System for Medical Visual Question Answering","venue":"arXiv.org","year":2023,"referenceCount":23,"citationCount":0,"influentialCitationCount":0,"publicationDate":"13/12/2023","authors":"Xiaojie Hong,Zixin Song,Liangzhi Li,Xiaoli Wang,Feiyan Liu","id":"d48fa3ed73817563130ef217d85011ce1fbe7470","summary":"A Benchmark Evaluation SysTem for Medical Visual Question Answering, denoted by BESTMVQA, is developed, which provides a useful tool for users to automatically build Med-V QA datasets, which helps overcoming the data insufficient problem.","score":5},{"url":"https://www.semanticscholar.org/paper/352252231462c24440bc0016638ea5fe8d4c6f7e","title":"UniDCP: Unifying Multiple Medical Vision-language Tasks via Dynamic Cross-modal Learnable Prompts","venue":"arXiv.org","year":2023,"referenceCount":67,"citationCount":0,"influentialCitationCount":0,"publicationDate":"18/12/2023","authors":"Chenlu Zhan,Yufei Zhang,Yu Lin,Gaoang Wang,Hongwei Wang","id":"352252231462c24440bc0016638ea5fe8d4c6f7e","summary":"UniDCP is the first Med-VLP model capable of performing all 8 medical uni-modal and cross-modal tasks over 14 corresponding datasets, consistently yielding superior results over diverse state-of-the-art methods.","score":5},{"url":"https://www.semanticscholar.org/paper/bca0bbd01ea917b7a9fe369288ea3ba03d3b1ff3","title":"A Survey of Large Language Models in Medicine: Progress, Application, and Challenge","venue":"arXiv.org","year":2023,"referenceCount":183,"citationCount":3,"influentialCitationCount":0,"publicationDate":2023,"authors":"Hongjian Zhou,Boyang Gu,Xinyu Zou,Yiru Li,Sam S. Chen,Peilin Zhou,Junling Liu,Y. Hua,Chengfeng Mao,Xian Wu,Zheng Li,Fenglin Liu","id":"bca0bbd01ea917b7a9fe369288ea3ba03d3b1ff3","summary":"This survey provides a comprehensive overview of the current progress, applications, and challenges faced by LLMs in medicine.","score":4},{"url":"https://www.semanticscholar.org/paper/ef321c6f174ac59916ac54ec40ad18bca5b58e5c","title":"PerceptionGPT: Effectively Fusing Visual Perception into LLM","venue":"arXiv.org","year":2023,"referenceCount":50,"citationCount":3,"influentialCitationCount":0,"publicationDate":"11/11/2023","authors":"Renjie Pi,Lewei Yao,Jiahui Gao,Jipeng Zhang,Tong Zhang","id":"ef321c6f174ac59916ac54ec40ad18bca5b58e5c","summary":"A novel end-to-end framework named PerceptionGPT, which efficiently and effectively equips the VLLMs with visual perception abilities by leveraging the representation power of LLMs' token embedding and demonstrates significant improvements over previous methods with much fewer trainable parameters and GPU hours.","score":4},{"url":"https://www.semanticscholar.org/paper/f32ea390686b1eee3ba5b53c7a85e9e9385d4b94","title":"Visual Program Distillation: Distilling Tools and Programmatic Reasoning into Vision-Language Models","venue":"arXiv.org","year":2023,"referenceCount":59,"citationCount":0,"influentialCitationCount":0,"publicationDate":"05/12/2023","authors":"Yushi Hu,Otilia Stretcu,Chun-Ta Lu,Krishnamurthy Viswanathan,Kenji Hata,Enming Luo,Ranjay Krishna,Ariel Fuxman","id":"f32ea390686b1eee3ba5b53c7a85e9e9385d4b94","summary":"Visual Program Distillation (VPD) is proposed, an instruction tuning framework that produces a vision-language model (VLM) capable of solving complex visual tasks with a single forward pass and improves the VLM's ability to count, understand spatial relations, and reason compositionally.","score":4},{"url":"https://www.semanticscholar.org/paper/5502d769595981009e43344f8914e287acca2359","title":"ModaVerse: Efficiently Transforming Modalities with LLMs","venue":"arXiv.org","year":2024,"referenceCount":56,"citationCount":0,"influentialCitationCount":0,"publicationDate":"12/01/2024","authors":"Xinyu Wang,Bohan Zhuang,Qi Wu","id":"5502d769595981009e43344f8914e287acca2359","summary":"ModaVerse is introduced, a Multi-modal Large Language Model (MLLM) capable of comprehending and transforming content across various modalities including images, videos, and audio, and a novel Input/Output (I/O) alignment mechanism that operates directly at the level of natural language.","score":4},{"url":"https://www.semanticscholar.org/paper/a5036f31f0e629dc661f120b8c3b1f374d479ab8","title":"Visual Instruction Tuning","venue":"arXiv.org","year":2023,"referenceCount":63,"citationCount":613,"influentialCitationCount":204,"publicationDate":"17/04/2023","authors":"Haotian Liu,Chunyuan Li,Qingyang Wu,Yong Jae Lee","id":"a5036f31f0e629dc661f120b8c3b1f374d479ab8","summary":"This paper presents LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding and introduces GPT-4 generated visual instruction tuning data, the model and code base publicly available.","score":4},{"url":"https://www.semanticscholar.org/paper/ca6a2bc279be5a3349a22bfd6866ed633d18734b","title":"MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":49,"citationCount":512,"influentialCitationCount":126,"publicationDate":"20/04/2023","authors":"Deyao Zhu,Jun Chen,Xiaoqian Shen,Xiang Li,Mohamed Elhoseiny","id":"ca6a2bc279be5a3349a22bfd6866ed633d18734b","summary":"MiniGPT-4 is presented, which aligns a frozen visual encoder with a frozen advanced LLM, Vicuna, using one projection layer to uncovers that properly aligning the visual features with an advanced large language model can possess numerous advanced multi-modal abilities demonstrated by G PT-4.","score":4},{"url":"https://www.semanticscholar.org/paper/7e32aac43e9f1df49e116add03327ee6f365dbf3","title":"mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality","venue":"arXiv.org","year":2023,"referenceCount":36,"citationCount":259,"influentialCitationCount":36,"publicationDate":"27/04/2023","authors":"Qinghao Ye,Haiyang Xu,Guohai Xu,Jiabo Ye,Ming Yan,Yi Zhou,Junyan Wang,Anwen Hu,Pengcheng Shi,Yaya Shi,Chenliang Li,Yuanhong Xu,Hehong Chen,Junfeng Tian,Qiang Qi,Ji Zhang,Feiyan Huang","id":"7e32aac43e9f1df49e116add03327ee6f365dbf3","summary":null,"score":4},{"url":"https://www.semanticscholar.org/paper/54a8b153ed04a872da878d695239bdc413dc782c","title":"InternGPT: Solving Vision-Centric Tasks by Interacting with Chatbots Beyond Language","venue":"arXiv.org","year":2023,"referenceCount":83,"citationCount":36,"influentialCitationCount":0,"publicationDate":"09/05/2023","authors":"Zhaoyang Liu,Yinan He,Wenhai Wang,Weiyun Wang,Yi Wang,Shoufa Chen,Qing-Long Zhang,Yang Yang,Qingyun Li,Jiashuo Yu,Kunchang Li,Zhe Chen,Xuecheng Yang,Xizhou Zhu,Yali Wang,Limin Wang,Ping Luo,Jifeng Dai,Yu Qiao","id":"54a8b153ed04a872da878d695239bdc413dc782c","summary":"By incorporating pointing instructions, the proposed iGPT significantly improves the efficiency of communication between users and chatbots, as well as the accuracy of chatbots in vision-centric tasks, especially in complicated visual scenarios where the number of objects is greater than 2.","score":4},{"url":"https://www.semanticscholar.org/paper/00cb69a9f280317d1c59ac5827551ee9b10642b8","title":"EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought","venue":"arXiv.org","year":2023,"referenceCount":72,"citationCount":58,"influentialCitationCount":3,"publicationDate":"24/05/2023","authors":"Yao Mu,Qinglong Zhang,Mengkang Hu,Wen Wang,Mingyu Ding,Jun Jin,Bin Wang,Jifeng Dai,Y. Qiao,Ping Luo","id":"00cb69a9f280317d1c59ac5827551ee9b10642b8","summary":"This work introduces EmbodiedGPT, an end-to-end multi-modal foundation model for embodied AI, empowering embodied agents with multi- modal understanding and execution capabilities, and significantly enhances the success rate of the embodied control task by extracting more effective features.","score":4},{"url":"https://www.semanticscholar.org/paper/d48cb91b9e555194f7494c4d4bb9815021d3ee45","title":"VideoChat: Chat-Centric Video Understanding","venue":"arXiv.org","year":2023,"referenceCount":65,"citationCount":112,"influentialCitationCount":20,"publicationDate":"10/05/2023","authors":"Kunchang Li,Yinan He,Yi Wang,Yizhuo Li,Wen Wang,Ping Luo,Yali Wang,Limin Wang,Yu Qiao","id":"d48cb91b9e555194f7494c4d4bb9815021d3ee45","summary":"An end-to-end chat-centric video understanding system that integrates video foundation models and large language models via a learnable neural interface, excelling in spatiotemporal reasoning, event localization, and causal relationship inference is initiated.","score":4},{"url":"https://www.semanticscholar.org/paper/9837349417e36ef5be06da0fd6c74042148bdaa2","title":"Visual Programming for Text-to-Image Generation and Evaluation","venue":"arXiv.org","year":2023,"referenceCount":68,"citationCount":17,"influentialCitationCount":3,"publicationDate":"24/05/2023","authors":"Jaemin Cho,Abhaysinh Zala,Mohit Bansal","id":"9837349417e36ef5be06da0fd6c74042148bdaa2","summary":"This work proposes two novel interpretable/explainable visual programming frameworks for text-to-image (T2I) generation and evaluation and introduces VPEval, an interpretable and explainable evaluation framework for T2I generation based on visual programming.","score":4},{"url":"https://www.semanticscholar.org/paper/9c3a9b4821daa03cb5369041d59d2714329a3811","title":"Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":53,"citationCount":31,"influentialCitationCount":3,"publicationDate":"24/05/2023","authors":"Gen Luo,Yiyi Zhou,Tianhe Ren,Shen Chen,Xiaoshuai Sun,Rongrong Ji","id":"9c3a9b4821daa03cb5369041d59d2714329a3811","summary":"A novel and affordable solution for the effective VL adaption of LLMs, called Mixture-of-Modality Adaptation (MMA), which adopts lightweight modules, i.e., adapters, to bridge the gap between LLMs and VL tasks, which also enables the joint optimization of the image and language models","score":4},{"url":"https://www.semanticscholar.org/paper/b458fc5261595f44b36325e5eaea1f874d65138f","title":"GPT4Tools: Teaching Large Language Model to Use Tools via Self-instruction","venue":"arXiv.org","year":2023,"referenceCount":63,"citationCount":56,"influentialCitationCount":11,"publicationDate":"30/05/2023","authors":"Rui Yang,Lin Song,Yanwei Li,Sijie Zhao,Yixiao Ge,Xiu Li,Ying Shan","id":"b458fc5261595f44b36325e5eaea1f874d65138f","summary":"The GPT4Tools based on self-instruct to enable open-source LLMs, such as LLaMA and OPT, to use tools, generates an instruction-following dataset by prompting an advanced teacher with various multi-modal contexts using the Low-Rank Adaptation (LoRA) optimization.","score":4},{"url":"https://www.semanticscholar.org/paper/d47524cd5c3c4b57af2e5a29f6f91c420310f236","title":"MIMIC-IT: Multi-Modal In-Context Instruction Tuning","venue":"arXiv.org","year":2023,"referenceCount":55,"citationCount":64,"influentialCitationCount":7,"publicationDate":"08/06/2023","authors":"Bo Li,Yuanhan Zhang,Liangyu Chen,Jinghao Wang,Fanyi Pu,Jingkang Yang,C. Li,Ziwei Liu","id":"d47524cd5c3c4b57af2e5a29f6f91c420310f236","summary":"MultI-Modal In-Context Instruction Tuning (MIMIC-IT), a dataset comprising 2.8 million multimodal instruction-response pairs, with 2.2 million unique instructions derived from images and videos, is presented and a large VLM named Otter is trained.","score":4},{"url":"https://www.semanticscholar.org/paper/4c4d176c6e28f48041f215d563f6ee8633534cff","title":"Valley: Video Assistant with Large Language model Enhanced abilitY","venue":"arXiv.org","year":2023,"referenceCount":39,"citationCount":33,"influentialCitationCount":8,"publicationDate":"12/06/2023","authors":"Ruipu Luo,Ziwang Zhao,Min Yang,Junwei Dong,Ming-Hui Qiu,Pengcheng Lu,Tao Wang,Zhongyu Wei","id":"4c4d176c6e28f48041f215d563f6ee8633534cff","summary":"A novel multi-modal foundation model capable of comprehending video, image, and language within a general framework is developed, and Qualitative experiments demonstrate that Valley has the potential to function as a highly effective video assistant that can make complex video understanding scenarios easy.","score":4},{"url":"https://www.semanticscholar.org/paper/d98536f24272e258b1d399074b64284d64786099","title":"AVIS: Autonomous Visual Information Seeking with Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":133,"citationCount":9,"influentialCitationCount":1,"publicationDate":"13/06/2023","authors":"Ziniu Hu,Ahmet Iscen,Chen Sun,Kai-Wei Chang,Yizhou Sun,David A. Ross,C. Schmid,A. Fathi","id":"d98536f24272e258b1d399074b64284d64786099","summary":"An autonomous information seeking visual question answering framework that leverages a Large Language Model to dynamically strategize the utilization of external tools and to investigate their outputs, thereby acquiring the indispensable knowledge needed to provide answers to the posed questions is proposed.","score":4},{"url":"https://www.semanticscholar.org/paper/ebddfdc5d845a788e8062eddbbf7a335737cb99b","title":"What Matters in Training a GPT4-Style Language Model with Multimodal Inputs?","venue":"arXiv.org","year":2023,"referenceCount":108,"citationCount":27,"influentialCitationCount":3,"publicationDate":"05/07/2023","authors":"Yan Zeng,Hanbo Zhang,Jiani Zheng,Jiangnan Xia,Guoqiang Wei,Yang Wei,Yuchen Zhang,Tao Kong","id":"ebddfdc5d845a788e8062eddbbf7a335737cb99b","summary":"Lynx is presented, which performs the most accurate multi-modal understanding while keeping the best multi- modal generation ability compared to existing open-sourced GPT4-style models.","score":4},{"url":"https://www.semanticscholar.org/paper/094883e42bb9a41f602c0715c1059bc431e33fb2","title":"GPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest","venue":"arXiv.org","year":2023,"referenceCount":96,"citationCount":57,"influentialCitationCount":6,"publicationDate":"07/07/2023","authors":"Shilong Zhang,Pei Sun,Shoufa Chen,Min Xiao,Wenqi Shao,Wenwei Zhang,Kai Chen,Ping Luo","id":"094883e42bb9a41f602c0715c1059bc431e33fb2","summary":"Spatial instruction tuning is proposed, which introduces the reference to the region-of-interest (RoI) in the instruction, which achieves a remarkable accuracy of 81.6%, surpassing all existing models by a significant margin and almost reaching human-level performance of 85.0%.","score":4},{"url":"https://www.semanticscholar.org/paper/2e52178416b93a0a7dc1cb7d21d04c7e72c2ce0f","title":"ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring Instruction Tuning","venue":"arXiv.org","year":2023,"referenceCount":39,"citationCount":14,"influentialCitationCount":2,"publicationDate":"18/07/2023","authors":"Liang Zhao,En Yu,Zheng Ge,Jinrong Yang,Hao-Ran Wei,Hongyu Zhou,Jian‐Yuan Sun,Yuang Peng,Runpei Dong,Chunrui Han,Xiangyu Zhang","id":"2e52178416b93a0a7dc1cb7d21d04c7e72c2ce0f","summary":"This study proposes ChatSpot, a unified end-to-end multimodal large language model that supports diverse forms of interactivity including mouse clicks, drag-and-drop, and drawing boxes, which provides a more flexible and seamless interactive experience.","score":4},{"url":"https://www.semanticscholar.org/paper/ad113d8b6f8f6dace7abd12dc88af520aaaf3fc7","title":"LISA: Reasoning Segmentation via Large Language Model","venue":"arXiv.org","year":2023,"referenceCount":64,"citationCount":56,"influentialCitationCount":14,"publicationDate":"01/08/2023","authors":"Xin Lai,Zhuotao Tian,Yukang Chen,Yanwei Li,Yuhui Yuan,Shu Liu,Jiaya Jia","id":"ad113d8b6f8f6dace7abd12dc88af520aaaf3fc7","summary":"This work proposes a new segmentation task -- reasoning segmentation, designed to output a segmentation mask given a complex and implicit query text, and presents LISA, which inherits the language generation capabilities of the multi-modal Large Language Model (LLM) while also possessing the ability to produce segmentation masks.","score":4},{"url":"https://www.semanticscholar.org/paper/d53945d4afb4528590d79e20de52883d29037e86","title":"FashionLOGO: Prompting Multimodal Large Language Models for Fashion Logo Embeddings","venue":"arXiv.org","year":2023,"referenceCount":44,"citationCount":0,"influentialCitationCount":0,"publicationDate":"17/08/2023","authors":"Yulin Su,Min Yang,Minghui Qiu,Jing Wang,Tao Wang","id":"d53945d4afb4528590d79e20de52883d29037e86","summary":"This work explores how MLLMs can improve logo embedding by prompting them to generate explicit textual knowledge through three types of prompts, including image OCR, brief captions, and detailed descriptions prompts, in a zero-shot setting and adopt a cross-attention transformer to enable image embedding queries to learn supplementary knowledge from textual embeddings automatically.","score":4},{"url":"https://www.semanticscholar.org/paper/eb5cf10406a8ad31e0ebe56b36571d5db4758a62","title":"PUMGPT: A Large Vision-Language Model for Product Understanding","venue":"arXiv.org","year":2023,"referenceCount":38,"citationCount":1,"influentialCitationCount":0,"publicationDate":"18/08/2023","authors":"Shuhui Wu,Zengming Tang,Zongyi Guo,Weiwei Zhang,Baoliang Cui,Haihong Tang,Weiming Lu","id":"eb5cf10406a8ad31e0ebe56b36571d5db4758a62","summary":"This paper presents PUMGPT, a large vision-language model that aims at unifying all product understanding tasks under a singular model structure, and proposes Layer-wise Adapters (LA), an approach that provides enhanced alignment with fewer visual tokens and enables parameter-efficient fine-tuning.","score":4},{"url":"https://www.semanticscholar.org/paper/6bcc6ab9c28805d4067e99b2cdc7524550fe80e1","title":"PointLLM: Empowering Large Language Models to Understand Point Clouds","venue":"arXiv.org","year":2023,"referenceCount":72,"citationCount":25,"influentialCitationCount":3,"publicationDate":"31/08/2023","authors":"Runsen Xu,Xiaolong Wang,Tai Wang,Yilun Chen,Jiangmiao Pang,Dahua Lin","id":"6bcc6ab9c28805d4067e99b2cdc7524550fe80e1","summary":"Experimental results reveal PointLLM's superior performance over existing 2D and 3D baselines, with a notable achievement in human-evaluated object captioning tasks where it surpasses human annotators in over 50% of the samples.","score":4},{"url":"https://www.semanticscholar.org/paper/3ec464696db25acc2c39a6d967ec3df09e06f633","title":"Multimodal Multi-Hop Question Answering Through a Conversation Between Tools and Efficiently Finetuned Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":33,"citationCount":0,"influentialCitationCount":0,"publicationDate":"16/09/2023","authors":"Hossein Rajabzadeh,Suyuchen Wang,Hyock Ju Kwon,Bang Liu","id":"3ec464696db25acc2c39a6d967ec3df09e06f633","summary":"A tool-interacting divide-and-conquer strategy enabling large language models (LLMs) to answer complex multimodal multi-hop questions, demonstrating the efficacy and generality of this approach.","score":4},{"url":"https://www.semanticscholar.org/paper/bee68767debbdc96d6f75947e544a8be98b869e3","title":"Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model: Explorations with GPT4-Vision and Beyond","venue":"arXiv.org","year":2023,"referenceCount":60,"citationCount":9,"influentialCitationCount":2,"publicationDate":"03/10/2023","authors":"Liang Chen,Yichi Zhang,Shuhuai Ren,Haozhe Zhao,Zefan Cai,Yuchi Wang,Tianyu Liu,Baobao Chang","id":"bee68767debbdc96d6f75947e544a8be98b869e3","summary":"This study introduces a new benchmark called PCA-EVAL, which evaluates embodied decision-making from the perspectives of Perception, Cognition, and Action and proposes HOLMES, a multi-agent cooperation framework that allows LLMs to leverage MLLMs and APIs to gather multimodal information for informed decision- making.","score":4},{"url":"https://www.semanticscholar.org/paper/36b923d97d7cfaf73d11c55c15ea46605ba974a5","title":"BiLL-VTG: Bridging Large Language Models and Lightweight Visual Tools for Video-based Texts Generation","venue":"arXiv.org","year":2023,"referenceCount":68,"citationCount":0,"influentialCitationCount":0,"publicationDate":"16/10/2023","authors":"Ji Qi,Kaixuan Ji,Jifan Yu,Duokang Wang,Bin Xu,Lei Hou,Juanzi Li","id":"36b923d97d7cfaf73d11c55c15ea46605ba974a5","summary":"BiLL-VTG is introduced, a fast adaptive framework that leverages large language models (LLMs) to reasoning on videos based on essential lightweight visual tools and an Instruction-oriented Video Events Recognition (InsOVER) algorithm based on the efficient Hungarian matching to localize corresponding video events using linguistic instructions, enabling LLMs to interact with long videos.","score":4},{"url":"https://www.semanticscholar.org/paper/807f336176070bd3f95b82a16f125ee99b7d2c80","title":"Woodpecker: Hallucination Correction for Multimodal Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":49,"citationCount":11,"influentialCitationCount":1,"publicationDate":"24/10/2023","authors":"Shukang Yin,Chaoyou Fu,Sirui Zhao,Tong Xu,Hao Wang,Dianbo Sui,Yunhang Shen,Ke Li,Xingguo Sun,Enhong Chen","id":"807f336176070bd3f95b82a16f125ee99b7d2c80","summary":"Implemented in a post-remedy manner, Woodpecker can easily serve different MLLMs, while being interpretable by accessing intermediate outputs of the five stages, and shows the huge potential of this new paradigm.","score":4},{"url":"https://www.semanticscholar.org/paper/52941cadbd340344f3e0a6f50719fe55b3de5088","title":"Multimodal Large Language Models: A Survey","venue":"BigData Congress [Services Society]","year":2023,"referenceCount":75,"citationCount":5,"influentialCitationCount":0,"publicationDate":"22/11/2023","authors":"Jiayang Wu,Wensheng Gan,Zefeng Chen,Shicheng Wan,Philip S. Yu","id":"52941cadbd340344f3e0a6f50719fe55b3de5088","summary":"A range of multimodal products are introduced, focusing on the efforts of major technology companies, and a compilation of the latest algorithms and commonly used datasets are presented, providing researchers with valuable resources for experimentation and evaluation.","score":4},{"url":"https://www.semanticscholar.org/paper/ecfff30e570498b6c87c5d1319a0cbcdf7a8b86d","title":"LLaVA-Plus: Learning to Use Tools for Creating Multimodal Agents","venue":"arXiv.org","year":2023,"referenceCount":52,"citationCount":8,"influentialCitationCount":1,"publicationDate":"09/11/2023","authors":"Shilong Liu,Hao Cheng,Haotian Liu,Hao Zhang,Feng Li,Tianhe Ren,Xueyan Zou,Jianwei Yang,Hang Su,Jun-Juan Zhu,Lei Zhang,Jianfeng Gao,Chun-yue Li","id":"ecfff30e570498b6c87c5d1319a0cbcdf7a8b86d","summary":"LLaVA-Plus is distinct in that the image query is directly grounded and actively engaged throughout the entire human-AI interaction sessions, significantly improving tool use performance and enabling new scenarios.","score":4},{"url":"https://www.semanticscholar.org/paper/5eea245cc12c55905d4df827d0c9776c5ddfa743","title":"Compositional Chain-of-Thought Prompting for Large Multimodal Models","venue":"arXiv.org","year":2023,"referenceCount":91,"citationCount":1,"influentialCitationCount":1,"publicationDate":"27/11/2023","authors":"Chancharik Mitra,Brandon Huang,Trevor Darrell,Roei Herzig","id":"5eea245cc12c55905d4df827d0c9776c5ddfa743","summary":"The proposed Compositional Chain-of-Thought (CCoT) approach not only improves LMM performance on several vision and language VL compositional benchmarks but also improves the performance of several popular LMMs on general multimodal benchmarks, without the need for fine-tuning or annotated ground-truth SGs.","score":4},{"url":"https://www.semanticscholar.org/paper/b240a1d8ec2860bdd7370daa3144268ce46ac018","title":"Vary: Scaling up the Vision Vocabulary for Large Vision-Language Models","venue":"arXiv.org","year":2023,"referenceCount":52,"citationCount":1,"influentialCitationCount":1,"publicationDate":"11/12/2023","authors":"Haoran Wei,Lingyu Kong,Jinyue Chen,Liang Zhao,Zheng Ge,Jinrong Yang,Jian‐Yuan Sun,Chunrui Han,Xiangyu Zhang","id":"b240a1d8ec2860bdd7370daa3144268ce46ac018","summary":"Compared to the popular BLIP-2, MiniGPT4, and LLaVA, Vary can maintain its vanilla capabilities while enjoying more excellent fine-grained perception and understanding ability and is competent in new document parsing features (OCR or markdown conversion).","score":4},{"url":"https://www.semanticscholar.org/paper/33c3dbf86dd6e338e2a49bba9c2e2193d1eca08a","title":"Vista-LLaMA: Reliable Video Narrator via Equal Distance to Visual Tokens","venue":"arXiv.org","year":2023,"referenceCount":39,"citationCount":0,"influentialCitationCount":0,"publicationDate":"12/12/2023","authors":"Fan Ma,Xiaojie Jin,Heng Wang,Yuchen Xian,Jiashi Feng,Yi Yang","id":"33c3dbf86dd6e338e2a49bba9c2e2193d1eca08a","summary":"This work proposes Vista-LLaMA, a novel framework that maintains the consistent distance between all visual tokens and any language tokens, irrespective of the generated text length, and presents a sequential visual projector that projects the current video frame into tokens of language space with the assistance of the previous frame.","score":4},{"url":"https://www.semanticscholar.org/paper/17a32c825bd746a2625eddc2728092171a9ef72a","title":"Jack of All Tasks, Master of Many: Designing General-purpose Coarse-to-Fine Vision-Language Model","venue":"arXiv.org","year":2023,"referenceCount":126,"citationCount":0,"influentialCitationCount":0,"publicationDate":"19/12/2023","authors":"Shraman Pramanick,Guangxing Han,Rui Hou,Sayan Nag,Ser-Nam Lim,Nicolas Ballas,Qifan Wang,Rama Chellappa,Amjad Almahairi","id":"17a32c825bd746a2625eddc2728092171a9ef72a","summary":"This work introduces VistaLLM, a powerful visual system that addresses coarse- and fine-grained VL tasks over single and multiple input images using a unified framework and addresses the lack of multi-image grounding datasets by introducing a novel task, AttCoSeg (Attribute-level Co-Segmentation), which boosts the model's reasoning and grounding capability over multiple input images.","score":4},{"url":"https://www.semanticscholar.org/paper/46f64681d7a0c7f80303bebb0d62a3b7acbcdcd9","title":"An Improved Baseline for Reasoning Segmentation with Large Language Model","venue":"arXiv.org","year":2023,"referenceCount":42,"citationCount":0,"influentialCitationCount":0,"publicationDate":"28/12/2023","authors":"Senqiao Yang,Tianyuan Qu,Xin Lai,Zhuotao Tian,Bohao Peng,Shu Liu,Jiaya Jia","id":"46f64681d7a0c7f80303bebb0d62a3b7acbcdcd9","summary":"LISA++ is introduced, an update to the existing LISA model, focusing on improving core functionalities while keeping the base architecture intact, and its adaptability and improved features highlight the versatility of the mask-as-embedding paradigm proposed by LISA and the potential as a foundational model for diverse applications.","score":4},{"url":"https://www.semanticscholar.org/paper/4a48d628e53f554eb6ef09a457ca855188b96171","title":"DoraemonGPT: Toward Understanding Dynamic Scenes with Large Language Models","venue":"","year":2024,"referenceCount":112,"citationCount":1,"influentialCitationCount":0,"publicationDate":"16/01/2024","authors":"Zongxin Yang,Guikun Chen,Xiaodi Li,Wenguan Wang,Yi Yang","id":"4a48d628e53f554eb6ef09a457ca855188b96171","summary":"DoraemonGPT is devised, a comprehensive and conceptually elegant system driven by LLMs to handle dynamic video tasks and a novel LLM-driven planner based on Monte Carlo Tree Search is introduced to efficiently explore the large planning space for scheduling various tools.","score":4},{"url":"https://www.semanticscholar.org/paper/4f2a56102bcbf0fe79379c4c27daecbccfb35a26","title":"MLLM-Tool: A Multimodal Large Language Model For Tool Agent Learning","venue":"","year":2024,"referenceCount":44,"citationCount":0,"influentialCitationCount":0,"publicationDate":"19/01/2024","authors":"Chenyu Wang,Weixin Luo,Qianyu Chen,Haonan Mai,Jindi Guo,Sixun Dong,Xiaohua Xuan,Zhengxin Li,Lin Ma,Shenghua Gao","id":"4f2a56102bcbf0fe79379c4c27daecbccfb35a26","summary":"This paper proposes MLLM-Tool, a system incorporating open-source LLMs and multi-modal encoders so that the learnt LLMs can be conscious of multi-modal input instruction and then select the function-matched tool correctly.","score":4},{"url":"https://www.semanticscholar.org/paper/140cfda71bfff852c3e205b7ad61854b78c76982","title":"Mastering Text-to-Image Diffusion: Recaptioning, Planning, and Generating with Multimodal LLMs","venue":"","year":2024,"referenceCount":75,"citationCount":0,"influentialCitationCount":0,"publicationDate":"22/01/2024","authors":"Ling Yang,Zhaochen Yu,Chenlin Meng,Minkai Xu,Stefano Ermon,Bin Cui","id":"140cfda71bfff852c3e205b7ad61854b78c76982","summary":"This paper proposes a brand new training-free text-to-image generation/editing framework, namely Recaption, Plan and Generate (RPG), harnessing the powerful chain-of-thought reasoning ability of multimodal LLMs to enhance the compositionality of text-to-image diffusion models.","score":4},{"url":"https://www.semanticscholar.org/paper/af5f256e9771bf9cd02451195e3a7ac693fde3ed","title":"Exploring the Reasoning Abilities of Multimodal Large Language Models (MLLMs): A Comprehensive Survey on Emerging Trends in Multimodal Reasoning","venue":"","year":2024,"referenceCount":178,"citationCount":0,"influentialCitationCount":0,"publicationDate":"10/01/2024","authors":"Yiqi Wang,Wentao Chen,Xiaotian Han,Xudong Lin,Haiteng Zhao,Yongfei Liu,Bohan Zhai,Jianbo Yuan,Quanzeng You,Hongxia Yang","id":"af5f256e9771bf9cd02451195e3a7ac693fde3ed","summary":"This survey comprehensively review the existing evaluation protocols of multimodal reasoning, categorize and illustrate the frontiers of MLLMs, introduce recent trends in applications of MLLMs on reasoning-intensive tasks, and finally discuss current practices and future directions.","score":4},{"url":"https://www.semanticscholar.org/paper/93886752191db25efd096a65af7b09df5c0a64e0","title":"Data-Centric Foundation Models in Computational Healthcare: A Survey","venue":"arXiv.org","year":2024,"referenceCount":316,"citationCount":0,"influentialCitationCount":0,"publicationDate":"04/01/2024","authors":"Yunkun Zhang,Jin Gao,Zheling Tan,Lingfeng Zhou,Kexin Ding,Mu Zhou,Shaoting Zhang,Dequan Wang","id":"93886752191db25efd096a65af7b09df5c0a64e0","summary":"A wide range of data-centric approaches in the FM era (from model pre-training to inference) towards improving the healthcare workflow are investigated and a promising outlook of FM-based analytics to enhance the performance of patient outcome and clinical workflow in the evolving landscape of healthcare and medicine is offered.","score":4},{"url":"https://www.semanticscholar.org/paper/20fcc01d12a50f1da2af71d85f0a269b3ba48b77","title":"LMEye: An Interactive Perception Network for Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":57,"citationCount":9,"influentialCitationCount":1,"publicationDate":"05/05/2023","authors":"Yunxin Li,Baotian Hu,Xinyu Chen,Lin Ma,M. Zhang","id":"20fcc01d12a50f1da2af71d85f0a269b3ba48b77","summary":"LMEye, a human-like eye with a play-and-plug interactive perception network, designed to enable dynamic interaction between LLMs and external vision information, is introduced, demonstrating that it significantly improves the zero-shot performance on various multimodal tasks compared to previous methods, with less parameters.","score":4},{"url":"https://www.semanticscholar.org/paper/8badb0587fef2ffc078b0cec549eb8ec96ed3ad4","title":"Self-Chained Image-Language Model for Video Localization and Question Answering","venue":"arXiv.org","year":2023,"referenceCount":96,"citationCount":22,"influentialCitationCount":6,"publicationDate":"11/05/2023","authors":"Shoubin Yu,Jaemin Cho,Prateek Yadav,Mohit Bansal","id":"8badb0587fef2ffc078b0cec549eb8ec96ed3ad4","summary":"Self-Chained Video Localization-Answering (SeViLA), a novel framework that leverages a single image-language model (BLIP-2) to tackle both temporal keyframe localization and QA on videos, and achieves the state-of-the-art in both fine-tuning and zero-shot settings.","score":4},{"url":"https://www.semanticscholar.org/paper/5d321194696f1f75cf9da045e6022b2f20ba5b9c","title":"Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding","venue":"Conference on Empirical Methods in Natural Language Processing","year":2023,"referenceCount":42,"citationCount":136,"influentialCitationCount":23,"publicationDate":"05/06/2023","authors":"Hang Zhang,Xin Li,Lidong Bing","id":"5d321194696f1f75cf9da045e6022b2f20ba5b9c","summary":"Video-LLaMA shows the ability to perceive and comprehend video content and generate meaningful responses grounded in the visual and auditory information presented in the videos.","score":4},{"url":"https://www.semanticscholar.org/paper/8efc20988021ce3b4b05dd44b13e27260ee9b99b","title":"Investigating Prompting Techniques for Zero- and Few-Shot Visual Question Answering","venue":"arXiv.org","year":2023,"referenceCount":59,"citationCount":1,"influentialCitationCount":0,"publicationDate":"16/06/2023","authors":"Rabiul Awal,Le Zhang,Aishwarya Agrawal","id":"8efc20988021ce3b4b05dd44b13e27260ee9b99b","summary":"Light is shed on the intricacies of prompting strategies in VLMs for VQA, emphasizing the synergistic use of captions, templates, and pre-processing to enhance model efficacy.","score":4},{"url":"https://www.semanticscholar.org/paper/659a12d71d8709c132ccd9ccd235f0024cae0239","title":"The All-Seeing Project: Towards Panoptic Visual Recognition and Understanding of the Open World","venue":"arXiv.org","year":2023,"referenceCount":115,"citationCount":16,"influentialCitationCount":3,"publicationDate":"03/08/2023","authors":"Weiyun Wang,Min Shi,Qingyun Li,Wen Wang,Zhenhang Huang,Linjie Xing,Zhe Chen,Hao Li,Xizhou Zhu,Zhiguo Cao,Yushi Chen,Tong Lu,Jifeng Dai,Y. Qiao","id":"659a12d71d8709c132ccd9ccd235f0024cae0239","summary":"The All-Seeing model (ASM), a unified framework for panoptic visual recognition and understanding, is developed with open-ended language prompts and locations, which allows it to generalize to various vision and language tasks with remarkable zero-shot performance.","score":4},{"url":"https://www.semanticscholar.org/paper/2e3dcf5a5d58ac210d0d87e9f918540a8373211a","title":"GIT-Mol: A Multi-modal Large Language Model for Molecular Science with Graph, Image, and Text","venue":"arXiv.org","year":2023,"referenceCount":63,"citationCount":6,"influentialCitationCount":1,"publicationDate":"14/08/2023","authors":"Peng Liu,Yiming Ren,Zhixiang Ren","id":"2e3dcf5a5d58ac210d0d87e9f918540a8373211a","summary":"GIT-Mol is introduced, a multi-modal large language model that integrates the Graph, Image, and Text information and proposes GIT-Former, a novel architecture that is capable of aligning all modalities into a unified latent space.","score":4},{"url":"https://www.semanticscholar.org/paper/7f807249c0ef0fe07d5e9c810684cd5daba0edc5","title":"De-fine: Decomposing and Refining Visual Programs with Auto-Feedback","venue":"arXiv.org","year":2023,"referenceCount":40,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/11/2023","authors":"Minghe Gao,Juncheng Li,Hao Fei,Liang Pang,Wei Ji,Guoming Wang,Wenqiao Zhang,Siliang Tang,Yueting Zhuang","id":"7f807249c0ef0fe07d5e9c810684cd5daba0edc5","summary":"This work introduces De-fine, a general framework that automatically decomposes complex tasks into simpler subtasks and refines programs through auto-feedback, which can improve logical reasoning performance by integrating the strengths of multiple models.","score":4},{"url":"https://www.semanticscholar.org/paper/b1721374889899950994f67029fe899de257c140","title":"A Foundational Multimodal Vision Language AI Assistant for Human Pathology","venue":"arXiv.org","year":2023,"referenceCount":126,"citationCount":1,"influentialCitationCount":0,"publicationDate":"13/12/2023","authors":"Ming Y. Lu,Bowen Chen,Drew F. K. Williamson,Richard J. Chen,Kenji Ikamura,Georg Gerber,Ivy Liang,L. Le,Tong Ding,Anil V. Parwani,Faisal Mahmood","id":"b1721374889899950994f67029fe899de257c140","summary":"PathChat is presented, a vision-language generalist AI assistant for human pathology using an in-house developed foundational vision encoder pretrained on 100 million histology images from over 100,000 patient cases and 1.18 million pathology image-caption pairs.","score":4},{"url":"https://www.semanticscholar.org/paper/55c6d16b550c606d62dd85084f0d373d8f087966","title":"VLAP: Efficient Video-Language Alignment via Frame Prompting and Distilling for Video Question Answering","venue":"arXiv.org","year":2023,"referenceCount":65,"citationCount":0,"influentialCitationCount":0,"publicationDate":"13/12/2023","authors":"Xijun Wang,Junbang Liang,Chun-Kai Wang,Kenan Deng,Yu Lou,Ming Lin,Shan Yang","id":"55c6d16b550c606d62dd85084f0d373d8f087966","summary":"This work proposes an efficient Video-Language Alignment via Frame-Prompting and Distilling (VLAP) network that addresses both efficient frame sampling and effective cross-modal alignment in a unified way and demonstrates the capability of selecting key frames with critical contents, thus improving the video-language alignment accuracy.","score":4},{"url":"https://www.semanticscholar.org/paper/ea6982a936a2b263bbf46ff6eb27fc0b63fddaf7","title":"VL-GPT: A Generative Pre-trained Transformer for Vision and Language Understanding and Generation","venue":"arXiv.org","year":2023,"referenceCount":55,"citationCount":3,"influentialCitationCount":1,"publicationDate":"14/12/2023","authors":"Jinguo Zhu,Xiaohan Ding,Yixiao Ge,Yuying Ge,Sijie Zhao,Hengshuang Zhao,Xiaohua Wang,Ying Shan","id":"ea6982a936a2b263bbf46ff6eb27fc0b63fddaf7","summary":null,"score":4},{"url":"https://www.semanticscholar.org/paper/1f5e1a036b24b9dd34c006ba3bb61119624f4fdb","title":"A Comprehensive Study of GPT-4V's Multimodal Capabilities in Medical Imaging","venue":"medRxiv","year":2023,"referenceCount":67,"citationCount":6,"influentialCitationCount":1,"publicationDate":"31/10/2023","authors":"Yingshu Li,Yunyi Liu,Zhanyu Wang,Xinyu Liang,Lingqiao Liu,Lei Wang,Leyang Cui,Zhaopeng Tu,Longyue Wang,Luping Zhou","id":"1f5e1a036b24b9dd34c006ba3bb61119624f4fdb","summary":"This paper presents a comprehensive evaluation of GPT-4V's capabilities across diverse medical imaging tasks, including Radiology Report Generation, Medical Visual Question Answering (VQA), and Visual Grounding, and finds the limitations of conventional evaluation metrics like the BLEU score.","score":4},{"url":"https://www.semanticscholar.org/paper/31a7d8c4a5ab6bab522494b57270249105c8748e","title":"BiomedGPT: A Unified and Generalist Biomedical Generative Pre-trained Transformer for Vision, Language, and Multimodal Tasks","venue":"arXiv.org","year":2023,"referenceCount":147,"citationCount":33,"influentialCitationCount":2,"publicationDate":2023,"authors":"Kai Zhang,Jun Yu,Zhilin Yan,Yixin Liu,Eashan Adhikarla,S. Fu,Xun Chen,Chen Chen,Yuyin Zhou,Xiang Li,Lifang He,B. Davison,Quanzheng Li,Yong Chen,Hongfang Liu,Lichao Sun","id":"31a7d8c4a5ab6bab522494b57270249105c8748e","summary":"A unified and generalist BiomedGPT model, which leverages self-supervision on large and diverse datasets to accept multi-modal inputs and perform a range of downstream tasks, which presents a significant step forward in developing unified and generalist models for biomedicine.","score":4},{"url":"https://www.semanticscholar.org/paper/8f3138f7ee5127faab265793be8ae278bc49d9b1","title":"PMC-CLIP: Contrastive Language-Image Pre-training using Biomedical Documents","venue":"International Conference on Medical Image Computing and Computer-Assisted Intervention","year":2023,"referenceCount":33,"citationCount":20,"influentialCitationCount":2,"publicationDate":"13/03/2023","authors":"Weixiong Lin,Ziheng Zhao,Xiaoman Zhang,Chaoyi Wu,Ya Zhang,Yanfeng Wang,Weidi Xie","id":"8f3138f7ee5127faab265793be8ae278bc49d9b1","summary":"PMC-OA, a biomedical dataset with 1.6M image-caption pairs collected from PubMedCentral's OpenAccess subset, is built and released, which is 8 times larger than before and achieves state-of-the-art results on various downstream tasks.","score":4},{"url":"https://www.semanticscholar.org/paper/bf40c9e7832e1b2887cbf5798455f91705ea11ba","title":"Masked Vision and Language Pre-training with Unimodal and Multimodal Contrastive Losses for Medical Visual Question Answering","venue":"International Conference on Medical Image Computing and Computer-Assisted Intervention","year":2023,"referenceCount":26,"citationCount":3,"influentialCitationCount":0,"publicationDate":"11/07/2023","authors":"Pengfei Li,Gang Liu,Jinlong He,Zixu Zhao,Shenjun Zhong","id":"bf40c9e7832e1b2887cbf5798455f91705ea11ba","summary":"This paper presents a novel self-supervised approach that learns unimodal and multimodal feature representations of input images and text using medical image caption datasets, by leveraging both unimmodal and multi-modal contrastive losses, along with masked language modeling and image text matching as pretraining objectives.","score":4},{"url":"https://www.semanticscholar.org/paper/df0ddb588a200d095743e9d26fc4a9318619766e","title":"Towards Generalist Foundation Model for Radiology","venue":"arXiv.org","year":2023,"referenceCount":59,"citationCount":20,"influentialCitationCount":1,"publicationDate":"04/08/2023","authors":"Chaoyi Wu,Xiaoman Zhang,Ya Zhang,Yanfeng Wang,Weidi Xie","id":"df0ddb588a200d095743e9d26fc4a9318619766e","summary":"This study constructs a large-scale Medical Multi-modal Dataset, MedMD, which consists of 16M 2D and 3D medical scans with high-quality text descriptions or reports across various data formats, modalities, and tasks, covering over 5000 distinct diseases, and proposes a new evaluation benchmark, RadBench, aiming to comprehensively assess the capability of foundation models in handling practical clinical problems.","score":4},{"url":"https://www.semanticscholar.org/paper/1e0d21dc2caf7b58342ddc8609fb30cdc1e27cd5","title":"MISS: A Generative Pretraining and Finetuning Approach for Med-VQA","venue":"arXiv.org","year":2024,"referenceCount":25,"citationCount":0,"influentialCitationCount":0,"publicationDate":"10/01/2024","authors":"Jiawei Chen,Dingkang Yang,Yue Jiang,Yuxuan Lei,Lihua Zhang","id":"1e0d21dc2caf7b58342ddc8609fb30cdc1e27cd5","summary":"A large-scale MultI-task Self-Supervised learning based framework (MISS) for medical VQA tasks that unify the text encoder and multimodal encoder and align image-text features through multi-task learning and proposes a Transfer-and-Caption method that extends the feature space of single-modal image datasets using large language models (LLMs), enabling those traditional medical vision field task data to be applied to VLP.","score":4},{"url":"https://www.semanticscholar.org/paper/a3711dbf296b5ddd97ba93826660cd3995611625","title":"Towards A Foundation Model for Generalist Robots: Diverse Skill Learning at Scale via Automated Task and Scene Generation","venue":"arXiv.org","year":2023,"referenceCount":111,"citationCount":4,"influentialCitationCount":0,"publicationDate":2023,"authors":"Zhou Xian,Théophile Gervet,Zhenjia Xu,Yi-Ling Qiao,Tsun-Hsuan Wang","id":"a3711dbf296b5ddd97ba93826660cd3995611625","summary":null,"score":3},{"url":"https://www.semanticscholar.org/paper/692bc40edf4785d88c39e0c0fe9f270541fecf8a","title":"Towards Generalist Robots: A Promising Paradigm via Generative Simulation","venue":"","year":2023,"referenceCount":124,"citationCount":1,"influentialCitationCount":0,"publicationDate":"17/05/2023","authors":"Zhou Xian,Théophile Gervet,Zhenjia Xu,Yi-Ling Qiao,Tsun-Hsuan Wang,Yian Wang","id":"692bc40edf4785d88c39e0c0fe9f270541fecf8a","summary":"This document presents a specific idea for mining knowledge in the latest large-scale foundation models for robotics research, which uses a fully automated generative pipeline which uses these models to generate diversified tasks, scenes and training supervisions at scale, thereby scaling up low-level skill learning and ultimately leading to a foundation model for robotics that empowers generalist robots.","score":3},{"url":"https://www.semanticscholar.org/paper/8ec7d50250203543a0098d99f04957b22bbe2c77","title":"How to Bridge the Gap between Modalities: A Comprehensive Survey on Multimodal Large Language Model","venue":"arXiv.org","year":2023,"referenceCount":117,"citationCount":0,"influentialCitationCount":0,"publicationDate":"10/11/2023","authors":"Shezheng Song,Xiaopeng Li,Shasha Li","id":"8ec7d50250203543a0098d99f04957b22bbe2c77","summary":"This paper aims to explore modality alignment methods for LLMs and their existing capabilities, and surveys existing modal alignment methods in MLLMs into four groups: Multimodal Converters that change data into something LLMs can understand, and Data-Driven methods that teach LLMs to understand specific types of data in a dataset.","score":3},{"url":"https://www.semanticscholar.org/paper/6a5525c316b9be7909c433a79e090ed731425083","title":"What Makes for Good Visual Tokenizers for Large Language Models?","venue":"arXiv.org","year":2023,"referenceCount":54,"citationCount":13,"influentialCitationCount":1,"publicationDate":"20/05/2023","authors":"Guangzhi Wang,Yixiao Ge,Xiaohan Ding,Mohan S. Kankanhalli,Ying Shan","id":"6a5525c316b9be7909c433a79e090ed731425083","summary":"A new MLLM equipped with a tailored Good Visual Tokenizer (GVT), which exhibits strong visual comprehension capability at multiple scales, without introducing extra parameters and task-specific fine-tuning.","score":3},{"url":"https://www.semanticscholar.org/paper/efc694164312006c543ef745611348ef64e68dda","title":"Towards Language Models That Can See: Computer Vision Through the LENS of Natural Language","venue":"arXiv.org","year":2023,"referenceCount":64,"citationCount":20,"influentialCitationCount":2,"publicationDate":"28/06/2023","authors":"William Berrios,Gautam Mittal,Tristan Thrush,Douwe Kiela,Amanpreet Singh","id":"efc694164312006c543ef745611348ef64e68dda","summary":"This work proposes LENS, a modular approach for tackling computer vision problems by leveraging the power of large language models (LLMs) with a language model to reason over outputs from a set of independent and highly descriptive vision modules that provide exhaustive information about an image.","score":3},{"url":"https://www.semanticscholar.org/paper/813ba033b8f593c98f9af44c5b4901408ba6f70a","title":"Towards a Visual-Language Foundation Model for Computational Pathology","venue":"arXiv.org","year":2023,"referenceCount":105,"citationCount":10,"influentialCitationCount":0,"publicationDate":"24/07/2023","authors":"Ming Y. Lu,Bowen Chen,Drew F. K. Williamson,Richard J. Chen,Ivy Liang,Tong Ding,Guillaume Jaume,I. Odintsov,Andrew Zhang,L. Le,G. Gerber,A. Parwani,Faisal Mahmood","id":"813ba033b8f593c98f9af44c5b4901408ba6f70a","summary":"This work introduces CONtrastive learning from Captions for Histopathology (CONCH), a visual-language foundation model developed using diverse sources of histopathology images, biomedical text, and notably over 1.17 million image-caption pairs via task-agnostic pretraining, with the potential to directly facilitate a wide array of machine learning-based workflows requiring minimal or no further supervised fine-tuning.","score":3},{"url":"https://www.semanticscholar.org/paper/94972e30504017156ef5b5debc419bf6edc67384","title":"MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities","venue":"arXiv.org","year":2023,"referenceCount":91,"citationCount":64,"influentialCitationCount":5,"publicationDate":"04/08/2023","authors":"Weihao Yu,Zhengyuan Yang,Linjie Li,Jianfeng Wang,Kevin Lin,Zicheng Liu,Xinchao Wang,Lijuan Wang","id":"94972e30504017156ef5b5debc419bf6edc67384","summary":"MM-Vet is proposed, an evaluation benchmark that examines large multimodal models (LMMs) on complicated multimodAL tasks and proposes an LLM-based evaluator for open-ended outputs that enables the evaluation across different question types and answer styles, resulting in a unified scoring metric.","score":3},{"url":"https://www.semanticscholar.org/paper/1fd31b74f5e1eeb67341982fd35a613c6fad10e0","title":"Link-Context Learning for Multimodal LLMs","venue":"arXiv.org","year":2023,"referenceCount":33,"citationCount":3,"influentialCitationCount":1,"publicationDate":"15/08/2023","authors":"Yan Tai,Weichen Fan,Zhao Zhang,Feng Zhu,Rui Zhao,Ziwei Liu","id":"1fd31b74f5e1eeb67341982fd35a613c6fad10e0","summary":"This work proposes link-context learning (LCL), which emphasizes \"reasoning from cause and effect\" to augment the learning capabilities of MLLMs and introduces the ISEKAI dataset, comprising exclusively of unseen generated image-label pairs designed for link- context learning.","score":3},{"url":"https://www.semanticscholar.org/paper/7a7128e9696dfedc24bf432c4b4c3aafa5e95a1a","title":"An Empirical Study of Scaling Instruct-Tuned Large Multimodal Models","venue":"arXiv.org","year":2023,"referenceCount":22,"citationCount":6,"influentialCitationCount":0,"publicationDate":"18/09/2023","authors":"Yadong Lu,Chunyuan Li,Haotian Liu,Jianwei Yang,Jianfeng Gao,Yelong Shen","id":"7a7128e9696dfedc24bf432c4b4c3aafa5e95a1a","summary":"An empirical study of scaling LLaVA up to 33B and 65B/70B and performance of LoRA/QLoRA tuning of LMM are comparable to the performance of full-model fine-tuning, finding that scaling LMM consistently enhances model performance and improves language capabilities.","score":3},{"url":"https://www.semanticscholar.org/paper/20ae101289965d36dbd93e9b8c47ec9deab03ed0","title":"What If the TV Was Off? Examining Counterfactual Reasoning Abilities of Multi-modal Language Models","venue":"2023 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)","year":2023,"referenceCount":52,"citationCount":1,"influentialCitationCount":0,"publicationDate":"02/10/2023","authors":"Letian Zhang,Xiaotong Zhai,Zhongkai Zhao,Xin Wen,Yongshuo Zong,Bingchen Zhao","id":"20ae101289965d36dbd93e9b8c47ec9deab03ed0","summary":"This work benchmarking the counterfactual reasoning ability of multimodal large language models found that all models exhibit a large performance drop compared to the results tested on questions without counterfactual presupposition, indicating that there still exists space for developing vision language models.","score":3},{"url":"https://www.semanticscholar.org/paper/ac2e5bf716aed246ca8914a6816ef73e00286099","title":"Beyond Segmentation: Road Network Generation with Multi-Modal LLMs","venue":"arXiv.org","year":2023,"referenceCount":34,"citationCount":2,"influentialCitationCount":0,"publicationDate":"15/10/2023","authors":"Sumedh Rasal,S. Boddhu","id":"ac2e5bf716aed246ca8914a6816ef73e00286099","summary":"An innovative approach to road network generation through the utilization of a multi-modal Large Language Model (LLM), specifically designed to process aerial images of road layouts and produce detailed, navigable road networks within the input images.","score":3},{"url":"https://www.semanticscholar.org/paper/0b395ed1c8b284e551172b728e83cf257e33729a","title":"HallusionBench: An Advanced Diagnostic Suite for Entangled Language Hallucination&Visual Illusion in Large Vision-Language Models","venue":"","year":2023,"referenceCount":57,"citationCount":2,"influentialCitationCount":0,"publicationDate":"23/10/2023","authors":"Tianrui Guan,Fuxiao Liu,Xiyang Wu,Ruiqi Xian,Zongxia Li,Xiaoyu Liu,Xijun Wang,Lichang Chen,Furong Huang,Yaser Yacoob,Dinesh Manocha,Tianyi Zhou","id":"0b395ed1c8b284e551172b728e83cf257e33729a","summary":"This work introduces HallusionBench, a comprehensive benchmark designed for the evaluation of image-context reasoning, and introduces a novel structure for these visual questions designed to establish control groups, which enables a quantitative analysis of the models' response tendencies, logical consistency, and various failure modes.","score":3},{"url":"https://www.semanticscholar.org/paper/8e5d42f5b98146d0784fe85e29c768a4989e1478","title":"Foundational Models in Medical Imaging: A Comprehensive Survey and Future Vision","venue":"arXiv.org","year":2023,"referenceCount":92,"citationCount":7,"influentialCitationCount":0,"publicationDate":"28/10/2023","authors":"Bobby Azad,Reza Azad,Sania Eskandari,Afshin Bozorgpour,A. Kazerouni,I. Rekik,D. Merhof","id":"8e5d42f5b98146d0784fe85e29c768a4989e1478","summary":"A methodical taxonomy of foundation models within the medical domain is offered, proposing a classification system primarily structured around training strategies, while also incorporating additional facets such as application domains, imaging modalities, specific organs of interest, and the algorithms integral to these models.","score":3},{"url":"https://www.semanticscholar.org/paper/c62711f6b5d8620ba36bc2c378ec6ab53f6e197c","title":"RoboGen: Towards Unleashing Infinite Data for Automated Robot Learning via Generative Simulation","venue":"arXiv.org","year":2023,"referenceCount":127,"citationCount":7,"influentialCitationCount":0,"publicationDate":"02/11/2023","authors":"Yufei Wang,Zhou Xian,Feng Chen,Tsun-Hsuan Wang,Yian Wang,Zackory M. Erickson,David Held,Chuang Gan","id":"c62711f6b5d8620ba36bc2c378ec6ab53f6e197c","summary":"RoboGen is presented, a generative robotic agent that automatically learns diverse robotic skills at scale via generative simulation and attempts to extract the extensive and versatile knowledge embedded in large-scale models and transfer them to the field of robotics.","score":3},{"url":"https://www.semanticscholar.org/paper/6ae4705139494fcb6b790b6dd6c4225b40ee40f8","title":"GLaMM: Pixel Grounding Large Multimodal Model","venue":"arXiv.org","year":2023,"referenceCount":63,"citationCount":8,"influentialCitationCount":2,"publicationDate":"06/11/2023","authors":"H. Rasheed,Muhammad Maaz,Sahal Shaji Mullappilly,Abdelrahman M. Shaker,Salman H. Khan,Hisham Cholakkal,R. Anwer,Erix Xing,Ming-Hsuan Yang,F. Khan","id":"6ae4705139494fcb6b790b6dd6c4225b40ee40f8","summary":"This work presents Grounding LMM (GLaMM), the first model that can generate natural language responses seamlessly intertwined with corresponding object segmentation masks and is flexible enough to accept both textual and optional visual prompts (region of interest) as input.","score":3},{"url":"https://www.semanticscholar.org/paper/cc9627c4475b9ad1c7e4105f3bceb1e7b59b7cab","title":"Zero-Shot Video Question Answering with Procedural Programs","venue":"arXiv.org","year":2023,"referenceCount":65,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/12/2023","authors":"Rohan Choudhury,Koichiro Niinuma,Kris M. Kitani,László A. Jeni","id":"cc9627c4475b9ad1c7e4105f3bceb1e7b59b7cab","summary":"This work proposes to answer zero-shot questions about videos by generating short procedural programs that derive a final answer from solving a sequence of visual subtasks, using a large language model to generate such programs from an input question and an API of visual modules in the prompt, then executes them to obtain the output.","score":3},{"url":"https://www.semanticscholar.org/paper/369b34826e23cb43bea9a91395e9603eacfa7420","title":"EgoPlan-Bench: Benchmarking Egocentric Embodied Planning with Multimodal Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":49,"citationCount":1,"influentialCitationCount":0,"publicationDate":"11/12/2023","authors":"Yi Chen,Yuying Ge,Yixiao Ge,Mingyu Ding,Bohao Li,Rui Wang,Rui-Lan Xu,Ying Shan,Xihui Liu","id":"369b34826e23cb43bea9a91395e9603eacfa7420","summary":"A benchmark with human annotations, EgoPlan-Bench, is introduced to quantitatively investigate the potential of MLLMs as embodied task planners in real-world scenarios and demonstrates that the model tuned on Ego plan-IT not only significantly improves performance on this benchmark, but also effectively acts as embodied planner in simulations.","score":3},{"url":"https://www.semanticscholar.org/paper/0a8a776054a087118f4f9523994ef084b2b2469a","title":"Advancing Large Multi-modal Models with Explicit Chain-of-Reasoning and Visual Question Generation","venue":"","year":2024,"referenceCount":88,"citationCount":0,"influentialCitationCount":0,"publicationDate":"18/01/2024","authors":"Kohei Uehara,Nabarun Goswami,Hanqin Wang,Toshiaki Baba,Kohtaro Tanaka,Tomohiro Hashimoto,Kai Wang,Rei Ito,Takagi Naoya,Ryo Umagami,Yingyi Wen,Tanachai Anakewat,Tatsuya Harada","id":"0a8a776054a087118f4f9523994ef084b2b2469a","summary":"This paper designs an LMM, which has high capabilities on region awareness to address the intricate requirements of image-text alignment, and introduces a system that can ask a question to acquire necessary knowledge, thereby enhancing the robustness and explicability of the reasoning process.","score":3},{"url":"https://www.semanticscholar.org/paper/44ccf252018f71898d52d89539f17d77a4f8d548","title":"Chart Understanding with Large Language Model","venue":"","year":null,"referenceCount":39,"citationCount":0,"influentialCitationCount":0,"publicationDate":null,"authors":"Yaser James,Will Li,John Feng","id":"44ccf252018f71898d52d89539f17d77a4f8d548","summary":"A baseline multimodal model is introduced that integrates text and charts to enhance the chart comprehension capabilities of existing models, offering more pertinent insights and information related to the depicted charts.","score":3},{"url":"https://www.semanticscholar.org/paper/5ce94181ea702f69c3651dce721d6bd8026b8106","title":"TPTU: Task Planning and Tool Usage of Large Language Model-based AI Agents","venue":"arXiv.org","year":2023,"referenceCount":91,"citationCount":28,"influentialCitationCount":5,"publicationDate":2023,"authors":"Jingqing Ruan,Yihong Chen,Bin Zhang,Zhiwei Xu,Tianpeng Bao,Guoqing Du,Shiwei Shi,Hangyu Mao,Xingyu Zeng,Rui Zhao","id":"5ce94181ea702f69c3651dce721d6bd8026b8106","summary":"A structured framework tailored for LLM-based AI Agents is proposed and two distinct types of agents are designed to execute the inference process, which emphasizes the substantial potential of these models, while also identifying areas that need more investigation and improvement.","score":3},{"url":"https://www.semanticscholar.org/paper/96a6df2b4aa50cfbd8984933e9c66b0763fc08a6","title":"Overleaf Example","venue":"","year":2023,"referenceCount":84,"citationCount":18,"influentialCitationCount":1,"publicationDate":2023,"authors":"Jianwei Yang,Hao Zhang,Feng Li,Xueyan Zou,Chun-yue Li,Jianfeng Gao","id":"96a6df2b4aa50cfbd8984933e9c66b0763fc08a6","summary":"The experiments show that GPT-4V with SoM outperforms the state-of-the-art fully-finetuned referring segmentation model on RefCOCOg in a zero-shot setting and the effectiveness of SoM on a wide range of fine-grained vision and multimodal tasks.","score":3},{"url":"https://www.semanticscholar.org/paper/6845bea94b2fb17d4377b3bb2bd10f73a959f9cc","title":"Reasoning with Language Model Prompting: A Survey","venue":"Annual Meeting of the Association for Computational Linguistics","year":2022,"referenceCount":219,"citationCount":105,"influentialCitationCount":3,"publicationDate":"19/12/2022","authors":"Shuofei Qiao,Yixin Ou,Ningyu Zhang,Xiang Chen,Yunzhi Yao,Shumin Deng,Chuanqi Tan,Fei Huang,Huajun Chen","id":"6845bea94b2fb17d4377b3bb2bd10f73a959f9cc","summary":"This paper provides a comprehensive survey of cutting-edge research on reasoning with language model prompting with comparisons and summaries and provides systematic resources to help beginners.","score":3},{"url":"https://www.semanticscholar.org/paper/170c97c7215f42edfb20c2248f954879e91ef86e","title":"Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":72,"citationCount":120,"influentialCitationCount":15,"publicationDate":"19/04/2023","authors":"Pan Lu,Baolin Peng,Hao Cheng,Michel Galley,Kai-Wei Chang,Y. Wu,Song-Chun Zhu,Jianfeng Gao","id":"170c97c7215f42edfb20c2248f954879e91ef86e","summary":"This paper demonstrates the effectiveness of Chameleon on two multi-modal knowledge-intensive reasoning tasks: ScienceQA and TabMWP and shows that the GPT-4-powered planner exhibits more consistent and rational tool selection via inferring potential constraints from instructions, compared to a ChatGPT- powered planner.","score":3},{"url":"https://www.semanticscholar.org/paper/43e6e8d6663d83f1b74cf5a2be7b040b0928f867","title":"X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages","venue":"arXiv.org","year":2023,"referenceCount":61,"citationCount":46,"influentialCitationCount":7,"publicationDate":"07/05/2023","authors":"Feilong Chen,Minglun Han,Haozhi Zhao,Qingyang Zhang,Jing Shi,Shuang Xu,Bo Xu","id":"43e6e8d6663d83f1b74cf5a2be7b040b0928f867","summary":"X-LLM is proposed, which converts Multi-modalities into foreign languages using X2L interfaces and inputs them into a large Language model (ChatGLM), and demonstrates impressive multimodel chat abilities.","score":3},{"url":"https://www.semanticscholar.org/paper/ca055cfb9d4d47124cc035c346f38577825fcacf","title":"Enhance Reasoning Ability of Visual-Language Models via Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":40,"citationCount":0,"influentialCitationCount":0,"publicationDate":"22/05/2023","authors":"Yueting Yang,Xintong Zhang,Wenjuan Han","id":"ca055cfb9d4d47124cc035c346f38577825fcacf","summary":"A method called TReE is proposed, which transfers the reasoning ability of a large language model to a visual language model in zero-shot scenarios, and contains three stages: observation, thinking, and re-thinking.","score":3},{"url":"https://www.semanticscholar.org/paper/66d755730f5d08a6f4fcc5e81f24982ba389dca9","title":"LayoutGPT: Compositional Visual Planning and Generation with Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":65,"citationCount":23,"influentialCitationCount":4,"publicationDate":"24/05/2023","authors":"Weixi Feng,Wanrong Zhu,Tsu-Jui Fu,Varun Jampani,Arjun Reddy Akula,Xuehai He,Sugato Basu,X. Wang,William Yang Wang","id":"66d755730f5d08a6f4fcc5e81f24982ba389dca9","summary":"This work proposes LayoutGPT, a method to compose in-context visual demonstrations in style sheet language to enhance the visual planning skills of LLMs, and shows superior performance in converting challenging language concepts like numerical and spatial relations to layout arrangements for faithful text-to-image generation.","score":3},{"url":"https://www.semanticscholar.org/paper/c6ac708b65b24c20f80831d518c1795ce8133ad5","title":"ChatBridge: Bridging Modalities with Large Language Model as a Language Catalyst","venue":"arXiv.org","year":2023,"referenceCount":72,"citationCount":15,"influentialCitationCount":3,"publicationDate":"25/05/2023","authors":"Zijia Zhao,Longteng Guo,Tongtian Yue,Si-Qing Chen,Shuai Shao,Xinxin Zhu,Zehuan Yuan,Jing Liu","id":"c6ac708b65b24c20f80831d518c1795ce8133ad5","summary":"It is shown that only language-paired two-modality data is sufficient to connect all modalities and ChatBridge, a novel multimodal language model that leverages the expressive capabilities of language as the catalyst to bridge the gap between various modalities, is presented.","score":3},{"url":"https://www.semanticscholar.org/paper/5ff2f5212713ec424662ac3c9e4aa5a8790d40cf","title":"ANPL: Towards Natural Programming with Interactive Decomposition","venue":"","year":2023,"referenceCount":71,"citationCount":1,"influentialCitationCount":0,"publicationDate":"29/05/2023","authors":"Di Huang,Ziyuan Nan,Xingui Hu,Pengwei Jin,Shaohui Peng,Yuanbo Wen,Rui Zhang,Zidong Du,Qi Guo,Yewen Pu,Yunji Chen","id":"5ff2f5212713ec424662ac3c9e4aa5a8790d40cf","summary":"This paper introduces ANPL, an interactive programming system that ensures users can always refine the generated code towards their specific programmatic intents via structured decompositions, and deploys ANPL on the Abstraction and Reasoning Corpus, a set of unique tasks that are challenging for state-of-the-art AI systems.","score":3},{"url":"https://www.semanticscholar.org/paper/50c1414fe41d0cb9db6f0933c9319aa124beac5d","title":"Contextual Object Detection with Multimodal Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":87,"citationCount":15,"influentialCitationCount":0,"publicationDate":"29/05/2023","authors":"Yuhang Zang,Wei Li,Jun Han,Kaiyang Zhou,Chen Change Loy","id":"50c1414fe41d0cb9db6f0933c9319aa124beac5d","summary":"The ContextDET is presented, a unified multimodal model that is capable of end-to-end differentiable modeling of visual-language contexts, so as to locate, identify, and associate visual objects with language inputs for human-AI interaction.","score":3},{"url":"https://www.semanticscholar.org/paper/af705d648b5b16daa3dcc593bc593f2574d76c07","title":"Grammar Prompting for Domain-Specific Language Generation with Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":101,"citationCount":3,"influentialCitationCount":0,"publicationDate":"30/05/2023","authors":"Bailin Wang,Zi Wang,Xuezhi Wang,Yuan Cao,R. Saurous,Yoon Kim","id":"af705d648b5b16daa3dcc593bc593f2574d76c07","summary":"Experiments demonstrate that grammar prompting can enable LLMs to perform competitively on a diverse set of DSL generation tasks, including semantic parsing, Overnight, GeoQuery, PDDL planning, and even molecule generation (SMILES).","score":3},{"url":"https://www.semanticscholar.org/paper/42ea55edb46395469aee1b760829657e65ab6577","title":"Zero-Shot 3D Shape Correspondence","venue":"ACM SIGGRAPH Conference and Exhibition on Computer Graphics and Interactive Techniques in Asia","year":2023,"referenceCount":67,"citationCount":6,"influentialCitationCount":0,"publicationDate":"05/06/2023","authors":"Ahmed Abdelreheem,Abdelrahman Eldesokey,M. Ovsjanikov,Peter Wonka","id":"42ea55edb46395469aee1b760829657e65ab6577","summary":"This work introduces a fully automatic method that exploits the exceptional reasoning capabilities of recent foundation models in language and vision to tackle difficult shape correspondence problems and produces highly plausible results in a zero-shot manner, especially between strongly non-isometric shapes.","score":3},{"url":"https://www.semanticscholar.org/paper/fed150a219f9c31bdb4920e615c7c9264c634736","title":"On the Challenges and Perspectives of Foundation Models for Medical Image Analysis","venue":"Medical Image Anal.","year":2023,"referenceCount":88,"citationCount":15,"influentialCitationCount":0,"publicationDate":"09/06/2023","authors":"Shaoting Zhang,Dimitris N. Metaxas","id":"fed150a219f9c31bdb4920e615c7c9264c634736","summary":"The \"spectrum\" of medical foundation models, ranging from general imaging models, modality-specific models, to organ/task- specific models, are illustrated and highlighted, to highlight their challenges, opportunities and applications.","score":3},{"url":"https://www.semanticscholar.org/paper/79150cb420d15830c8d36f0e91eea1b02e177f0f","title":"Sticker820K: Empowering Interactive Retrieval with Stickers","venue":"arXiv.org","year":2023,"referenceCount":36,"citationCount":0,"influentialCitationCount":0,"publicationDate":"12/06/2023","authors":"Sijie Zhao,Yixiao Ge,Zhongang Qi,Lin Song,Xiaohan Ding,Zehua Xie,Ying Shan","id":"79150cb420d15830c8d36f0e91eea1b02e177f0f","summary":"The StickerCLIP is proposed as a benchmark model on the Sticker820K dataset, demonstrating strong superiority over the CLIP for the text-to-image retrieval task, and the recently popularized LLM is extended by means of prompt tuning, integrating its ability for sticker retrieval and allowing users to retrieve stickers through instructions.","score":3},{"url":"https://www.semanticscholar.org/paper/697e0add95e880bd42e00bef838181e105f91981","title":"MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":60,"citationCount":110,"influentialCitationCount":23,"publicationDate":"23/06/2023","authors":"Chaoyou Fu,Peixian Chen,Yunhang Shen,Yulei Qin,Mengdan Zhang,Xu Lin,Zhenyu Qiu,Wei Lin,Jinrui Yang,Xiawu Zheng,Ke Li,Xing Sun,Rongrong Ji","id":"697e0add95e880bd42e00bef838181e105f91981","summary":"This paper presents the first comprehensive MLLM Evaluation benchmark MME, which measures both perception and cognition abilities on a total of 14 subtasks and suggests that existing MLLMs still have a large room for improvement, but also reveals the potential directions for the subsequent model optimization.","score":3},{"url":"https://www.semanticscholar.org/paper/ea566f87f6253bb2d32cf7b61cd3e2535a0c3f42","title":"mPLUG-DocOwl: Modularized Multimodal Large Language Model for Document Understanding","venue":"arXiv.org","year":2023,"referenceCount":37,"citationCount":21,"influentialCitationCount":5,"publicationDate":"04/07/2023","authors":"Jiabo Ye,Anwen Hu,Haiyang Xu,Qinghao Ye,Mingshi Yan,Yuhao Dan,Chenlin Zhao,Guohai Xu,Chenliang Li,Junfeng Tian,Qiang Qi,Ji Zhang,Feiyan Huang","id":"ea566f87f6253bb2d32cf7b61cd3e2535a0c3f42","summary":"Experimental results show that the proposed mPLUG-DocOwl model outperforms existing multi-modal models, demonstrating its strong ability of document understanding, and also generalizes well on various downstream tasks.","score":3},{"url":"https://www.semanticscholar.org/paper/451a3f03aca4aa87b93981364842137417549e58","title":"SVIT: Scaling up Visual Instruction Tuning","venue":"arXiv.org","year":2023,"referenceCount":55,"citationCount":28,"influentialCitationCount":2,"publicationDate":"09/07/2023","authors":"Bo Zhao,Boya Wu,Tiejun Huang","id":"451a3f03aca4aa87b93981364842137417549e58","summary":"Scale up Visual Instruction Tuning (SVIT) by constructing a dataset of 4.2 million visual instruction tuning data and proposing a new data recipe to select subset with better diversity and balance, which evokes model's superior capabilities.","score":3},{"url":"https://www.semanticscholar.org/paper/962ccf1fc49c83817fb031e5b24b81b19cdfb89d","title":"BuboGPT: Enabling Visual Grounding in Multi-Modal LLMs","venue":"arXiv.org","year":2023,"referenceCount":33,"citationCount":29,"influentialCitationCount":1,"publicationDate":"17/07/2023","authors":"Yang Zhao,Zhijie Lin,Daquan Zhou,Zilong Huang,Jiashi Feng,Bingyi Kang","id":"962ccf1fc49c83817fb031e5b24b81b19cdfb89d","summary":"BozoGPT is a multi-modal LLM with visual grounding that can perform cross-modAL interaction between vision, audio and language, providing fine-grained understanding of visual objects and other given modalities and performs consistently well when provided by arbitrary modalities.","score":3},{"url":"https://www.semanticscholar.org/paper/446fb5dead075a1a08862662738f462e9a0e91c8","title":"Tool Documentation Enables Zero-Shot Tool-Usage with Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":85,"citationCount":15,"influentialCitationCount":0,"publicationDate":"01/08/2023","authors":"Cheng-Yu Hsieh,Sibei Chen,Chun-Liang Li,Yasuhisa Fujii,Alexander J. Ratner,Chen-Yu Lee,Ranjay Krishna,Tomas Pfister","id":"446fb5dead075a1a08862662738f462e9a0e91c8","summary":"This work advocates the use of tool documentation, descriptions for the individual tool usage, over demonstrations, and shows that tool documentation is significantly more valuable than demonstrations, with zero-shot documentation significantly outperforming few-shot without documentation.","score":3},{"url":"https://www.semanticscholar.org/paper/dd0612ce863f64b0f69d0d9f708c52e829f6f859","title":"TPTU: Large Language Model-based AI Agents for Task Planning and Tool Usage","venue":"","year":2023,"referenceCount":95,"citationCount":3,"influentialCitationCount":0,"publicationDate":"07/08/2023","authors":"Jingqing Ruan,Yihong Chen,Bin Zhang,Zhiwei Xu,Tianpeng Bao,Guoqing Du,Shiwei Shi,Hangyu Mao,Xingyu Zeng,Rui Zhao","id":"dd0612ce863f64b0f69d0d9f708c52e829f6f859","summary":"A structured framework tailored for LLM-based AI Agents is proposed and two distinct types of agents are designed to execute the inference process, which emphasizes the substantial potential of these models, while also identifying areas that need more investigation and improvement.","score":3},{"url":"https://www.semanticscholar.org/paper/4f2be887e991efa85f7b874e7ab871080a745c39","title":"CAESURA: Language Models as Multi-Modal Query Planners","venue":"arXiv.org","year":2023,"referenceCount":30,"citationCount":0,"influentialCitationCount":0,"publicationDate":"07/08/2023","authors":"Matthias Urban,Carsten Binnig","id":"4f2be887e991efa85f7b874e7ab871080a745c39","summary":"This paper proposes Language-Model-Driven Query Planning, a new paradigm of query planning that uses Language Models to translate natural language queries into executable query plans that can contain complex operators that are able to process arbitrary modalities.","score":3},{"url":"https://www.semanticscholar.org/paper/da96ec9c32d63292e506ba8f8ea8e838df998c02","title":"StableLLaVA: Enhanced Visual Instruction Tuning with Synthesized Image-Dialogue Data","venue":"arXiv.org","year":2023,"referenceCount":42,"citationCount":7,"influentialCitationCount":1,"publicationDate":"20/08/2023","authors":"Yanda Li,Chi Zhang,Gang Yu,Zhibin Wang,Bin Fu,Guosheng Lin,Chunhua Shen,Ling Chen,Yunchao Wei","id":"da96ec9c32d63292e506ba8f8ea8e838df998c02","summary":"This work proposes a novel data collection methodology that synchronously synthesizes images and dialogues for visual instruction tuning, marrying the abilities of ChatGPT and text-to-image generative models to yield a diverse and controllable dataset with varied image content.","score":3},{"url":"https://www.semanticscholar.org/paper/ef1ebdb24ce45fb57e271783a0c9a877fe0e79c3","title":"Position-Enhanced Visual Instruction Tuning for Multimodal Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":34,"citationCount":17,"influentialCitationCount":1,"publicationDate":"25/08/2023","authors":"Chi Chen,Ruoyu Qin,Fuwen Luo,Xiaoyue Mi,Peng Li,Maosong Sun,Yang Liu","id":"ef1ebdb24ce45fb57e271783a0c9a877fe0e79c3","summary":"This paper proposes Position-enhanced Visual Instruction Tuning (PVIT), which extends the functionality of MLLMs by integrating an additional region-level vision encoder, which promotes a more detailed comprehension of images for the MLLM.","score":3},{"url":"https://www.semanticscholar.org/paper/fa75a55760e6ea49b39b83cb85c99a22e1088254","title":"NExT-GPT: Any-to-Any Multimodal LLM","venue":"arXiv.org","year":2023,"referenceCount":111,"citationCount":64,"influentialCitationCount":6,"publicationDate":"11/09/2023","authors":"Shengqiong Wu,Hao Fei,Leigang Qu,Wei Ji,Tat-Seng Chua","id":"fa75a55760e6ea49b39b83cb85c99a22e1088254","summary":"This research showcases the promising possibility of building an AI agent capable of modeling universal modalities, paving the way for more human-like AI research in the community.","score":3},{"url":"https://www.semanticscholar.org/paper/d39182113cd4176ead48027b4fc05fe06ec6aaca","title":"Language Models as Black-Box Optimizers for Vision-Language Models","venue":"arXiv.org","year":2023,"referenceCount":107,"citationCount":2,"influentialCitationCount":0,"publicationDate":"12/09/2023","authors":"Samuel Yu,Shihong Liu,Zhiqiu Lin,Deepak Pathak,Deva Ramanan","id":"d39182113cd4176ead48027b4fc05fe06ec6aaca","summary":"This work proposes employing chat-based LLMs to search for the best text prompt for VLMs and highlights the advantage of conversational feedback that incorporates both positive and negative prompts, suggesting that LLMs can utilize the implicit gradient direction in textual feedback for a more efficient search.","score":3},{"url":"https://www.semanticscholar.org/paper/e7d09b6f2bc878cf2c993acf675f409d0b55f35a","title":"MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens","venue":"arXiv.org","year":2023,"referenceCount":38,"citationCount":18,"influentialCitationCount":0,"publicationDate":"03/10/2023","authors":"Kaizhi Zheng,Xuehai He,Xin Eric Wang","id":"e7d09b6f2bc878cf2c993acf675f409d0b55f35a","summary":"This work introduces an innovative interleaved vision-and-language generation technique anchored by the concept of \"generative vokens,\" which acts as the bridge for harmonized image-text outputs and exhibits substantial improvement over the baseline Divter model on the MMDialog dataset.","score":3},{"url":"https://www.semanticscholar.org/paper/84f9bc5f89dac53662fb467b6af8ff26415ca3e7","title":"InstructDET: Diversifying Referring Object Detection with Generalized Instructions","venue":"arXiv.org","year":2023,"referenceCount":62,"citationCount":1,"influentialCitationCount":0,"publicationDate":"08/10/2023","authors":"Ronghao Dang,Jiangyan Feng,Haodong Zhang,Chongjian Ge,Lin Song,Lijun Gong,Chengju Liu,Qi Chen,Feng Zhu,Rui Zhao,Yibing Song","id":"84f9bc5f89dac53662fb467b6af8ff26415ca3e7","summary":"A data-centric method for referring object detection (ROD) that localizes target objects based on user instructions and shows that a conventional ROD model surpasses existing methods on standard REC datasets and the authors' InDET test set.","score":3},{"url":"https://www.semanticscholar.org/paper/33095b1334bed852e3652bd9d7da3f4df0cdf485","title":"ViCor: Bridging Visual Understanding and Commonsense Reasoning with Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":34,"citationCount":1,"influentialCitationCount":0,"publicationDate":"09/10/2023","authors":"KAI-QING Zhou,Kwonjoon Lee,Teruhisa Misu,Xin Eric Wang","id":"33095b1334bed852e3652bd9d7da3f4df0cdf485","summary":"This work explores the synergistic capabilities of pre-trained vision-and-language models (VLMs) and large language models (LLMs) for visual commonsense reasoning (VCR) and suggests a collaborative approach where LLMs, when uncertain about their reasoning, actively direct VLMs to concentrate on and gather relevant visual elements to support potential commonsense inferences.","score":3},{"url":"https://www.semanticscholar.org/paper/7f1ba5630c3baa09b11cc665b3f71cdb117e5ffb","title":"OpenLEAF: Open-Domain Interleaved Image-Text Generation and Evaluation","venue":"arXiv.org","year":2023,"referenceCount":40,"citationCount":2,"influentialCitationCount":0,"publicationDate":"11/10/2023","authors":"Jie An,Zhengyuan Yang,Linjie Li,Jianfeng Wang,K. Lin,Zicheng Liu,Lijuan Wang,Jiebo Luo","id":"7f1ba5630c3baa09b11cc665b3f71cdb117e5ffb","summary":"A new interleaved generation framework based on prompting large-language models (LLMs) and pre-trained text-to-image (T2I) models, namely OpenLEAF is proposed, which can generate high-quality image-text content for various domains and applications.","score":3},{"url":"https://www.semanticscholar.org/paper/b3e9f249dd2e09ec111496f6b533101e8217a5b0","title":"Multimodal Large Language Model for Visual Navigation","venue":"arXiv.org","year":2023,"referenceCount":46,"citationCount":1,"influentialCitationCount":0,"publicationDate":"12/10/2023","authors":"Yao-Hung Tsai,Vansh Dhar,Jialu Li,Bowen Zhang,Jian Zhang","id":"b3e9f249dd2e09ec111496f6b533101e8217a5b0","summary":"This work designs a model that combines a simple text prompt, current observations, and a history collector model that gathers information from previous observations as input and provides a probability distribution of possible actions that the agent can take during navigation.","score":3},{"url":"https://www.semanticscholar.org/paper/1d14a708622917da4b9820ada6d32af24fc1651a","title":"Idea2Img: Iterative Self-Refinement with GPT-4V(ision) for Automatic Image Design and Generation","venue":"arXiv.org","year":2023,"referenceCount":56,"citationCount":5,"influentialCitationCount":0,"publicationDate":"12/10/2023","authors":"Zhengyuan Yang,Jianfeng Wang,Linjie Li,Kevin Lin,Chung-Ching Lin,Zicheng Liu,Lijuan Wang","id":"1d14a708622917da4b9820ada6d32af24fc1651a","summary":null,"score":3},{"url":"https://www.semanticscholar.org/paper/f8b8f926bbfa327c86c40796131fe2695db81126","title":"DDCoT: Duty-Distinct Chain-of-Thought Prompting for Multimodal Reasoning in Language Models","venue":"arXiv.org","year":2023,"referenceCount":75,"citationCount":4,"influentialCitationCount":1,"publicationDate":"25/10/2023","authors":"Ge Zheng,Bin Yang,Jiajin Tang,Hong-Yu Zhou,Sibei Yang","id":"f8b8f926bbfa327c86c40796131fe2695db81126","summary":"This study proposes a novel DDCoT prompting that maintains a critical attitude through negative-space prompting and incorporates multimodality into reasoning by first dividing the reasoning responsibility of LLMs into reasoning and recognition and then integrating the visual recognition capability of visual models into the joint reasoning process.","score":3},{"url":"https://www.semanticscholar.org/paper/c020f15be1dee20f9e2e0c5a6f05f272b5508325","title":"LLaVA-Interactive: An All-in-One Demo for Image Chat, Segmentation, Generation and Editing","venue":"arXiv.org","year":2023,"referenceCount":36,"citationCount":4,"influentialCitationCount":1,"publicationDate":"01/11/2023","authors":"Wei-Ge Chen,Irina Spiridonova,Jianwei Yang,Jianfeng Gao,Chun-yue Li","id":"c020f15be1dee20f9e2e0c5a6f05f272b5508325","summary":"The development of LLaVA-Interactive is extremely cost-efficient as the system combines three multimodal skills of pre-built AI models without additional model training: visual chat of L LaVA, image segmentation from SEEM, as well as image generation and editing from GLIGEN.","score":3},{"url":"https://www.semanticscholar.org/paper/76a3f4a79ae9a00db2f2b5f6877021d8deb96ada","title":"SPHINX: The Joint Mixing of Weights, Tasks, and Visual Embeddings for Multi-modal Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":90,"citationCount":10,"influentialCitationCount":1,"publicationDate":"13/11/2023","authors":"Ziyi Lin,Chris Liu,Renrui Zhang,Peng Gao,Longtian Qiu,Han Xiao,Han Qiu,Chen Lin,Wenqi Shao,Keqin Chen,Jiaming Han,Siyuan Huang,Yichi Zhang,Xuming He,Hongsheng Li,Y. Qiao","id":"76a3f4a79ae9a00db2f2b5f6877021d8deb96ada","summary":"SPHINX, a versatile multi-modal large language model (MLLM) with a joint mixing of model weights, tuning tasks, and visual embeddings is presented, and an efficient strategy aiming to better capture fine-grained appearances of high-resolution images is proposed.","score":3},{"url":"https://www.semanticscholar.org/paper/2fb605f67fee79cad94952ddfe0f686e926f49f5","title":"GPT-4V in Wonderland: Large Multimodal Models for Zero-Shot Smartphone GUI Navigation","venue":"arXiv.org","year":2023,"referenceCount":58,"citationCount":7,"influentialCitationCount":0,"publicationDate":"13/11/2023","authors":"An Yan,Zhengyuan Yang,Wanrong Zhu,K. Lin,Linjie Li,Jianfeng Wang,Jianwei Yang,Yiwu Zhong,Julian McAuley,Jianfeng Gao,Zicheng Liu,Lijuan Wang","id":"2fb605f67fee79cad94952ddfe0f686e926f49f5","summary":"The findings demonstrate that large multimodal models, specifically GPT-4V, excel in zero-shot GUI navigation through its advanced screen interpretation, action reasoning, and precise action localization capabilities.","score":3},{"url":"https://www.semanticscholar.org/paper/5f370f52e24d185ae44bb0ea18cbd4be2aab0d15","title":"VLM-Eval: A General Evaluation on Video Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":49,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/11/2023","authors":"Shuailin Li,Yuang Zhang,Yucheng Zhao,Qiuyue Wang,Fan Jia,Ying-Hao Liu,Tiancai Wang","id":"5f370f52e24d185ae44bb0ea18cbd4be2aab0d15","summary":"This paper introduces a unified evaluation that encompasses multiple video tasks, including captioning, question and answering, retrieval, and action recognition, and proposes a simple baseline: Video-LLaVA, which uses a single linear projection and outperforms existing video LLMs.","score":3},{"url":"https://www.semanticscholar.org/paper/451539c0d0f5f5785ff58d09ca5e67a5f129f9de","title":"A Survey on Multimodal Large Language Models for Autonomous Driving","venue":"arXiv.org","year":2023,"referenceCount":203,"citationCount":11,"influentialCitationCount":0,"publicationDate":"21/11/2023","authors":"Can Cui,Yunsheng Ma,Xu Cao,Wenqian Ye,Yang Zhou,Kaizhao Liang,Jintai Chen,Juanwu Lu,Zichong Yang,Kuei-Da Liao,Tianren Gao,Erlong Li,Kun Tang,Zhipeng Cao,Tongxi Zhou,Ao Liu,Xinrui Yan,Shuqi Mei,Jianguo Cao,Ziran Wang,Chao Zheng","id":"451539c0d0f5f5785ff58d09ca5e67a5f129f9de","summary":"This paper introduces the background of Multimodal Large Language Models (MLLMs), the multimodal models development using LLMs, and the history of autonomous driving, and overviews existing MLLM tools for driving, transportation, and map systems together with existing datasets and benchmarks.","score":3},{"url":"https://www.semanticscholar.org/paper/ee2c769943f9e46c3bbee117d1ecf14566b7bf1f","title":"Boosting the Power of Small Multimodal Reasoning Models to Match Larger Models with Self-Consistency Training","venue":"arXiv.org","year":2023,"referenceCount":62,"citationCount":0,"influentialCitationCount":0,"publicationDate":"23/11/2023","authors":"Cheng Tan,Jingxuan Wei,Zhangyang Gao,Linzhuang Sun,Siyuan Li,Xihong Yang,Stan Z. Li","id":"ee2c769943f9e46c3bbee117d1ecf14566b7bf1f","summary":"This work proposes MC-CoT, a self-consistency training strategy that generates multiple rationales and answers, subsequently selecting the most accurate through a voting process, and demonstrates that this approach significantly improves model performance across various benchmarks.","score":3},{"url":"https://www.semanticscholar.org/paper/7b0a186b0140ee91fb13991c9c7187f3dc3b0670","title":"Visual Programming for Zero-shot Open-Vocabulary 3D Visual Grounding","venue":"arXiv.org","year":2023,"referenceCount":67,"citationCount":0,"influentialCitationCount":0,"publicationDate":"26/11/2023","authors":"Zhihao Yuan,Jinke Ren,Chun-Mei Feng,Hengshuang Zhao,Shuguang Cui,Zhen Li","id":"7b0a186b0140ee91fb13991c9c7187f3dc3b0670","summary":"This work proposes a novel visual programming approach for zero-shot open-vocabulary 3DVG, leveraging the capabilities of large language models (LLMs) and develops an innovative language-object correlation module to extend the scope of existing 3D object detectors into open- Vocabulary scenarios.","score":3},{"url":"https://www.semanticscholar.org/paper/a756b584f8f8b4307e52895ae2120bc339580ad8","title":"See and Think: Embodied Agent in Virtual Environment","venue":"arXiv.org","year":2023,"referenceCount":66,"citationCount":0,"influentialCitationCount":0,"publicationDate":"26/11/2023","authors":"Zhonghan Zhao,Wenhao Chai,Xuan Wang,Li Boyi,Shengyu Hao,Shidong Cao,Tianbo Ye,Jenq-Neng Hwang,Gaoang Wang","id":"a756b584f8f8b4307e52895ae2120bc339580ad8","summary":"This paper proposes STEVE, a comprehensive and visionary embodied agent in the Minecraft virtual environment that consists of three key components: vision perception, language instruction, and code action.","score":3},{"url":"https://www.semanticscholar.org/paper/769a924d0af014acec326f50c15c5d70d258a969","title":"LLMGA: Multimodal Large Language Model based Generation Assistant","venue":"arXiv.org","year":2023,"referenceCount":65,"citationCount":1,"influentialCitationCount":0,"publicationDate":"27/11/2023","authors":"Bin Xia,Shiyin Wang,Yingfan Tao,Yitong Wang,Jiaya Jia","id":"769a924d0af014acec326f50c15c5d70d258a969","summary":"This paper introduces a Multimodal Large Language Model-based Generation Assistant (LLMGA), leveraging the vast reservoir of knowledge and proficiency in reasoning, comprehension, and response inherent in Large Language Models to assist users in image generation and editing.","score":3},{"url":"https://www.semanticscholar.org/paper/486c2df78cbb770a90a55f7fa3fe19102fba2c24","title":"LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":73,"citationCount":0,"influentialCitationCount":0,"publicationDate":"28/11/2023","authors":"Yanwei Li,Chengyao Wang,Jiaya Jia","id":"486c2df78cbb770a90a55f7fa3fe19102fba2c24","summary":"A novel method to tackle the token generation challenge in Vision Language Models (VLMs) for video and image understanding, called LLaMA-VID, which empowers existing frameworks to support hour-long videos and pushes their upper limit with an extra context token.","score":3},{"url":"https://www.semanticscholar.org/paper/9e2bac2777eebe603a39f69221689493609d4149","title":"MLLMs-Augmented Visual-Language Representation Learning","venue":"arXiv.org","year":2023,"referenceCount":69,"citationCount":0,"influentialCitationCount":0,"publicationDate":"30/11/2023","authors":"Yanqing Liu,Kai Wang,Wenqi Shao,Ping Luo,Yu Qiao,Mike Zheng Shou,Kaipeng Zhang,Yang You","id":"9e2bac2777eebe603a39f69221689493609d4149","summary":"This work demonstrates that multi-modal large language models (MLLMs) can enhance visual-language representation learning by improving data quality and proposes text shearing to maintain the same length for extended captions as that of the original captions.","score":3},{"url":"https://www.semanticscholar.org/paper/ef4e4e4b52d4379ab5387d8dc53da87e561e78db","title":"Good Questions Help Zero-Shot Image Reasoning","venue":"arXiv.org","year":2023,"referenceCount":36,"citationCount":0,"influentialCitationCount":0,"publicationDate":"04/12/2023","authors":"Kaiwen Yang,Tao Shen,Xinmei Tian,Xiubo Geng,Chongyang Tao,Dacheng Tao,Tianyi Zhou","id":"ef4e4e4b52d4379ab5387d8dc53da87e561e78db","summary":"Question-Driven Visual Exploration (QVix) is introduced, a novel prompting strategy that enhances the exploratory capabilities of LVLMs in zero-shot reasoning tasks and significantly outperforms existing methods, highlighting its effectiveness in bridging the gap between complex visual data and LVL Ms' exploratory abilities.","score":3},{"url":"https://www.semanticscholar.org/paper/b92289123a94f6076505487adfb4513bd3495c1d","title":"LEGO: Learning EGOcentric Action Frame Generation via Visual Instruction Tuning","venue":"arXiv.org","year":2023,"referenceCount":103,"citationCount":0,"influentialCitationCount":0,"publicationDate":"06/12/2023","authors":"Bolin Lai,Xiaoliang Dai,Lawrence Chen,Guan Pang,James M. Rehg,Miao Liu","id":"b92289123a94f6076505487adfb4513bd3495c1d","summary":"This paper finetune a visual large language model (VLLM) via visual instruction tuning for curating the enriched action descriptions to address a novel problem -- egocentric action frame generation.","score":3},{"url":"https://www.semanticscholar.org/paper/95d791ad14db2c779daa67ca7fdc3a75214c42eb","title":"3DAxiesPrompts: Unleashing the 3D Spatial Task Capabilities of GPT-4V","venue":"arXiv.org","year":2023,"referenceCount":85,"citationCount":0,"influentialCitationCount":0,"publicationDate":"15/12/2023","authors":"Dingning Liu,Xiaomeng Dong,Renrui Zhang,Xu Luo,Peng Gao,Xiaoshui Huang,Yongshun Gong,Zhihui Wang","id":"95d791ad14db2c779daa67ca7fdc3a75214c42eb","summary":"This work presents a new visual prompting method called 3DAxiesPrompts (3DAP) to unleash the capabilities of GPT-4V in performing 3D spatial tasks, and creates a 3D coordinate system tailored to 3D imagery, complete with annotated scale information.","score":3},{"url":"https://www.semanticscholar.org/paper/35a17f896847614a71df772bbe2b66ae231cabc7","title":"CLOVA: A Closed-Loop Visual Assistant with Tool Usage and Update","venue":"arXiv.org","year":2023,"referenceCount":83,"citationCount":0,"influentialCitationCount":0,"publicationDate":"18/12/2023","authors":"Zhi Gao,Yuntao Du,Xintong Zhang,Xiaojian Ma,Wenjuan Han,Song-Chun Zhu,Qing Li","id":"35a17f896847614a71df772bbe2b66ae231cabc7","summary":"Experiments show that CLOVA outperforms tool-usage methods by 5% in visual question answering and multiple-image reasoning tasks, by 10% in knowledge tagging tasks, and by 20% in image editing tasks, highlighting the significance of the learning capability for general visual assistants.","score":3},{"url":"https://www.semanticscholar.org/paper/24fc9ad715372358bd0108eeb7c944b915963293","title":"ASSISTGUI: Task-Oriented Desktop Graphical User Interface Automation","venue":"arXiv.org","year":2023,"referenceCount":53,"citationCount":1,"influentialCitationCount":0,"publicationDate":"20/12/2023","authors":"Difei Gao,Lei Ji,Zechen Bai,Mingyu Ouyang,Peiran Li,Dongxing Mao,Qinchen Wu,Weichen Zhang,Peiyi Wang,Xiangwu Guo,Hengxu Wang,Luowei Zhou,Mike Zheng Shou","id":"24fc9ad715372358bd0108eeb7c944b915963293","summary":"An advanced Actor-Critic Embodied Agent framework is proposed, which incorporates a sophisticated GUI parser driven by an LLM-agent and an enhanced reasoning mechanism adept at handling lengthy procedural tasks that outshine existing methods in performance.","score":3},{"url":"https://www.semanticscholar.org/paper/c672ec79f55cef8f7a32cd8dddfa981b893f1567","title":"V*: Guided Visual Search as a Core Mechanism in Multimodal LLMs","venue":"arXiv.org","year":2023,"referenceCount":60,"citationCount":1,"influentialCitationCount":0,"publicationDate":"21/12/2023","authors":"Penghao Wu,Saining Xie","id":"c672ec79f55cef8f7a32cd8dddfa981b893f1567","summary":"This work introduces V*, an LLM-guided visual search mechanism that employs the world knowledge in LLMs for efficient visual querying and results in a new MLLM meta-architecture, named Show, sEArch, and TelL (SEAL).","score":3},{"url":"https://www.semanticscholar.org/paper/82dc8edc49f9edf4a53056aedcdfb339be070166","title":"IQAGPT: Image Quality Assessment with Vision-language and ChatGPT Models","venue":"arXiv.org","year":2023,"referenceCount":46,"citationCount":0,"influentialCitationCount":0,"publicationDate":"25/12/2023","authors":"Zhihao Chen,Bin Hu,Chuang Niu,Tao Chen,Yuxin Li,Hongming Shan,Ge Wang","id":"82dc8edc49f9edf4a53056aedcdfb339be070166","summary":"IQAGPT is introduced, an innovative image quality assessment system integrating an image quality captioning VLM with ChatGPT for generating quality scores and textual reports that outperforms GPT-4 and CLIP-IQA and multi-task classification and regression models that solely rely on images.","score":3},{"url":"https://www.semanticscholar.org/paper/5f58863dd6474d6f127be995b5871e7c60f2792f","title":"Video Understanding with Large Language Models: A Survey","venue":"arXiv.org","year":2023,"referenceCount":218,"citationCount":0,"influentialCitationCount":0,"publicationDate":"29/12/2023","authors":"Yunlong Tang,Jing Bi,Siting Xu,Luchuan Song,Susan Liang,Teng Wang,Daoan Zhang,Jie An,Jingyang Lin,Rongyi Zhu,A. Vosoughi,Chao Huang,Zeliang Zhang,Feng Zheng,Jianguo Zhang,Ping Luo,Jiebo Luo,Chenliang Xu","id":"5f58863dd6474d6f127be995b5871e7c60f2792f","summary":"The emergent capabilities of Vid-LLMs are surprisingly advanced, particularly their ability for open-ended spatial-temporal reasoning combined with commonsense knowledge, suggesting a promising path for future video understanding.","score":3},{"url":"https://www.semanticscholar.org/paper/575f403261d5f99526f0b4dfc8644352d6c4467a","title":"DocLLM: A layout-aware generative language model for multimodal document understanding","venue":"arXiv.org","year":2023,"referenceCount":62,"citationCount":0,"influentialCitationCount":0,"publicationDate":"31/12/2023","authors":"Dongsheng Wang,Natraj Raman,Mathieu Sibue,Zhiqiang Ma,Petr Babkin,Simerjot Kaur,Yulong Pei,Armineh Nourbakhsh,Xiaomo Liu","id":"575f403261d5f99526f0b4dfc8644352d6c4467a","summary":"DocLLM is presented, a lightweight extension to traditional large language models (LLMs) for reasoning over visual documents, taking into account both textual semantics and spatial layout, and outperforms SotA LLMs on 14 out of 16 datasets across all tasks, and generalizes well to 4 out of 5 previously unseen datasets.","score":3},{"url":"https://www.semanticscholar.org/paper/a06d3e9e90008c64c45a0029d580541d5f646771","title":"If LLM Is the Wizard, Then Code Is the Wand: A Survey on How Code Empowers Large Language Models to Serve as Intelligent Agents","venue":"arXiv.org","year":2024,"referenceCount":168,"citationCount":1,"influentialCitationCount":0,"publicationDate":"01/01/2024","authors":"Ke Yang,Jiateng Liu,John Wu,Chaoqi Yang,Y. Fung,Sha Li,Zixuan Huang,Xu Cao,Xingyao Wang,Yiquan Wang,Heng Ji,Chengxiang Zhai","id":"a06d3e9e90008c64c45a0029d580541d5f646771","summary":"An overview of the various benefits of integrating code into LLMs' training data and how these profound capabilities of LLMs, brought by code, have led to their emergence as intelligent agents in situations where the ability to understand instructions, decompose goals, plan and execute actions, and refine from feedback are crucial to their success on downstream tasks are traced.","score":3},{"url":"https://www.semanticscholar.org/paper/9eab4104973f5de650544729a4a69d84c594da92","title":"A Vision Check-up for Language Models","venue":"arXiv.org","year":2024,"referenceCount":48,"citationCount":0,"influentialCitationCount":0,"publicationDate":"03/01/2024","authors":"Pratyusha Sharma,Tamar Rott Shaham,Manel Baradad,Stephanie Fu,Adrian Rodriguez-Munoz,Shivam Duggal,Phillip Isola,Antonio Torralba","id":"9eab4104973f5de650544729a4a69d84c594da92","summary":"Although LLM-generated images do not look like natural images, results on image generation and the ability of models to correct these generated images indicate that precise modeling of strings can teach language models about numerous aspects of the visual world.","score":3},{"url":"https://www.semanticscholar.org/paper/002d2c4569d070a55fe69c25ebccad8e9ddae572","title":"Incorporating Visual Experts to Resolve the Information Loss in Multimodal Large Language Models","venue":"arXiv.org","year":2024,"referenceCount":58,"citationCount":0,"influentialCitationCount":0,"publicationDate":"06/01/2024","authors":"Xin He,Longhui Wei,Lingxi Xie,Qi Tian","id":"002d2c4569d070a55fe69c25ebccad8e9ddae572","summary":"A novel method is introduced that incorporates multi-task encoders and visual tools into the existing MLLMs training and inference pipeline, aiming to provide a more comprehensive and accurate summarization of visual inputs.","score":3},{"url":"https://www.semanticscholar.org/paper/23957040943f883542f47850c709b9e7f9d6fa55","title":"Prompting Large Vision-Language Models for Compositional Reasoning","venue":"","year":2024,"referenceCount":34,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/01/2024","authors":"Timothy Ossowski,Ming Jiang,Junjie Hu","id":"23957040943f883542f47850c709b9e7f9d6fa55","summary":"This paper makes an exploratory step using a novel generative method that prompts large vision-language models (e.g., GPT-4) to depict images and perform compositional reasoning, and outperforms other embedding-based methods on the Winoground dataset, and obtains further improvement of up to 10% accuracy when enhanced with the optimal description.","score":3},{"url":"https://www.semanticscholar.org/paper/23684a07517870cffd1f97fafbaae16ba22bd2b7","title":"Large AI Models in Health Informatics: Applications, Challenges, and the Future","venue":"IEEE journal of biomedical and health informatics","year":2023,"referenceCount":348,"citationCount":30,"influentialCitationCount":0,"publicationDate":"21/03/2023","authors":"Jianing Qiu,Lin Li,Jiankai Sun,Jiachuan Peng,Peilun Shi,Rui Zhang,Yinzhao Dong,Kyle Lam,F. P. Lo,Bo Xiao,Wu Yuan,Dong Xu,Benny P. L. Lo","id":"23684a07517870cffd1f97fafbaae16ba22bd2b7","summary":"Seven key sectors in which large AI models are applicable and might have substantial influence, including: 1) bioinformatics; 2) medical diagnosis; 3) medical imaging; 4) medical informatics; 5) medical education; 6) public health; and 7) medical robotics are identified.","score":3},{"url":"https://www.semanticscholar.org/paper/c9dbdae8146b9f97e254f5d26fd6efde96eaa703","title":"Med-Flamingo: a Multimodal Medical Few-shot Learner","venue":"arXiv.org","year":2023,"referenceCount":29,"citationCount":25,"influentialCitationCount":5,"publicationDate":"27/07/2023","authors":"Michael Moor,Qian Huang,Shirley Wu,Michihiro Yasunaga,C. Zakka,Yashodhara Dalmia,E. Reis,P. Rajpurkar,J. Leskovec","id":"c9dbdae8146b9f97e254f5d26fd6efde96eaa703","summary":"Med-Flamingo improves performance in generative medical VQA by up to 20\\% in clinician's rating and firstly enables multimodal medical few-shot adaptations, such as rationale generation, as well as releasing the model, code, and evaluation app.","score":3},{"url":"https://www.semanticscholar.org/paper/5b038c1a93967072cc76689fd805e756f804cc42","title":"Large Models for Time Series and Spatio-Temporal Data: A Survey and Outlook","venue":"arXiv.org","year":2023,"referenceCount":343,"citationCount":12,"influentialCitationCount":1,"publicationDate":"16/10/2023","authors":"Ming Jin,Qingsong Wen,Yuxuan Liang,Chaoli Zhang,Siqiao Xue,Xue Wang,James Y. Zhang,Yi Wang,Haifeng Chen,Xiaoli Li,Shirui Pan,Vincent S. Tseng,Yu Zheng,Lei Chen,Hui Xiong","id":"5b038c1a93967072cc76689fd805e756f804cc42","summary":"This survey coalesces the latest strides in large model-centric research on time series and spatio-temporal data, underscoring the solid foundations, current advances, practical applications, abundant resources, and future research opportunities.","score":3},{"url":"https://www.semanticscholar.org/paper/2b3554a8fea6f123fc04bd3e120f2293f227e1b2","title":"InstructMol: Multi-Modal Integration for Building a Versatile and Reliable Molecular Assistant in Drug Discovery","venue":"arXiv.org","year":2023,"referenceCount":85,"citationCount":2,"influentialCitationCount":1,"publicationDate":"27/11/2023","authors":"He Cao,Zijing Liu,Xingyu Lu,Yuan Yao,Yu Li","id":"2b3554a8fea6f123fc04bd3e120f2293f227e1b2","summary":"InstructMol, a multi-modal LLM, effectively aligns molecular structures with natural language via an instruction-tuning approach, utilizing a two-stage training strategy that adeptly combines limited domain-specific data with molecular and textual information.","score":3},{"url":"https://www.semanticscholar.org/paper/28fbbf98bac1bb941162df553ca034d600cb59a6","title":"Rephrase, Augment, Reason: Visual Grounding of Questions for Vision-Language Models","venue":"arXiv.org","year":2023,"referenceCount":108,"citationCount":1,"influentialCitationCount":0,"publicationDate":"09/10/2023","authors":"Archiki Prasad,Elias Stengel-Eskin,Mohit Bansal","id":"28fbbf98bac1bb941162df553ca034d600cb59a6","summary":"Rephrase, Augment and Reason (RepARe), a gradient-free framework that extracts salient details about the image using the underlying LVLM as a captioner and reasoner, in order to propose modifications to the original question, is presented.","score":3},{"url":"https://www.semanticscholar.org/paper/beb3e8acd816bac1a5b7fccfd073f79048877e33","title":"Frozen Transformers in Language Models Are Effective Visual Encoder Layers","venue":"arXiv.org","year":2023,"referenceCount":82,"citationCount":1,"influentialCitationCount":0,"publicationDate":"19/10/2023","authors":"Ziqi Pang,Ziyang Xie,Yunze Man,Yu-Xiong Wang","id":"beb3e8acd816bac1a5b7fccfd073f79048877e33","summary":"This paper reveals that large language models (LLMs), despite being trained solely on textual data, are surprisingly strong encoders for purely visual tasks in the absence of language and proposes the information filtering hypothesis to explain the effectiveness of pre-trained LLMs in visual encoding.","score":3},{"url":"https://www.semanticscholar.org/paper/cf193b5b34178a444cb9bd9f51beb4124b753935","title":"HalluciDoctor: Mitigating Hallucinatory Toxicity in Visual Instruction Data","venue":"arXiv.org","year":2023,"referenceCount":38,"citationCount":0,"influentialCitationCount":0,"publicationDate":"22/11/2023","authors":"Qifan Yu,Juncheng Li,Longhui Wei,Liang Pang,Wentao Ye,Bosheng Qin,Siliang Tang,Qi Tian,Yueting Zhuang","id":"cf193b5b34178a444cb9bd9f51beb4124b753935","summary":"This work presents a novel hallucination detection and elimination framework, HalluciDoctor, based on the cross-checking paradigm, which is used to identify and eliminate hallucinations in the training data automatically and indicates that spurious correlations arising from long-tail object co-occurrences contribute to hallucinations.","score":3},{"url":"https://www.semanticscholar.org/paper/cb5f53cd7d43ac6f48abf4dde6fc58e53a083b0e","title":"Accelerating the integration of ChatGPT and other large‐scale AI models into biomedical research and healthcare","venue":"MedComm – Future Medicine","year":2023,"referenceCount":99,"citationCount":20,"influentialCitationCount":0,"publicationDate":"17/05/2023","authors":"Ding‐Qiao Wang,Long‐Yu Feng,Jin‐Guo Ye,Jin‐Gen Zou,Yingfeng Zheng","id":"cb5f53cd7d43ac6f48abf4dde6fc58e53a083b0e","summary":"","score":3},{"url":"https://www.semanticscholar.org/paper/e9f0223f8dce8b04d37d1f56e6c976b5d0cb5956","title":"A Foundation LAnguage-Image model of the Retina (FLAIR): Encoding expert knowledge in text supervision","venue":"arXiv.org","year":2023,"referenceCount":108,"citationCount":1,"influentialCitationCount":0,"publicationDate":"15/08/2023","authors":"Julio Silva-Rodríguez,H. Chakor,R. Kobbi,J. Dolz,Ismail Ben Ayed","id":"e9f0223f8dce8b04d37d1f56e6c976b5d0cb5956","summary":"Interestingly, FLAIR outperforms by a large margin more generalist, larger-scale image-language models, which emphasizes the potential of embedding experts' domain knowledge and the limitations of generalist models in medical imaging.","score":3},{"url":"https://www.semanticscholar.org/paper/d92c797f587ce7f1b001920ab9e6b7d31960bd77","title":"RemoteCLIP: A Vision Language Foundation Model for Remote Sensing","venue":"arXiv.org","year":2023,"referenceCount":108,"citationCount":14,"influentialCitationCount":2,"publicationDate":"19/06/2023","authors":"F. Liu,Delong Chen,Zhan-Rong Guan,Xiaocong Zhou,Jiale Zhu,Jun Zhou","id":"d92c797f587ce7f1b001920ab9e6b7d31960bd77","summary":"RemoteCLIP is proposed, the first vision-language foundation model for remote sensing that aims to learn robust visual features with rich semantics, as well as aligned text embeddings for seamless downstream application and consistently outperforms baseline foundation models across different model scales.","score":3},{"url":"https://www.semanticscholar.org/paper/13b5b69355555e0c8b702261c5de3b4172ba653c","title":"The Art of SOCRATIC QUESTIONING: Zero-shot Multimodal Reasoning with Recursive Thinking and Self-Questioning","venue":"arXiv.org","year":2023,"referenceCount":27,"citationCount":8,"influentialCitationCount":0,"publicationDate":2023,"authors":"Jingyuan Qi,Zhiyang Xu,Ying Shen,Minqian Liu,dingnan jin,Qifan Wang,Lifu Huang","id":"13b5b69355555e0c8b702261c5de3b4172ba653c","summary":"Qualitative analysis clearly demonstrates the intermediate thinking steps elicited by S OCRATIC Q UESTIONING are similar to the human’s recursively thinking process of a complex reasoning problem.","score":3},{"url":"https://www.semanticscholar.org/paper/fc8988585c6846fdeee33b34779a6a87b92c3e86","title":"Equivariant Similarity for Vision-Language Foundation Models","venue":"IEEE International Conference on Computer Vision","year":2023,"referenceCount":83,"citationCount":9,"influentialCitationCount":3,"publicationDate":"25/03/2023","authors":"Tan Wang,Kevin Lin,Linjie Li,Chung-Ching Lin,Zhengyuan Yang,Hanwang Zhang,Zicheng Liu,Lijuan Wang","id":"fc8988585c6846fdeee33b34779a6a87b92c3e86","summary":"This study proposes EqSim, a regularization loss that can be efficiently calculated from any two matched training pairs and easily pluggable into existing image-text retrieval fine-tuning and presents a new challenging benchmark EqBen, the first to focus on \"visual-minimal change\".","score":3},{"url":"https://www.semanticscholar.org/paper/0046306876ff2d5600699327e52bc29fa5e9ec91","title":"Transfer Visual Prompt Generator across LLMs","venue":"arXiv.org","year":2023,"referenceCount":60,"citationCount":41,"influentialCitationCount":6,"publicationDate":"02/05/2023","authors":"Ao Zhang,Hao Fei,Yuan Yao,Wei Ji,Li Li,Zhiyuan Liu,Tat-seng Chua","id":"0046306876ff2d5600699327e52bc29fa5e9ec91","summary":"This work investigates the VPG transferability across LLMs, and develops a two-stage transfer framework named VPGTrans, which is simple yet highly effective and demonstrated to significantly speed up the transfer learning process without compromising performance.","score":3},{"url":"https://www.semanticscholar.org/paper/3130643a5d02f0e849d83bb1f85577a924081f36","title":"Paxion: Patching Action Knowledge in Video-Language Foundation Models","venue":"arXiv.org","year":2023,"referenceCount":63,"citationCount":7,"influentialCitationCount":1,"publicationDate":"18/05/2023","authors":"Zhenhailong Wang,Ansel Blume,Sha Li,Genglin Liu,Jaemin Cho,Zineng Tang,Mohit Bansal,Heng Ji","id":"3130643a5d02f0e849d83bb1f85577a924081f36","summary":"This work proposes a novel framework, Paxion, along with a new Discriminative Video Dynamics Modeling (DVDM) objective, and introduces the DVDM objective to train the Knowledge Patcher, which forces the model to encode the correlation between the action text and the correct ordering of video frames.","score":3},{"url":"https://www.semanticscholar.org/paper/6fe2a4f7594abadafc9feb05e96ab7e9e634ce8f","title":"Album Storytelling with Iterative Story-aware Captioning and Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":58,"citationCount":3,"influentialCitationCount":1,"publicationDate":"22/05/2023","authors":"Munan Ning,Yujia Xie,Dongdong Chen,Zeyin Song,Lu Yuan,Yonghong Tian,Qixiang Ye,Liuliang Yuan","id":"6fe2a4f7594abadafc9feb05e96ab7e9e634ce8f","summary":"This work proposes a new iterative album storytelling pipeline, which starts with an initial story and builds a story-aware caption model to refine the captions using the whole story as guidance, then feeds into the LLMs to generate a new refined story.","score":3},{"url":"https://www.semanticscholar.org/paper/c5b7f9e2af19db0c2bc9d57a113c17aa9d4d6fda","title":"S-CLIP: Semi-supervised Vision-Language Learning using Few Specialist Captions","venue":"","year":2023,"referenceCount":100,"citationCount":0,"influentialCitationCount":0,"publicationDate":"23/05/2023","authors":"Sangwoo Mo,Min-Kyung Kim,Kyungmin Lee,Jinwoo Shin","id":"c5b7f9e2af19db0c2bc9d57a113c17aa9d4d6fda","summary":"S-CLIP is proposed, a semi-supervised learning method for training CLIP that utilizes additional unpaired images and employs two pseudo-labeling strategies specifically designed for contrastive learning and the language modality.","score":3},{"url":"https://www.semanticscholar.org/paper/08b562aa8066c2342f0d03824221dea18f0a18d2","title":"Cream: Visually-Situated Natural Language Understanding with Contrastive Reading Model and Frozen Large Language Models","venue":"Conference on Empirical Methods in Natural Language Processing","year":2023,"referenceCount":78,"citationCount":2,"influentialCitationCount":0,"publicationDate":"24/05/2023","authors":"Geewook Kim,Hodong Lee,D. Kim,Haeji Jung,S. Park,Yoon Kim,Sangdoo Yun,T. Kil,Bado Lee,Seunghyun Park","id":"08b562aa8066c2342f0d03824221dea18f0a18d2","summary":"This paper introduces Contrastive Reading Model (Cream), a novel neural architecture designed to enhance the language-image understanding capability of LLMs by capturing intricate details that are often overlooked in existing methods.","score":3},{"url":"https://www.semanticscholar.org/paper/b634f9ba35123d40f0af8d96a9c154025cf2cf2a","title":"Contrasting Intra-Modal and Ranking Cross-Modal Hard Negatives to Enhance Visio-Linguistic Compositional Understanding","venue":"","year":2023,"referenceCount":68,"citationCount":0,"influentialCitationCount":0,"publicationDate":"15/06/2023","authors":"Le Zhang,Rabiul Awal,Aishwarya Agrawal","id":"b634f9ba35123d40f0af8d96a9c154025cf2cf2a","summary":"A simple and effective method to improve compositional reasoning in VLMs by refining and expanding the standard image-text contrastive learning framework and does not require specific annotations and does not incur extra parameters.","score":3},{"url":"https://www.semanticscholar.org/paper/0983883619a0ca597d055d0e58da2f514052913d","title":"Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration","venue":"arXiv.org","year":2023,"referenceCount":54,"citationCount":43,"influentialCitationCount":5,"publicationDate":"15/06/2023","authors":"Chenyang Lyu,Minghao Wu,Longyue Wang,Xinting Huang,Bingshuai Liu,Zefeng Du,Shuming Shi,Zhaopeng Tu","id":"0983883619a0ca597d055d0e58da2f514052913d","summary":"This work proposes Macaw-LLM, a novel multi-modal LLM that seamlessly integrates visual, audio, and textual information and builds on the work of previous work on instruction-tuned large language models to handle diverse data modalities and address complex real-world scenarios.","score":3},{"url":"https://www.semanticscholar.org/paper/41c6028c620debae00ca5b30e2db5977225fec57","title":"mBLIP: Efficient Bootstrapping of Multilingual Vision-LLMs","venue":"arXiv.org","year":2023,"referenceCount":84,"citationCount":0,"influentialCitationCount":0,"publicationDate":"13/07/2023","authors":"Gregor Geigle,Abhay Jain,R. Timofte,Goran Glavavs","id":"41c6028c620debae00ca5b30e2db5977225fec57","summary":null,"score":3},{"url":"https://www.semanticscholar.org/paper/fc6a2f7478f68adefd69e2071f27e38aa1647f2f","title":"Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond","venue":"","year":2023,"referenceCount":85,"citationCount":40,"influentialCitationCount":9,"publicationDate":"24/08/2023","authors":"Jinze Bai,Shuai Bai,Shusheng Yang,Shijie Wang,Sinan Tan,Peng Wang,Junyang Lin,Chang Zhou,Jingren Zhou","id":"fc6a2f7478f68adefd69e2071f27e38aa1647f2f","summary":"The Qwen-VL series, a set of large-scale vision-language models (LVLMs) designed to perceive and understand both texts and images, set new records for generalist models under similar model scales on a broad range of visual-centric benchmarks.","score":3},{"url":"https://www.semanticscholar.org/paper/772724892819d7e6f15ce536753fdc32d022c0e0","title":"A Survey on Image-text Multimodal Models","venue":"arXiv.org","year":2023,"referenceCount":229,"citationCount":0,"influentialCitationCount":0,"publicationDate":"23/09/2023","authors":"Ruifeng Guo,Jingxuan Wei,Linzhuang Sun,Bihui Yu,Guiyong Chang,Dawei Liu,Sibo Zhang,Zhengbing Yao,Mingjun Xu,Liping Bu","id":"772724892819d7e6f15ce536753fdc32d022c0e0","summary":"An exhaustive overview of the present research landscape of image-text multimodal models is offered, introducing a novel classification that segments their evolution into three distinct phases, based on their time of introduction and subsequent impact on the discipline.","score":3},{"url":"https://www.semanticscholar.org/paper/96c43227831c4c3b12b7c64809e78674cea3a8a1","title":"DeepSpeed-VisualChat: Multi-Round Multi-Image Interleave Chat via Multi-Modal Causal Attention","venue":"arXiv.org","year":2023,"referenceCount":47,"citationCount":3,"influentialCitationCount":1,"publicationDate":"25/09/2023","authors":"Z. Yao,Xiaoxia Wu,Conglong Li,Minjia Zhang,Heyang Qi,Olatunji Ruwase,A. Awan,Samyam Rajbhandari,Yuxiong He","id":"96c43227831c4c3b12b7c64809e78674cea3a8a1","summary":"The DeepSpeed-VisualChat framework is presented, designed to optimize Large Language Models by incorporating multi-modal capabilities, with a focus on enhancing the proficiency of Large Vision and Language Models in handling interleaved inputs.","score":3},{"url":"https://www.semanticscholar.org/paper/11a4284e335ba39330b59d9f42ca3272a6166991","title":"A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future","venue":"arXiv.org","year":2023,"referenceCount":210,"citationCount":16,"influentialCitationCount":1,"publicationDate":"27/09/2023","authors":"Zheng Chu,Jingchang Chen,Qianglong Chen,Weijiang Yu,Tao He,Haotian Wang,Weihua Peng,Ming Liu,Bing Qin,Ting Liu","id":"11a4284e335ba39330b59d9f42ca3272a6166991","summary":"A thorough survey of the current research according to the taxonomies of methods within the domain of chain-of-thought reasoning, and describes XoT with frontier applications, covering planning, tool use, and distillation.","score":3},{"url":"https://www.semanticscholar.org/paper/a5d27bf7a2155d4ca016565a78b52ee90f81624c","title":"Beyond Task Performance: Evaluating and Reducing the Flaws of Large Multimodal Models with In-Context Learning","venue":"arXiv.org","year":2023,"referenceCount":98,"citationCount":3,"influentialCitationCount":0,"publicationDate":"01/10/2023","authors":"Mustafa Shukor,Alexandre Ramé,Corentin Dancette,M. Cord","id":"a5d27bf7a2155d4ca016565a78b52ee90f81624c","summary":"The training-free in-context learning (ICL) as a solution is explored, and the effect of ICL on LMMs flaws is nuanced; despite its effectiveness for improved explainability, answer abstention, ICL only slightly improves instruction following, does not improve compositional abilities, and actually even amplifies hallucinations.","score":3},{"url":"https://www.semanticscholar.org/paper/f220d3218f84340d1e06e01b89d9c3d64e61edd1","title":"SimVLG: Simple and Efficient Pretraining of Visual Language Generative Models","venue":"arXiv.org","year":2023,"referenceCount":59,"citationCount":1,"influentialCitationCount":0,"publicationDate":"05/10/2023","authors":"Yiren Jian,Tingkai Liu,Yunzhe Tao,Soroush Vosoughi,HX Yang","id":"f220d3218f84340d1e06e01b89d9c3d64e61edd1","summary":"A one-stage, single-loss framework for the pre-training of computationally intensive vision-language generative models, leveraging frozen pre-trained large language models (LLMs), which effectively compacts the visual information while preserving the richness of semantic content, leading to fast convergence without sacrificing performance.","score":3},{"url":"https://www.semanticscholar.org/paper/56025f5034f7aebe1b7292284d33d3d0e3317614","title":"Deep Tensor Network","venue":"arXiv.org","year":2023,"referenceCount":90,"citationCount":0,"influentialCitationCount":0,"publicationDate":"18/11/2023","authors":"Yifan Zhang","id":"56025f5034f7aebe1b7292284d33d3d0e3317614","summary":"The introduction of the Tensor Attention and Tensor Interaction Mechanism is introduced, a groundbreaking approach that leverages the tensor category to enhance the computational efficiency and the expressiveness of deep networks, and can even be generalized into the quantum realm.","score":3},{"url":"https://www.semanticscholar.org/paper/09157a8c0e7d7263ac035690118ddcbe295cee5c","title":"ShapeGPT: 3D Shape Generation with A Unified Multi-modal Language Model","venue":"arXiv.org","year":2023,"referenceCount":60,"citationCount":0,"influentialCitationCount":0,"publicationDate":"29/11/2023","authors":"Fukun Yin,Xin Chen,C. Zhang,Biao Jiang,Zibo Zhao,Jiayuan Fan,Gang Yu,Taihao Li,Tao Chen","id":"09157a8c0e7d7263ac035690118ddcbe295cee5c","summary":"This work presents ShapeGPT, a shape-included multi-modal framework to leverage strong pre-trained language models to address multiple shape-relevant tasks, and achieves comparable performance across shape- relevant tasks, including text-to-shape, shape- to-text, shape completion, and shape editing.","score":3},{"url":"https://www.semanticscholar.org/paper/769b794fe9f97268007676171f246d45e0631014","title":"Towards More Unified In-context Visual Understanding","venue":"arXiv.org","year":2023,"referenceCount":59,"citationCount":0,"influentialCitationCount":0,"publicationDate":"05/12/2023","authors":"Dianmo Sheng,Dongdong Chen,Zhentao Tan,Qiankun Liu,Qi Chu,Jianmin Bao,Tao Gong,Bin Liu,Shengwei Xu,Nenghai Yu","id":"769b794fe9f97268007676171f246d45e0631014","summary":"A new ICL framework for visual understanding with multi-modal output enabled is presented, capable of handling in-context vision understanding tasks with multimodal output in a unified pipeline and achieves competitive performance compared with specialized models and previous ICL baselines.","score":3}]}
{"papers":[{"url":"https://www.semanticscholar.org/paper/d6d3604f369bb0415cbe814e43ca3131323b03e2","title":"Otter: A Multi-Modal Model with In-Context Instruction Tuning","venue":"arXiv.org","year":2023,"referenceCount":36,"citationCount":2,"influentialCitationCount":0,"publicationDate":"05/05/2023","authors":"Bo Li,Yuanhan Zhang,Liangyu Chen,Jinghao Wang,Jingkang Yang,Ziwei Liu","id":"d6d3604f369bb0415cbe814e43ca3131323b03e2","summary":"Otter, a multi-modal model based on OpenFlamingo (open-sourced version of DeepMind's Flamingo), trained on MIMIC-IT and showcasing improved instruction-following ability and in-context learning is introduced.","score":6},{"url":"https://www.semanticscholar.org/paper/570079bbdd8758dfe865097e05719313c9c1301a","title":"LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model","venue":"arXiv.org","year":2023,"referenceCount":77,"citationCount":7,"influentialCitationCount":0,"publicationDate":"28/04/2023","authors":"Peng Gao,Jiaming Han,Renrui Zhang,Ziyi Lin,Shijie Geng,Aojun Zhou,W. Zhang,Pan Lu,Conghui He,Xiangyu Yue,Hongsheng Li,Y. Qiao","id":"570079bbdd8758dfe865097e05719313c9c1301a","summary":"This work augments LLaMA-Adapter by unlocking more learnable parameters and proposes an early fusion strategy to feed visual tokens only into the early LLM layers, contributing to better visual knowledge incorporation and achieves strong multi-modal reasoning with only a small-scale image-text and instruction dataset.","score":5},{"url":"https://www.semanticscholar.org/paper/0ebc861f5478561f12941e6b48aad30574e996d8","title":"Video ChatCaptioner: Towards Enriched Spatiotemporal Descriptions","venue":"arXiv.org","year":2023,"referenceCount":42,"citationCount":4,"influentialCitationCount":0,"publicationDate":"09/04/2023","authors":"Jun Chen,Deyao Zhu,Kilichbek Haydarov,Xiang Li,Mohamed Elhoseiny","id":"0ebc861f5478561f12941e6b48aad30574e996d8","summary":"This work introduces Video ChatCaptioner, an innovative approach for creating more comprehensive spatiotemporal video descriptions, specifically designed to select frames for posing video content-driven questions and shows promise as a method for enhancing video content.","score":5},{"url":"https://www.semanticscholar.org/paper/e1ff32753e20e48b4b01e40b5e820254396e6e70","title":"LMEye: An Interactive Perception Network for Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":48,"citationCount":0,"influentialCitationCount":0,"publicationDate":"05/05/2023","authors":"Yunxin Li,Baotian Hu,Xinyu Chen,Lin Ma,M. Zhang","id":"e1ff32753e20e48b4b01e40b5e820254396e6e70","summary":"Interactive Perception Network (IPN), aiming to achieve a LVLM by incorporating the image understanding capability into Large Language Models (LLMs), significantly improves the zero-shot performance of LVLMs on various multimodal tasks compared to previous methods.","score":5},{"url":"https://www.semanticscholar.org/paper/42a30dc5470f54ec249f25d3c31e05d7c376c8e3","title":"VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks","venue":"arXiv.org","year":2023,"referenceCount":78,"citationCount":3,"influentialCitationCount":0,"publicationDate":"18/05/2023","authors":"Wen Wang,Zhe Chen,Xiaokang Chen,Jiannan Wu,Xizhou Zhu,Gang Zeng,Ping Luo,Tong Lu,Jie Zhou,Y. Qiao,Jifeng Dai","id":"42a30dc5470f54ec249f25d3c31e05d7c376c8e3","summary":"This work presents an LLM-based framework for vision-centric tasks, termed VisionLLM, which provides a unified perspective for vision and language tasks by treating images as a foreign language and aligning vision-focused tasks with language tasks that can be flexibly defined and managed using language instructions.","score":5},{"url":"https://www.semanticscholar.org/paper/7e32aac43e9f1df49e116add03327ee6f365dbf3","title":"mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality","venue":"arXiv.org","year":2023,"referenceCount":32,"citationCount":12,"influentialCitationCount":2,"publicationDate":"27/04/2023","authors":"Qinghao Ye,Haiyang Xu,Guohai Xu,Jiabo Ye,Ming Yan,Yi Zhou,Junyan Wang,Anwen Hu,Pengcheng Shi,Yaya Shi,Chenliang Li,Yuanhong Xu,Hehong Chen,Junfeng Tian,Qiang Qi,J. Zhang,Feiyan Huang","id":"7e32aac43e9f1df49e116add03327ee6f365dbf3","summary":null,"score":4},{"url":"https://www.semanticscholar.org/paper/ca45c746b69fdba8e73c1872937b4f054c6c4cc1","title":"InternGPT: Solving Vision-Centric Tasks by Interacting with Chatbots Beyond Language","venue":"arXiv.org","year":2023,"referenceCount":79,"citationCount":5,"influentialCitationCount":0,"publicationDate":"09/05/2023","authors":"Zhaoyang Liu,Yinan He,Wenhai Wang,Weiyun Wang,Yi Wang,Shoufa Chen,Qing-Long Zhang,Yang Yang,Qingyun Li,Jiashuo Yu,Kunchang Li,Zhe Chen,Xuecheng Yang,Xizhou Zhu,Yali Wang,Limin Wang,Ping Luo,Jifeng Dai,Yu Qiao","id":"ca45c746b69fdba8e73c1872937b4f054c6c4cc1","summary":"By incorporating pointing instructions, the proposed iGPT significantly improves the efficiency of communication between users and chatbots, as well as the accuracy of chatbots in vision-centric tasks, especially in complicated visual scenarios where the number of objects is greater than 2.","score":4},{"url":"https://www.semanticscholar.org/paper/d48cb91b9e555194f7494c4d4bb9815021d3ee45","title":"VideoChat: Chat-Centric Video Understanding","venue":"arXiv.org","year":2023,"referenceCount":59,"citationCount":6,"influentialCitationCount":0,"publicationDate":"10/05/2023","authors":"Kunchang Li,Yinan He,Yi Wang,Yizhuo Li,Wen Wang,Ping Luo,Yali Wang,Limin Wang,Yu Qiao","id":"d48cb91b9e555194f7494c4d4bb9815021d3ee45","summary":"VideoChat is introduced, an end-to-end chat-centric video understanding system that integrates video foundation models and large language models via a learnable neural interface, excelling in spatiotemporal reasoning, event localization, and causal relationship inference.","score":4},{"url":"https://www.semanticscholar.org/paper/4194149154f68dd7b1dc60f51bff2ca68448345a","title":"PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering","venue":"arXiv.org","year":2023,"referenceCount":38,"citationCount":0,"influentialCitationCount":0,"publicationDate":"17/05/2023","authors":"Xiaoman Zhang,Chaoyi Wu,Ziheng Zhao,Weixiong Lin,Ya Zhang,Yanfeng Wang,Weidi Xie","id":"4194149154f68dd7b1dc60f51bff2ca68448345a","summary":"This paper proposes a generative-based model for medical visual understanding by aligning visual information from a pre-trained vision encoder with a large language model, and establishes a scalable pipeline to construct a large-scale medical visual question-answering dataset.","score":4},{"url":"https://www.semanticscholar.org/paper/a3711dbf296b5ddd97ba93826660cd3995611625","title":"Towards A Foundation Model for Generalist Robots: Diverse Skill Learning at Scale via Automated Task and Scene Generation","venue":"arXiv.org","year":2023,"referenceCount":111,"citationCount":0,"influentialCitationCount":0,"publicationDate":"17/05/2023","authors":"Zhou Xian,Théophile Gervet,Zhenjia Xu,Yi-Ling Qiao,Tsun-Hsuan Wang","id":"a3711dbf296b5ddd97ba93826660cd3995611625","summary":"This document presents a specific idea for mining knowledge in the latest large-scale foundation models for robotics research, and advocates for using them to generate diversified tasks and scenes at scale, thereby scaling up low-level skill learning and ultimately leading to a foundation model for robotics that empowers generalist robots.","score":3},{"url":"https://www.semanticscholar.org/paper/f500523b8362ad8f838da14ff0a2498d0143cd1e","title":"Reasoning with Language Model Prompting: A Survey","venue":"arXiv.org","year":2022,"referenceCount":198,"citationCount":30,"influentialCitationCount":1,"publicationDate":"19/12/2022","authors":"Shuofei Qiao,Yixin Ou,Ningyu Zhang,Xiang Chen,Yunzhi Yao,Shumin Deng,Chuanqi Tan,Fei Huang,Huajun Chen","id":"f500523b8362ad8f838da14ff0a2498d0143cd1e","summary":"This paper provides a comprehensive survey of cutting-edge research on reasoning with language model prompting with comparisons and summaries and provides systematic resources to help beginners.","score":3},{"url":"https://www.semanticscholar.org/paper/170c97c7215f42edfb20c2248f954879e91ef86e","title":"Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":65,"citationCount":22,"influentialCitationCount":3,"publicationDate":"19/04/2023","authors":"Pan Lu,Baolin Peng,Hao Cheng,Michel Galley,Kai-Wei Chang,Y. Wu,Song-Chun Zhu,Jianfeng Gao","id":"170c97c7215f42edfb20c2248f954879e91ef86e","summary":"This paper presents Chameleon, an AI system that mitigates LLM limitations by augmenting LLMs with plug-and-play modules for compositional reasoning, and shows that the GPT-4-powered planner exhibits more consistent and rational tool selection via inferring potential constraints from instructions, compared to a ChatGPT- powered planner.","score":3},{"url":"https://www.semanticscholar.org/paper/1a8eb2cae1833df3bf12fe3b41b03d60b4a4a98d","title":"Visual Instruction Tuning","venue":"arXiv.org","year":2023,"referenceCount":55,"citationCount":34,"influentialCitationCount":18,"publicationDate":"17/04/2023","authors":"Haotian Liu,Chunyuan Li,Qingyang Wu,Yong Jae Lee","id":"1a8eb2cae1833df3bf12fe3b41b03d60b4a4a98d","summary":"This paper presents LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding and introduces GPT-4 generated visual instruction tuning data, the model and code base publicly available.","score":3},{"url":"https://www.semanticscholar.org/paper/43e6e8d6663d83f1b74cf5a2be7b040b0928f867","title":"X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages","venue":"arXiv.org","year":2023,"referenceCount":61,"citationCount":2,"influentialCitationCount":0,"publicationDate":"07/05/2023","authors":"Feilong Chen,Minglun Han,Haozhi Zhao,Qingyang Zhang,Jing Shi,Shuang Xu,Bo Xu","id":"43e6e8d6663d83f1b74cf5a2be7b040b0928f867","summary":"X-LLM is proposed, which converts Multi-modalities into foreign languages using X2L interfaces and inputs them into a large Language model (ChatGLM), and demonstrates impressive multimodel chat abilities.","score":3},{"url":"https://www.semanticscholar.org/paper/6a5525c316b9be7909c433a79e090ed731425083","title":"What Makes for Good Visual Tokenizers for Large Language Models?","venue":"arXiv.org","year":2023,"referenceCount":54,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/05/2023","authors":"Guangzhi Wang,Yixiao Ge,Xiaohan Ding,Mohan S. Kankanhalli,Ying Shan","id":"6a5525c316b9be7909c433a79e090ed731425083","summary":"A new MLLM equipped with a tailored Good Visual Tokenizer (GVT), which exhibits strong visual comprehension capability at multiple scales, without introducing extra parameters and task-specific fine-tuning.","score":3},{"url":"https://www.semanticscholar.org/paper/ca055cfb9d4d47124cc035c346f38577825fcacf","title":"Enhance Reasoning Ability of Visual-Language Models via Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":35,"citationCount":0,"influentialCitationCount":0,"publicationDate":"22/05/2023","authors":"Yueting Yang,Xintong Zhang,Wenjuan Han","id":"ca055cfb9d4d47124cc035c346f38577825fcacf","summary":"A method called TReE is proposed, which transfers the reasoning ability of a large language model to a visual language model in zero-shot scenarios, and contains three stages: observation, thinking, and re-thinking.","score":3},{"url":"https://www.semanticscholar.org/paper/9a22b33b529484c912d1ea9f8698369d4546a1c1","title":"Transfer Visual Prompt Generator across LLMs","venue":"arXiv.org","year":2023,"referenceCount":34,"citationCount":2,"influentialCitationCount":0,"publicationDate":"02/05/2023","authors":"Ao Zhang,Hao Fei,Yuan Yao,Wei Ji,Li Li,Zhiyuan Liu,Tat-Seng Chua","id":"9a22b33b529484c912d1ea9f8698369d4546a1c1","summary":"This work investigates the VPG transferability across LLMs, and designs a two-stage transfer framework named VPGTrans, which is simple yet highly effective and demonstrated to significantly speed up the transfer learning process without compromising performance.","score":3},{"url":"https://www.semanticscholar.org/paper/280b7d7e3ed4c069cc974a70780f88041cbf0717","title":"Paxion: Patching Action Knowledge in Video-Language Foundation Models","venue":"arXiv.org","year":2023,"referenceCount":62,"citationCount":0,"influentialCitationCount":0,"publicationDate":"18/05/2023","authors":"Zhenhailong Wang,Ansel Blume,Sha Li,Genglin Liu,Jaemin Cho,Zineng Tang,Mohit Bansal,Heng Ji","id":"280b7d7e3ed4c069cc974a70780f88041cbf0717","summary":"This work proposes a novel framework, Paxion, along with a new Discriminative Video Dynamics Modeling (DVDM) objective, which effectively fills the gap in action knowledge understanding, while maintaining or improving performance on a wide spectrum of both object- and action-centric downstream tasks.","score":3},{"url":"https://www.semanticscholar.org/paper/bfa9df38bd8597d8cdf0c3497fe5e5a5e31f646f","title":"ONE-PEACE: Exploring One General Representation Model Toward Unlimited Modalities","venue":"arXiv.org","year":2023,"referenceCount":169,"citationCount":0,"influentialCitationCount":0,"publicationDate":"18/05/2023","authors":"Peng Wang,Shijie Wang,Junyang Lin,Shuai Bai,Xiaohuan Zhou,Jingren Zhou,Xinggang Wang,Chang Zhou","id":"bfa9df38bd8597d8cdf0c3497fe5e5a5e31f646f","summary":"This work releases ONE-PEACE, a highly extensible model with 4B parameters that can seamlessly align and integrate representations across vision, audio, and language modalities, and develops two modality-agnostic pretraining tasks, which align the semantic space of different modalities and capture fine-grained details within modalities concurrently.","score":3},{"url":"https://www.semanticscholar.org/paper/6fe2a4f7594abadafc9feb05e96ab7e9e634ce8f","title":"Album Storytelling with Iterative Story-aware Captioning and Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":57,"citationCount":0,"influentialCitationCount":0,"publicationDate":"22/05/2023","authors":"Munan Ning,Yujia Xie,Dongdong Chen,Zeyin Song,Lu Yuan,Yonghong Tian,Qixiang Ye,Liuliang Yuan","id":"6fe2a4f7594abadafc9feb05e96ab7e9e634ce8f","summary":"This work proposes a new iterative album storytelling pipeline, which starts with an initial story and builds a story-aware caption model to refine the captions using the whole story as guidance, then feeds into the LLMs to generate a new refined story.","score":3},{"url":"https://www.semanticscholar.org/paper/cb5f53cd7d43ac6f48abf4dde6fc58e53a083b0e","title":"Accelerating the integration of ChatGPT and other large‐scale AI models into biomedical research and healthcare","venue":"MedComm – Future Medicine","year":2023,"referenceCount":99,"citationCount":0,"influentialCitationCount":0,"publicationDate":"17/05/2023","authors":"Ding‐Qiao Wang,Long‐Yu Feng,Jin‐Guo Ye,Jin‐Gen Zou,Yingfeng Zheng","id":"cb5f53cd7d43ac6f48abf4dde6fc58e53a083b0e","summary":"","score":3},{"url":"https://www.semanticscholar.org/paper/e9480d62e216f77d5556b7eda769daa4c92d004d","title":"VQAMix: Conditional Triplet Mixup for Medical Visual Question Answering","venue":"IEEE Transactions on Medical Imaging","year":2022,"referenceCount":55,"citationCount":7,"influentialCitationCount":3,"publicationDate":"21/06/2022","authors":"Haifan Gong,Guanqi Chen,Mingzhi Mao,Z. Li,Guanbin Li","id":"e9480d62e216f77d5556b7eda769daa4c92d004d","summary":"This paper proposes a simple yet effective data augmentation method, VQAMix, which generates more labeled training samples by linearly combining a pair of VQA samples, which can be easily embedded into any visual-language model to boost performance.","score":3},{"url":"https://www.semanticscholar.org/paper/ac4d13b6a4f9fb67337099f4602135a0351f5c99","title":"Towards Medical Artificial General Intelligence via Knowledge-Enhanced Multimodal Pretraining","venue":"arXiv.org","year":2023,"referenceCount":60,"citationCount":0,"influentialCitationCount":0,"publicationDate":"26/04/2023","authors":"Bingqian Lin,Zicong Chen,Mingjie Li,Haokun Lin,Hang Xu,Yi Zhu,Jian-zhuo Liu,Wenjia Cai,Lei Yang,Shen Zhao,Chenfei Wu,Ling Chen,Xiaojun Chang,Yi Yang,L. Xing,Xiaodan Liang","id":"ac4d13b6a4f9fb67337099f4602135a0351f5c99","summary":"The proposed MOTOR successfully mimics the human practice of fulfilling a\"medical student\" to accelerate the process of becoming a\"specialist\" and believes that this work makes a significant stride in realizing MAGI.","score":3},{"url":"https://www.semanticscholar.org/paper/ac7771c332da42b29a913b116bd6ef622cbf89cf","title":"TaskMatrix.AI: Completing Tasks by Connecting Foundation Models with Millions of APIs","venue":"arXiv.org","year":2023,"referenceCount":27,"citationCount":22,"influentialCitationCount":1,"publicationDate":"29/03/2023","authors":"Yaobo Liang,Chenfei Wu,Ting Song,Wenshan Wu,Yan Xia,Yu Liu,Yangyiwen Ou,Shuai Lu,Lei Ji,Shaoguang Mao,Yun Wang,Linjun Shou,Ming Gong,Nan Duan","id":"ac7771c332da42b29a913b116bd6ef622cbf89cf","summary":"The vision of how to build such an ecosystem is presented, each key component is explained, and study cases are used to illustrate both the feasibility of this vision and the main challenges the authors need to address next.","score":2},{"url":"https://www.semanticscholar.org/paper/01f9b773408115a16fe872147348db175789e82f","title":"Tool Learning with Foundation Models","venue":"arXiv.org","year":2023,"referenceCount":224,"citationCount":10,"influentialCitationCount":2,"publicationDate":"17/04/2023","authors":"Yujia Qin,Shengding Hu,Yankai Lin,Weize Chen,Ning Ding,Ganqu Cui,Zheni Zeng,Yufei Huang,Chaojun Xiao,Chi Han,Y. Fung,Yusheng Su,Huadong Wang,Cheng Qian,Runchu Tian,Kunlun Zhu,Shi Liang,Xingyu Shen,Bokai Xu,Zhen Zhang,Yining Ye,Bo Li,Ziwei Tang,Jing Yi,Yu Zhu,Zhenning Dai,Lan Yan,Xin Cong,Ya-Ting Lu,Weilin Zhao,Yuxiang Huang,Jun-Han Yan,Xu Han,Xian Sun,Dahai Li,Jason Phang,Cheng Yang,Tongshuang Wu,Heng Ji,Zhiyuan Liu,Maosong Sun","id":"01f9b773408115a16fe872147348db175789e82f","summary":"A systematic investigation of tool learning is presented, including its cognitive origins, the paradigm shift of foundation models, and the complementary roles of tools and models to inspire future research in integrating tools with foundation models.","score":2},{"url":"https://www.semanticscholar.org/paper/2195676f111ad492c50f4d4c96abb2bd3d72f7fc","title":"Instruct2Act: Mapping Multi-modality Instructions to Robotic Actions with Large Language Model","venue":"arXiv.org","year":2023,"referenceCount":45,"citationCount":1,"influentialCitationCount":1,"publicationDate":"18/05/2023","authors":"Siyuan Huang,Zhengkai Jiang,Hao-Wen Dong,Y. Qiao,Peng Gao,Hongsheng Li","id":"2195676f111ad492c50f4d4c96abb2bd3d72f7fc","summary":"This paper presents Instruct2Act, a framework that utilizes Large Language Models to map multi-modal instructions to sequential actions for robotic manipulation tasks, employing the LLM model to generate Python programs that constitute a comprehensive perception, planning, and action loop for robotic tasks.","score":2},{"url":"https://www.semanticscholar.org/paper/13a5140fc0b269c408ecfc666cb297410bc753c5","title":"Matcher: Segment Anything with One Shot Using All-Purpose Feature Matching","venue":"arXiv.org","year":2023,"referenceCount":44,"citationCount":0,"influentialCitationCount":0,"publicationDate":"22/05/2023","authors":"Yang Liu,Muzhi Zhu,Hengtao Li,Hao Chen,Xinlong Wang,Chunhua Shen","id":"13a5140fc0b269c408ecfc666cb297410bc753c5","summary":"This work presents Matcher, which segments anything with one shot by integrating an all-purpose feature extraction model and a class-agnostic segmentation model, and proposes a novel instance-level matching strategy for controllable mask merging.","score":2},{"url":"https://www.semanticscholar.org/paper/d1120d67b700e4dfe8b39eb1e48fbdea4e1a0c43","title":"HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace","venue":"arXiv.org","year":2023,"referenceCount":39,"citationCount":60,"influentialCitationCount":10,"publicationDate":"30/03/2023","authors":"Yongliang Shen,Kaitao Song,Xu Tan,D. Li,Weiming Lu,Y. Zhuang","id":"d1120d67b700e4dfe8b39eb1e48fbdea4e1a0c43","summary":"HuggingGPT is a framework that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities to solve AI tasks and is able to cover numerous sophisticated AI tasks in different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks.","score":2},{"url":"https://www.semanticscholar.org/paper/7562e25b666cba841b1dd5cf6e700978922beb04","title":"SkinGPT: A Dermatology Diagnostic System with Vision Large Language Model","venue":"arXiv.org","year":2023,"referenceCount":35,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/04/2023","authors":"Juexiao Zhou,Xin Gao","id":"7562e25b666cba841b1dd5cf6e700978922beb04","summary":"SkinGPT is the first dermatology diagnostic system that utilizes an advanced vision-based large language model, incorporating a fine-tuned version of MiniGPT-4 with a vast collection of in-house skin disease images, accompanied by doctor's notes.","score":2},{"url":"https://www.semanticscholar.org/paper/c56a51728678e5b2e3ff95e51caf21d267439c36","title":"ChatVideo: A Tracklet-centric Multimodal and Versatile Video Understanding System","venue":"arXiv.org","year":2023,"referenceCount":43,"citationCount":2,"influentialCitationCount":0,"publicationDate":"27/04/2023","authors":"Junke Wang,Dongdong Chen,Chong Luo,Xiyang Dai,Lu Yuan,Zuxuan Wu,Yu-Gang Jiang","id":"c56a51728678e5b2e3ff95e51caf21d267439c36","summary":"The vision for multimodal and versatile video understanding is presented and a prototype system, built upon a tracklet-centric paradigm, which treats tracklets as the basic video unit and employs various Video Foundation Models to annotate their properties e.g., appearance, motion, etc.","score":2},{"url":"https://www.semanticscholar.org/paper/8f95859cd6ccbe2c039fe8214f76c22382ebb9c3","title":"Caption Anything: Interactive Image Description with Diverse Multimodal Controls","venue":"arXiv.org","year":2023,"referenceCount":48,"citationCount":1,"influentialCitationCount":1,"publicationDate":"04/05/2023","authors":"Teng Wang,Jinrui Zhang,Junjie Fei,Yixiao Ge,Hao Zheng,Yun-Qiu Tang,Zhe Li,Mingqi Gao,Shanshan Zhao,Ying Shan,Feng Zheng","id":"8f95859cd6ccbe2c039fe8214f76c22382ebb9c3","summary":"Caption AnyThing (CAT) is presented, a foundation model augmented image captioning framework supporting a wide range of multimodel controls: 1) visual controls, including points, boxes, and trajectories; 2) language controls, such as sentiment, length, language, and factuality.","score":2},{"url":"https://www.semanticscholar.org/paper/80c44fab16852ea9599411da14de7079c4514172","title":"Vision-Language Models in Remote Sensing: Current Progress and Future Trends","venue":"arXiv.org","year":2023,"referenceCount":195,"citationCount":0,"influentialCitationCount":0,"publicationDate":"09/05/2023","authors":"Congcong Wen,Yuan Hu,Xiang Li,Zhenghang Yuan,Xiao Xiang Zhu","id":"80c44fab16852ea9599411da14de7079c4514172","summary":"A comprehensive review of the research on vision-language models in remote sensing, summarizing the latest progress, highlighting the current challenges, and identifying potential research opportunities is provided.","score":2},{"url":"https://www.semanticscholar.org/paper/affd5837492019cf252f4bf89afcda3708d2abac","title":"InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning","venue":"arXiv.org","year":2023,"referenceCount":48,"citationCount":5,"influentialCitationCount":1,"publicationDate":"11/05/2023","authors":"Wenliang Dai,Junnan Li,Dongxu Li,A. M. H. Tiong,Junqi Zhao,Weisheng Wang,Boyang Li,Pascale Fung,Steven Hoi","id":"affd5837492019cf252f4bf89afcda3708d2abac","summary":"A systematic and comprehensive study on vision-language instruction tuning based on the pre-trained BLIP-2 models, which achieves state-of-the-art zero-shot performance across all 13 held-out datasets and qualitatively demonstrate the advantages of InstructBLIP over concurrent multimodal models.","score":2},{"url":"https://www.semanticscholar.org/paper/59d0da2d295fe4761431b6b1fcc805ba0df1e172","title":"ArtGPT-4: Artistic Vision-Language Understanding with Adapter-enhanced MiniGPT-4","venue":"arXiv.org","year":2023,"referenceCount":41,"citationCount":0,"influentialCitationCount":0,"publicationDate":"12/05/2023","authors":"Zheng Yuan,HU Xue,Xinyi Wang,Yongming Liu,Zhuanzhe Zhao,Kun Wang","id":"59d0da2d295fe4761431b6b1fcc805ba0df1e172","summary":"A novel multimodal model called ArtGPT-4 has been proposed to address some challenges in image understanding, particularly in artistic pictures, and novel benchmarks for evaluating the performance of vision-language models are proposed.","score":2},{"url":"https://www.semanticscholar.org/paper/c8e41bde9c80b986bf981f54029db489ee0b7809","title":"On the Hidden Mystery of OCR in Large Multimodal Models","venue":"arXiv.org","year":2023,"referenceCount":48,"citationCount":1,"influentialCitationCount":0,"publicationDate":"13/05/2023","authors":"Yuliang Liu,Zhang Li,Hongliang Li,Wenwen Yu,Mingxin Huang,Dezhi Peng,Mingyu Liu,Mingrui Chen,Chunyuan Li,Lianwen Jin,Xiang Bai","id":"c8e41bde9c80b986bf981f54029db489ee0b7809","summary":"It is demonstrated that even the current most powerful large multimodal models cannot match domain-specific methods in traditional text tasks and face greater challenges in more complex tasks.","score":2},{"url":"https://www.semanticscholar.org/paper/3f0c4d50050e8d74993b020897abaee8d1e8054d","title":"Evaluating Object Hallucination in Large Vision-Language Models","venue":"arXiv.org","year":2023,"referenceCount":42,"citationCount":2,"influentialCitationCount":0,"publicationDate":"17/05/2023","authors":"Yifan Li,Yifan Du,Kun Zhou,Jinpeng Wang,Wayne Xin Zhao,Ji-rong Wen","id":"3f0c4d50050e8d74993b020897abaee8d1e8054d","summary":"This work presents the first systematic study on object hallucination of LVLMs, and designs an improved evaluation method by proposing a polling-based query method called POPE, which can evaluate the object hallucinated objects in a more stable and flexible way.","score":2},{"url":"https://www.semanticscholar.org/paper/a1f83d8507677ad0ff47ee44d0577fe086d4b01a","title":"X-IQE: eXplainable Image Quality Evaluation for Text-to-Image Generation with Visual Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":56,"citationCount":0,"influentialCitationCount":0,"publicationDate":"18/05/2023","authors":"Yixiong Chen","id":"a1f83d8507677ad0ff47ee44d0577fe086d4b01a","summary":"A novel explainable image quality evaluation approach called X-IQE, which leverages visual large language models (LLMs) to evaluate text-to-image generation methods by generating textual explanations by utilizing a hierarchical Chain of Thought to enable MiniGPT-4 to produce self-consistent, unbiased texts that are highly correlated with human evaluation.","score":2},{"url":"https://www.semanticscholar.org/paper/1bdd5fc17cc580efe998304692639c57c857cc84","title":"Going Denser with Open-Vocabulary Part Segmentation","venue":"arXiv.org","year":2023,"referenceCount":94,"citationCount":0,"influentialCitationCount":0,"publicationDate":"18/05/2023","authors":"Pei Sun,Shoufa Chen,Chenchen Zhu,Fanyi Xiao,Ping Luo,Saining Xie,Zhicheng Yan","id":"1bdd5fc17cc580efe998304692639c57c857cc84","summary":"A detector with the ability to predict both open-vocabulary objects and their part segmentation and training on the joint of part-level, object-level and image-level data to build the multi-granularity alignment between language and image.","score":2},{"url":"https://www.semanticscholar.org/paper/33f9ddca2469bf4831dcab085e1620792b1a6a80","title":"LLM Itself Can Read and Generate CXR Images","venue":"arXiv.org","year":2023,"referenceCount":49,"citationCount":0,"influentialCitationCount":0,"publicationDate":"19/05/2023","authors":"Suhyeon Lee,Won Jun Kim,Jong-Chul Ye","id":"33f9ddca2469bf4831dcab085e1620792b1a6a80","summary":"This work presents a novel method to fine-tune a pre-trained LLM to read and generate images like text without any structural changes, extra training objectives, or the need for training an ad-hoc network while still preserving the of the instruction-following capability of the LLM.","score":2},{"url":"https://www.semanticscholar.org/paper/f9bfc6d9ba1665b73af3323d46c7642b852759ef","title":"VideoLLM: Modeling Video Sequence with Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":101,"citationCount":0,"influentialCitationCount":0,"publicationDate":"22/05/2023","authors":"Guo Chen,Yin-Dong Zheng,Jiahao Wang,Jilan Xu,Yifei Huang,Junting Pan,Yi Wang,Yali Wang,Y. Qiao,Tong Lu,Limin Wang","id":"f9bfc6d9ba1665b73af3323d46c7642b852759ef","summary":"This work proposes a novel framework called VideoLLM that leverages the sequence reasoning capabilities of pre-trained LLMs from natural language processing (NLP) for video sequence understanding and demonstrates that the understanding and Reasoning capabilities of LLMs can be effectively transferred to video understanding tasks.","score":2},{"url":"https://www.semanticscholar.org/paper/43a55dbd95c9d5cd82de8db276f41adeec4a937d","title":"Interactive Data Synthesis for Systematic Vision Adaptation via LLMs-AIGCs Collaboration","venue":"arXiv.org","year":2023,"referenceCount":37,"citationCount":0,"influentialCitationCount":0,"publicationDate":"22/05/2023","authors":"Qifan Yu,Juncheng Li,Wentao Ye,Siliang Tang,Yueting Zhuang","id":"43a55dbd95c9d5cd82de8db276f41adeec4a937d","summary":"This work extensively study how LLMs communicate with AIGC model to achieve more controllable image generation and makes the first attempt to collaborate them for automatic data augmentation for a variety of downstream tasks.","score":2},{"url":"https://www.semanticscholar.org/paper/de8121dc3d2c69bdab172f37e31168ddf2e6e62f","title":"Segment Everything Everywhere All at Once","venue":"arXiv.org","year":2023,"referenceCount":65,"citationCount":14,"influentialCitationCount":5,"publicationDate":"13/04/2023","authors":"Xueyan Zou,Jianwei Yang,Hao Zhang,Feng Li,Linjie Li,Jianfeng Gao,Yong Jae Lee","id":"de8121dc3d2c69bdab172f37e31168ddf2e6e62f","summary":"SEEM is a promptable, interactive model for Segmenting Everything Everywhere all at once in an image by incorporating learnable memory prompts to retain dialog history information via mask-guided cross-attention.","score":2},{"url":"https://www.semanticscholar.org/paper/508d9b43832790b4d35f4ae1fa76e9712859d6aa","title":"Vision and Structured-Language Pretraining for Cross-Modal Food Retrieval","venue":"","year":2022,"referenceCount":85,"citationCount":0,"influentialCitationCount":0,"publicationDate":"08/12/2022","authors":"Mustafa Shukor,Nicolas Thome,M. Cord","id":"508d9b43832790b4d35f4ae1fa76e9712859d6aa","summary":"VLPCook outperforms current SoTA by a significant margin on the task of Cross-Modal Food Retrieval on the large Recipe1M dataset and validates the generalization of the approach to other tasks and domains with structured text such as the Medical domain on the ROCO dataset.","score":2},{"url":"https://www.semanticscholar.org/paper/1329484d20c470b84e46ed7453786cee0acad2e0","title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":43,"citationCount":141,"influentialCitationCount":43,"publicationDate":"30/01/2023","authors":"Junnan Li,Dongxu Li,S. Savarese,Steven Hoi","id":"1329484d20c470b84e46ed7453786cee0acad2e0","summary":"BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods, and is demonstrated's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.","score":2},{"url":"https://www.semanticscholar.org/paper/fbfef4723d8c8467d7bd523e1d0b703cce0e0f9c","title":"Language Is Not All You Need: Aligning Perception with Language Models","venue":"arXiv.org","year":2023,"referenceCount":67,"citationCount":45,"influentialCitationCount":6,"publicationDate":"27/02/2023","authors":"Shaohan Huang,Li Dong,Wenhui Wang,Y. Hao,Saksham Singhal,Shuming Ma,Tengchao Lv,Lei Cui,O. Mohammed,Qiang Liu,Kriti Aggarwal,Zewen Chi,Johan Bjorck,Vishrav Chaudhary,Subhojit Som,Xia Song,Furu Wei","id":"fbfef4723d8c8467d7bd523e1d0b703cce0e0f9c","summary":"This work introduces Kosmos-1, a Multimodal Large Language Model (MLLM) that can perceive general modalities, learn in context, and follow instructions, and shows that MLLMs can benefit from cross-modal transfer, i.e., transfer knowledge from language to multimodals, and from multimodal to language.","score":2},{"url":"https://www.semanticscholar.org/paper/69cfdc8df16ae63b7acba4ac6f727f78b86893c3","title":"ChatGPT Asks, BLIP-2 Answers: Automatic Questioning Towards Enriched Visual Descriptions","venue":"arXiv.org","year":2023,"referenceCount":51,"citationCount":8,"influentialCitationCount":2,"publicationDate":"12/03/2023","authors":"Deyao Zhu,Jun Chen,Kilichbek Haydarov,Xiaoqian Shen,Wenxuan Zhang,Mohamed Elhoseiny","id":"69cfdc8df16ae63b7acba4ac6f727f78b86893c3","summary":"This paper introduces ChatCaptioner, a novel automatic-questioning method deployed in image captioning that identifies 53% more objects within the image than BLIP-2 alone measured by WordNet synset matching.","score":2},{"url":"https://www.semanticscholar.org/paper/9d12916dd46df7a6446cbec0bc4d054f7dafcdab","title":"Scaling Vision-Language Models with Sparse Mixture of Experts","venue":"arXiv.org","year":2023,"referenceCount":74,"citationCount":2,"influentialCitationCount":1,"publicationDate":"13/03/2023","authors":"Sheng Shen,Z. Yao,Chunyuan Li,Trevor Darrell,K. Keutzer,Yuxiong He","id":"9d12916dd46df7a6446cbec0bc4d054f7dafcdab","summary":"The effectiveness of MoE in scaling vision-language models is explored, demonstrating its potential to achieve state-of-the-art performance on a range of benchmarks over dense models of equivalent computational cost.","score":2},{"url":"https://www.semanticscholar.org/paper/4396e30f28eb49bb07c63cf62ca90415ebbe43d4","title":"IRGen: Generative Modeling for Image Retrieval","venue":"arXiv.org","year":2023,"referenceCount":110,"citationCount":1,"influentialCitationCount":0,"publicationDate":"17/03/2023","authors":"Yidan Zhang,Ting Zhang,Dong Chen,Yujing Wang,Qi Chen,Xingxu Xie,Hao Sun,Weiwei Deng,Qi Zhang,Fan Yang,Mao Yang,Q. Liao,B. Guo","id":"4396e30f28eb49bb07c63cf62ca90415ebbe43d4","summary":"The framework, IRGen, is a unified model that enables end-to-end differentiable search, thus achieving superior performance thanks to direct optimization and tackling the key technical challenge of converting an image into quite a short sequence of semantic units in order to enable efficient and effective retrieval.","score":2},{"url":"https://www.semanticscholar.org/paper/3049c992adbd56e29c4d957ee0c4e9d05fe3c6d1","title":"EVA-02: A Visual Representation for Neon Genesis","venue":"arXiv.org","year":2023,"referenceCount":146,"citationCount":4,"influentialCitationCount":1,"publicationDate":"20/03/2023","authors":"Yuxin Fang,Quan Sun,Xinggang Wang,Tiejun Huang,Xinlong Wang,Yue Cao","id":"3049c992adbd56e29c4d957ee0c4e9d05fe3c6d1","summary":"EVA-02, a next-generation Transformer-based visual representation pre-trained to reconstruct strong and robust language-aligned vision features via masked image modeling, is launched, demonstrating superior performance compared to prior state-of-the-art approaches across various representative vision tasks, while utilizing significantly fewer parameters and compute budgets.","score":2},{"url":"https://www.semanticscholar.org/paper/d064075c47e358f604034d06df4b985356757c71","title":"Equivariant Similarity for Vision-Language Foundation Models","venue":"arXiv.org","year":2023,"referenceCount":73,"citationCount":0,"influentialCitationCount":0,"publicationDate":"25/03/2023","authors":"Tan Wang,Kevin Lin,Linjie Li,Chung-Ching Lin,Zhengyuan Yang,Hanwang Zhang,Zicheng Liu,Lijuan Wang","id":"d064075c47e358f604034d06df4b985356757c71","summary":"This study proposes EqSim, a regularization loss that can be efficiently calculated from any two matched training pairs and easily pluggable into existing image-text retrieval fine-tuning and presents a new challenging benchmark EqBen, the first to focus on \"visual-minimal change\".","score":2},{"url":"https://www.semanticscholar.org/paper/b259d853b71a2d03cefa844bb9343b8e3ed816b1","title":"LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention","venue":"arXiv.org","year":2023,"referenceCount":63,"citationCount":24,"influentialCitationCount":2,"publicationDate":"28/03/2023","authors":"Renrui Zhang,Jiaming Han,Aojun Zhou,Xiangfei Hu,Shilin Yan,Pan Lu,Hongsheng Li,Peng Gao,Y. Qiao","id":"b259d853b71a2d03cefa844bb9343b8e3ed816b1","summary":"A zero-init attention mechanism with zero gating is proposed, which adaptively injects the new instructional cues into LLaMA, while effectively preserves its pre-trained knowledge.","score":2},{"url":"https://www.semanticscholar.org/paper/c84e2801512069acbc63f1a7f73273281939428c","title":"A Study of Autoregressive Decoders for Multi-Tasking in Computer Vision","venue":"arXiv.org","year":2023,"referenceCount":98,"citationCount":1,"influentialCitationCount":0,"publicationDate":"30/03/2023","authors":"L. Beyer,Bo Wan,Gagan Madan,Filip Pavetic,A. Steiner,Alexander Kolesnikov,André Susano Pinto,Emanuele Bugliarello,Xiao Wang,Qihang Yu,Liang-Chieh Chen,Xiaohua Zhai","id":"c84e2801512069acbc63f1a7f73273281939428c","summary":"This work takes a close look at autoregressive decoders for multi-task learning in multimodal computer vision, including classification, captioning, visual question answering, and optical character recognition and compares these to well-tuned single-task baselines to highlight the cost incurred by multi-tasking.","score":2},{"url":"https://www.semanticscholar.org/paper/db1c83ef73d2f7731b0dd255835f2f26db749e17","title":"Not All Features Matter: Enhancing Few-shot CLIP with Adaptive Prior Refinement","venue":"arXiv.org","year":2023,"referenceCount":100,"citationCount":2,"influentialCitationCount":0,"publicationDate":"03/04/2023","authors":"Xiang-yu Zhu,Renrui Zhang,Bowei He,A-Long Zhou,D. Wang,Bingyan Zhao,Peng Gao","id":"db1c83ef73d2f7731b0dd255835f2f26db749e17","summary":"This paper proposes APE, an Adaptive Prior rEfinement method for CLIP's pre-trained knowledge, which achieves superior accuracy with high computational efficiency and introduces two model variants, a training-free APE and aTraining-required APE-T.","score":2},{"url":"https://www.semanticscholar.org/paper/a43a3fadc9190e61b34f59a913f1716e443519e4","title":"On the Opportunities and Challenges of Foundation Models for Geospatial Artificial Intelligence","venue":"arXiv.org","year":2023,"referenceCount":158,"citationCount":7,"influentialCitationCount":0,"publicationDate":"13/04/2023","authors":"Gengchen Mai,Weiming Huang,Jin Sun,Suhang Song,Deepak Mishra,Ninghao Liu,Song Gao,Tianming Liu,G. Cong,Yingjie Hu,Chris Cundy,Ziyuan Li,Rui Zhu,Ni Lao","id":"a43a3fadc9190e61b34f59a913f1716e443519e4","summary":"It is proposed that one of the major challenges of developing a FM for GeoAI is to address the multimodality nature of geospatial tasks and the possibility of a multimodal foundation model which can reason over various types ofGeoAI data through geosp spatial alignments is suggested.","score":2},{"url":"https://www.semanticscholar.org/paper/b7d73f22d861f526541575a3b17449bd3c58ca74","title":"MVP-SEG: Multi-View Prompt Learning for Open-Vocabulary Semantic Segmentation","venue":"arXiv.org","year":2023,"referenceCount":56,"citationCount":0,"influentialCitationCount":0,"publicationDate":"14/04/2023","authors":"Jie Guo,Qimeng Wang,Yan Gao,Xiaolong Jiang,Xu Tang,Yao Hu,Baochang Zhang","id":"b7d73f22d861f526541575a3b17449bd3c58ca74","summary":"Experiments show that the multi-view prompts learned from seen categories have strong generalization to unseen categories, and MVP-SEG+ which combines the knowledge transfer stage significantly outperforms previous methods on several benchmarks, justifying that MVP- SEG does lead to better focus on different local parts.","score":2},{"url":"https://www.semanticscholar.org/paper/c3068e2a9f4cd374c7ff3be1b8f877b3d653e880","title":"Multimodal Neural Databases","venue":"arXiv.org","year":2023,"referenceCount":38,"citationCount":0,"influentialCitationCount":0,"publicationDate":"02/05/2023","authors":"Giovanni Trappolini,Andrea Santilli,E. Rodolà,A. Halevy,F. Silvestri","id":"c3068e2a9f4cd374c7ff3be1b8f877b3d653e880","summary":"This paper presents the first architecture able to answer complex database-like queries that involve reasoning over different input modalities, such as text and images, at scale, and presents the potential of these new techniques to process unstructured data coming from different modalities.","score":2},{"url":"https://www.semanticscholar.org/paper/0340c850e033abbf71c7214e403c8fe2be5ef91f","title":"Visual Tuning","venue":"arXiv.org","year":2023,"referenceCount":289,"citationCount":0,"influentialCitationCount":0,"publicationDate":"10/05/2023","authors":"Bruce X. B. Yu,Jianlong Chang,Haixin Wang,Lin Liu,Shijie Wang,Zhiyu Wang,Junfan Lin,Lingxi Xie,Haojie Li,Zhouchen Lin,Qi Tian,Chang Wen Chen","id":"0340c850e033abbf71c7214e403c8fe2be5ef91f","summary":"This survey characterizes a large and thoughtful selection of recent works, providing a systematic and comprehensive overview of existing work and models and categorizes recent visual tuning techniques into five groups: prompt tuning, adapter tuning, parameter tuning, and remapping tuning.","score":2},{"url":"https://www.semanticscholar.org/paper/b07fa63a6d2f39900f0f2cae8f58cd5507010aad","title":"Multi-Prompt with Depth Partitioned Cross-Modal Learning","venue":"arXiv.org","year":2023,"referenceCount":43,"citationCount":0,"influentialCitationCount":0,"publicationDate":"10/05/2023","authors":"Yiqi Wang,Xianda Guo,Zheng Hua Zhu,Yingjie Tian","id":"b07fa63a6d2f39900f0f2cae8f58cd5507010aad","summary":"This study introduces the Partitioned Multi-modal Prompt (PMPO), a multi- modal prompting technique that extends the soft prompt from a single learnable prompt to multiple prompts, and incorporates prior information from manually designed templates and learnable multi-prompts, thus improving the generalization capabilities of the approach.","score":2},{"url":"https://www.semanticscholar.org/paper/8badb0587fef2ffc078b0cec549eb8ec96ed3ad4","title":"Self-Chained Image-Language Model for Video Localization and Question Answering","venue":"arXiv.org","year":2023,"referenceCount":80,"citationCount":0,"influentialCitationCount":0,"publicationDate":"11/05/2023","authors":"Shoubin Yu,Jaemin Cho,Prateek Yadav,Mohit Bansal","id":"8badb0587fef2ffc078b0cec549eb8ec96ed3ad4","summary":"Self-Chained Video Localization-Answering (SeViLA), a novel framework that leverages a single image-language model (BLIP-2) to tackle both temporal keyframe localization and QA on videos, outperforms several strong baselines/previous works on five video QA and event prediction tasks, and achieves the state-of-the-art in both fine-tuning and zero-shot settings.","score":2},{"url":"https://www.semanticscholar.org/paper/38be7643bcad936739550a1802220eb53ca9b1df","title":"Simple Token-Level Confidence Improves Caption Correctness","venue":"arXiv.org","year":2023,"referenceCount":73,"citationCount":0,"influentialCitationCount":0,"publicationDate":"11/05/2023","authors":"Suzanne Petryk,Spencer Whitehead,Joseph Gonzalez,Trevor Darrell,Anna Rohrbach,Marcus Rohrbach","id":"38be7643bcad936739550a1802220eb53ca9b1df","summary":"This work explores Token-Level Confidence, or TLC, as a simple yet surprisingly effective method to assess caption correctness, and fine-tune a vision-language model on image captioning, input an image and proposed caption to the model, and aggregate either algebraic or learned token confidences over words or sequences to estimate image-caption consistency.","score":2},{"url":"https://www.semanticscholar.org/paper/8ae2e81495d426419e6fd96940b651002c046b61","title":"Enhancing Vision-Language Pre-Training with Jointly Learned Questioner and Dense Captioner","venue":"arXiv.org","year":2023,"referenceCount":57,"citationCount":0,"influentialCitationCount":0,"publicationDate":"19/05/2023","authors":"Zikang Liu,Sihan Chen,Longteng Guo,Handong Li,Xingjian He,Jing Liu","id":"8ae2e81495d426419e6fd96940b651002c046b61","summary":"A novel method called Joint QA and DC GEneration (JADE), which utilizes a pre-trained multimodal model and easily-crawled image-text pairs to automatically generate and filter large-scale VQA and dense captioning datasets.","score":2},{"url":"https://www.semanticscholar.org/paper/6118eb18023429fa8bad64b7a1d95533127a62d7","title":"Prompt ChatGPT In MNER: Improved multimodal named entity recognition method based on auxiliary refining knowledge from ChatGPT","venue":"arXiv.org","year":2023,"referenceCount":48,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/05/2023","authors":"Jinyuan Li,Han Li,Zhuo Pan,Gang Pan","id":"6118eb18023429fa8bad64b7a1d95533127a62d7","summary":"This paper uses ChatGPT as an implicit knowledge engine to acquire auxiliary refined knowledge, thereby bolstering the model's performance in MNER tasks and significantly outperforms all existing state-of-the-art methods on two classic MNER datasets.","score":2},{"url":"https://www.semanticscholar.org/paper/d183cc170400e43535c5e2c37121c37ee0ba23dc","title":"Open-Ended Medical Visual Question Answering Through Prefix Tuning of Language Models","venue":"arXiv.org","year":2023,"referenceCount":33,"citationCount":2,"influentialCitationCount":1,"publicationDate":"10/03/2023","authors":"Tom van Sonsbeek,Mohammad Mahdi Derakhshani,Ivona Najdenkoska,Cees G. M. Snoek,M. Worring","id":"d183cc170400e43535c5e2c37121c37ee0ba23dc","summary":"This work focuses on open-ended VQA and motivated by the recent advances in language models consider it as a generative task, and introduces a novel method particularly suited for small, domain-specific, medical datasets.","score":2},{"url":"https://www.semanticscholar.org/paper/456a70485aafc12dfed4fb7354668d72aae9b658","title":"SecureBERT: A Domain-Specific Language Model for Cybersecurity","venue":"","year":2022,"referenceCount":33,"citationCount":1,"influentialCitationCount":0,"publicationDate":"06/04/2022","authors":"Ehsan Aghaei,Xi Niu,W. Shadid,E. Al-Shaer","id":"456a70485aafc12dfed4fb7354668d72aae9b658","summary":"This paper proposes SecureBERT, a cybersecurity language model capable of capturing text connotations in cybersecurity text and therefore successful in automation for many critical cybersecurity tasks that would otherwise rely on human expertise and time-consuming manual procedures.","score":2},{"url":"https://www.semanticscholar.org/paper/5d8fd04c436367b18b35e28332ee8e452a477f3f","title":"Medical Image Understanding with Pretrained Vision Language Models: A Comprehensive Study","venue":"arXiv.org","year":2022,"referenceCount":53,"citationCount":5,"influentialCitationCount":0,"publicationDate":"30/09/2022","authors":"Ziyuan Qin,Huahui Yi,Qicheng Lao,Kang Li","id":"5d8fd04c436367b18b35e28332ee8e452a477f3f","summary":"It is shown that well-designed medical prompts are the key to elicit knowledge from pre-trained VLMs, and by prompting with expressive attributes that are shared between domains, the VLM can carry the knowledge across domains and improve its generalization.","score":2},{"url":"https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a","title":"Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond","venue":"arXiv.org","year":2023,"referenceCount":127,"citationCount":7,"influentialCitationCount":1,"publicationDate":"26/04/2023","authors":"Jingfeng Yang,Hongye Jin,Ruixiang Tang,Xiaotian Han,Qizhang Feng,Haoming Jiang,Bing Yin,Xia Hu","id":"131c6f328c11706de2c43cd16e0b7c5d5e610b6a","summary":"A comprehensive and practical guide for practitioners and end-users working with Large Language Models in their downstream natural language processing (NLP) tasks, enabling the successful implementation of these models in a wide range of NLP tasks.","score":2},{"url":"https://www.semanticscholar.org/paper/f646d3056ca02daa99820917b3ba48a43a0022e2","title":"SUR-adapter: Enhancing Text-to-Image Pre-trained Diffusion Models with Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":54,"citationCount":0,"influentialCitationCount":0,"publicationDate":"09/05/2023","authors":"Shan Zhong,Zhongzhan Huang,Wushao Wen,Jinghui Qin,Liang Lin","id":"f646d3056ca02daa99820917b3ba48a43a0022e2","summary":"This work proposes a simple-yet-effective parameter-efficient fine-tuning approach called the Semantic Understanding and Reasoning adapter (SUR-adapter) for pre-trained diffusion models that can make text-to-image diffusion models easier to use with better user experience.","score":2},{"url":"https://www.semanticscholar.org/paper/052a5e2bcc999810ee6f1eedcf758c528e4f125f","title":"Retrieving Multimodal Information for Augmented Generation: A Survey","venue":"arXiv.org","year":2023,"referenceCount":150,"citationCount":5,"influentialCitationCount":0,"publicationDate":"20/03/2023","authors":"Ruochen Zhao,Hailin Chen,Weishi Wang,Fangkai Jiao,Xuan Long Do,Chengwei Qin,Bosheng Ding,Xiaobao Guo,Minzhi Li,Xingxuan Li,Shafiq R. Joty","id":"052a5e2bcc999810ee6f1eedcf758c528e4f125f","summary":"This survey provides an in-depth review of retrieval-augmented generation in different modalities and discusses potential future directions of this emerging field.","score":2},{"url":"https://www.semanticscholar.org/paper/357fc385caf2e5b9898c9140fa3ac9955e6bb3c6","title":"TeamS at VQA-Med 2021: BBN-Orchestra for Long-tailed Medical Visual Question Answering","venue":"Conference and Labs of the Evaluation Forum","year":2021,"referenceCount":13,"citationCount":6,"influentialCitationCount":0,"publicationDate":2021,"authors":"Sedigheh Eslami,Gerard de Melo,C. Meinel","id":"357fc385caf2e5b9898c9140fa3ac9955e6bb3c6","summary":"The proposed BBN-Orchestra is an ensemble of bilateral-branch networks (BBN) and successfully reduces overfitting to train and validation data in addition to effectively modeling the imbalanced long-tailed image distribution.","score":2},{"url":"https://www.semanticscholar.org/paper/3c83f80f06633ff4598d33c2959f8e4cdcad3e93","title":"Cross-Modal Self-Attention with Multi-Task Pre-Training for Medical Visual Question Answering","venue":"International Conference on Multimedia Retrieval","year":2021,"referenceCount":31,"citationCount":27,"influentialCitationCount":1,"publicationDate":"01/05/2021","authors":"Haifan Gong,Guanqi Chen,Sishuo Liu,Yizhou Yu,Guanbin Li","id":"3c83f80f06633ff4598d33c2959f8e4cdcad3e93","summary":"This work reformulates image feature pre-training as a multi-task learning paradigm and witness its extraordinary superiority, forcing it to take into account the applicability of features for the specific image comprehension task.","score":2},{"url":"https://www.semanticscholar.org/paper/97af09b1436e768019aed4023cd1f9e3ccb9a635","title":"Medical Visual Question Answering: A Survey","venue":"arXiv.org","year":2021,"referenceCount":96,"citationCount":13,"influentialCitationCount":1,"publicationDate":"19/11/2021","authors":"Zhihong Lin,Donghao Zhang,Qingyi Tao,Danli Shi,Gholamreza Haffari,Qi Wu,M. He,Z. Ge","id":"97af09b1436e768019aed4023cd1f9e3ccb9a635","summary":"This research presents a novel and scalable approaches to integrate 3D image recognition and 3D speech recognition into the clinical practice of ophthalmology.","score":2},{"url":"https://www.semanticscholar.org/paper/8c9a9a1bbba2a3e3bab34bce533b3b2acfda32b0","title":"Medical visual question answering based on question-type reasoning and semantic space constraint","venue":"Artif. Intell. Medicine","year":2022,"referenceCount":57,"citationCount":2,"influentialCitationCount":0,"publicationDate":"01/06/2022","authors":"Meiling Wang,Xiaohai He,Luping Liu,L. Qing,Honggang Chen,Yan Liu,Chao Ren","id":"8c9a9a1bbba2a3e3bab34bce533b3b2acfda32b0","summary":"A novel Med-VQA framework is proposed to alleviate the above-mentioned problems, which employed a question-type reasoning module severally to closed-ended and open-ended questions, thereby extracting the important information contained in the questions through an attention mechanism and filtering the noise to extract more valuable question features.","score":2},{"url":"https://www.semanticscholar.org/paper/2ac3bacbbee520b701707ebcf7b9ca7a3f233129","title":"Medical visual question answering via corresponding feature fusion combined with semantic attention.","venue":"Mathematical biosciences and engineering : MBE","year":2022,"referenceCount":15,"citationCount":2,"influentialCitationCount":0,"publicationDate":"20/07/2022","authors":"Han Zhu,Xiaohai He,Meiling Wang,Mozhi Zhang,L. Qing","id":"2ac3bacbbee520b701707ebcf7b9ca7a3f233129","summary":"A corresponding feature fusion (CFF) method to strengthen the interactions of specific features from corresponding radiology images and questions and a semantic attention (SA) module for textual feature extraction is proposed.","score":2},{"url":"https://www.semanticscholar.org/paper/ef2edea434e487f288d4eed6f9b1dc480b917211","title":"Adversarial Learning to Improve Question Image Embedding in Medical Visual Question Answering","venue":"Moratuwa Engineering Research Conference","year":2022,"referenceCount":20,"citationCount":0,"influentialCitationCount":0,"publicationDate":"27/07/2022","authors":"Kaveesha Silva,Thanuja Maheepala,Kasun Tharaka,Thanuja D. Ambegoda","id":"ef2edea434e487f288d4eed6f9b1dc480b917211","summary":"A new method for training VQA models that utilizes adversarial learning to improve the question-image embedding and demonstrates how this embedding can be used as the ideal embedding for answer inference.","score":2},{"url":"https://www.semanticscholar.org/paper/b047b3b7d76b79958e23b0fcab985be22b1ce42d","title":"Alternating Cross-attention Vision-Language Model for Efficient Learning with Medical Image and Report without Curation","venue":"arXiv.org","year":2022,"referenceCount":47,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Sangjoon Park,Eunha Lee,Jeonghyeon Lee,Jong-Chul Ye","id":"b047b3b7d76b79958e23b0fcab985be22b1ce42d","summary":"It is experimentally demonstrated that the pre-trained MAX-VL model outperforms the current state-of-the-art vision language models in various vision-language tasks and suggested the clinical utility for the diagnosis of newly emerging diseases and human error detection as well as showed the widespread applicability of the model in different domain data.","score":2},{"url":"https://www.semanticscholar.org/paper/79478a2ac67b9fdbeadcde13faa2d84eb239e080","title":"Vision-Language Pretraining Enables Radiographs and Reports to be Learned without Curation","venue":"","year":2022,"referenceCount":50,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Sangjoon Park,Eunha Lee,Jeonghyeon Lee,Jong-Chul Ye","id":"79478a2ac67b9fdbeadcde13faa2d84eb239e080","summary":"It is experimentally demonstrated that the pre-trained medical X-VL model outperforms the current state-of-the-art models in various vision-language tasks in medical domains, which suggests the potential of the model for widespread applicability in different medical applications.","score":2},{"url":"https://www.semanticscholar.org/paper/2441230bd2f3cca924d597b3044ad63aaff269ec","title":"Self-supervised Co-learning of Uncurated Images and Reports Enables Oversight AI in Radiology","venue":"","year":2022,"referenceCount":49,"citationCount":4,"influentialCitationCount":0,"publicationDate":2022,"authors":"Sangjoon Park,Eunha Lee,K. Shin,Jeonghyeon Lee,Jong-Chul Ye","id":"2441230bd2f3cca924d597b3044ad63aaff269ec","summary":"A self-supervised model tailored for efficient vision-language pre-training that exploits cross attention in the radiological images and - trained medical X-VL model outperforms the current state-of-the-art models in various vision- language tasks in medical domains.","score":2},{"url":"https://www.semanticscholar.org/paper/0cbd644254462341a897d4bfa0134637662c3ab5","title":"A Transformer-based Medical Visual Question Answering Model","venue":"International Conference on Pattern Recognition","year":2022,"referenceCount":25,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/08/2022","authors":"Lei Liu,Xiangdong Su,Hui Guo,Daobin Zhu","id":"0cbd644254462341a897d4bfa0134637662c3ab5","summary":"Experimental results demonstrate that the Transformer structure not only ensures the stability of the model performance, but also accelerates its convergence, and the MQAT model outperforms the existing state-of-the-art methods.","score":2},{"url":"https://www.semanticscholar.org/paper/56d8d9fff399f798da97a69e891de4eeb4568d4f","title":"MHKD-MVQA: Multimodal Hierarchical Knowledge Distillation for Medical Visual Question Answering","venue":"IEEE International Conference on Bioinformatics and Biomedicine","year":2022,"referenceCount":49,"citationCount":1,"influentialCitationCount":0,"publicationDate":"06/12/2022","authors":"Jianfeng Wang,Shuokang Huang,Huifang Du,Yu Qin,Haofen Wang,Wenqiang Zhang","id":"56d8d9fff399f798da97a69e891de4eeb4568d4f","summary":"This work proposes multimodal hierarchical knowledge distillation for medical VQA (MHKD-MVQA), which distill knowledge from not only the output but also the intermediate layers, which leverages the knowledge from limited samples to a greater extent and achieves state-of-the-art performance.","score":2},{"url":"https://www.semanticscholar.org/paper/5942335fdd35d1651aaabd7af4db129a29ed2a85","title":"How Well Apply Multimodal Mixup and Simple MLPs Backbone to Medical Visual Question Answering?","venue":"IEEE International Conference on Bioinformatics and Biomedicine","year":2022,"referenceCount":26,"citationCount":0,"influentialCitationCount":0,"publicationDate":"06/12/2022","authors":"Lei Liu,Xiangdong Su","id":"5942335fdd35d1651aaabd7af4db129a29ed2a85","summary":"This paper designs a Med-VQA model which employs multi-layer perceptrons (MLPs) as the backbone network for feature extraction and modal fusion and designs a multimodal mixup (M-Mixup) to augment images and questions separately, which effectively alleviates the problem of insufficient training samples in the Med- VQA task.","score":2},{"url":"https://www.semanticscholar.org/paper/a627232a97a7a63f8399d157f0b022eb1ccd547c","title":"Biomedical Question Answering: A Survey of Approaches and Challenges","venue":"ACM Computing Surveys","year":2021,"referenceCount":246,"citationCount":34,"influentialCitationCount":1,"publicationDate":"10/02/2021","authors":"Qiao Jin,Zheng Yuan,Guangzhi Xiong,Qian Yu,Huaiyuan Ying,Chuanqi Tan,Mosha Chen,Songfang Huang,Xiaozhong Liu,Sheng Yu","id":"a627232a97a7a63f8399d157f0b022eb1ccd547c","summary":"This survey identifies and characterize several key challenges in BQA that might lead to this issue, and discusses some potential future directions to explore.","score":2},{"url":"https://www.semanticscholar.org/paper/2580d3fc39fed3989f10665559a955b847b7eb7f","title":"Medical Visual Question Answering via Conditional Reasoning and Contrastive Learning","venue":"IEEE Transactions on Medical Imaging","year":2022,"referenceCount":72,"citationCount":0,"influentialCitationCount":0,"publicationDate":"26/12/2022","authors":"Bo Liu,Li-Ming Zhan,Li Xu,Xiao-Ming Wu","id":"2580d3fc39fed3989f10665559a955b847b7eb7f","summary":"A novel conditional reasoning mechanism with a question- Conditioned reasoning component and a type-conditioned reasoning strategy to learn effective reasoning skills for different Med-VQA tasks adaptively and to pre-train a visual feature extractor via contrastive learning on large amounts of unlabeled radiology images.","score":2},{"url":"https://www.semanticscholar.org/paper/940f303c2530a52c5fd3c52c9c64ceea4b53ab05","title":"Diversity Learning Based on Multi-Latent Space for Medical Image Visual Question Generation","venue":"Italian National Conference on Sensors","year":2023,"referenceCount":59,"citationCount":0,"influentialCitationCount":0,"publicationDate":"17/01/2023","authors":"He Zhu,Ren Togo,Takahiro Ogawa,M. Haseyama","id":"940f303c2530a52c5fd3c52c9c64ceea4b53ab05","summary":"A diversity learning-based visual question generation model using a multi-latent space to generate informative question sets from medical images that works with an answering model for interactive automated clinical diagnosis and generates datasets to replace the process of annotation that incurs huge labor costs.","score":2},{"url":"https://www.semanticscholar.org/paper/8200be2e8b9af243ee72a9d919a4f7fbe82a17d2","title":"Medical knowledge-based network for Patient-oriented Visual Question Answering","venue":"Information Processing & Management","year":2023,"referenceCount":37,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/03/2023","authors":"Jian Huang,Yihao Chen,Yong Li,Zhenguo Yang,Xuehao Gong,Fuhui Wang,Xiaohong Xu,Wenyin Liu","id":"8200be2e8b9af243ee72a9d919a4f7fbe82a17d2","summary":"","score":2},{"url":"https://www.semanticscholar.org/paper/17ca48ad1b944c897863f04ba9ffa72674dce1ce","title":"Parallel multi-head attention and term-weighted question embedding for medical visual question answering","venue":"Multimedia tools and applications","year":2023,"referenceCount":56,"citationCount":0,"influentialCitationCount":0,"publicationDate":"11/03/2023","authors":"Sruthy Manmadhan,Binsu C. Kovoor","id":"17ca48ad1b944c897863f04ba9ffa72674dce1ce","summary":"The proposed MaMVQA model achieved significantly increased accuracy in predicting answers to both close-ended and open-ended questions and outperforms previous state-of-the-art methods in terms of accuracy while requiring no external data to train the model.","score":2},{"url":"https://www.semanticscholar.org/paper/4cf4528e3b19a22bcfb041d09be53bd3095bcef8","title":"Q2ATransformer: Improving Medical VQA via an Answer Querying Decoder","venue":"arXiv.org","year":2023,"referenceCount":15,"citationCount":0,"influentialCitationCount":0,"publicationDate":"04/04/2023","authors":"Yunyi Liu,Zhanyu Wang,Dong Xu,Luping Zhou","id":"4cf4528e3b19a22bcfb041d09be53bd3095bcef8","summary":"This paper proposes a new Transformer based framework for medical VQA (named as Q2ATransformer), which integrates the advantages of both the classification and the generation approaches and provides a unified treatment for the close-end and open-end questions.","score":2},{"url":"https://www.semanticscholar.org/paper/c5bcc78ae708b29edb03481e12213eca53c28963","title":"A multi-modal model based on transformers for medical visual question answering","venue":"International Conference on Artificial Intelligence and Computer Engineering (ICAICE 2022)","year":2023,"referenceCount":12,"citationCount":0,"influentialCitationCount":0,"publicationDate":"28/04/2023","authors":"Mingchun Huang,Ming Xu,Fuhuang Liu,Liyan Chen","id":"c5bcc78ae708b29edb03481e12213eca53c28963","summary":"The IIF module that can improve the model's ability to obtain visual feature is proposed and QAM is designed to help the model analyze the question better.","score":2},{"url":"https://www.semanticscholar.org/paper/385376b8aa48c25403f17d6206db7c09b67e1314","title":"Prompt Engineering for Healthcare: Methodologies and Applications","venue":"arXiv.org","year":2023,"referenceCount":105,"citationCount":0,"influentialCitationCount":0,"publicationDate":"28/04/2023","authors":"Jiaqi Wang,Enze Shi,Sigang Yu,Zihao Wu,Chong Ma,Haixing Dai,Qiushi Yang,Yanqing Kang,Jinru Wu,Huawen Hu,Chenxi Yue,Haiyang Zhang,Yi-Hsueh Liu,Xiang Li,Bao Ge,Dajiang Zhu,Yixuan Yuan,Dinggang Shen,Tianming Liu,Shu Zhang","id":"385376b8aa48c25403f17d6206db7c09b67e1314","summary":"This review will introduce the latest advances in prompt engineering in the field of natural language processing (NLP) for the medical domain and highlight its significant contributions to healthcare NLP applications such as question-answering systems, text summarization, and machine translation.","score":1},{"url":"https://www.semanticscholar.org/paper/e62937949aa18caeebaf1263ef5d86d447acbc6e","title":"Pre-trained Language Models in Biomedical Domain: A Systematic Survey","venue":"arXiv.org","year":2021,"referenceCount":379,"citationCount":24,"influentialCitationCount":3,"publicationDate":"11/10/2021","authors":"Benyou Wang,Qianqian Xie,Jiahuan Pei,P. Tiwari,Zhao Li,Jie Fu","id":"e62937949aa18caeebaf1263ef5d86d447acbc6e","summary":"The recent progress of pre-trained language models in the biomedical domain and their applications in biomedical downstream tasks are summarized and a taxonomy of existing biomedical PLMs is proposed.","score":1},{"url":"https://www.semanticscholar.org/paper/e3c70b0b71b51872bbdaa0f4bf2b56908f97abec","title":"MedKLIP: Medical Knowledge Enhanced Language-Image Pre-Training","venue":"medRxiv","year":2023,"referenceCount":67,"citationCount":6,"influentialCitationCount":1,"publicationDate":"05/01/2023","authors":"Chaoyi Wu,Xiaoman Zhang,Ya Zhang,Yanfeng Wang,Weidi Xie","id":"e3c70b0b71b51872bbdaa0f4bf2b56908f97abec","summary":"This paper adopts a novel report filter to extract the medical entities, and proposes a novel entity embedding module by querying an external knowledge description base to exploit the rich context of additional information that the medical domain affords, and implicitly build relationships between entities in the language embedding space.","score":1},{"url":"https://www.semanticscholar.org/paper/10a8e7a7e07256178665f90074c5c41b071e73d3","title":"MDF-Net: Multimodal Dual-Fusion Network for Abnormality Detection using CXR Images and Clinical Data","venue":"arXiv.org","year":2023,"referenceCount":49,"citationCount":0,"influentialCitationCount":0,"publicationDate":"26/02/2023","authors":"Chih-Jou Hsieh,Isabel Blanco Nobre,Sandra Costa Sousa,Chun Ouyang,Margot Brereton,J. Nascimento,Joaquim Jorge,Catarina Moreira","id":"10a8e7a7e07256178665f90074c5c41b071e73d3","summary":"Results show that incorporating patients' clinical data in a DL model together with the proposed fusion methods improves the performance of disease localization in chest X-rays by 12\\% in terms of Average Precision compared to a standard Mask R-CNN using only chestX-rays.","score":1},{"url":"https://www.semanticscholar.org/paper/a39adffc4e6de100a950f4476e113bfc402119f2","title":"Knowledge-enhanced Visual-Language Pre-training on Chest Radiology Images","venue":"","year":2023,"referenceCount":73,"citationCount":0,"influentialCitationCount":0,"publicationDate":"27/02/2023","authors":"Xiaoman Zhang,Chaoyi Wu,Ya Zhang,Yanfeng Wang,Weidi Xie","id":"a39adffc4e6de100a950f4476e113bfc402119f2","summary":"KAD is evaluated on four external X-ray datasets and it is demonstrated that its zero-shot performance is not only comparable to that of fully-supervised models, but also superior to the average of three expert radiologists for three pathologies with statistical significance.","score":1},{"url":"https://www.semanticscholar.org/paper/8700c5af25450bf8e84b94783344b054d268738b","title":"Bi-VLGM : Bi-Level Class-Severity-Aware Vision-Language Graph Matching for Text Guided Medical Image Segmentation","venue":"","year":2023,"referenceCount":48,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/05/2023","authors":"Chen Wenting,Liu Jie,Yuan Yixuan","id":"8700c5af25450bf8e84b94783344b054d268738b","summary":"A Bi-level class-severity-aware Vision-Language Graph Matching (Bi-VLGM) for text guided medical image segmentation and its superiority to existing methods is introduced.","score":1},{"url":"https://www.semanticscholar.org/paper/201e3519a2b77e3d4852a909d8edfa19796e219e","title":"Generalist Vision Foundation Models for Medical Imaging: A Case Study of Segment Anything Model on Zero-Shot Medical Segmentation","venue":"arXiv.org","year":2023,"referenceCount":37,"citationCount":1,"influentialCitationCount":0,"publicationDate":"25/04/2023","authors":"Peilun Shi,Jianing Qiu,Sai Mu Dalike Abaxi,Hao Wei,F. P. Lo,Wu Yuan","id":"201e3519a2b77e3d4852a909d8edfa19796e219e","summary":"The study indicates the versatility of generalist vision foundation models on solving specific tasks in medical imaging, and their great potential to achieve desired performance through fine-turning and eventually tackle the challenges of accessing large diverse medical datasets and the complexity of medical domains.","score":1},{"url":"https://www.semanticscholar.org/paper/80785017029cab501fcdb90b98985cd2b36e1fb8","title":"Evaluation of GPT-3.5 and GPT-4 for supporting real-world information needs in healthcare delivery","venue":"arXiv.org","year":2023,"referenceCount":38,"citationCount":0,"influentialCitationCount":0,"publicationDate":"26/04/2023","authors":"Debadutta Dash,Rahul Thapa,J. Banda,Akshay Swaminathan,Morgan Cheatham,M. Kashyap,N. Kotecha,Jonathan H. Chen,S. Gombar,L. Downing,Rachel A. Pedreira,Ethan Goh,A. Arnaout,Garret K. Morris,H. Magon,M. Lungren,E. Horvitz,N. Shah","id":"80785017029cab501fcdb90b98985cd2b36e1fb8","summary":"It is suggested that while general purpose LLMs are able to provide safe and credible responses, they often do not meet the specific information need of a given question.","score":1},{"url":"https://www.semanticscholar.org/paper/d0ee000f30420953f10dfcfd608a7f9ad40f1635","title":"Humans are Still Better than ChatGPT: Case of the IEEEXtreme Competition","venue":"arXiv.org","year":2023,"referenceCount":34,"citationCount":1,"influentialCitationCount":0,"publicationDate":"10/05/2023","authors":"A. Koubâa,B. Qureshi,Adel Ammar,Zahid Khan,W. Boulila,L. Ghouti","id":"d0ee000f30420953f10dfcfd608a7f9ad40f1635","summary":"This paper presents an instance where human performance excels in typical tasks suited for ChatGPT, specifically in the domain of computer programming, and provides evidence that contrary to popular belief, human programmers maintain a competitive edge overChatGPT in certain aspects of problem-solving within the programming context.","score":1},{"url":"https://www.semanticscholar.org/paper/e1a12117f15a6ee07133851a51439394bb9e7406","title":"ChemCrow: Augmenting large-language models with chemistry tools","venue":"","year":2023,"referenceCount":78,"citationCount":4,"influentialCitationCount":0,"publicationDate":"11/04/2023","authors":"A. Bran,Sam Cox,Andrew D. White,P. Schwaller","id":"e1a12117f15a6ee07133851a51439394bb9e7406","summary":"This study introduces ChemCrow, an LLM chemistry agent designed to accomplish tasks across organic synthesis, drug discovery, and materials design by integrating 13 expert-designed tools, which augments the LLM performance in chemistry, and new capabilities emerge.","score":1},{"url":"https://www.semanticscholar.org/paper/66d4631c28582b496d8e77653556893a9a6d8c3a","title":"MultiModal-GPT: A Vision and Language Model for Dialogue with Humans","venue":"arXiv.org","year":2023,"referenceCount":17,"citationCount":5,"influentialCitationCount":0,"publicationDate":"08/05/2023","authors":"T. Gong,Chengqi Lyu,Shilong Zhang,Yudong Wang,Miao Zheng,Qianmengke Zhao,Kuikun Liu,Wenwei Zhang,Ping Luo,Kai Chen","id":"66d4631c28582b496d8e77653556893a9a6d8c3a","summary":"A vision and language model named MultiModal-GPT to conduct multi-round dialogue with humans and finds the quality of training data is vital for the dialogue performance, where few data containing short answers can lead the model to respond shortly to any instructions.","score":1},{"url":"https://www.semanticscholar.org/paper/2d3905c1a92c28c056dff1225d89e4ca72ac4d8e","title":"Man vs the machine: The Struggle for Effective Text Anonymisation in the Age of Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":38,"citationCount":1,"influentialCitationCount":0,"publicationDate":"22/03/2023","authors":"C. Patsakis,Nikolaos Lykousas","id":"2d3905c1a92c28c056dff1225d89e4ca72ac4d8e","summary":"An experiment is conducted using GPT over anonymised texts of famous people to determine whether such trained networks can deanonymise them and introduces a novel methodology that employs Large Language Models to improve the anonymity of texts.","score":1},{"url":"https://www.semanticscholar.org/paper/6139b6bc065b24562cb7f4f08227a42f5766138f","title":"Diffusion Models: A Comprehensive Survey of Methods and Applications","venue":"arXiv.org","year":2022,"referenceCount":364,"citationCount":121,"influentialCitationCount":7,"publicationDate":"02/09/2022","authors":"Ling Yang,Zhilong Zhang,Shenda Hong,Runsheng Xu,Yue Zhao,Yingxia Shao,Wentao Zhang,Ming-Hsuan Yang,Bin Cui","id":"6139b6bc065b24562cb7f4f08227a42f5766138f","summary":"An overview of the rapidly expanding body of work on diffusion models is provided, categorizing the research into three key areas: efficient sampling, improved likelihood estimation, and handling data with special structures.","score":1},{"url":"https://www.semanticscholar.org/paper/09ca5072a76796c65e5936b6fb4968afead61944","title":"Semantics-Empowered Communication: A Tutorial-cum-Survey","venue":"arXiv.org","year":2022,"referenceCount":243,"citationCount":2,"influentialCitationCount":1,"publicationDate":"16/12/2022","authors":"Zhilin Lu,Rongpeng Li,Kun Lu,Xianfu Chen,E. Hossain,Zhifeng Zhao,Honggang Zhang","id":"09ca5072a76796c65e5936b6fb4968afead61944","summary":"This work proposes to categorize the critical enabling techniques by explicit and implicit reasoning-based methods, and elaborate on how they evolve and contribute to modern content&channel semantics-empowered communications.","score":1},{"url":"https://www.semanticscholar.org/paper/419eb47fea3931c4098232f44ccbc216275d3f56","title":"Decoding Visual Neural Representations by Multimodal Learning of Brain-Visual-Linguistic Features","venue":"IEEE Transactions on Pattern Analysis and Machine Intelligence","year":2022,"referenceCount":87,"citationCount":2,"influentialCitationCount":1,"publicationDate":"13/10/2022","authors":"Changde Du,Kaicheng Fu,Jinpeng Li,Huiguang He","id":"419eb47fea3931c4098232f44ccbc216275d3f56","summary":"The BraVL model can be trained under various semi-supervised scenarios to incorporate the visual and textual features obtained from the extra categories and constructed three trimodal matching datasets, leading to some interesting conclusions and cognitive insights.","score":1},{"url":"https://www.semanticscholar.org/paper/9fe9af7cf3d54b707a7be3c53ce94b77dcc3bae5","title":"A Short Survey of Viewing Large Language Models in Legal Aspect","venue":"arXiv.org","year":2023,"referenceCount":25,"citationCount":6,"influentialCitationCount":0,"publicationDate":"16/03/2023","authors":"Zhongxiang Sun","id":"9fe9af7cf3d54b707a7be3c53ce94b77dcc3bae5","summary":"","score":1},{"url":"https://www.semanticscholar.org/paper/74e8ae03a385e72f5ae377667ba9858fb3e0bfa0","title":"Unleashing the Power of Edge-Cloud Generative AI in Mobile Networks: A Survey of AIGC Services","venue":"arXiv.org","year":2023,"referenceCount":232,"citationCount":3,"influentialCitationCount":0,"publicationDate":"28/03/2023","authors":"Minrui Xu,Hongyang Du,D. Niyato,Jiawen Kang,Zehui Xiong,Shiwen Mao,Zhu Han,A. Jamalipour,Dong In Kim,X. Shen,Victor C. M. Leung,H. V. Poor","id":"74e8ae03a385e72f5ae377667ba9858fb3e0bfa0","summary":"This survey paper focuses on the deployment of AIGC applications, e.g., ChatGPT and Dall-E, at mobile edge networks, that provide personalized and customized AigC services in real time while maintaining user privacy.","score":1},{"url":"https://www.semanticscholar.org/paper/1d29334cfbe9a1a943082058876f0c22d44c62fd","title":"A Survey of Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":415,"citationCount":66,"influentialCitationCount":7,"publicationDate":"31/03/2023","authors":"Wayne Xin Zhao,Kun Zhou,Junyi Li,Tianyi Tang,Xiaolei Wang,Yupeng Hou,Yingqian Min,Beichen Zhang,Junjie Zhang,Zican Dong,Yifan Du,Chen Yang,Yushuo Chen,Z. Chen,Jinhao Jiang,Ruiyang Ren,Yifan Li,Xinyu Tang,Zikang Liu,Peiyu Liu,J. Nie,Ji-rong Wen","id":"1d29334cfbe9a1a943082058876f0c22d44c62fd","summary":"A review of the recent advances of large language models by introducing the background, key findings, and mainstream techniques, and focusing on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation.","score":1},{"url":"https://www.semanticscholar.org/paper/26ccfbd8bfad44aeed695c12579ff7126adbfae9","title":"Summary of ChatGPT/GPT-4 Research and Perspective Towards the Future of Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":106,"citationCount":17,"influentialCitationCount":2,"publicationDate":"04/04/2023","authors":"Yi-Hsien Liu,Tianle Han,Siyuan Ma,Jia-Yu Zhang,Yuanyu Yang,Jiaming Tian,Haoyang He,Antong Li,Mengshen He,Zheng Liu,Zihao Wu,Dajiang Zhu,Xiang Li,Ning Qiang,Dingang Shen,Tianming Liu,Bao Ge","id":"26ccfbd8bfad44aeed695c12579ff7126adbfae9","summary":"A comprehensive survey of ChatGPT and GPT-4, state-of-the-art large language models from the GPT series, and their prospective applications across diverse domains, encompassing trend analysis, word cloud representation, and distribution analysis across various application domains is presented.","score":1},{"url":"https://www.semanticscholar.org/paper/9fd980237e7fdfa4c103a2dc08657e73adf847c4","title":"OpenAGI: When LLM Meets Domain Experts","venue":"arXiv.org","year":2023,"referenceCount":42,"citationCount":5,"influentialCitationCount":0,"publicationDate":"10/04/2023","authors":"Yingqiang Ge,Wenyue Hua,Jianchao Ji,Juntao Tan,Shuyuan Xu,Yongfeng Zhang","id":"9fd980237e7fdfa4c103a2dc08657e73adf847c4","summary":"OpenAGI is developed, an open-source AGI research platform, specifically designed to offer complex, multi-step tasks and accompanied by task-specific datasets, evaluation metrics, and a diverse range of extensible models.","score":1},{"url":"https://www.semanticscholar.org/paper/6316cbb4f1e7dba5806a3310ec7f89f3571bc3db","title":"Boosting Cross-task Transferability of Adversarial Patches with Visual Relations","venue":"arXiv.org","year":2023,"referenceCount":22,"citationCount":0,"influentialCitationCount":0,"publicationDate":"11/04/2023","authors":"Tony Ma,Songze Li,Yisong Xiao,Shunchang Liu","id":"6316cbb4f1e7dba5806a3310ec7f89f3571bc3db","summary":"A novel Visual Relation-based cross-task Adversarial Patch generation method called VRAP is proposed, which aims to evaluate the robustness of various visual tasks, especially those involving visual reasoning, such as Visual Question Answering and Image Captioning.","score":1},{"url":"https://www.semanticscholar.org/paper/be2b0396de9431bae931642516a1d3e4906329f5","title":"Low-code LLM: Visual Programming over LLMs","venue":"arXiv.org","year":2023,"referenceCount":27,"citationCount":1,"influentialCitationCount":0,"publicationDate":"17/04/2023","authors":"Yuzhe Cai,Shaoguang Mao,Wenshan Wu,Zehua Wang,Yaobo Liang,Tao Ge,Chenfei Wu,Wang You,Ting Song,Yan Xia,Jonathan Tien,Nan Duan","id":"be2b0396de9431bae931642516a1d3e4906329f5","summary":"A novel human-LLM interaction framework that incorporates six types of simple low-code visual programming interactions, all supported by clicking, dragging, or text editing, to achieve more controllable and stable responses.","score":1},{"url":"https://www.semanticscholar.org/paper/93cedc10eda0e89757c1d1de67d78cb7e1b5c55b","title":"Learning to Program with Natural Language","venue":"arXiv.org","year":2023,"referenceCount":30,"citationCount":1,"influentialCitationCount":0,"publicationDate":"20/04/2023","authors":"Yiduo Guo,Yaobo Liang,Chenfei Wu,Wenshan Wu,Dongyan Zhao,Nan Duan","id":"93cedc10eda0e89757c1d1de67d78cb7e1b5c55b","summary":"The Learning to Program (LP) method is proposed to ask LLMs themselves to learn natural language programs from the training dataset of complex tasks and then use the learned program to guide inference.","score":1},{"url":"https://www.semanticscholar.org/paper/4c8ef2db0c77aba453783f5211ebafc6695d3835","title":"ChatABL: Abductive Learning via Natural Language Interaction with ChatGPT","venue":"arXiv.org","year":2023,"referenceCount":69,"citationCount":1,"influentialCitationCount":0,"publicationDate":"21/04/2023","authors":"Tianyang Zhong,Yaonai Wei,Li Yang,Zihao Wu,Zheng Liu,Xiaozheng Wei,WenJu Sun,Junjie Yao,Chongfei Ma,Xiang Li,Dajiang Zhu,Xi Jiang,Jun-Feng Han,Dinggang Shen,Tianming Liu,Tuo Zhang","id":"4c8ef2db0c77aba453783f5211ebafc6695d3835","summary":"This paper presents a novel method (ChatABL) for integrating LLMs into the ABL framework, aiming at unifying the three abilities in a more user-friendly and understandable manner, and is the first attempt to explore a new pattern for further approaching human-level cognitive ability via natural language interaction with ChatGPT.","score":1},{"url":"https://www.semanticscholar.org/paper/7a5c31341e7ec22409e175542368eb76e08900aa","title":"The Potential of Visual ChatGPT For Remote Sensing","venue":"arXiv.org","year":2023,"referenceCount":36,"citationCount":0,"influentialCitationCount":0,"publicationDate":"25/04/2023","authors":"L. Osco,Eduardo Lopes de Lemos,W. Gonçalves,A. P. Ramos,J. M. Junior","id":"7a5c31341e7ec22409e175542368eb76e08900aa","summary":"","score":1},{"url":"https://www.semanticscholar.org/paper/8bc617c9139648d7a92991d70c671230bac7b2e2","title":"AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head","venue":"arXiv.org","year":2023,"referenceCount":46,"citationCount":10,"influentialCitationCount":1,"publicationDate":"25/04/2023","authors":"Rongjie Huang,Mingze Li,Dongchao Yang,Jiatong Shi,Xuankai Chang,Zhenhui Ye,Yuning Wu,Zhiqing Hong,Jia-Bin Huang,Jinglin Liu,Yixiang Ren,Zhou Zhao,Shinji Watanabe","id":"8bc617c9139648d7a92991d70c671230bac7b2e2","summary":"A multi-modal AI system named AudioGPT is proposed, which complements LLMs with 1) foundation models to process complex audio information and solve numerous understanding and generation tasks; and 2) the input/output interface (ASR, TTS) to support spoken dialogue.","score":1},{"url":"https://www.semanticscholar.org/paper/37ba1833e844f5178f91f50d82bfff616551e6ad","title":"The Role of Summarization in Generative Agents: A Preliminary Perspective","venue":"arXiv.org","year":2023,"referenceCount":11,"citationCount":0,"influentialCitationCount":0,"publicationDate":"02/05/2023","authors":"Xiachong Feng,Xiaocheng Feng,Bing Qin","id":"37ba1833e844f5178f91f50d82bfff616551e6ad","summary":"","score":1},{"url":"https://www.semanticscholar.org/paper/c77d908ba29567445a9a4ad1bd4461d441cce174","title":"AutoML-GPT: Automatic Machine Learning with GPT","venue":"arXiv.org","year":2023,"referenceCount":33,"citationCount":0,"influentialCitationCount":0,"publicationDate":"04/05/2023","authors":"Shujian Zhang,Chengyue Gong,Lemeng Wu,Xingchao Liu,Mi Zhou","id":"c77d908ba29567445a9a4ad1bd4461d441cce174","summary":"The AutoML-GPT is presented, which employs GPT as the bridge to diverse AI models and dynamically trains models with optimized hyperparameters and achieves remarkable results in computer vision, natural language processing, and other challenging areas.","score":1},{"url":"https://www.semanticscholar.org/paper/d473847dff63e3f5d238251cb23597f8205f72f2","title":"Generating Virtual On-body Accelerometer Data from Virtual Textual Descriptions for Human Activity Recognition","venue":"arXiv.org","year":2023,"referenceCount":33,"citationCount":0,"influentialCitationCount":0,"publicationDate":"04/05/2023","authors":"Zi-Jian Leng,HyeokHyen Kwon,T. Plotz","id":"d473847dff63e3f5d238251cb23597f8205f72f2","summary":"This work introduces an automated pipeline that first uses ChatGPT to generate diverse textual descriptions of activities that are later converted to streams of virtual IMU data and demonstrates how HAR models can be improved through the generation of virtual training data that do not require any manual effort.","score":1},{"url":"https://www.semanticscholar.org/paper/e0dc8e113dbdd2896fb6420ac93e0b976c47f2a2","title":"Augmented Large Language Models with Parametric Knowledge Guiding","venue":"arXiv.org","year":2023,"referenceCount":57,"citationCount":0,"influentialCitationCount":0,"publicationDate":"08/05/2023","authors":"Ziyang Luo,Can Xu,Pu Zhao,Xiubo Geng,Chongyang Tao,Jing Ma,Qingwei Lin,Daxin Jiang","id":"e0dc8e113dbdd2896fb6420ac93e0b976c47f2a2","summary":"The novel Parametric Knowledge Guiding (PKG) framework is proposed, which equips LLMs with a knowledge-guiding module to access relevant knowledge without altering the LLMs' parameters, and is based on open-source language models, allowing offline memory of any knowledge that LLMs require.","score":1},{"url":"https://www.semanticscholar.org/paper/09bd70e820427a711ed4bff29985d2885c8a379e","title":"Autonomous GIS: the next-generation AI-powered GIS","venue":"arXiv.org","year":2023,"referenceCount":64,"citationCount":0,"influentialCitationCount":0,"publicationDate":"10/05/2023","authors":"Zhenlong Li,H. Ning","id":"09bd70e820427a711ed4bff29985d2885c8a379e","summary":"Although still in its infancy and lacking several important modules such as logging and code testing, LLM-Geo demonstrates a potential path towards next-generation AI-powered GIS, and advocates for the GIScience community to dedicate more effort to the research and development of autonomous GIS.","score":1},{"url":"https://www.semanticscholar.org/paper/8dbb29f93292d8b1b861c322d232fe087b2ef7b1","title":"Small Models are Valuable Plug-ins for Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":35,"citationCount":3,"influentialCitationCount":0,"publicationDate":"15/05/2023","authors":"Canwen Xu,Yichong Xu,Shuo Wang,Yang Liu,Chenguang Zhu,Julian McAuley","id":"8dbb29f93292d8b1b861c322d232fe087b2ef7b1","summary":"The experiments demonstrate that SuperICL can improve performance beyond state-of-the-art fine-tuned models while addressing the instability problem of in-context learning, and can enhance the capabilities of smaller models, such as multilinguality and interpretability.","score":1},{"url":"https://www.semanticscholar.org/paper/7787efaf502421eac9b6b0fd946a82e1ecf4c8c9","title":"Generating coherent comic with rich story using ChatGPT and Stable Diffusion","venue":"arXiv.org","year":2023,"referenceCount":17,"citationCount":0,"influentialCitationCount":0,"publicationDate":"16/05/2023","authors":"Ze Jin,Zorina Song","id":"7787efaf502421eac9b6b0fd946a82e1ecf4c8c9","summary":"A novel way to evaluate AI-generated stories is introduced, and SOTA performance on character fidelity and art style is achieved by fine-tuning stable diffusion using LoRA, ControlNet, etc.","score":1},{"url":"https://www.semanticscholar.org/paper/43f17d08125a07f101a52170f068c0354a2f8ada","title":"Chain-of-Symbol Prompting Elicits Planning in Large Langauge Models","venue":"arXiv.org","year":2023,"referenceCount":26,"citationCount":0,"influentialCitationCount":0,"publicationDate":"17/05/2023","authors":"Hanxu Hu,Hongyuan Lu,Huajian Zhang,Wai Lam,Yue Zhang","id":"43f17d08125a07f101a52170f068c0354a2f8ada","summary":"A novel method called CoS (Chain-of-Symbol Prompting) that represents the complex environments with condensed symbolic spatial representations during the chained intermediate thinking steps and is easy to use and does not need additional training on LLMs.","score":1},{"url":"https://www.semanticscholar.org/paper/dc26beda5a94fba16194a25e35857df2aa04d8c2","title":"Empower Large Language Model to Perform Better on Industrial Domain-Specific Question Answering","venue":"arXiv.org","year":2023,"referenceCount":41,"citationCount":0,"influentialCitationCount":0,"publicationDate":"19/05/2023","authors":"Zezhong Wang,Fan Yang,Pu Zhao,Lu Wang,Jue Zhang,Mohit Garg,Qingwei Lin,Dongmei Zhang","id":"dc26beda5a94fba16194a25e35857df2aa04d8c2","summary":"This paper provides a benchmark Question Answering (QA) dataset named MSQA, which is about Microsoft products and IT technical problems encountered by customers, and proposes a new model interaction paradigm that can empower LLM to achieve better performance on domain-specific tasks where it is not proficient.","score":1},{"url":"https://www.semanticscholar.org/paper/fbd4a876cee20eaf98f344aca597a55338f663f5","title":"Examining the Inter-Consistency of Large Language Models: An In-depth Analysis via Debate","venue":"arXiv.org","year":2023,"referenceCount":40,"citationCount":0,"influentialCitationCount":0,"publicationDate":"19/05/2023","authors":"Kai Xiong,Xiao Ding,Yixin Cao,Ting Liu,Bing Qin","id":"fbd4a876cee20eaf98f344aca597a55338f663f5","summary":"Through extensive experiments on the commonsense reasoning task, LLMs not only become more inter-consistent but also achieve higher performance and the importance of a competent judge, such as GPT-4, is highlighted.","score":1},{"url":"https://www.semanticscholar.org/paper/eb291a2e237774b162d9c51c21c4868795589e94","title":"Diving into the Inter-Consistency of Large Language Models: An Insightful Analysis through Debate","venue":"","year":2023,"referenceCount":42,"citationCount":0,"influentialCitationCount":0,"publicationDate":2023,"authors":"Kai Xiong,Xiao Ding,Yixin Cao,Ting Liu,Bing Qin","id":"eb291a2e237774b162d9c51c21c4868795589e94","summary":"A formal debate framework is designed to delve into the inter-consistency problem among LLMs with three-stage debate: fair debate, mismatched debate, and roundtable debate and shows how a much stronger LLM would be dominant in mismatched debates, while it will be easily misled by relatively weaker LLMs in a more complex debate scenario such as round table debate.","score":1},{"url":"https://www.semanticscholar.org/paper/205d2ed0906440f07a0275d7d6a63bced60951fc","title":"InstructVid2Vid: Controllable Video Editing with Natural Language Instructions","venue":"arXiv.org","year":2023,"referenceCount":61,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/05/2023","authors":"Bosheng Qin,Juncheng Li,Siliang Tang,Tat-Seng Chua,Yueting Zhuang","id":"205d2ed0906440f07a0275d7d6a63bced60951fc","summary":"Experiments demonstrate that InstructVid2Vid is able to generate high-quality, temporally coherent videos and perform diverse edits, including attribute editing, change of background, and style transfer, which highlight the versatility and effectiveness of the proposed method.","score":1},{"url":"https://www.semanticscholar.org/paper/0f19e94f30b99d6c4b349900057cdae9262034f9","title":"The Contribution of Knowledge in Visiolinguistic Learning: A Survey on Tasks and Challenges","venue":"arXiv.org","year":2023,"referenceCount":140,"citationCount":0,"influentialCitationCount":0,"publicationDate":"04/03/2023","authors":"Maria Lymperaiou,G. Stamou","id":"0f19e94f30b99d6c4b349900057cdae9262034f9","summary":"This survey analyzes tasks that have benefited from hybrid approaches to visiolinguistic learning, and categorizes existing knowledge sources and types, proceeding to discussion regarding the KG vs LLM dilemma and its potential impact to future hybrid approaches.","score":1},{"url":"https://www.semanticscholar.org/paper/09840a5c151f858ed0eaf1db2a4d3741516f693b","title":"ICL-D3IE: In-Context Learning with Diverse Demonstrations Updating for Document Information Extraction","venue":"arXiv.org","year":2023,"referenceCount":55,"citationCount":6,"influentialCitationCount":0,"publicationDate":"09/03/2023","authors":"Jiabang He,Lei Wang,Yingpeng Hu,Ning Liu,Hui-juan Liu,Xingdong Xu,Hengtao Shen","id":"09840a5c151f858ed0eaf1db2a4d3741516f693b","summary":"A simple but effective in-context learning framework called ICL-D3IE, which enables LLMs to perform DIE with different types of demonstration examples and enables GPT-3/ChatGPT to achieve superior performance when compared to previous pre-trained methods fine-tuned with full training in both the in-dist distribution (ID) setting and in the out-of-distribution (OOD) setting.","score":1},{"url":"https://www.semanticscholar.org/paper/5dea6facab090a070be1444920230689e7189599","title":"SurgicalGPT: End-to-End Language-Vision GPT for Visual Question Answering in Surgery","venue":"arXiv.org","year":2023,"referenceCount":25,"citationCount":1,"influentialCitationCount":0,"publicationDate":"19/04/2023","authors":"L. Seenivasan,Mobarakol Islam,Gokul Kannan,Hongliang Ren","id":"5dea6facab090a070be1444920230689e7189599","summary":"An end-to-end trainable Language-Vision GPT model that expands the GPT2 model to include vision input (image) and extensively study and present the effects of token sequencing, token type and pose embedding for vision tokens in the LV-GPT model.","score":1},{"url":"https://www.semanticscholar.org/paper/d00ca5c49415d3a45bfcf3fabaf0a60a1c52a6ff","title":"PromptCap: Prompt-Guided Task-Aware Image Captioning","venue":"arXiv.org","year":2022,"referenceCount":100,"citationCount":15,"influentialCitationCount":0,"publicationDate":"15/11/2022","authors":"Yushi Hu,Hang Hua,Zhengyuan Yang,Weijia Shi,Noah A. Smith,Jiebo Luo","id":"d00ca5c49415d3a45bfcf3fabaf0a60a1c52a6ff","summary":"PromptCap (Prompt-guided image Captioning), a captioning model designed to serve as a better connector between images and black-box LMs, achieves state-of-the-art accuracy on knowledge-based VQA tasks and generalizes well to unseen domains.","score":1},{"url":"https://www.semanticscholar.org/paper/1367dcff4ccb927a5e95c452041288b3f0dd0eff","title":"Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation","venue":"arXiv.org","year":2022,"referenceCount":67,"citationCount":38,"influentialCitationCount":12,"publicationDate":"22/12/2022","authors":"Jay Zhangjie Wu,Yixiao Ge,Xintao Wang,Weixian Lei,Yuchao Gu,W. Hsu,Ying Shan,Xiaohu Qie,Mike Zheng Shou","id":"1367dcff4ccb927a5e95c452041288b3f0dd0eff","summary":"A new T2V generation setting, where only one text-video pair is presented, and Tune-A-Video, which involves a tailored spatio-temporal attention mechanism and an efficient one-shot tuning strategy, is introduced.","score":1},{"url":"https://www.semanticscholar.org/paper/00c1ff63468305ea3fa430c2b3aef156d580c4ff","title":"P ROMPT C AP : Prompt-Guided Image Captioning for VQA with GPT-3","venue":"","year":2023,"referenceCount":72,"citationCount":1,"influentialCitationCount":0,"publicationDate":2023,"authors":"Yushi Hu,Hang Hua,Zhengyuan Yang,Weijia Shi,Noah A. Smith,Jiebo Luo","id":"00c1ff63468305ea3fa430c2b3aef156d580c4ff","summary":"P ROMPT C AP is a captioning model designed to serve as a better connector between images and black-box LMs that outperforms generic captions by a large margin and achieves state-of-the-art accuracy on knowledge-based VQA tasks.","score":1},{"url":"https://www.semanticscholar.org/paper/3062bb79d12ff55c29c8731211a84e8cf344e235","title":"Vision Learners Meet Web Image-Text Pairs","venue":"arXiv.org","year":2023,"referenceCount":88,"citationCount":0,"influentialCitationCount":0,"publicationDate":"17/01/2023","authors":"Bingchen Zhao,Quan Cui,Hao Wu,O. Yoshie,Cheng Yang","id":"3062bb79d12ff55c29c8731211a84e8cf344e235","summary":"A new visual representation pre-training method, MUlti-modal Generator~(MUG), that learns from scalable web sourced image-text data that achieves state-of-the-art transfer performance on a variety of tasks and demonstrates promising scaling properties.","score":1},{"url":"https://www.semanticscholar.org/paper/a082b61a7d9d6c890861661be919fd9190893b38","title":"Bias-to-Text: Debiasing Unknown Visual Biases through Language Interpretation","venue":"","year":2023,"referenceCount":102,"citationCount":0,"influentialCitationCount":0,"publicationDate":"26/01/2023","authors":"Younghyun Kim,Sangwoo Mo,Minkyu Kim,Kyungmin Lee,Jaeho Lee,Jinwoo Shin","id":"a082b61a7d9d6c890861661be919fd9190893b38","summary":"The bias-to-text (B2T) framework is introduced, which uses language interpretation to identify and mitigate biases in vision models, such as image classifiers and text- to-image generative models, and its effectiveness on various image classification and generation tasks is demonstrated.","score":1},{"url":"https://www.semanticscholar.org/paper/fccada3fc530ea98d612126399f13ecb0844fc21","title":"Contrast with Reconstruct: Contrastive 3D Representation Learning Guided by Generative Pretraining","venue":"arXiv.org","year":2023,"referenceCount":103,"citationCount":5,"influentialCitationCount":1,"publicationDate":"05/02/2023","authors":"Zekun Qi,Runpei Dong,Guo Fan,Zheng Ge,Xiangyu Zhang,Kaisheng Ma,Li Yi","id":"fccada3fc530ea98d612126399f13ecb0844fc21","summary":"ReCon is trained to learn from both generative modeling teachers and single/cross-modal contrastive teachers through ensemble distillation, where the generative student guides the contrastive student.","score":1},{"url":"https://www.semanticscholar.org/paper/a3ff4df653b6970898c04e6b768e58b99786d073","title":"Learning gain differences between ChatGPT and human tutor generated algebra hints","venue":"arXiv.org","year":2023,"referenceCount":30,"citationCount":5,"influentialCitationCount":2,"publicationDate":"14/02/2023","authors":"Z. Pardos,Shreya Bhandari","id":"a3ff4df653b6970898c04e6b768e58b99786d073","summary":"This paper conducts the first learning gain evaluation of ChatGPT by comparing the efficacy of its hints with hints authored by human tutors with 77 participants across two algebra topic areas, Elementary Algebra and Intermediate Algebra.","score":1},{"url":"https://www.semanticscholar.org/paper/c8f98f28f1f28a9f5db7c4b4d9a6b7853a100214","title":"Can Pre-trained Vision and Language Models Answer Visual Information-Seeking Questions?","venue":"arXiv.org","year":2023,"referenceCount":58,"citationCount":1,"influentialCitationCount":0,"publicationDate":"23/02/2023","authors":"Yang Chen,Hexiang Hu,Yi Luan,Haitian Sun,Soravit Changpinyo,Alan Ritter,Ming-Wei Chang","id":"c8f98f28f1f28a9f5db7c4b4d9a6b7853a100214","summary":"The analysis shows that it is challenging for the state-of-the-art multi-modal pre-trained models to answer visual information seeking questions, but this capability is improved through fine-tuning on the automated InfoSeek dataset.","score":1},{"url":"https://www.semanticscholar.org/paper/467b839cb8a2475477ca004df94b797d967ad057","title":"Human-Art: A Versatile Human-Centric Dataset Bridging Natural and Artificial Scenes","venue":"arXiv.org","year":2023,"referenceCount":88,"citationCount":2,"influentialCitationCount":0,"publicationDate":"05/03/2023","authors":"Xu Ju,Ailing Zeng,Jianan Wang,Qian Xu,Lei Zhang","id":"467b839cb8a2475477ca004df94b797d967ad057","summary":"The Human-Art dataset is introduced and contains 50k high-quality images with over 123k person instances from 5 natural and 15 artificial scenarios, which are annotated with bounding boxes, keypoints, self-contact points, and text information for humans represented in both 2D and 3D.","score":1},{"url":"https://www.semanticscholar.org/paper/a7b3a868a80dbe97689135c99b1a6b6e10dcdfe5","title":"A Comprehensive Survey of AI-Generated Content (AIGC): A History of Generative AI from GAN to ChatGPT","venue":"arXiv.org","year":2023,"referenceCount":280,"citationCount":16,"influentialCitationCount":0,"publicationDate":"07/03/2023","authors":"Yihan Cao,Siyu Li,Yixin Liu,Zhiling Yan,Yutong Dai,Philip S. Yu,Lichao Sun","id":"a7b3a868a80dbe97689135c99b1a6b6e10dcdfe5","summary":"This survey provides a comprehensive review on the history of generative models, and basic components, recent advances in AIGC from unimmodal interaction and multimodal interaction, and introduces the generation tasks and relative models of text and image.","score":1},{"url":"https://www.semanticscholar.org/paper/6a4ef6c4799dc871a4253c0536126d397ca3ec1e","title":"Interpretable Visual Question Answering Referring to Outside Knowledge","venue":"arXiv.org","year":2023,"referenceCount":29,"citationCount":0,"influentialCitationCount":0,"publicationDate":"08/03/2023","authors":"He Zhu,Ren Togo,Takahiro Ogawa,M. Haseyama","id":"6a4ef6c4799dc871a4253c0536126d397ca3ec1e","summary":"A novel multimodal interpretable VQA model that can answer the question more accurately and generate diverse explanations and can outperform state-of-the-art methods regarding answer accuracy and explanation rationality.","score":1},{"url":"https://www.semanticscholar.org/paper/e5a7be5b9e6c368a1839455bfbb51bc07ed161f1","title":"ODIN: On-demand Data Formulation to Mitigate Dataset Lock-in","venue":"arXiv.org","year":2023,"referenceCount":38,"citationCount":0,"influentialCitationCount":0,"publicationDate":"13/03/2023","authors":"SP Choi,Jihun Lee,HyeongSeok Ahn,S. Jung,Bumsoo Kang","id":"e5a7be5b9e6c368a1839455bfbb51bc07ed161f1","summary":"This work evaluated ODIN on various datasets in terms of model accuracy and data diversity to demonstrate its potential, and conducted post-experiments for further investigation.","score":1},{"url":"https://www.semanticscholar.org/paper/aa75ec0ee1aa18d3b0603d5a425e92eabcb7ac02","title":"Breaking Common Sense: WHOOPS! A Vision-and-Language Benchmark of Synthetic and Compositional Images","venue":"arXiv.org","year":2023,"referenceCount":44,"citationCount":4,"influentialCitationCount":1,"publicationDate":"13/03/2023","authors":"Nitzan Bitton-Guetta,Yonatan Bitton,Jack Hessel,Ludwig Schmidt,Y. Elovici,G. Stanovsky,Roy Schwartz","id":"aa75ec0ee1aa18d3b0603d5a425e92eabcb7ac02","summary":"This work introduces WHOOPS!, a new dataset and benchmark for visual commonsense, comprised of purposefully commonsense-defying images created by designers using publicly-available image generation tools like Midjourney and introduces a difficult explanation generation task, where models must identify and explain why a given image is unusual.","score":1},{"url":"https://www.semanticscholar.org/paper/cf41ae462687f81ce95b27113c6a4f9c2751de42","title":"Vision-Language Models as Success Detectors","venue":"arXiv.org","year":2023,"referenceCount":54,"citationCount":3,"influentialCitationCount":0,"publicationDate":"13/03/2023","authors":"Yuqing Du,Ksenia Konyushkova,Misha Denil,A. Raju,Jessica Landon,Felix Hill,N. D. Freitas,Serkan Cabi","id":"cf41ae462687f81ce95b27113c6a4f9c2751de42","summary":"This work focuses on developing robust success detectors that leverage large, pretrained vision-language models and human reward annotations, and investigates the generalisation properties of a Flamingo-based success detection model across unseen language and visual changes in the first two domains.","score":1},{"url":"https://www.semanticscholar.org/paper/3c39a600adb254f7520f513ed9c3412c9c62f17f","title":"MRIS: A Multi-modal Retrieval Approach for Image Synthesis on Diverse Modalities","venue":"arXiv.org","year":2023,"referenceCount":27,"citationCount":0,"influentialCitationCount":0,"publicationDate":"17/03/2023","authors":"Boqi Chen,M. Niethammer","id":"3c39a600adb254f7520f513ed9c3412c9c62f17f","summary":"The proposed method outperforms direct image synthesis and that the synthesized thickness maps retain information relevant to downstream tasks such as progression prediction and Kellgren-Lawrence grading (KLG).","score":1},{"url":"https://www.semanticscholar.org/paper/049a62ac86f59f2a912cd59f1cb179b82c4ae6b9","title":"TIFA: Accurate and Interpretable Text-to-Image Faithfulness Evaluation with Question Answering","venue":"arXiv.org","year":2023,"referenceCount":62,"citationCount":7,"influentialCitationCount":4,"publicationDate":"21/03/2023","authors":"Yushi Hu,Benlin Liu,Jungo Kasai,Yizhong Wang,Mari Ostendorf,Ranjay Krishna,Noah A. Smith","id":"049a62ac86f59f2a912cd59f1cb179b82c4ae6b9","summary":"This work introduces TIFA (Text-to-Image Faithfulness evaluation with question Answering), an automatic evaluation metric that measures the faithfulness of a generated image to its text input via visual question answering (VQA).","score":1},{"url":"https://www.semanticscholar.org/paper/7733cf84e5447339dd57ca96133e14e36c29e0e7","title":"Contrastive Alignment of Vision to Language Through Parameter-Efficient Transfer Learning","venue":"arXiv.org","year":2023,"referenceCount":60,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/03/2023","authors":"Zaid Khan,Yun Fu","id":"7733cf84e5447339dd57ca96133e14e36c29e0e7","summary":"The feasibility and benefits of parameter-efficient contrastive vision-language alignment through transfer learning are explored: creating a model such as CLIP by minimally updating an already-trained vision and language model is found.","score":1},{"url":"https://www.semanticscholar.org/paper/285dae5c2f2ef55c70971094a1ddd45afe720eee","title":"Fundamentals of Generative Large Language Models and Perspectives in Cyber-Defense","venue":"arXiv.org","year":2023,"referenceCount":100,"citationCount":1,"influentialCitationCount":0,"publicationDate":"21/03/2023","authors":"Andrei Kucharavy,Z. Schillaci,Loic Mar'echal,Maxime Wursch,L. Dolamic,Remi Sabonnadiere,Dimitri Percia David,Alain Mermoud,Vincent Lenders","id":"285dae5c2f2ef55c70971094a1ddd45afe720eee","summary":"This review aims to provide a brief overview of the history, state of the art, and implications of Generative Language Models in terms of their principles, abilities, limitations, and future prospects -- especially in the context of cyber-defense, with a focus on the Swiss operational environment.","score":1},{"url":"https://www.semanticscholar.org/paper/0d3817ae7fecc204c7c79a039dc47ae88890d5f3","title":"ChatGPT for Shaping the Future of Dentistry: The Potential of Multi-Modal Large Language Model","venue":"arXiv.org","year":2023,"referenceCount":46,"citationCount":1,"influentialCitationCount":0,"publicationDate":"23/03/2023","authors":"Hanyao Huang,Ou Zheng,Dongdong Wang,Jiayi Yin,Zijin Wang,Shengxuan Ding,H. Yin,Chuan Xu,R. Yang,Q. Zheng,B. Shi","id":"0d3817ae7fecc204c7c79a039dc47ae88890d5f3","summary":"","score":1},{"url":"https://www.semanticscholar.org/paper/d84616f108ccbd958735fef7622e58d148b32139","title":"Make-It-3D: High-Fidelity 3D Creation from A Single Image with Diffusion Prior","venue":"arXiv.org","year":2023,"referenceCount":63,"citationCount":4,"influentialCitationCount":1,"publicationDate":"24/03/2023","authors":"Junshu Tang,Tengfei Wang,Bo Zhang,Ting Zhang,Ran Yi,Lizhuang Ma,Dong Chen","id":"d84616f108ccbd958735fef7622e58d148b32139","summary":"This work leverages prior knowledge from a well-trained 2D diffusion model to act as 3D-aware supervision for 3D creation and presents the first attempt to achieve high-quality3D creation from a single image for general objects.","score":1},{"url":"https://www.semanticscholar.org/paper/a08b7123a7158f1a7fbbc18e8b5aaebd47980ecf","title":"EVA-CLIP: Improved Training Techniques for CLIP at Scale","venue":"arXiv.org","year":2023,"referenceCount":52,"citationCount":7,"influentialCitationCount":2,"publicationDate":"27/03/2023","authors":"Quan Sun,Yuxin Fang,Ledell Yu Wu,Xinlong Wang,Yue Cao","id":"a08b7123a7158f1a7fbbc18e8b5aaebd47980ecf","summary":"This paper proposes a series of models that significantly improve the efficiency and effectiveness of CLIP training, and incorporates new techniques for representation learning, optimization, and augmentation, enabling EVA-CLIP to achieve superior performance compared to previous CLIP models with the same number of parameters but significantly smaller training costs.","score":1},{"url":"https://www.semanticscholar.org/paper/1c7471996a4f2e08a5ef592a6ffcde65a034a1e4","title":"GlyphDraw: Learning to Draw Chinese Characters in Image Synthesis Models Coherently","venue":"arXiv.org","year":2023,"referenceCount":42,"citationCount":1,"influentialCitationCount":1,"publicationDate":2023,"authors":"Jiancang Ma,Mingjun Zhao,Chen Chen,Ruichen Wang,Di Niu,H. Lu,Xiaodong Lin","id":"1c7471996a4f2e08a5ef592a6ffcde65a034a1e4","summary":"GlyphDraw is a general learning framework aiming at endowing image generation models with the capacity to generate images embedded with coherent text, and is the first work in the field of image synthesis to address the generation of Chinese characters.","score":1},{"url":"https://www.semanticscholar.org/paper/4d94dcc6c9c261c8edcd0f3c5a1318a98a45b79d","title":"HRS-Bench: Holistic, Reliable and Scalable Benchmark for Text-to-Image Models","venue":"arXiv.org","year":2023,"referenceCount":73,"citationCount":1,"influentialCitationCount":1,"publicationDate":"11/04/2023","authors":"Eslam Mohamed Bakr,Pengzhan Sun,Xiaoqian Shen,Faizan Farooq Khan,Li Erran Li,Mohamed Elhoseiny","id":"4d94dcc6c9c261c8edcd0f3c5a1318a98a45b79d","summary":"HRS-Bench is introduced, a concrete evaluation benchmark for T2I models that is Holistic, Reliable, and Scalable and measures 13 skills that can be categorized into five major categories: accuracy, robustness, generalization, fairness, and bias.","score":1},{"url":"https://www.semanticscholar.org/paper/f12ebcc9a0a7296cc6c85b243a003f7205c68b3d","title":"What does CLIP know about a red circle? Visual prompt engineering for VLMs","venue":"arXiv.org","year":2023,"referenceCount":63,"citationCount":2,"influentialCitationCount":0,"publicationDate":"13/04/2023","authors":"Aleksandar Shtedritski,C. Rupprecht,A. Vedaldi","id":"f12ebcc9a0a7296cc6c85b243a003f7205c68b3d","summary":"This work explores the idea of visual prompt engineering for solving computer vision tasks beyond classification by editing in image space instead of text, and discovers an emergent ability of CLIP, where, by simply drawing a red circle around an object, it can direct the model's attention to that region, while also maintaining global information.","score":1},{"url":"https://www.semanticscholar.org/paper/f603b9dc81dfea56d437e967b724636d4d72d000","title":"LLM as A Robotic Brain: Unifying Egocentric Memory and Control","venue":"arXiv.org","year":2023,"referenceCount":24,"citationCount":1,"influentialCitationCount":0,"publicationDate":"19/04/2023","authors":"Jinjie Mai,Jun Chen,Bing-chuan Li,Guocheng Qian,Mohamed Elhoseiny,Bernard Ghanem","id":"f603b9dc81dfea56d437e967b724636d4d72d000","summary":"A novel and generalizable framework called LLM-Brain: using Large-scale Language Model as a robotic brain to unify egocentric memory and control, utilizing a zero-shot learning approach.","score":1},{"url":"https://www.semanticscholar.org/paper/8fce3142bc144bdc08bf0cab1db908c7ad3f8454","title":"Contrastive Language, Action, and State Pre-training for Robot Learning","venue":"arXiv.org","year":2023,"referenceCount":30,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/04/2023","authors":"Krishan Rana,Andrew Melnik,N. Sunderhauf","id":"8fce3142bc144bdc08bf0cab1db908c7ad3f8454","summary":"The method, Contrastive Language, Action, and State Pre-training (CLASP), extends the CLIP formulation by incorporating distributional learning, capturing the inherent complexities and one-to-many relationships in behaviour-text alignment.","score":1},{"url":"https://www.semanticscholar.org/paper/598b3961f767c1ad40cbb393afd936de4e30d578","title":"SATIN: A Multi-Task Metadataset for Classifying Satellite Imagery using Vision-Language Models","venue":"arXiv.org","year":2023,"referenceCount":86,"citationCount":0,"influentialCitationCount":0,"publicationDate":"23/04/2023","authors":"J. Roberts,K. Han,Samuel Albanie","id":"598b3961f767c1ad40cbb393afd936de4e30d578","summary":"This work introduces SATellite ImageNet (SATIN), a metadataset curated from 27 existing remotely sensed datasets, and comprehensively evaluates the zero-shot transfer classification capabilities of a broad range of vision-language (VL) models on SATIN.","score":1},{"url":"https://www.semanticscholar.org/paper/59dfa986cc7468561d2c19cdb43f816406ea30d8","title":"TR0N: Translator Networks for 0-Shot Plug-and-Play Conditional Generation","venue":"arXiv.org","year":2023,"referenceCount":85,"citationCount":0,"influentialCitationCount":0,"publicationDate":"26/04/2023","authors":"Zhao-Qian Liu,Noël Vouitsis,S. Gorti,Jimmy Ba,G. Loaiza-Ganem","id":"59dfa986cc7468561d2c19cdb43f816406ea30d8","summary":"TR0N is proposed, a highly general framework to turn pre-trained unconditional generative models, such as GANs and VAEs, into conditional models, and shows how to turn unconditional models into class-conditional ones with the help of a classifier, and also into text-to-image models by leveraging CLIP.","score":1},{"url":"https://www.semanticscholar.org/paper/b1c26d02a44407de29ee11205d62a9ae72d51057","title":"Segment Anything is A Good Pseudo-label Generator for Weakly Supervised Semantic Segmentation","venue":"arXiv.org","year":2023,"referenceCount":45,"citationCount":1,"influentialCitationCount":1,"publicationDate":"02/05/2023","authors":"Peng-Tao Jiang,Yuqi Yang","id":"b1c26d02a44407de29ee11205d62a9ae72d51057","summary":"This report attempts to explore the potential of 'prompt to masks' from the powerful class-agnostic large segmentation model, segment-anything, by using different weak labels to be used as prompts to the segment- Anything model, generating precise class masks.","score":1},{"url":"https://www.semanticscholar.org/paper/b7e6408841054dcf6efcd9cd77de2561841210b4","title":"Personalize Segment Anything Model with One Shot","venue":"arXiv.org","year":2023,"referenceCount":61,"citationCount":4,"influentialCitationCount":1,"publicationDate":"04/05/2023","authors":"Renrui Zhang,Zhengkai Jiang,Ziyu Guo,Shilin Yan,Junting Pan,Hao-Wen Dong,Peng Gao,Hongsheng Li","id":"b7e6408841054dcf6efcd9cd77de2561841210b4","summary":"A training-free Personalization approach for SAM, termed as PerSAM, which effectively adapt SAM for private use without any training, and can enhance DreamBooth to personalize Stable Diffusion for text-to-image generation, which discards the background disturbance for better target appearance learning.","score":1},{"url":"https://www.semanticscholar.org/paper/042459a38c1efb4118030309f0bcd7d5ed77d83f","title":"Image Captioners Sometimes Tell More Than Images They See","venue":"arXiv.org","year":2023,"referenceCount":24,"citationCount":0,"influentialCitationCount":0,"publicationDate":"04/05/2023","authors":"Honori Udo,Takafumi Koshinaka","id":"042459a38c1efb4118030309f0bcd7d5ed77d83f","summary":"This work evaluates several image captioning models with respect to a disaster image classification task, CrisisNLP, and shows that descriptive text classifiers can sometimes achieve higher accuracy than standard image-based classifiers and that fusing an image- based classifier with a descriptive textclassifier can provide improvement in accuracy.","score":1},{"url":"https://www.semanticscholar.org/paper/69fa5dabbdc98ec63c3eaf95c0f70ec50699f0ce","title":"Towards Applying Powerful Large AI Models in Classroom Teaching: Opportunities, Challenges and Prospects","venue":"arXiv.org","year":2023,"referenceCount":53,"citationCount":0,"influentialCitationCount":0,"publicationDate":"05/05/2023","authors":"Kehui Tan,Tianqi Pang,Chenyou Fan","id":"69fa5dabbdc98ec63c3eaf95c0f70ec50699f0ce","summary":"","score":1},{"url":"https://www.semanticscholar.org/paper/1509f216ef556a68aa5639d07b61882945651e60","title":"ChinaOpen: A Dataset for Open-world Multimodal Learning","venue":"arXiv.org","year":2023,"referenceCount":39,"citationCount":0,"influentialCitationCount":0,"publicationDate":"10/05/2023","authors":"Aozhu Chen,Ziyuan Wang,Chengbo Dong,Kaibin Tian,Ruixiang Zhao,Xun Liang,Zhanhui Kang,Xirong Li","id":"1509f216ef556a68aa5639d07b61882945651e60","summary":"An extensive evaluation of the state-of-the-art single-task / multi-task models on the new dataset, ChinaOpen, is conducted, resulting in a number of novel findings and insights.","score":1},{"url":"https://www.semanticscholar.org/paper/240bc60c98c9b860c27c6f962992618a6775cab1","title":"Musketeer (All for One, and One for All): A Generalist Vision-Language Model with Task Explanation Prompts","venue":"arXiv.org","year":2023,"referenceCount":68,"citationCount":0,"influentialCitationCount":0,"publicationDate":"11/05/2023","authors":"Zhaoyang Zhang,Yantao Shen,Kunyu Shi,Zhaowei Cai,Jun Fang,Siqi Deng,Hao Yang,Davide Modolo,Z. Tu,S. Soatto","id":"240bc60c98c9b860c27c6f962992618a6775cab1","summary":"A sequence-to-sequence vision-language model whose parameters are jointly trained on all tasks and fully shared among multiple tasks, resulting in a single model which is named Musketeer.","score":1},{"url":"https://www.semanticscholar.org/paper/65051f6836a4a618586c01deff43b46ab5e3f887","title":"Measuring Progress in Fine-grained Vision-and-Language Understanding","venue":"arXiv.org","year":2023,"referenceCount":66,"citationCount":1,"influentialCitationCount":1,"publicationDate":"12/05/2023","authors":"Emanuele Bugliarello,Laurent Sartran,Aishwarya Agrawal,Lisa Anne Hendricks,Aida Nematzadeh","id":"65051f6836a4a618586c01deff43b46ab5e3f887","summary":"This work investigates four competitive V&L models on four fine-grained benchmarks, and finds that X-VLM consistently outperforms other baselines, and that modelling innovations can impact performance more than scaling Web data, which even degrades performance sometimes.","score":1},{"url":"https://www.semanticscholar.org/paper/4b203ee52e27cbf27d210dd671951150729a8259","title":"ULIP-2: Towards Scalable Multimodal Pre-training for 3D Understanding","venue":"arXiv.org","year":2023,"referenceCount":60,"citationCount":0,"influentialCitationCount":0,"publicationDate":"14/05/2023","authors":"Le Xue,Ning Yu,Shu Zhang,Junnan Li,Roberto Mart'in-Mart'in,Jiajun Wu,Caiming Xiong,Ran Xu,Juan Carlos Niebles,S. Savarese","id":"4b203ee52e27cbf27d210dd671951150729a8259","summary":"ULIP-2 is introduced, a tri-modal pre-training framework that leverages state-of-the-art large multimodal models to automatically generate holistic language counterparts for 3D objects that does not require any 3D annotations, and is therefore scalable to large datasets.","score":1},{"url":"https://www.semanticscholar.org/paper/adf4655e6bb531ec1a03d8ec9e8c5c63ae771fb6","title":"Edit As You Wish: Video Description Editing with Multi-grained Commands","venue":"arXiv.org","year":2023,"referenceCount":43,"citationCount":0,"influentialCitationCount":0,"publicationDate":"15/05/2023","authors":"Linli Yao,Yuanmeng Zhang,Ziheng Wang,Xinglin Hou,T. Ge,Yuning Jiang,Qin Jin","id":"adf4655e6bb531ec1a03d8ec9e8c5c63ae771fb6","summary":"A novel Video Description Editing (VDEdit) task to automatically revise an existing video description guided by flexible user requests is proposed and a unified framework to convert the {operation, position, attribute} triplet into a textual control sequence to handle multi-grained editing commands is proposed.","score":1},{"url":"https://www.semanticscholar.org/paper/83a734dee0809a46bc7189c12cd9956927d14836","title":"Bridging the Domain Gap: Self-Supervised 3D Scene Understanding with Foundation Models","venue":"arXiv.org","year":2023,"referenceCount":68,"citationCount":0,"influentialCitationCount":0,"publicationDate":"15/05/2023","authors":"Zhimin Chen,Bing Li","id":"83a734dee0809a46bc7189c12cd9956927d14836","summary":"This paper introduces a novel method that employs foundation models to generate highly accurate object-level masks and semantic text information at the object level, and notably outshines state-of-the-art methods in 3D object detection and semantic segmentation tasks.","score":1},{"url":"https://www.semanticscholar.org/paper/5f51eda9f7abddca027941d50fb0b6bf6f508eff","title":"Make-A-Protagonist: Generic Video Editing with An Ensemble of Experts","venue":"arXiv.org","year":2023,"referenceCount":43,"citationCount":1,"influentialCitationCount":0,"publicationDate":"15/05/2023","authors":"Yuyang Zhao,Enze Xie,Lanqing Hong,Zhenguo Li,G. Lee","id":"5f51eda9f7abddca027941d50fb0b6bf6f508eff","summary":"This work proposes a generic video editing framework called Make-A-Protagonist, which utilizes textual and visual clues to edit videos with the goal of empowering individuals to become the protagonists, and proposes a visual-textual-based video generation model that employs mask-guided denoising sampling to generate the desired output.","score":1},{"url":"https://www.semanticscholar.org/paper/3ef08e54c8f97804a89383b497347c8a7eefff7d","title":"A Video Is Worth 4096 Tokens: Verbalize Story Videos To Understand Them In Zero Shot","venue":"arXiv.org","year":2023,"referenceCount":49,"citationCount":1,"influentialCitationCount":0,"publicationDate":"16/05/2023","authors":"Aanisha Bhattacharya,Yaman Kumar Singla,Balaji Krishnamurthy,R. Shah,Changan Chen","id":"3ef08e54c8f97804a89383b497347c8a7eefff7d","summary":"This work proposes verbalizing story videos to generate their descriptions in natural language and then performing video-understanding tasks on the generated story as opposed to the original video, demonstrating that this method, despite being zero-shot, achieves significantly better results than supervised baselines for video understanding.","score":1},{"url":"https://www.semanticscholar.org/paper/1a3d6119d9513ad27fa4fc3262e517ec6a6d2261","title":"FastComposer: Tuning-Free Multi-Subject Image Generation with Localized Attention","venue":"arXiv.org","year":2023,"referenceCount":38,"citationCount":0,"influentialCitationCount":0,"publicationDate":"17/05/2023","authors":"Guangxuan Xiao,Tianwei Yin,W. Freeman,F. Durand,Song Han","id":"1a3d6119d9513ad27fa4fc3262e517ec6a6d2261","summary":"FastComposer enables efficient, personalized, multi-subject text-to-image generation without fine-tuning, and proposes delayed subject conditioning in the denoising step to maintain both identity and editability in subject-driven image generation.","score":1},{"url":"https://www.semanticscholar.org/paper/9b3ff4c05be2ee57021099ab07fadfb77440be45","title":"IMAD: IMage-Augmented multi-modal Dialogue","venue":"","year":2023,"referenceCount":67,"citationCount":0,"influentialCitationCount":0,"publicationDate":"17/05/2023","authors":"Moskvoretskii Viktor,Frolov Anton,Kuznetsov Denis","id":"9b3ff4c05be2ee57021099ab07fadfb77440be45","summary":"This work proposes a two-stage approach to automatically construct a multi-modal dialogue dataset that can serve as a validated dataset for this task and proposes a baseline model trained on this dataset, which outperforms modeltrained on the same data without images and BlenderBot.","score":1},{"url":"https://www.semanticscholar.org/paper/413bc628ef54effa9d59f93aa7ffd86a38fd1143","title":"What You See is What You Read? Improving Text-Image Alignment Evaluation","venue":"arXiv.org","year":2023,"referenceCount":60,"citationCount":1,"influentialCitationCount":0,"publicationDate":"17/05/2023","authors":"Michal Yarom,Yonatan Bitton,Soravit Changpinyo,Roee Aharoni,Jonathan Herzig,Oran Lang,E. Ofek,Idan Szpektor","id":"413bc628ef54effa9d59f93aa7ffd86a38fd1143","summary":"Two automatic methods to determine alignment are described, the first involving a pipeline based on question generation and visual question answering models, and the second employing an end-to-end classification approach by finetuning multimodal pretrained models.","score":1},{"url":"https://www.semanticscholar.org/paper/c3eee48481b3b8f4be18026e389fadf9a53ad192","title":"Content-based Unrestricted Adversarial Attack","venue":"arXiv.org","year":2023,"referenceCount":57,"citationCount":0,"influentialCitationCount":0,"publicationDate":"18/05/2023","authors":"Zhaoyu Chen,Bo Li,Shuang Wu,Kaixun Jiang,Shouhong Ding,Wenqiang Zhang","id":"c3eee48481b3b8f4be18026e389fadf9a53ad192","summary":"This work proposes a novel unrestricted attack framework called Content-based Unrestricted Adversarial Attack based on Stable Diffusion that can generate high transferable unrestricted adversarial examples with various adversarial contents and demonstrates the efficacy of ACA.","score":1},{"url":"https://www.semanticscholar.org/paper/2da2f8284c03a0c0df0e8b13e90bf215dd5ac786","title":"Listen, Think, and Understand","venue":"arXiv.org","year":2023,"referenceCount":49,"citationCount":0,"influentialCitationCount":0,"publicationDate":"18/05/2023","authors":"Yuan Gong,Hongyin Luo,Alexander H. Liu,Leonid Karlinsky,James Glass","id":"2da2f8284c03a0c0df0e8b13e90bf215dd5ac786","summary":"LTU is the first audio-enabled large language model that bridges audio perception with advanced reasoning, and demonstrates strong performance and generalization ability on conventional audio tasks such as classification and captioning.","score":1},{"url":"https://www.semanticscholar.org/paper/86ea4aa29241149c3999301f0285d8cbb8542b11","title":"Vision-Language Pre-training with Object Contrastive Learning for 3D Scene Understanding","venue":"arXiv.org","year":2023,"referenceCount":52,"citationCount":0,"influentialCitationCount":0,"publicationDate":"18/05/2023","authors":"Zhang Tao,Su He,D. Tao,Bin Chen,Zhi Wang,Shutao Xia","id":"86ea4aa29241149c3999301f0285d8cbb8542b11","summary":"This work carefully investigates three common tasks in semantic 3D scene understanding, and derives key insights into the development of a pre-training model 3DVLP, which transfers flexibly on 3D vision-language downstream tasks.","score":1},{"url":"https://www.semanticscholar.org/paper/daf34122a0c38531aeeb55069ba98e564c263d53","title":"MedBLIP: Bootstrapping Language-Image Pre-training from 3D Medical Images and Texts","venue":"arXiv.org","year":2023,"referenceCount":35,"citationCount":0,"influentialCitationCount":0,"publicationDate":"18/05/2023","authors":"Qiuhui Chen,Xinyue Hu,Zirui Wang,Yi Hong","id":"daf34122a0c38531aeeb55069ba98e564c263d53","summary":"This paper considers developing a VLP model in the medical domain for making computer-aided diagnoses (CAD) based on image scans and text descriptions in electronic health records, as done in practice, and presents a lightweight CAD system MedBLIP, a new paradigm for bootstrapping VLP from off-the-shelf frozen pre-trained image encoders and frozen large language models.","score":1},{"url":"https://www.semanticscholar.org/paper/8ce6ad6d8a73757309d3b9f525cf15cb68e32397","title":"Efficient Prompting via Dynamic In-Context Learning","venue":"arXiv.org","year":2023,"referenceCount":62,"citationCount":0,"influentialCitationCount":0,"publicationDate":"18/05/2023","authors":"Wangchunshu Zhou,Yuchen Jiang,Ryan Cotterell,Mrinmaya Sachan","id":"8ce6ad6d8a73757309d3b9f525cf15cb68e32397","summary":"DynaICL is proposed, a recipe for efficient prompting with black-box generalist models that dynamically allocate in-context examples according to the input complexity and the computational budget and it is found that a meta controller trained on a certain backbone model and tasks can successfully generalize to unseen models and tasks.","score":1},{"url":"https://www.semanticscholar.org/paper/757940cef62b06f6abdb427d5a7fe61d512a2e3f","title":"UniControl: A Unified Diffusion Model for Controllable Visual Generation In the Wild","venue":"arXiv.org","year":2023,"referenceCount":58,"citationCount":0,"influentialCitationCount":0,"publicationDate":"18/05/2023","authors":"Can Qin,Shu Zhang,Ning Yu,Yihao Feng,Xinyi Yang,Yingbo Zhou,Haiquan Wang,Juan Carlos Niebles,Caiming Xiong,S. Savarese,S. Ermon,Yun Fu,Ran Xu","id":"757940cef62b06f6abdb427d5a7fe61d512a2e3f","summary":"This work introduces UniControl, a new generative foundation model that consolidates a wide array of controllable condition-to-image (C2I) tasks within a singular framework, while still allowing for arbitrary language prompts, and enables pixel-level-precise image generation.","score":1},{"url":"https://www.semanticscholar.org/paper/972501b057e2b84d6ce6506f70bcac697bab7872","title":"LLMScore: Unveiling the Power of Large Language Models in Text-to-Image Synthesis Evaluation","venue":"arXiv.org","year":2023,"referenceCount":51,"citationCount":0,"influentialCitationCount":0,"publicationDate":"18/05/2023","authors":"Yujie Lu,Xianjun Yang,Xiujun Li,X. Wang,William Yang Wang","id":"972501b057e2b84d6ce6506f70bcac697bab7872","summary":"This work proposes LLMScore, a new framework that offers evaluation scores with multi-granularity compositionality that achieves Kendall's tau correlation with human evaluations that is 58.8% and 31.2% higher than the commonly-used text-image matching metrics CLIP and BLIP, respectively.","score":1},{"url":"https://www.semanticscholar.org/paper/a90f2b1fa484a9ee9a6efa13c1734f7510eaf044","title":"VideoFactory: Swap Attention in Spatiotemporal Diffusions for Text-to-Video Generation","venue":"arXiv.org","year":2023,"referenceCount":64,"citationCount":1,"influentialCitationCount":0,"publicationDate":"18/05/2023","authors":"Wenjing Wang,Huan Yang,Zixi Tuo,Huiguo He,Junchen Zhu,Jianlong Fu,Jiaying Liu","id":"a90f2b1fa484a9ee9a6efa13c1734f7510eaf044","summary":"Object metrics and user studies demonstrate the superiority of the novel approach that strengthens the interaction between spatial and temporal perceptions in 3D windows in terms of per-frame quality, temporal correlation, and text-video alignment, with clear margins.","score":1},{"url":"https://www.semanticscholar.org/paper/9f411fda2ad5b141a3115f707bcf5ee865b3fb94","title":"Any-to-Any Generation via Composable Diffusion","venue":"arXiv.org","year":2023,"referenceCount":58,"citationCount":0,"influentialCitationCount":0,"publicationDate":"19/05/2023","authors":"Zineng Tang,Ziyi Yang,Chenguang Zhu,Michael Zeng,Mohit Bansal","id":"9f411fda2ad5b141a3115f707bcf5ee865b3fb94","summary":"CoDi employs a novel composable generation strategy which involves building a shared multimodal space by bridging alignment in the diffusion process, enabling the synchronized generation of intertwined modalities, such as temporally aligned video and audio.","score":1},{"url":"https://www.semanticscholar.org/paper/3c2fcef50b952097a31cfe1b9e1b1b89d5599744","title":"ViT-TTS: Visual Text-to-Speech with Scalable Diffusion Transformer","venue":"arXiv.org","year":2023,"referenceCount":51,"citationCount":0,"influentialCitationCount":0,"publicationDate":"22/05/2023","authors":"Huadai Liu,Rongjie Huang,Xuan Lin,Wenqiang Xu,Maozong Zheng,Hong Chen,Jinzheng He,Zhou Zhao","id":"3c2fcef50b952097a31cfe1b9e1b1b89d5599744","summary":"ViT-TTS is proposed, the first visual TTS model with scalable diffusion transformers and introduces a self-supervised learning framework to enhance both the visual-text encoder and denoiser decoder and leverage the diffusion transformer scalable in terms of parameters and capacity to learn visual scene information.","score":1},{"url":"https://www.semanticscholar.org/paper/251445d8b22b1c25ccad96f284c085dca49b57f3","title":"UVOSAM: A Mask-free Paradigm for Unsupervised Video Object Segmentation via Segment Anything Model","venue":"arXiv.org","year":2023,"referenceCount":56,"citationCount":0,"influentialCitationCount":0,"publicationDate":"22/05/2023","authors":"Zhenghao Zhang,Zhichao Wei,Shengfan Zhang,Zuozhuo Dai,Siyu Zhu","id":"251445d8b22b1c25ccad96f284c085dca49b57f3","summary":"This paper proposes a novel paradigm called UVOSAM, which leverages SAM for unsupervised video object segmentation without requiring video mask labels, and introduces a video salient object tracking network that automatically generates trajectories for prominent foreground objects.","score":1},{"url":"https://www.semanticscholar.org/paper/684c8dccfe7afc1b05057ebd5df0c90379443796","title":"AudioToken: Adaptation of Text-Conditioned Diffusion Models for Audio-to-Image Generation","venue":"arXiv.org","year":2023,"referenceCount":39,"citationCount":0,"influentialCitationCount":0,"publicationDate":"22/05/2023","authors":"Guy Yariv,Itai Gat,Lior Wolf,Yossi Adi,Idan Schwartz","id":"684c8dccfe7afc1b05057ebd5df0c90379443796","summary":"Using a pre-trained audio encoding model, the proposed method encodes audio into a new token, which can be considered as an adaptation layer between the audio and text representations, making the proposed approach appealing for lightweight optimization.","score":1},{"url":"https://www.semanticscholar.org/paper/0cc49320f77e384a9acde7fa9c1b7c776a4f04a4","title":"If at First You Don't Succeed, Try, Try Again: Faithful Diffusion-based Text-to-Image Generation by Selection","venue":"arXiv.org","year":2023,"referenceCount":70,"citationCount":0,"influentialCitationCount":0,"publicationDate":"22/05/2023","authors":"Shyamgopal Karthik,Karsten Roth,Massimiliano Mancini,Zeynep Akata","id":"0cc49320f77e384a9acde7fa9c1b7c776a4f04a4","summary":"This work shows that large T2I diffusion models are more faithful than usually assumed, and can generate images faithful to even complex prompts without the need to manipulate the generative process, and introduces a straightforward pipeline that generates candidate images for a text prompt and picks the best one according to an automatic scoring system.","score":1},{"url":"https://www.semanticscholar.org/paper/994a1ce6677b496bd3c0c63aceafc6556005e994","title":"GLIGEN: Open-Set Grounded Text-to-Image Generation","venue":"arXiv.org","year":2023,"referenceCount":79,"citationCount":31,"influentialCitationCount":7,"publicationDate":"17/01/2023","authors":"Yuheng Li,Haotian Liu,Qingyang Wu,Fangzhou Mu,Jianwei Yang,Jianfeng Gao,Chunyuan Li,Yong Jae Lee","id":"994a1ce6677b496bd3c0c63aceafc6556005e994","summary":"A novel approach that builds upon and extends the functionality of existing pre-trained text-to-image diffusion models by enabling them to also be conditioned on grounding inputs, and achieves open-world grounded text2img generation with caption and bounding box condition inputs.","score":1},{"url":"https://www.semanticscholar.org/paper/33827bb0bb8188817083be024614f82bec002c42","title":"A Simple Framework for Open-Vocabulary Segmentation and Detection","venue":"arXiv.org","year":2023,"referenceCount":62,"citationCount":2,"influentialCitationCount":0,"publicationDate":"14/03/2023","authors":"Hao Zhang,Feng Li,Xueyan Zou,Siyi Liu,Chun-yue Li,Jianfeng Gao,Jianwei Yang,Lei Zhang","id":"33827bb0bb8188817083be024614f82bec002c42","summary":"OpenSeeD is the first to explore the potential of joint training on segmentation and detection, and hope it can be received as a strong baseline for developing a single model for both tasks in open world.","score":1},{"url":"https://www.semanticscholar.org/paper/f8e37aa69b3c9b743055e648851c530b18dc54d2","title":"Neural Implicit Vision-Language Feature Fields","venue":"arXiv.org","year":2023,"referenceCount":51,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/03/2023","authors":"Kenneth Blomqvist,Francesco Milano,Jen Jen Chung,Lionel Ott,R. Siegwart","id":"f8e37aa69b3c9b743055e648851c530b18dc54d2","summary":"This work presents a zero-shot volumetric open-vocabulary semantic scene segmentation method that builds on the insight that it can fuse image features from a vision-language model into a neural implicit representation and shows that the resulting feature field can be segmented into different classes by assigning points to natural language text prompts.","score":1},{"url":"https://www.semanticscholar.org/paper/0931888d5cd7c26427cc116af2ac33863552da27","title":"SAM.MD: Zero-shot medical image segmentation capabilities of the Segment Anything Model","venue":"arXiv.org","year":2023,"referenceCount":8,"citationCount":10,"influentialCitationCount":0,"publicationDate":"10/04/2023","authors":"Saikat Roy,T. Wald,Gregor Koehler,Maximilian R. Rokuss,Nico Disch,Julius Holzschuh,David Zimmerer,K. Maier-Hein","id":"0931888d5cd7c26427cc116af2ac33863552da27","summary":"It is shown that SAM generalizes well to CT data, making it a potential catalyst for the advancement of semi-automatic segmentation tools for clinicians, and can serve as a highly potent starting point for further adaptations of such models to the intricacies of the medical domain.","score":1},{"url":"https://www.semanticscholar.org/paper/9570a3abed7c339ea2fa8d89ad1ee0f459e42fc1","title":"Transformer-Based Visual Segmentation: A Survey","venue":"arXiv.org","year":2023,"referenceCount":343,"citationCount":7,"influentialCitationCount":0,"publicationDate":"19/04/2023","authors":"Xiangtai Li,Henghui Ding,Wenwei Zhang,Haobo Yuan,Jiangmiao Pang,Guangliang Cheng,Kai Chen,Ziwei Liu,Chen Change Loy","id":"9570a3abed7c339ea2fa8d89ad1ee0f459e42fc1","summary":"This survey provides a thorough overview of transformer-based visual segmentation, summarizing recent advancements and summarizes a meta-architecture that unifies all recent transformer- based approaches.","score":1},{"url":"https://www.semanticscholar.org/paper/e18dd8d2dd71b56950172ca9988b3873a5c8023f","title":"Advancing Referring Expression Segmentation Beyond Single Image","venue":"arXiv.org","year":2023,"referenceCount":57,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/05/2023","authors":"Yixuan Wu,Zhao Zhang,Xie Chi,Feng Zhu,Rui Zhao","id":"e18dd8d2dd71b56950172ca9988b3873a5c8023f","summary":"This work proposes a more realistic and general setting, named Group-wise Referring Expression Segmentation (GRES), which expands RES to a collection of related images, allowing the described objects to be present in a subset of input images.","score":1},{"url":"https://www.semanticscholar.org/paper/0aee8204fbe3b88dae425eb2827f5c7d380ea630","title":"Instance-Level Semantic Maps for Vision Language Navigation","venue":"arXiv.org","year":2023,"referenceCount":26,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/05/2023","authors":"Laksh Nanwani,Anmol Agarwal,Kanishk Jain,Raghav Prabhakar,Aaron Monis,Aditya Mathur,Krishna Murthy Jatavallabhula,A. Hafez,Vineet Gandhi,K. M. Krishna","id":"0aee8204fbe3b88dae425eb2827f5c7d380ea630","summary":"This work addresses the limitation of instance-level information into spatial map representation using a community detection algorithm and by utilizing word ontology learned by large language models (LLMs) to perform open-set semantic associations in the mapping representation.","score":1},{"url":"https://www.semanticscholar.org/paper/26a2a15c16c78f586169e4768720187c1ef14f8a","title":"UP OP : U NIFIED AND P ROGRESSIVE P RUNING FOR C OMPRESSING V ISION -L ANGUAGE T RANSFORMERS","venue":"","year":null,"referenceCount":0,"citationCount":0,"influentialCitationCount":0,"publicationDate":null,"authors":"","id":"26a2a15c16c78f586169e4768720187c1ef14f8a","summary":"The Unified and Progressive Pruning (UPop) is proposed that compresses vison-language Transformers via pruning and enables zero-cost subnetwork selection after searching countless multimodal subnetworks, and the searched subnetwork can be used without any retraining.","score":1},{"url":"https://www.semanticscholar.org/paper/0d0269f8533a33c3c310fd0a59815aa16a0c47ff","title":"Perception Test : A Diagnostic Benchmark for Multimodal Models","venue":"","year":2022,"referenceCount":39,"citationCount":2,"influentialCitationCount":0,"publicationDate":2022,"authors":"Viorica Patraucean,Lucas Smaira,Ankush Gupta,Adrià Recasens Continente,L. Markeeva,Dylan,Banarse,Mateusz Malinowski,Yezhou Yang,Carl Doersch,Tatiana Matejovicova,Yury Sulsky,Antoine,Miech,Skanda Koppula,A. Fréchette,H. Klimczak,R. Koster,Junlin Zhang,Stephanie,Winkler,Y. Aytar,Simon Osindero,D. Damen,Andrew Zisserman,João Carreira","id":"0d0269f8533a33c3c310fd0a59815aa16a0c47ff","summary":"The Perception Test introduces real-world videos designed to show perceptually interesting situations and defines multiple tasks that require understanding of memory, abstract patterns, physics, and semantics – across visual, audio, and text modalities.","score":1},{"url":"https://www.semanticscholar.org/paper/363270facc8a3ff229a2688f29f16800d45092ff","title":"A Unified View of Masked Image Modeling","venue":"","year":2022,"referenceCount":57,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Rahul Desai","id":"363270facc8a3ff229a2688f29f16800d45092ff","summary":"Under the unified view, a simple yet effective method, termed as MaskDistill, which reconstructs normalized semantic features from teacher models at the masked positions, conditioning on corrupted input images, achieves comparable or superior performance than state-of-the-art methods.","score":1},{"url":"https://www.semanticscholar.org/paper/660ddea322b193c7428f2a3149ebe3d24ff9d88d","title":"Image-and-Language Understanding from Pixels Only","venue":"arXiv.org","year":2022,"referenceCount":77,"citationCount":6,"influentialCitationCount":1,"publicationDate":2022,"authors":"M. Tschannen,Basil Mustafa,N. Houlsby","id":"660ddea322b193c7428f2a3149ebe3d24ff9d88d","summary":"This work explores an additional uniﬁcation: the use of a pure pixel-based model to perform image, text, and multimodal tasks and exploits the fact that CLIPPO does not require a tokenizer to show that it can achieve strong performance on multilingual multi-modal retrieval without modi ﬁcations.","score":1},{"url":"https://www.semanticscholar.org/paper/fa717a2e31f0cef4e26921f3b147a98644d2e64c","title":"Focal Modulation Networks","venue":"Neural Information Processing Systems","year":2022,"referenceCount":118,"citationCount":33,"influentialCitationCount":6,"publicationDate":"22/03/2022","authors":"Jianwei Yang,Chunyuan Li,Jianfeng Gao","id":"fa717a2e31f0cef4e26921f3b147a98644d2e64c","summary":"Focal modulation networks (FocalNets in short), where self-attention is completely replaced by a focal modulation mechanism for modeling token interactions in vision, exhibit clear superiority on the tasks of image classification, object detection, and segmentation.","score":1},{"url":"https://www.semanticscholar.org/paper/c26bb68806a992bf4fc85b5639e1657a445c4781","title":"On the Representation Collapse of Sparse Mixture of Experts","venue":"Neural Information Processing Systems","year":2022,"referenceCount":49,"citationCount":16,"influentialCitationCount":1,"publicationDate":"20/04/2022","authors":"Zewen Chi,Li Dong,Shaohan Huang,Damai Dai,Shuming Ma,Barun Patra,Saksham Singhal,Payal Bajaj,Xia Song,Furu Wei","id":"c26bb68806a992bf4fc85b5639e1657a445c4781","summary":"This work proposes to estimate the routing scores between tokens and experts on a low-dimensional hypersphere and achieves more consistent routing than the baseline mixture-of-experts methods.","score":1},{"url":"https://www.semanticscholar.org/paper/5598c4ece8ffc69a7eb584d16f6de6629044e76a","title":"Vision Transformer Adapter for Dense Predictions","venue":"arXiv.org","year":2022,"referenceCount":107,"citationCount":76,"influentialCitationCount":11,"publicationDate":"17/05/2022","authors":"Zhe Chen,Yuchen Duan,Wenhai Wang,Junjun He,Tong Lu,Jifeng Dai,Y. Qiao","id":"5598c4ece8ffc69a7eb584d16f6de6629044e76a","summary":"The ViT-Adapter is proposed, which allows plain ViT to achieve comparable performance to vision-specific transformers and facilitate future research and is verified on multiple dense prediction tasks, including object detection, instance segmentation, and semantic segmentation.","score":1},{"url":"https://www.semanticscholar.org/paper/53ae1072fd04080e4fc2c9205ebcbc2683d7264c","title":"Sparse Mixture-of-Experts are Domain Generalizable Learners","venue":"","year":2022,"referenceCount":96,"citationCount":1,"influentialCitationCount":0,"publicationDate":"08/06/2022","authors":"Bo Li,Yifei Shen,Jingkang Yang,Yezhen Wang,Jiawei Ren,Tong Che,Jun Zhang,Ziwei Liu","id":"53ae1072fd04080e4fc2c9205ebcbc2683d7264c","summary":"A formal framework to characterize a network's robustness to distribution shifts by studying its architecture's alignment with the correlations in the dataset is developed and guided to propose a novel DG model built upon vision transformers, namely Generalizable Mixture-of-Experts (GMoE).","score":1},{"url":"https://www.semanticscholar.org/paper/e617e103269488c0dae861066ccbacc0375a0efc","title":"Bridge-Tower: Building Bridges Between Encoders in Vision-Language Representation Learning","venue":"arXiv.org","year":2022,"referenceCount":109,"citationCount":9,"influentialCitationCount":2,"publicationDate":"17/06/2022","authors":"Xiao Xu,Chenfei Wu,Shachar Rosenman,Vasudev Lal,Nan Duan","id":"e617e103269488c0dae861066ccbacc0375a0efc","summary":"This paper proposes BridgeTower, which introduces multiple bridge layers that build a connection between the top layers of uni- modal encoders and each layer of the cross-modal encoder, which enables effective bottom-up cross-Modal alignment and fusion between visual and textual representations of different semantic levels of pre-trained uni -modal Encoder.","score":1}]}
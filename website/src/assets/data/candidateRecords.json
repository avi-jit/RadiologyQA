{"papers":[{"url":"https://www.semanticscholar.org/paper/570079bbdd8758dfe865097e05719313c9c1301a","title":"LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model","venue":"arXiv.org","year":2023,"referenceCount":77,"citationCount":1,"influentialCitationCount":0,"publicationDate":"28/04/2023","authors":"Peng Gao,Jiaming Han,Renrui Zhang,Ziyi Lin,Shijie Geng,Aojun Zhou,W. Zhang,Pan Lu,Conghui He,Xiangyu Yue,Hongsheng Li,Y. Qiao","id":"570079bbdd8758dfe865097e05719313c9c1301a","summary":"This work augments LLaMA-Adapter by unlocking more learnable parameters and proposes an early fusion strategy to feed visual tokens only into the early LLM layers, contributing to better visual knowledge incorporation and achieves strong multi-modal reasoning with only a small-scale image-text and instruction dataset.","score":5},{"url":"https://www.semanticscholar.org/paper/47e3a2efde1df91a4d90a8f008f39a55f983f6bc","title":"Video ChatCaptioner: Towards Enriched Spatiotemporal Descriptions","venue":"arXiv.org","year":2023,"referenceCount":42,"citationCount":3,"influentialCitationCount":0,"publicationDate":"09/04/2023","authors":"Jun Chen,Deyao Zhu,Kilichbek Haydarov,Xiang Li,Mohamed Elhoseiny","id":"47e3a2efde1df91a4d90a8f008f39a55f983f6bc","summary":"This work introduces Video ChatCaptioner, an innovative approach for creating more comprehensive spatiotemporal video descriptions, specifically designed to select frames for posing video content-driven questions and shows promise as a method for enhancing video content.","score":5},{"url":"https://www.semanticscholar.org/paper/7e32aac43e9f1df49e116add03327ee6f365dbf3","title":"mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality","venue":"arXiv.org","year":2023,"referenceCount":32,"citationCount":1,"influentialCitationCount":0,"publicationDate":"27/04/2023","authors":"Qinghao Ye,Haiyang Xu,Guohai Xu,Jiabo Ye,Ming Yan,Yi Zhou,Junyan Wang,Anwen Hu,Pengcheng Shi,Yaya Shi,Chenliang Li,Yuanhong Xu,Hehong Chen,Junfeng Tian,Qiang Qi,J. Zhang,Feiyan Huang","id":"7e32aac43e9f1df49e116add03327ee6f365dbf3","summary":null,"score":4},{"url":"https://www.semanticscholar.org/paper/4c746d8a8368dbea13c7eb7cccfd5d0cc15edb31","title":"Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":59,"citationCount":4,"influentialCitationCount":1,"publicationDate":"19/04/2023","authors":"Pan Lu,Baolin Peng,Hao Cheng,Michel Galley,Kai-Wei Chang,Y. Wu,Song-Chun Zhu,Jianfeng Gao","id":"4c746d8a8368dbea13c7eb7cccfd5d0cc15edb31","summary":"Chameleon is introduced, a plug-and-play compositional reasoning framework that augments LLMs to help address these challenges and showcases the adaptability and effectiveness of Chameleon on two tasks: ScienceQA and TabMWP.","score":3},{"url":"https://www.semanticscholar.org/paper/1a8eb2cae1833df3bf12fe3b41b03d60b4a4a98d","title":"Visual Instruction Tuning","venue":"arXiv.org","year":2023,"referenceCount":55,"citationCount":7,"influentialCitationCount":3,"publicationDate":"17/04/2023","authors":"Haotian Liu,Chunyuan Li,Qingyang Wu,Yong Jae Lee","id":"1a8eb2cae1833df3bf12fe3b41b03d60b4a4a98d","summary":"This paper presents LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding and introduces GPT-4 generated visual instruction tuning data, the model and code base publicly available.","score":3},{"url":"https://www.semanticscholar.org/paper/e9480d62e216f77d5556b7eda769daa4c92d004d","title":"VQAMix: Conditional Triplet Mixup for Medical Visual Question Answering","venue":"IEEE Transactions on Medical Imaging","year":2022,"referenceCount":55,"citationCount":6,"influentialCitationCount":2,"publicationDate":"21/06/2022","authors":"Haifan Gong,Guanqi Chen,Mingzhi Mao,Z. Li,Guanbin Li","id":"e9480d62e216f77d5556b7eda769daa4c92d004d","summary":"This paper proposes a simple yet effective data augmentation method, VQAMix, which generates more labeled training samples by linearly combining a pair of VQA samples, which can be easily embedded into any visual-language model to boost performance.","score":3},{"url":"https://www.semanticscholar.org/paper/ac4d13b6a4f9fb67337099f4602135a0351f5c99","title":"Towards Medical Artificial General Intelligence via Knowledge-Enhanced Multimodal Pretraining","venue":"arXiv.org","year":2023,"referenceCount":60,"citationCount":0,"influentialCitationCount":0,"publicationDate":"26/04/2023","authors":"Bingqian Lin,Zicong Chen,Mingjie Li,Haokun Lin,Hang Xu,Yi Zhu,Jian-zhuo Liu,Wenjia Cai,Lei Yang,Shen Zhao,Chenfei Wu,Ling Chen,Xiaojun Chang,Yi Yang,L. Xing,Xiaodan Liang","id":"ac4d13b6a4f9fb67337099f4602135a0351f5c99","summary":"The proposed MOTOR successfully mimics the human practice of fulfilling a\"medical student\" to accelerate the process of becoming a\"specialist\" and believes that this work makes a significant stride in realizing MAGI.","score":3},{"url":"https://www.semanticscholar.org/paper/ac7771c332da42b29a913b116bd6ef622cbf89cf","title":"TaskMatrix.AI: Completing Tasks by Connecting Foundation Models with Millions of APIs","venue":"arXiv.org","year":2023,"referenceCount":27,"citationCount":9,"influentialCitationCount":1,"publicationDate":"29/03/2023","authors":"Yaobo Liang,Chenfei Wu,Ting Song,Wenshan Wu,Yan Xia,Yu Liu,Yangyiwen Ou,Shuai Lu,Lei Ji,Shaoguang Mao,Yun Wang,Linjun Shou,Ming Gong,Nan Duan","id":"ac7771c332da42b29a913b116bd6ef622cbf89cf","summary":"The vision of how to build such an ecosystem is presented, each key component is explained, and study cases are used to illustrate both the feasibility of this vision and the main challenges the authors need to address next.","score":2},{"url":"https://www.semanticscholar.org/paper/01f9b773408115a16fe872147348db175789e82f","title":"Tool Learning with Foundation Models","venue":"arXiv.org","year":2023,"referenceCount":224,"citationCount":3,"influentialCitationCount":0,"publicationDate":"17/04/2023","authors":"Yujia Qin,Shengding Hu,Yankai Lin,Weize Chen,Ning Ding,Ganqu Cui,Zheni Zeng,Yufei Huang,Chaojun Xiao,Chi Han,Y. Fung,Yusheng Su,Huadong Wang,Cheng Qian,Runchu Tian,Kunlun Zhu,Shi Liang,Xingyu Shen,Bokai Xu,Zhen Zhang,Yining Ye,Bo Li,Ziwei Tang,Jing Yi,Yu Zhu,Zhenning Dai,Lan Yan,Xin Cong,Ya-Ting Lu,Weilin Zhao,Yuxiang Huang,Jun-Han Yan,Xu Han,Xian Sun,Dahai Li,Jason Phang,Cheng Yang,Tongshuang Wu,Heng Ji,Zhiyuan Liu,Maosong Sun","id":"01f9b773408115a16fe872147348db175789e82f","summary":"A systematic investigation of tool learning is presented, including its cognitive origins, the paradigm shift of foundation models, and the complementary roles of tools and models to inspire future research in integrating tools with foundation models.","score":2},{"url":"https://www.semanticscholar.org/paper/bf54ccf6e5c9a7da47a0909471002881913f02ba","title":"HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in HuggingFace","venue":"arXiv.org","year":2023,"referenceCount":25,"citationCount":22,"influentialCitationCount":5,"publicationDate":"30/03/2023","authors":"Yongliang Shen,Kaitao Song,Xu Tan,D. Li,Weiming Lu,Y. Zhuang","id":"bf54ccf6e5c9a7da47a0909471002881913f02ba","summary":"HuggingGPT is a framework that leverages LLMs (e.g., ChatGPT) to connect various AI models in machine learning communities to solve AI tasks and is able to cover numerous sophisticated AI tasks in different modalities and domains and achieve impressive results in language, vision, speech, and other challenging tasks.","score":2},{"url":"https://www.semanticscholar.org/paper/7562e25b666cba841b1dd5cf6e700978922beb04","title":"SkinGPT: A Dermatology Diagnostic System with Vision Large Language Model","venue":"arXiv.org","year":2023,"referenceCount":35,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/04/2023","authors":"Juexiao Zhou,Xin Gao","id":"7562e25b666cba841b1dd5cf6e700978922beb04","summary":"SkinGPT is the first dermatology diagnostic system that utilizes an advanced vision-based large language model, incorporating a fine-tuned version of MiniGPT-4 with a vast collection of in-house skin disease images, accompanied by doctor's notes.","score":2},{"url":"https://www.semanticscholar.org/paper/c56a51728678e5b2e3ff95e51caf21d267439c36","title":"ChatVideo: A Tracklet-centric Multimodal and Versatile Video Understanding System","venue":"arXiv.org","year":2023,"referenceCount":43,"citationCount":0,"influentialCitationCount":0,"publicationDate":"27/04/2023","authors":"Junke Wang,Dongdong Chen,Chong Luo,Xiyang Dai,Lu Yuan,Zuxuan Wu,Yu-Gang Jiang","id":"c56a51728678e5b2e3ff95e51caf21d267439c36","summary":"The vision for multimodal and versatile video understanding is presented and a prototype system, built upon a tracklet-centric paradigm, which treats tracklets as the basic video unit and employs various Video Foundation Models to annotate their properties e.g., appearance, motion, etc.","score":2},{"url":"https://www.semanticscholar.org/paper/de8121dc3d2c69bdab172f37e31168ddf2e6e62f","title":"Segment Everything Everywhere All at Once","venue":"arXiv.org","year":2023,"referenceCount":65,"citationCount":5,"influentialCitationCount":2,"publicationDate":"13/04/2023","authors":"Xueyan Zou,Jianwei Yang,Hao Zhang,Feng Li,Linjie Li,Jianfeng Gao,Yong Jae Lee","id":"de8121dc3d2c69bdab172f37e31168ddf2e6e62f","summary":"SEEM is a promptable, interactive model for Segmenting Everything Everywhere all at once in an image by incorporating learnable memory prompts to retain dialog history information via mask-guided cross-attention.","score":2},{"url":"https://www.semanticscholar.org/paper/508d9b43832790b4d35f4ae1fa76e9712859d6aa","title":"Vision and Structured-Language Pretraining for Cross-Modal Food Retrieval","venue":"","year":2022,"referenceCount":85,"citationCount":0,"influentialCitationCount":0,"publicationDate":"08/12/2022","authors":"Mustafa Shukor,Nicolas Thome,M. Cord","id":"508d9b43832790b4d35f4ae1fa76e9712859d6aa","summary":"VLPCook outperforms current SoTA by a significant margin on the task of Cross-Modal Food Retrieval on the large Recipe1M dataset and validates the generalization of the approach to other tasks and domains with structured text such as the Medical domain on the ROCO dataset.","score":2},{"url":"https://www.semanticscholar.org/paper/1329484d20c470b84e46ed7453786cee0acad2e0","title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":43,"citationCount":59,"influentialCitationCount":14,"publicationDate":"30/01/2023","authors":"Junnan Li,Dongxu Li,S. Savarese,Steven Hoi","id":"1329484d20c470b84e46ed7453786cee0acad2e0","summary":"BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods, and is demonstrated's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.","score":2},{"url":"https://www.semanticscholar.org/paper/fbfef4723d8c8467d7bd523e1d0b703cce0e0f9c","title":"Language Is Not All You Need: Aligning Perception with Language Models","venue":"arXiv.org","year":2023,"referenceCount":67,"citationCount":25,"influentialCitationCount":2,"publicationDate":"27/02/2023","authors":"Shaohan Huang,Li Dong,Wenhui Wang,Y. Hao,Saksham Singhal,Shuming Ma,Tengchao Lv,Lei Cui,O. Mohammed,Qiang Liu,Kriti Aggarwal,Zewen Chi,Johan Bjorck,Vishrav Chaudhary,Subhojit Som,Xia Song,Furu Wei","id":"fbfef4723d8c8467d7bd523e1d0b703cce0e0f9c","summary":"This work introduces Kosmos-1, a Multimodal Large Language Model (MLLM) that can perceive general modalities, learn in context, and follow instructions, and shows that MLLMs can benefit from cross-modal transfer, i.e., transfer knowledge from language to multimodals, and from multimodal to language.","score":2},{"url":"https://www.semanticscholar.org/paper/69cfdc8df16ae63b7acba4ac6f727f78b86893c3","title":"ChatGPT Asks, BLIP-2 Answers: Automatic Questioning Towards Enriched Visual Descriptions","venue":"arXiv.org","year":2023,"referenceCount":51,"citationCount":4,"influentialCitationCount":0,"publicationDate":"12/03/2023","authors":"Deyao Zhu,Jun Chen,Kilichbek Haydarov,Xiaoqian Shen,Wenxuan Zhang,Mohamed Elhoseiny","id":"69cfdc8df16ae63b7acba4ac6f727f78b86893c3","summary":"This paper introduces ChatCaptioner, a novel automatic-questioning method deployed in image captioning that identifies 53% more objects within the image than BLIP-2 alone measured by WordNet synset matching.","score":2},{"url":"https://www.semanticscholar.org/paper/9d12916dd46df7a6446cbec0bc4d054f7dafcdab","title":"Scaling Vision-Language Models with Sparse Mixture of Experts","venue":"arXiv.org","year":2023,"referenceCount":74,"citationCount":0,"influentialCitationCount":0,"publicationDate":"13/03/2023","authors":"Sheng Shen,Z. Yao,Chunyuan Li,Trevor Darrell,K. Keutzer,Yuxiong He","id":"9d12916dd46df7a6446cbec0bc4d054f7dafcdab","summary":"The effectiveness of MoE in scaling vision-language models is explored, demonstrating its potential to achieve state-of-the-art performance on a range of benchmarks over dense models of equivalent computational cost.","score":2},{"url":"https://www.semanticscholar.org/paper/4396e30f28eb49bb07c63cf62ca90415ebbe43d4","title":"IRGen: Generative Modeling for Image Retrieval","venue":"arXiv.org","year":2023,"referenceCount":110,"citationCount":0,"influentialCitationCount":0,"publicationDate":"17/03/2023","authors":"Yidan Zhang,Ting Zhang,Dong Chen,Yujing Wang,Qi Chen,Xingxu Xie,Hao Sun,Weiwei Deng,Qi Zhang,Fan Yang,Mao Yang,Q. Liao,B. Guo","id":"4396e30f28eb49bb07c63cf62ca90415ebbe43d4","summary":"The framework, IRGen, is a unified model that enables end-to-end differentiable search, thus achieving superior performance thanks to direct optimization and tackling the key technical challenge of converting an image into quite a short sequence of semantic units in order to enable efficient and effective retrieval.","score":2},{"url":"https://www.semanticscholar.org/paper/3049c992adbd56e29c4d957ee0c4e9d05fe3c6d1","title":"EVA-02: A Visual Representation for Neon Genesis","venue":"arXiv.org","year":2023,"referenceCount":146,"citationCount":2,"influentialCitationCount":1,"publicationDate":"20/03/2023","authors":"Yuxin Fang,Quan Sun,Xinggang Wang,Tiejun Huang,Xinlong Wang,Yue Cao","id":"3049c992adbd56e29c4d957ee0c4e9d05fe3c6d1","summary":"EVA-02, a next-generation Transformer-based visual representation pre-trained to reconstruct strong and robust language-aligned vision features via masked image modeling, is launched, demonstrating superior performance compared to prior state-of-the-art approaches across various representative vision tasks, while utilizing significantly fewer parameters and compute budgets.","score":2},{"url":"https://www.semanticscholar.org/paper/d064075c47e358f604034d06df4b985356757c71","title":"Equivariant Similarity for Vision-Language Foundation Models","venue":"arXiv.org","year":2023,"referenceCount":73,"citationCount":0,"influentialCitationCount":0,"publicationDate":"25/03/2023","authors":"Tan Wang,Kevin Lin,Linjie Li,Chung-Ching Lin,Zhengyuan Yang,Hanwang Zhang,Zicheng Liu,Lijuan Wang","id":"d064075c47e358f604034d06df4b985356757c71","summary":"This study proposes EqSim, a regularization loss that can be efficiently calculated from any two matched training pairs and easily pluggable into existing image-text retrieval fine-tuning and presents a new challenging benchmark EqBen, the first to focus on \"visual-minimal change\".","score":2},{"url":"https://www.semanticscholar.org/paper/b259d853b71a2d03cefa844bb9343b8e3ed816b1","title":"LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention","venue":"arXiv.org","year":2023,"referenceCount":62,"citationCount":10,"influentialCitationCount":0,"publicationDate":"28/03/2023","authors":"Renrui Zhang,Jiaming Han,Aojun Zhou,Xiangfei Hu,Shilin Yan,Pan Lu,Hongsheng Li,Peng Gao,Y. Qiao","id":"b259d853b71a2d03cefa844bb9343b8e3ed816b1","summary":"A zero-init attention mechanism with zero gating is proposed, which adaptively injects the new instructional cues into LLaMA, while effectively preserves its pre-trained knowledge.","score":2},{"url":"https://www.semanticscholar.org/paper/c84e2801512069acbc63f1a7f73273281939428c","title":"A Study of Autoregressive Decoders for Multi-Tasking in Computer Vision","venue":"arXiv.org","year":2023,"referenceCount":98,"citationCount":0,"influentialCitationCount":0,"publicationDate":"30/03/2023","authors":"L. Beyer,Bo Wan,Gagan Madan,Filip Pavetic,A. Steiner,Alexander Kolesnikov,André Susano Pinto,Emanuele Bugliarello,Xiao Wang,Qihang Yu,Liang-Chieh Chen,Xiaohua Zhai","id":"c84e2801512069acbc63f1a7f73273281939428c","summary":"This work takes a close look at autoregressive decoders for multi-task learning in multimodal computer vision, including classification, captioning, visual question answering, and optical character recognition and compares these to well-tuned single-task baselines to highlight the cost incurred by multi-tasking.","score":2},{"url":"https://www.semanticscholar.org/paper/db1c83ef73d2f7731b0dd255835f2f26db749e17","title":"Not All Features Matter: Enhancing Few-shot CLIP with Adaptive Prior Refinement","venue":"arXiv.org","year":2023,"referenceCount":100,"citationCount":2,"influentialCitationCount":0,"publicationDate":"03/04/2023","authors":"Xiang-yu Zhu,Renrui Zhang,Bowei He,A-Long Zhou,D. Wang,Bingyan Zhao,Peng Gao","id":"db1c83ef73d2f7731b0dd255835f2f26db749e17","summary":"This paper proposes APE, an Adaptive Prior rEfinement method for CLIP's pre-trained knowledge, which achieves superior accuracy with high computational efficiency and introduces two model variants, a training-free APE and aTraining-required APE-T.","score":2},{"url":"https://www.semanticscholar.org/paper/a43a3fadc9190e61b34f59a913f1716e443519e4","title":"On the Opportunities and Challenges of Foundation Models for Geospatial Artificial Intelligence","venue":"arXiv.org","year":2023,"referenceCount":158,"citationCount":4,"influentialCitationCount":0,"publicationDate":"13/04/2023","authors":"Gengchen Mai,Weiming Huang,Jin Sun,Suhang Song,Deepak Mishra,Ninghao Liu,Song Gao,Tianming Liu,G. Cong,Yingjie Hu,Chris Cundy,Ziyuan Li,Rui Zhu,Ni Lao","id":"a43a3fadc9190e61b34f59a913f1716e443519e4","summary":"It is proposed that one of the major challenges of developing a FM for GeoAI is to address the multimodality nature of geospatial tasks and the possibility of a multimodal foundation model which can reason over various types ofGeoAI data through geosp spatial alignments is suggested.","score":2},{"url":"https://www.semanticscholar.org/paper/b7d73f22d861f526541575a3b17449bd3c58ca74","title":"MVP-SEG: Multi-View Prompt Learning for Open-Vocabulary Semantic Segmentation","venue":"arXiv.org","year":2023,"referenceCount":56,"citationCount":0,"influentialCitationCount":0,"publicationDate":"14/04/2023","authors":"Jie Guo,Qimeng Wang,Yan Gao,Xiaolong Jiang,Xu Tang,Yao Hu,Baochang Zhang","id":"b7d73f22d861f526541575a3b17449bd3c58ca74","summary":"Experiments show that the multi-view prompts learned from seen categories have strong generalization to unseen categories, and MVP-SEG+ which combines the knowledge transfer stage significantly outperforms previous methods on several benchmarks, justifying that MVP- SEG does lead to better focus on different local parts.","score":2},{"url":"https://www.semanticscholar.org/paper/d183cc170400e43535c5e2c37121c37ee0ba23dc","title":"Open-Ended Medical Visual Question Answering Through Prefix Tuning of Language Models","venue":"arXiv.org","year":2023,"referenceCount":33,"citationCount":1,"influentialCitationCount":1,"publicationDate":"10/03/2023","authors":"Tom van Sonsbeek,Mohammad Mahdi Derakhshani,Ivona Najdenkoska,Cees G. M. Snoek,M. Worring","id":"d183cc170400e43535c5e2c37121c37ee0ba23dc","summary":"This work focuses on open-ended VQA and motivated by the recent advances in language models consider it as a generative task, and introduces a novel method particularly suited for small, domain-specific, medical datasets.","score":2},{"url":"https://www.semanticscholar.org/paper/456a70485aafc12dfed4fb7354668d72aae9b658","title":"SecureBERT: A Domain-Specific Language Model for Cybersecurity","venue":"","year":2022,"referenceCount":33,"citationCount":1,"influentialCitationCount":0,"publicationDate":"06/04/2022","authors":"Ehsan Aghaei,Xi Niu,W. Shadid,E. Al-Shaer","id":"456a70485aafc12dfed4fb7354668d72aae9b658","summary":"This paper proposes SecureBERT, a cybersecurity language model capable of capturing text connotations in cybersecurity text and therefore successful in automation for many critical cybersecurity tasks that would otherwise rely on human expertise and time-consuming manual procedures.","score":2},{"url":"https://www.semanticscholar.org/paper/5d8fd04c436367b18b35e28332ee8e452a477f3f","title":"Medical Image Understanding with Pretrained Vision Language Models: A Comprehensive Study","venue":"arXiv.org","year":2022,"referenceCount":53,"citationCount":5,"influentialCitationCount":0,"publicationDate":"30/09/2022","authors":"Ziyuan Qin,Huahui Yi,Qicheng Lao,Kang Li","id":"5d8fd04c436367b18b35e28332ee8e452a477f3f","summary":"It is shown that well-designed medical prompts are the key to elicit knowledge from pre-trained VLMs, and by prompting with expressive attributes that are shared between domains, the VLM can carry the knowledge across domains and improve its generalization.","score":2},{"url":"https://www.semanticscholar.org/paper/131c6f328c11706de2c43cd16e0b7c5d5e610b6a","title":"Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond","venue":"arXiv.org","year":2023,"referenceCount":127,"citationCount":2,"influentialCitationCount":0,"publicationDate":"26/04/2023","authors":"Jingfeng Yang,Hongye Jin,Ruixiang Tang,Xiaotian Han,Qizhang Feng,Haoming Jiang,Bing Yin,Xia Hu","id":"131c6f328c11706de2c43cd16e0b7c5d5e610b6a","summary":"A comprehensive and practical guide for practitioners and end-users working with Large Language Models in their downstream natural language processing (NLP) tasks, enabling the successful implementation of these models in a wide range of NLP tasks.","score":2},{"url":"https://www.semanticscholar.org/paper/052a5e2bcc999810ee6f1eedcf758c528e4f125f","title":"Retrieving Multimodal Information for Augmented Generation: A Survey","venue":"arXiv.org","year":2023,"referenceCount":150,"citationCount":1,"influentialCitationCount":0,"publicationDate":"20/03/2023","authors":"Ruochen Zhao,Hailin Chen,Weishi Wang,Fangkai Jiao,Xu Do,Chengwei Qin,Bosheng Ding,Xiaobao Guo,Minzhi Li,Xingxuan Li,Shafiq R. Joty","id":"052a5e2bcc999810ee6f1eedcf758c528e4f125f","summary":"This survey provides an in-depth review of retrieval-augmented generation in different modalities and discusses potential future directions of this emerging field.","score":2},{"url":"https://www.semanticscholar.org/paper/357fc385caf2e5b9898c9140fa3ac9955e6bb3c6","title":"TeamS at VQA-Med 2021: BBN-Orchestra for Long-tailed Medical Visual Question Answering","venue":"Conference and Labs of the Evaluation Forum","year":2021,"referenceCount":13,"citationCount":6,"influentialCitationCount":0,"publicationDate":2021,"authors":"Sedigheh Eslami,Gerard de Melo,C. Meinel","id":"357fc385caf2e5b9898c9140fa3ac9955e6bb3c6","summary":"The proposed BBN-Orchestra is an ensemble of bilateral-branch networks (BBN) and successfully reduces overfitting to train and validation data in addition to effectively modeling the imbalanced long-tailed image distribution.","score":2},{"url":"https://www.semanticscholar.org/paper/3c83f80f06633ff4598d33c2959f8e4cdcad3e93","title":"Cross-Modal Self-Attention with Multi-Task Pre-Training for Medical Visual Question Answering","venue":"International Conference on Multimedia Retrieval","year":2021,"referenceCount":31,"citationCount":26,"influentialCitationCount":1,"publicationDate":"01/05/2021","authors":"Haifan Gong,Guanqi Chen,Sishuo Liu,Yizhou Yu,Guanbin Li","id":"3c83f80f06633ff4598d33c2959f8e4cdcad3e93","summary":"This work reformulates image feature pre-training as a multi-task learning paradigm and witness its extraordinary superiority, forcing it to take into account the applicability of features for the specific image comprehension task.","score":2},{"url":"https://www.semanticscholar.org/paper/97af09b1436e768019aed4023cd1f9e3ccb9a635","title":"Medical Visual Question Answering: A Survey","venue":"arXiv.org","year":2021,"referenceCount":96,"citationCount":12,"influentialCitationCount":1,"publicationDate":"19/11/2021","authors":"Zhihong Lin,Donghao Zhang,Qingyi Tao,Danli Shi,Gholamreza Haffari,Qi Wu,M. He,Z. Ge","id":"97af09b1436e768019aed4023cd1f9e3ccb9a635","summary":"This research presents a novel and scalable approaches to integrate 3D image recognition and 3D speech recognition into the clinical practice of ophthalmology.","score":2},{"url":"https://www.semanticscholar.org/paper/8c9a9a1bbba2a3e3bab34bce533b3b2acfda32b0","title":"Medical visual question answering based on question-type reasoning and semantic space constraint","venue":"Artif. Intell. Medicine","year":2022,"referenceCount":57,"citationCount":2,"influentialCitationCount":0,"publicationDate":"01/06/2022","authors":"Meiling Wang,Xiaohai He,Luping Liu,L. Qing,Honggang Chen,Yan Liu,Chao Ren","id":"8c9a9a1bbba2a3e3bab34bce533b3b2acfda32b0","summary":"A novel Med-VQA framework is proposed to alleviate the above-mentioned problems, which employed a question-type reasoning module severally to closed-ended and open-ended questions, thereby extracting the important information contained in the questions through an attention mechanism and filtering the noise to extract more valuable question features.","score":2},{"url":"https://www.semanticscholar.org/paper/2ac3bacbbee520b701707ebcf7b9ca7a3f233129","title":"Medical visual question answering via corresponding feature fusion combined with semantic attention.","venue":"Mathematical biosciences and engineering : MBE","year":2022,"referenceCount":15,"citationCount":2,"influentialCitationCount":0,"publicationDate":"20/07/2022","authors":"Han Zhu,Xiaohai He,Meiling Wang,Mozhi Zhang,L. Qing","id":"2ac3bacbbee520b701707ebcf7b9ca7a3f233129","summary":"A corresponding feature fusion (CFF) method to strengthen the interactions of specific features from corresponding radiology images and questions and a semantic attention (SA) module for textual feature extraction is proposed.","score":2},{"url":"https://www.semanticscholar.org/paper/ef2edea434e487f288d4eed6f9b1dc480b917211","title":"Adversarial Learning to Improve Question Image Embedding in Medical Visual Question Answering","venue":"Moratuwa Engineering Research Conference","year":2022,"referenceCount":20,"citationCount":0,"influentialCitationCount":0,"publicationDate":"27/07/2022","authors":"Kaveesha Silva,Thanuja Maheepala,Kasun Tharaka,Thanuja D. Ambegoda","id":"ef2edea434e487f288d4eed6f9b1dc480b917211","summary":"A new method for training VQA models that utilizes adversarial learning to improve the question-image embedding and demonstrates how this embedding can be used as the ideal embedding for answer inference.","score":2},{"url":"https://www.semanticscholar.org/paper/b047b3b7d76b79958e23b0fcab985be22b1ce42d","title":"Alternating Cross-attention Vision-Language Model for Efficient Learning with Medical Image and Report without Curation","venue":"arXiv.org","year":2022,"referenceCount":47,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Sangjoon Park,Eunha Lee,Jeonghyeon Lee,Jong-Chul Ye","id":"b047b3b7d76b79958e23b0fcab985be22b1ce42d","summary":"It is experimentally demonstrated that the pre-trained MAX-VL model outperforms the current state-of-the-art vision language models in various vision-language tasks and suggested the clinical utility for the diagnosis of newly emerging diseases and human error detection as well as showed the widespread applicability of the model in different domain data.","score":2},{"url":"https://www.semanticscholar.org/paper/79478a2ac67b9fdbeadcde13faa2d84eb239e080","title":"Vision-Language Pretraining Enables Radiographs and Reports to be Learned without Curation","venue":"","year":2022,"referenceCount":50,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Sangjoon Park,Eunha Lee,Jeonghyeon Lee,Jong-Chul Ye","id":"79478a2ac67b9fdbeadcde13faa2d84eb239e080","summary":"It is experimentally demonstrated that the pre-trained medical X-VL model outperforms the current state-of-the-art models in various vision-language tasks in medical domains, which suggests the potential of the model for widespread applicability in different medical applications.","score":2},{"url":"https://www.semanticscholar.org/paper/2441230bd2f3cca924d597b3044ad63aaff269ec","title":"Self-supervised Co-learning of Uncurated Images and Reports Enables Oversight AI in Radiology","venue":"","year":2022,"referenceCount":49,"citationCount":4,"influentialCitationCount":0,"publicationDate":2022,"authors":"Sangjoon Park,Eunha Lee,K. Shin,Jeonghyeon Lee,Jong-Chul Ye","id":"2441230bd2f3cca924d597b3044ad63aaff269ec","summary":"A self-supervised model tailored for efficient vision-language pre-training that exploits cross attention in the radiological images and - trained medical X-VL model outperforms the current state-of-the-art models in various vision- language tasks in medical domains.","score":2},{"url":"https://www.semanticscholar.org/paper/0cbd644254462341a897d4bfa0134637662c3ab5","title":"A Transformer-based Medical Visual Question Answering Model","venue":"International Conference on Pattern Recognition","year":2022,"referenceCount":25,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/08/2022","authors":"Lei Liu,Xiangdong Su,Hui Guo,Daobin Zhu","id":"0cbd644254462341a897d4bfa0134637662c3ab5","summary":"Experimental results demonstrate that the Transformer structure not only ensures the stability of the model performance, but also accelerates its convergence, and the MQAT model outperforms the existing state-of-the-art methods.","score":2},{"url":"https://www.semanticscholar.org/paper/56d8d9fff399f798da97a69e891de4eeb4568d4f","title":"MHKD-MVQA: Multimodal Hierarchical Knowledge Distillation for Medical Visual Question Answering","venue":"IEEE International Conference on Bioinformatics and Biomedicine","year":2022,"referenceCount":49,"citationCount":1,"influentialCitationCount":0,"publicationDate":"06/12/2022","authors":"Jianfeng Wang,Shuokang Huang,Huifang Du,Yu Qin,Haofen Wang,Wenqiang Zhang","id":"56d8d9fff399f798da97a69e891de4eeb4568d4f","summary":"This work proposes multimodal hierarchical knowledge distillation for medical VQA (MHKD-MVQA), which distill knowledge from not only the output but also the intermediate layers, which leverages the knowledge from limited samples to a greater extent and achieves state-of-the-art performance.","score":2},{"url":"https://www.semanticscholar.org/paper/5942335fdd35d1651aaabd7af4db129a29ed2a85","title":"How Well Apply Multimodal Mixup and Simple MLPs Backbone to Medical Visual Question Answering?","venue":"IEEE International Conference on Bioinformatics and Biomedicine","year":2022,"referenceCount":26,"citationCount":0,"influentialCitationCount":0,"publicationDate":"06/12/2022","authors":"Lei Liu,Xiangdong Su","id":"5942335fdd35d1651aaabd7af4db129a29ed2a85","summary":"This paper designs a Med-VQA model which employs multi-layer perceptrons (MLPs) as the backbone network for feature extraction and modal fusion and designs a multimodal mixup (M-Mixup) to augment images and questions separately, which effectively alleviates the problem of insufficient training samples in the Med- VQA task.","score":2},{"url":"https://www.semanticscholar.org/paper/a627232a97a7a63f8399d157f0b022eb1ccd547c","title":"Biomedical Question Answering: A Survey of Approaches and Challenges","venue":"ACM Computing Surveys","year":2021,"referenceCount":246,"citationCount":33,"influentialCitationCount":1,"publicationDate":"10/02/2021","authors":"Qiao Jin,Zheng Yuan,Guangzhi Xiong,Qian Yu,Huaiyuan Ying,Chuanqi Tan,Mosha Chen,Songfang Huang,Xiaozhong Liu,Sheng Yu","id":"a627232a97a7a63f8399d157f0b022eb1ccd547c","summary":"This survey identifies and characterize several key challenges in BQA that might lead to this issue, and discusses some potential future directions to explore.","score":2},{"url":"https://www.semanticscholar.org/paper/940f303c2530a52c5fd3c52c9c64ceea4b53ab05","title":"Diversity Learning Based on Multi-Latent Space for Medical Image Visual Question Generation","venue":"Italian National Conference on Sensors","year":2023,"referenceCount":59,"citationCount":0,"influentialCitationCount":0,"publicationDate":"17/01/2023","authors":"He Zhu,Ren Togo,Takahiro Ogawa,M. Haseyama","id":"940f303c2530a52c5fd3c52c9c64ceea4b53ab05","summary":"A diversity learning-based visual question generation model using a multi-latent space to generate informative question sets from medical images that works with an answering model for interactive automated clinical diagnosis and generates datasets to replace the process of annotation that incurs huge labor costs.","score":2},{"url":"https://www.semanticscholar.org/paper/8200be2e8b9af243ee72a9d919a4f7fbe82a17d2","title":"Medical knowledge-based network for Patient-oriented Visual Question Answering","venue":"Information Processing & Management","year":2023,"referenceCount":37,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/03/2023","authors":"Jian Huang,Yihao Chen,Yong Li,Zhenguo Yang,Xuehao Gong,Fuhui Wang,Xiaohong Xu,Wenyin Liu","id":"8200be2e8b9af243ee72a9d919a4f7fbe82a17d2","summary":"","score":2},{"url":"https://www.semanticscholar.org/paper/17ca48ad1b944c897863f04ba9ffa72674dce1ce","title":"Parallel multi-head attention and term-weighted question embedding for medical visual question answering","venue":"Multimedia tools and applications","year":2023,"referenceCount":56,"citationCount":0,"influentialCitationCount":0,"publicationDate":"11/03/2023","authors":"Sruthy Manmadhan,Binsu C. Kovoor","id":"17ca48ad1b944c897863f04ba9ffa72674dce1ce","summary":"The proposed MaMVQA model achieved significantly increased accuracy in predicting answers to both close-ended and open-ended questions and outperforms previous state-of-the-art methods in terms of accuracy while requiring no external data to train the model.","score":2},{"url":"https://www.semanticscholar.org/paper/4cf4528e3b19a22bcfb041d09be53bd3095bcef8","title":"Q2ATransformer: Improving Medical VQA via an Answer Querying Decoder","venue":"arXiv.org","year":2023,"referenceCount":15,"citationCount":0,"influentialCitationCount":0,"publicationDate":"04/04/2023","authors":"Yunyi Liu,Zhanyu Wang,Dong Xu,Luping Zhou","id":"4cf4528e3b19a22bcfb041d09be53bd3095bcef8","summary":"This paper proposes a new Transformer based framework for medical VQA (named as Q2ATransformer), which integrates the advantages of both the classification and the generation approaches and provides a unified treatment for the close-end and open-end questions.","score":2},{"url":"https://www.semanticscholar.org/paper/385376b8aa48c25403f17d6206db7c09b67e1314","title":"Prompt Engineering for Healthcare: Methodologies and Applications","venue":"arXiv.org","year":2023,"referenceCount":105,"citationCount":0,"influentialCitationCount":0,"publicationDate":"28/04/2023","authors":"Jiaqi Wang,Enze Shi,Sigang Yu,Zihao Wu,Chong Ma,Haixing Dai,Qiushi Yang,Yanqing Kang,Jinru Wu,Huawen Hu,Chenxi Yue,Haiyang Zhang,Yi-Hsueh Liu,Xiang Li,Bao Ge,Dajiang Zhu,Yixuan Yuan,Dinggang Shen,Tianming Liu,Shu Zhang","id":"385376b8aa48c25403f17d6206db7c09b67e1314","summary":"This review will introduce the latest advances in prompt engineering in the field of natural language processing (NLP) for the medical domain and highlight its significant contributions to healthcare NLP applications such as question-answering systems, text summarization, and machine translation.","score":1},{"url":"https://www.semanticscholar.org/paper/e62937949aa18caeebaf1263ef5d86d447acbc6e","title":"Pre-trained Language Models in Biomedical Domain: A Systematic Survey","venue":"arXiv.org","year":2021,"referenceCount":379,"citationCount":22,"influentialCitationCount":3,"publicationDate":"11/10/2021","authors":"Benyou Wang,Qianqian Xie,Jiahuan Pei,P. Tiwari,Zhao Li,Jie Fu","id":"e62937949aa18caeebaf1263ef5d86d447acbc6e","summary":"The recent progress of pre-trained language models in the biomedical domain and their applications in biomedical downstream tasks are summarized and a taxonomy of existing biomedical PLMs is proposed.","score":1},{"url":"https://www.semanticscholar.org/paper/e3c70b0b71b51872bbdaa0f4bf2b56908f97abec","title":"MedKLIP: Medical Knowledge Enhanced Language-Image Pre-Training","venue":"medRxiv","year":2023,"referenceCount":67,"citationCount":5,"influentialCitationCount":1,"publicationDate":"05/01/2023","authors":"Chaoyi Wu,Xiaoman Zhang,Ya Zhang,Yanfeng Wang,Weidi Xie","id":"e3c70b0b71b51872bbdaa0f4bf2b56908f97abec","summary":"This paper adopts a novel report filter to extract the medical entities, and proposes a novel entity embedding module by querying an external knowledge description base to exploit the rich context of additional information that the medical domain affords, and implicitly build relationships between entities in the language embedding space.","score":1},{"url":"https://www.semanticscholar.org/paper/10a8e7a7e07256178665f90074c5c41b071e73d3","title":"MDF-Net: Multimodal Dual-Fusion Network for Abnormality Detection using CXR Images and Clinical Data","venue":"arXiv.org","year":2023,"referenceCount":49,"citationCount":0,"influentialCitationCount":0,"publicationDate":"26/02/2023","authors":"Chih-Jou Hsieh,Isabel Blanco Nobre,Sandra Costa Sousa,Chun Ouyang,Margot Brereton,J. Nascimento,Joaquim Jorge,Catarina Moreira","id":"10a8e7a7e07256178665f90074c5c41b071e73d3","summary":"Results show that incorporating patients' clinical data in a DL model together with the proposed fusion methods improves the performance of disease localization in chest X-rays by 12\\% in terms of Average Precision compared to a standard Mask R-CNN using only chestX-rays.","score":1},{"url":"https://www.semanticscholar.org/paper/a39adffc4e6de100a950f4476e113bfc402119f2","title":"Knowledge-enhanced Visual-Language Pre-training on Chest Radiology Images","venue":"","year":2023,"referenceCount":73,"citationCount":0,"influentialCitationCount":0,"publicationDate":"27/02/2023","authors":"Xiaoman Zhang,Chaoyi Wu,Ya Zhang,Yanfeng Wang,Weidi Xie","id":"a39adffc4e6de100a950f4476e113bfc402119f2","summary":"KAD is evaluated on four external X-ray datasets and it is demonstrated that its zero-shot performance is not only comparable to that of fully-supervised models, but also superior to the average of three expert radiologists for three pathologies with statistical significance.","score":1},{"url":"https://www.semanticscholar.org/paper/201e3519a2b77e3d4852a909d8edfa19796e219e","title":"Generalist Vision Foundation Models for Medical Imaging: A Case Study of Segment Anything Model on Zero-Shot Medical Segmentation","venue":"arXiv.org","year":2023,"referenceCount":37,"citationCount":0,"influentialCitationCount":0,"publicationDate":"25/04/2023","authors":"Peilun Shi,Jianing Qiu,Sai Mu Dalike Abaxi,Hao Wei,F. P. Lo,Wu Yuan","id":"201e3519a2b77e3d4852a909d8edfa19796e219e","summary":"The study indicates the versatility of generalist vision foundation models on solving specific tasks in medical imaging, and their great potential to achieve desired performance through fine-turning and eventually tackle the challenges of accessing large diverse medical datasets and the complexity of medical domains.","score":1},{"url":"https://www.semanticscholar.org/paper/80785017029cab501fcdb90b98985cd2b36e1fb8","title":"Evaluation of GPT-3.5 and GPT-4 for supporting real-world information needs in healthcare delivery","venue":"arXiv.org","year":2023,"referenceCount":38,"citationCount":0,"influentialCitationCount":0,"publicationDate":"26/04/2023","authors":"Debadutta Dash,Rahul Thapa,J. Banda,Akshay Swaminathan,Morgan Cheatham,M. Kashyap,N. Kotecha,Jonathan H. Chen,S. Gombar,L. Downing,Rachel A. Pedreira,Ethan Goh,A. Arnaout,Garret K. Morris,H. Magon,M. Lungren,E. Horvitz,N. Shah","id":"80785017029cab501fcdb90b98985cd2b36e1fb8","summary":"It is suggested that while general purpose LLMs are able to provide safe and credible responses, they often do not meet the specific information need of a given question.","score":1},{"url":"https://www.semanticscholar.org/paper/e1a12117f15a6ee07133851a51439394bb9e7406","title":"ChemCrow: Augmenting large-language models with chemistry tools","venue":"","year":2023,"referenceCount":78,"citationCount":3,"influentialCitationCount":0,"publicationDate":"11/04/2023","authors":"A. Bran,Sam Cox,Andrew D. White,P. Schwaller","id":"e1a12117f15a6ee07133851a51439394bb9e7406","summary":"This study introduces ChemCrow, an LLM chemistry agent designed to accomplish tasks across organic synthesis, drug discovery, and materials design by integrating 13 expert-designed tools, which augments the LLM performance in chemistry, and new capabilities emerge.","score":1},{"url":"https://www.semanticscholar.org/paper/2d3905c1a92c28c056dff1225d89e4ca72ac4d8e","title":"Man vs the machine: The Struggle for Effective Text Anonymisation in the Age of Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":38,"citationCount":0,"influentialCitationCount":0,"publicationDate":"22/03/2023","authors":"C. Patsakis,Nikolaos Lykousas","id":"2d3905c1a92c28c056dff1225d89e4ca72ac4d8e","summary":"An experiment is conducted using GPT over anonymised texts of famous people to determine whether such trained networks can deanonymise them and introduces a novel methodology that employs Large Language Models to improve the anonymity of texts.","score":1},{"url":"https://www.semanticscholar.org/paper/6139b6bc065b24562cb7f4f08227a42f5766138f","title":"Diffusion Models: A Comprehensive Survey of Methods and Applications","venue":"arXiv.org","year":2022,"referenceCount":364,"citationCount":101,"influentialCitationCount":6,"publicationDate":"02/09/2022","authors":"Ling Yang,Zhilong Zhang,Shenda Hong,Runsheng Xu,Yue Zhao,Yingxia Shao,Wentao Zhang,Ming-Hsuan Yang,Bin Cui","id":"6139b6bc065b24562cb7f4f08227a42f5766138f","summary":"An overview of the rapidly expanding body of work on diffusion models is provided, categorizing the research into three key areas: efficient sampling, improved likelihood estimation, and handling data with special structures.","score":1},{"url":"https://www.semanticscholar.org/paper/09ca5072a76796c65e5936b6fb4968afead61944","title":"Semantics-Empowered Communication: A Tutorial-cum-Survey","venue":"arXiv.org","year":2022,"referenceCount":243,"citationCount":2,"influentialCitationCount":1,"publicationDate":"16/12/2022","authors":"Zhilin Lu,Rongpeng Li,Kun Lu,Xianfu Chen,E. Hossain,Zhifeng Zhao,Honggang Zhang","id":"09ca5072a76796c65e5936b6fb4968afead61944","summary":"This work proposes to categorize the critical enabling techniques by explicit and implicit reasoning-based methods, and elaborate on how they evolve and contribute to modern content&channel semantics-empowered communications.","score":1},{"url":"https://www.semanticscholar.org/paper/419eb47fea3931c4098232f44ccbc216275d3f56","title":"Decoding Visual Neural Representations by Multimodal Learning of Brain-Visual-Linguistic Features","venue":"IEEE Transactions on Pattern Analysis and Machine Intelligence","year":2022,"referenceCount":87,"citationCount":2,"influentialCitationCount":1,"publicationDate":"13/10/2022","authors":"Changde Du,Kaicheng Fu,Jinpeng Li,Huiguang He","id":"419eb47fea3931c4098232f44ccbc216275d3f56","summary":"The BraVL model can be trained under various semi-supervised scenarios to incorporate the visual and textual features obtained from the extra categories and constructed three trimodal matching datasets, leading to some interesting conclusions and cognitive insights.","score":1},{"url":"https://www.semanticscholar.org/paper/9fe9af7cf3d54b707a7be3c53ce94b77dcc3bae5","title":"A Short Survey of Viewing Large Language Models in Legal Aspect","venue":"arXiv.org","year":2023,"referenceCount":25,"citationCount":3,"influentialCitationCount":0,"publicationDate":"16/03/2023","authors":"Zhongxiang Sun","id":"9fe9af7cf3d54b707a7be3c53ce94b77dcc3bae5","summary":"","score":1},{"url":"https://www.semanticscholar.org/paper/74e8ae03a385e72f5ae377667ba9858fb3e0bfa0","title":"Unleashing the Power of Edge-Cloud Generative AI in Mobile Networks: A Survey of AIGC Services","venue":"arXiv.org","year":2023,"referenceCount":232,"citationCount":2,"influentialCitationCount":0,"publicationDate":"28/03/2023","authors":"Minrui Xu,Hongyang Du,D. Niyato,Jiawen Kang,Zehui Xiong,Shiwen Mao,Zhu Han,A. Jamalipour,Dong In Kim,X. Shen,Victor C. M. Leung,H. V. Poor","id":"74e8ae03a385e72f5ae377667ba9858fb3e0bfa0","summary":"This survey paper focuses on the deployment of AIGC applications, e.g., ChatGPT and Dall-E, at mobile edge networks, that provide personalized and customized AigC services in real time while maintaining user privacy.","score":1},{"url":"https://www.semanticscholar.org/paper/1d29334cfbe9a1a943082058876f0c22d44c62fd","title":"A Survey of Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":415,"citationCount":17,"influentialCitationCount":2,"publicationDate":"31/03/2023","authors":"Wayne Xin Zhao,Kun Zhou,Junyi Li,Tianyi Tang,Xiaolei Wang,Yupeng Hou,Yingqian Min,Beichen Zhang,Junjie Zhang,Zican Dong,Yifan Du,Chen Yang,Yushuo Chen,Z. Chen,Jinhao Jiang,Ruiyang Ren,Yifan Li,Xinyu Tang,Zikang Liu,Peiyu Liu,J. Nie,Ji-rong Wen","id":"1d29334cfbe9a1a943082058876f0c22d44c62fd","summary":"A review of the recent advances of large language models by introducing the background, key findings, and mainstream techniques, and focusing on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation.","score":1},{"url":"https://www.semanticscholar.org/paper/248ee36169895ddcaf3963eb76fb24e1d8ef2f81","title":"Summary of ChatGPT/GPT-4 Research and Perspective Towards the Future of Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":104,"citationCount":10,"influentialCitationCount":1,"publicationDate":"04/04/2023","authors":"Yi-Hsien Liu,Tianle Han,Siyuan Ma,Jia-Yu Zhang,Yuanyu Yang,Jiaming Tian,Haoyang He,Antong Li,Mengshen He,Zheng Liu,Zihao Wu,Dajiang Zhu,Xiang Li,Ning Qiang,Dingang Shen,Tianming Liu,Bao Ge","id":"248ee36169895ddcaf3963eb76fb24e1d8ef2f81","summary":"A comprehensive survey of ChatGPT and GPT-4, state-of-the-art large language models from the GPT series, and their prospective applications across diverse domains, encompassing trend analysis, word cloud representation, and distribution analysis across various application domains is presented.","score":1},{"url":"https://www.semanticscholar.org/paper/9fd980237e7fdfa4c103a2dc08657e73adf847c4","title":"OpenAGI: When LLM Meets Domain Experts","venue":"arXiv.org","year":2023,"referenceCount":42,"citationCount":3,"influentialCitationCount":0,"publicationDate":"10/04/2023","authors":"Yingqiang Ge,Wenyue Hua,Jianchao Ji,Juntao Tan,Shuyuan Xu,Yongfeng Zhang","id":"9fd980237e7fdfa4c103a2dc08657e73adf847c4","summary":"OpenAGI is developed, an open-source AGI research platform, specifically designed to offer complex, multi-step tasks and accompanied by task-specific datasets, evaluation metrics, and a diverse range of extensible models.","score":1},{"url":"https://www.semanticscholar.org/paper/6316cbb4f1e7dba5806a3310ec7f89f3571bc3db","title":"Boosting Cross-task Transferability of Adversarial Patches with Visual Relations","venue":"arXiv.org","year":2023,"referenceCount":22,"citationCount":0,"influentialCitationCount":0,"publicationDate":"11/04/2023","authors":"Tony Ma,Songze Li,Yisong Xiao,Shunchang Liu","id":"6316cbb4f1e7dba5806a3310ec7f89f3571bc3db","summary":"A novel Visual Relation-based cross-task Adversarial Patch generation method called VRAP is proposed, which aims to evaluate the robustness of various visual tasks, especially those involving visual reasoning, such as Visual Question Answering and Image Captioning.","score":1},{"url":"https://www.semanticscholar.org/paper/be2b0396de9431bae931642516a1d3e4906329f5","title":"Low-code LLM: Visual Programming over LLMs","venue":"arXiv.org","year":2023,"referenceCount":27,"citationCount":1,"influentialCitationCount":0,"publicationDate":"17/04/2023","authors":"Yuzhe Cai,Shaoguang Mao,Wenshan Wu,Zehua Wang,Yaobo Liang,Tao Ge,Chenfei Wu,Wang You,Ting Song,Yan Xia,Jonathan Tien,Nan Duan","id":"be2b0396de9431bae931642516a1d3e4906329f5","summary":"A novel human-LLM interaction framework that incorporates six types of simple low-code visual programming interactions, all supported by clicking, dragging, or text editing, to achieve more controllable and stable responses.","score":1},{"url":"https://www.semanticscholar.org/paper/93cedc10eda0e89757c1d1de67d78cb7e1b5c55b","title":"Learning to Program with Natural Language","venue":"arXiv.org","year":2023,"referenceCount":30,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/04/2023","authors":"Yiduo Guo,Yaobo Liang,Chenfei Wu,Wenshan Wu,Dongyan Zhao,Nan Duan","id":"93cedc10eda0e89757c1d1de67d78cb7e1b5c55b","summary":"The Learning to Program (LP) method is proposed to ask LLMs themselves to learn natural language programs from the training dataset of complex tasks and then use the learned program to guide inference.","score":1},{"url":"https://www.semanticscholar.org/paper/4c8ef2db0c77aba453783f5211ebafc6695d3835","title":"ChatABL: Abductive Learning via Natural Language Interaction with ChatGPT","venue":"arXiv.org","year":2023,"referenceCount":69,"citationCount":1,"influentialCitationCount":0,"publicationDate":"21/04/2023","authors":"Tianyang Zhong,Yaonai Wei,Li Yang,Zihao Wu,Zheng Liu,Xiaozheng Wei,WenJu Sun,Junjie Yao,Chongfei Ma,Xiang Li,Dajiang Zhu,Xi Jiang,Jun-Feng Han,Dinggang Shen,Tianming Liu,Tuo Zhang","id":"4c8ef2db0c77aba453783f5211ebafc6695d3835","summary":"This paper presents a novel method (ChatABL) for integrating LLMs into the ABL framework, aiming at unifying the three abilities in a more user-friendly and understandable manner, and is the first attempt to explore a new pattern for further approaching human-level cognitive ability via natural language interaction with ChatGPT.","score":1},{"url":"https://www.semanticscholar.org/paper/7a5c31341e7ec22409e175542368eb76e08900aa","title":"The Potential of Visual ChatGPT For Remote Sensing","venue":"arXiv.org","year":2023,"referenceCount":36,"citationCount":0,"influentialCitationCount":0,"publicationDate":"25/04/2023","authors":"L. Osco,Eduardo Lopes de Lemos,W. Gonçalves,A. P. Ramos,J. M. Junior","id":"7a5c31341e7ec22409e175542368eb76e08900aa","summary":"","score":1},{"url":"https://www.semanticscholar.org/paper/8bc617c9139648d7a92991d70c671230bac7b2e2","title":"AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head","venue":"arXiv.org","year":2023,"referenceCount":46,"citationCount":0,"influentialCitationCount":0,"publicationDate":"25/04/2023","authors":"Rongjie Huang,Mingze Li,Dongchao Yang,Jiatong Shi,Xuankai Chang,Zhenhui Ye,Yuning Wu,Zhiqing Hong,Jia-Bin Huang,Jinglin Liu,Yixiang Ren,Zhou Zhao,Shinji Watanabe","id":"8bc617c9139648d7a92991d70c671230bac7b2e2","summary":"A multi-modal AI system named AudioGPT is proposed, which complements LLMs with 1) foundation models to process complex audio information and solve numerous understanding and generation tasks; and 2) the input/output interface (ASR, TTS) to support spoken dialogue.","score":1},{"url":"https://www.semanticscholar.org/paper/0f19e94f30b99d6c4b349900057cdae9262034f9","title":"The Contribution of Knowledge in Visiolinguistic Learning: A Survey on Tasks and Challenges","venue":"arXiv.org","year":2023,"referenceCount":140,"citationCount":0,"influentialCitationCount":0,"publicationDate":"04/03/2023","authors":"Maria Lymperaiou,G. Stamou","id":"0f19e94f30b99d6c4b349900057cdae9262034f9","summary":"This survey analyzes tasks that have benefited from hybrid approaches to visiolinguistic learning, and categorizes existing knowledge sources and types, proceeding to discussion regarding the KG vs LLM dilemma and its potential impact to future hybrid approaches.","score":1},{"url":"https://www.semanticscholar.org/paper/09840a5c151f858ed0eaf1db2a4d3741516f693b","title":"ICL-D3IE: In-Context Learning with Diverse Demonstrations Updating for Document Information Extraction","venue":"arXiv.org","year":2023,"referenceCount":55,"citationCount":3,"influentialCitationCount":0,"publicationDate":"09/03/2023","authors":"Jiabang He,Lei Wang,Yingpeng Hu,Ning Liu,Hui-juan Liu,Xingdong Xu,Hengtao Shen","id":"09840a5c151f858ed0eaf1db2a4d3741516f693b","summary":"A simple but effective in-context learning framework called ICL-D3IE, which enables LLMs to perform DIE with different types of demonstration examples and enables GPT-3/ChatGPT to achieve superior performance when compared to previous pre-trained methods fine-tuned with full training in both the in-dist distribution (ID) setting and in the out-of-distribution (OOD) setting.","score":1},{"url":"https://www.semanticscholar.org/paper/5dea6facab090a070be1444920230689e7189599","title":"SurgicalGPT: End-to-End Language-Vision GPT for Visual Question Answering in Surgery","venue":"arXiv.org","year":2023,"referenceCount":25,"citationCount":1,"influentialCitationCount":0,"publicationDate":"19/04/2023","authors":"L. Seenivasan,Mobarakol Islam,Gokul Kannan,Hongliang Ren","id":"5dea6facab090a070be1444920230689e7189599","summary":"An end-to-end trainable Language-Vision GPT model that expands the GPT2 model to include vision input (image) and extensively study and present the effects of token sequencing, token type and pose embedding for vision tokens in the LV-GPT model.","score":1},{"url":"https://www.semanticscholar.org/paper/d00ca5c49415d3a45bfcf3fabaf0a60a1c52a6ff","title":"PromptCap: Prompt-Guided Task-Aware Image Captioning","venue":"arXiv.org","year":2022,"referenceCount":100,"citationCount":9,"influentialCitationCount":0,"publicationDate":"15/11/2022","authors":"Yushi Hu,Hang Hua,Zhengyuan Yang,Weijia Shi,Noah A. Smith,Jiebo Luo","id":"d00ca5c49415d3a45bfcf3fabaf0a60a1c52a6ff","summary":"PromptCap (Prompt-guided image Captioning), a captioning model designed to serve as a better connector between images and black-box LMs, achieves state-of-the-art accuracy on knowledge-based VQA tasks and generalizes well to unseen domains.","score":1},{"url":"https://www.semanticscholar.org/paper/1367dcff4ccb927a5e95c452041288b3f0dd0eff","title":"Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation","venue":"arXiv.org","year":2022,"referenceCount":67,"citationCount":24,"influentialCitationCount":6,"publicationDate":"22/12/2022","authors":"Jay Zhangjie Wu,Yixiao Ge,Xintao Wang,Weixian Lei,Yuchao Gu,W. Hsu,Ying Shan,Xiaohu Qie,Mike Zheng Shou","id":"1367dcff4ccb927a5e95c452041288b3f0dd0eff","summary":"A new T2V generation setting, where only one text-video pair is presented, and Tune-A-Video, which involves a tailored spatio-temporal attention mechanism and an efficient one-shot tuning strategy, is introduced.","score":1},{"url":"https://www.semanticscholar.org/paper/00c1ff63468305ea3fa430c2b3aef156d580c4ff","title":"P ROMPT C AP : Prompt-Guided Image Captioning for VQA with GPT-3","venue":"","year":2023,"referenceCount":72,"citationCount":1,"influentialCitationCount":0,"publicationDate":2023,"authors":"Yushi Hu,Hang Hua,Zhengyuan Yang,Weijia Shi,Noah A. Smith,Jiebo Luo","id":"00c1ff63468305ea3fa430c2b3aef156d580c4ff","summary":"P ROMPT C AP is a captioning model designed to serve as a better connector between images and black-box LMs that outperforms generic captions by a large margin and achieves state-of-the-art accuracy on knowledge-based VQA tasks.","score":1},{"url":"https://www.semanticscholar.org/paper/3062bb79d12ff55c29c8731211a84e8cf344e235","title":"Vision Learners Meet Web Image-Text Pairs","venue":"arXiv.org","year":2023,"referenceCount":88,"citationCount":0,"influentialCitationCount":0,"publicationDate":"17/01/2023","authors":"Bingchen Zhao,Quan Cui,Hao Wu,O. Yoshie,Cheng Yang","id":"3062bb79d12ff55c29c8731211a84e8cf344e235","summary":"A new visual representation pre-training method, MUlti-modal Generator~(MUG), that learns from scalable web sourced image-text data that achieves state-of-the-art transfer performance on a variety of tasks and demonstrates promising scaling properties.","score":1},{"url":"https://www.semanticscholar.org/paper/a3ff4df653b6970898c04e6b768e58b99786d073","title":"Learning gain differences between ChatGPT and human tutor generated algebra hints","venue":"arXiv.org","year":2023,"referenceCount":30,"citationCount":4,"influentialCitationCount":1,"publicationDate":"14/02/2023","authors":"Z. Pardos,Shreya Bhandari","id":"a3ff4df653b6970898c04e6b768e58b99786d073","summary":"This paper conducts the first learning gain evaluation of ChatGPT by comparing the efficacy of its hints with hints authored by human tutors with 77 participants across two algebra topic areas, Elementary Algebra and Intermediate Algebra.","score":1},{"url":"https://www.semanticscholar.org/paper/c8f98f28f1f28a9f5db7c4b4d9a6b7853a100214","title":"Can Pre-trained Vision and Language Models Answer Visual Information-Seeking Questions?","venue":"arXiv.org","year":2023,"referenceCount":58,"citationCount":1,"influentialCitationCount":0,"publicationDate":"23/02/2023","authors":"Yang Chen,Hexiang Hu,Yi Luan,Haitian Sun,Soravit Changpinyo,Alan Ritter,Ming-Wei Chang","id":"c8f98f28f1f28a9f5db7c4b4d9a6b7853a100214","summary":"The analysis shows that it is challenging for the state-of-the-art multi-modal pre-trained models to answer visual information seeking questions, but this capability is improved through fine-tuning on the automated InfoSeek dataset.","score":1},{"url":"https://www.semanticscholar.org/paper/467b839cb8a2475477ca004df94b797d967ad057","title":"Human-Art: A Versatile Human-Centric Dataset Bridging Natural and Artificial Scenes","venue":"arXiv.org","year":2023,"referenceCount":88,"citationCount":2,"influentialCitationCount":0,"publicationDate":"05/03/2023","authors":"Xu Ju,Ailing Zeng,Jianan Wang,Qian Xu,Lei Zhang","id":"467b839cb8a2475477ca004df94b797d967ad057","summary":"The Human-Art dataset is introduced and contains 50k high-quality images with over 123k person instances from 5 natural and 15 artificial scenarios, which are annotated with bounding boxes, keypoints, self-contact points, and text information for humans represented in both 2D and 3D.","score":1},{"url":"https://www.semanticscholar.org/paper/a7b3a868a80dbe97689135c99b1a6b6e10dcdfe5","title":"A Comprehensive Survey of AI-Generated Content (AIGC): A History of Generative AI from GAN to ChatGPT","venue":"arXiv.org","year":2023,"referenceCount":280,"citationCount":8,"influentialCitationCount":0,"publicationDate":"07/03/2023","authors":"Yihan Cao,Siyu Li,Yixin Liu,Zhiling Yan,Yutong Dai,Philip S. Yu,Lichao Sun","id":"a7b3a868a80dbe97689135c99b1a6b6e10dcdfe5","summary":"This survey provides a comprehensive review on the history of generative models, and basic components, recent advances in AIGC from unimmodal interaction and multimodal interaction, and introduces the generation tasks and relative models of text and image.","score":1},{"url":"https://www.semanticscholar.org/paper/6a4ef6c4799dc871a4253c0536126d397ca3ec1e","title":"Interpretable Visual Question Answering Referring to Outside Knowledge","venue":"arXiv.org","year":2023,"referenceCount":29,"citationCount":0,"influentialCitationCount":0,"publicationDate":"08/03/2023","authors":"He Zhu,Ren Togo,Takahiro Ogawa,M. Haseyama","id":"6a4ef6c4799dc871a4253c0536126d397ca3ec1e","summary":"A novel multimodal interpretable VQA model that can answer the question more accurately and generate diverse explanations and can outperform state-of-the-art methods regarding answer accuracy and explanation rationality.","score":1},{"url":"https://www.semanticscholar.org/paper/e5a7be5b9e6c368a1839455bfbb51bc07ed161f1","title":"ODIN: On-demand Data Formulation to Mitigate Dataset Lock-in","venue":"arXiv.org","year":2023,"referenceCount":38,"citationCount":0,"influentialCitationCount":0,"publicationDate":"13/03/2023","authors":"SP Choi,Jihun Lee,HyeongSeok Ahn,S. Jung,Bumsoo Kang","id":"e5a7be5b9e6c368a1839455bfbb51bc07ed161f1","summary":"This work evaluated ODIN on various datasets in terms of model accuracy and data diversity to demonstrate its potential, and conducted post-experiments for further investigation.","score":1},{"url":"https://www.semanticscholar.org/paper/aa75ec0ee1aa18d3b0603d5a425e92eabcb7ac02","title":"Breaking Common Sense: WHOOPS! A Vision-and-Language Benchmark of Synthetic and Compositional Images","venue":"arXiv.org","year":2023,"referenceCount":44,"citationCount":2,"influentialCitationCount":0,"publicationDate":"13/03/2023","authors":"Nitzan Bitton-Guetta,Yonatan Bitton,Jack Hessel,Ludwig Schmidt,Y. Elovici,G. Stanovsky,Roy Schwartz","id":"aa75ec0ee1aa18d3b0603d5a425e92eabcb7ac02","summary":"This work introduces WHOOPS!, a new dataset and benchmark for visual commonsense, comprised of purposefully commonsense-defying images created by designers using publicly-available image generation tools like Midjourney and introduces a difficult explanation generation task, where models must identify and explain why a given image is unusual.","score":1},{"url":"https://www.semanticscholar.org/paper/cf41ae462687f81ce95b27113c6a4f9c2751de42","title":"Vision-Language Models as Success Detectors","venue":"arXiv.org","year":2023,"referenceCount":54,"citationCount":2,"influentialCitationCount":0,"publicationDate":"13/03/2023","authors":"Yuqing Du,Ksenia Konyushkova,Misha Denil,A. Raju,Jessica Landon,Felix Hill,N. D. Freitas,Serkan Cabi","id":"cf41ae462687f81ce95b27113c6a4f9c2751de42","summary":"This work focuses on developing robust success detectors that leverage large, pretrained vision-language models and human reward annotations, and investigates the generalisation properties of a Flamingo-based success detection model across unseen language and visual changes in the first two domains.","score":1},{"url":"https://www.semanticscholar.org/paper/3c39a600adb254f7520f513ed9c3412c9c62f17f","title":"MRIS: A Multi-modal Retrieval Approach for Image Synthesis on Diverse Modalities","venue":"arXiv.org","year":2023,"referenceCount":27,"citationCount":0,"influentialCitationCount":0,"publicationDate":"17/03/2023","authors":"Boqi Chen,M. Niethammer","id":"3c39a600adb254f7520f513ed9c3412c9c62f17f","summary":"The proposed method outperforms direct image synthesis and that the synthesized thickness maps retain information relevant to downstream tasks such as progression prediction and Kellgren-Lawrence grading (KLG).","score":1},{"url":"https://www.semanticscholar.org/paper/049a62ac86f59f2a912cd59f1cb179b82c4ae6b9","title":"TIFA: Accurate and Interpretable Text-to-Image Faithfulness Evaluation with Question Answering","venue":"arXiv.org","year":2023,"referenceCount":62,"citationCount":1,"influentialCitationCount":0,"publicationDate":"21/03/2023","authors":"Yushi Hu,Benlin Liu,Jungo Kasai,Yizhong Wang,Mari Ostendorf,Ranjay Krishna,Noah A. Smith","id":"049a62ac86f59f2a912cd59f1cb179b82c4ae6b9","summary":"This work introduces TIFA (Text-to-Image Faithfulness evaluation with question Answering), an automatic evaluation metric that measures the faithfulness of a generated image to its text input via visual question answering (VQA).","score":1},{"url":"https://www.semanticscholar.org/paper/7733cf84e5447339dd57ca96133e14e36c29e0e7","title":"Contrastive Alignment of Vision to Language Through Parameter-Efficient Transfer Learning","venue":"arXiv.org","year":2023,"referenceCount":60,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/03/2023","authors":"Zaid Khan,Yun Fu","id":"7733cf84e5447339dd57ca96133e14e36c29e0e7","summary":"The feasibility and benefits of parameter-efficient contrastive vision-language alignment through transfer learning are explored: creating a model such as CLIP by minimally updating an already-trained vision and language model is found.","score":1},{"url":"https://www.semanticscholar.org/paper/285dae5c2f2ef55c70971094a1ddd45afe720eee","title":"Fundamentals of Generative Large Language Models and Perspectives in Cyber-Defense","venue":"arXiv.org","year":2023,"referenceCount":100,"citationCount":1,"influentialCitationCount":0,"publicationDate":"21/03/2023","authors":"Andrei Kucharavy,Z. Schillaci,Loic Mar'echal,Maxime Wursch,L. Dolamic,Remi Sabonnadiere,Dimitri Percia David,Alain Mermoud,Vincent Lenders","id":"285dae5c2f2ef55c70971094a1ddd45afe720eee","summary":"This review aims to provide a brief overview of the history, state of the art, and implications of Generative Language Models in terms of their principles, abilities, limitations, and future prospects -- especially in the context of cyber-defense, with a focus on the Swiss operational environment.","score":1},{"url":"https://www.semanticscholar.org/paper/0d3817ae7fecc204c7c79a039dc47ae88890d5f3","title":"ChatGPT for Shaping the Future of Dentistry: The Potential of Multi-Modal Large Language Model","venue":"arXiv.org","year":2023,"referenceCount":46,"citationCount":1,"influentialCitationCount":0,"publicationDate":"23/03/2023","authors":"Hanyao Huang,Ou Zheng,Dongdong Wang,Jiayi Yin,Zijin Wang,Shengxuan Ding,H. Yin,Chuan Xu,R. Yang,Q. Zheng,B. Shi","id":"0d3817ae7fecc204c7c79a039dc47ae88890d5f3","summary":"","score":1},{"url":"https://www.semanticscholar.org/paper/d84616f108ccbd958735fef7622e58d148b32139","title":"Make-It-3D: High-Fidelity 3D Creation from A Single Image with Diffusion Prior","venue":"arXiv.org","year":2023,"referenceCount":63,"citationCount":1,"influentialCitationCount":0,"publicationDate":"24/03/2023","authors":"Junshu Tang,Tengfei Wang,Bo Zhang,Ting Zhang,Ran Yi,Lizhuang Ma,Dong Chen","id":"d84616f108ccbd958735fef7622e58d148b32139","summary":"This work leverages prior knowledge from a well-trained 2D diffusion model to act as 3D-aware supervision for 3D creation and presents the first attempt to achieve high-quality3D creation from a single image for general objects.","score":1},{"url":"https://www.semanticscholar.org/paper/a08b7123a7158f1a7fbbc18e8b5aaebd47980ecf","title":"EVA-CLIP: Improved Training Techniques for CLIP at Scale","venue":"arXiv.org","year":2023,"referenceCount":52,"citationCount":0,"influentialCitationCount":0,"publicationDate":"27/03/2023","authors":"Quan Sun,Yuxin Fang,Ledell Yu Wu,Xinlong Wang,Yue Cao","id":"a08b7123a7158f1a7fbbc18e8b5aaebd47980ecf","summary":"This paper proposes a series of models that significantly improve the efficiency and effectiveness of CLIP training, and incorporates new techniques for representation learning, optimization, and augmentation, enabling EVA-CLIP to achieve superior performance compared to previous CLIP models with the same number of parameters but significantly smaller training costs.","score":1},{"url":"https://www.semanticscholar.org/paper/1c7471996a4f2e08a5ef592a6ffcde65a034a1e4","title":"GlyphDraw: Learning to Draw Chinese Characters in Image Synthesis Models Coherently","venue":"arXiv.org","year":2023,"referenceCount":42,"citationCount":0,"influentialCitationCount":0,"publicationDate":"31/03/2023","authors":"Jiancang Ma,Mingjun Zhao,Chen Chen,Ruichen Wang,Di Niu,H. Lu,Xiaodong Lin","id":"1c7471996a4f2e08a5ef592a6ffcde65a034a1e4","summary":"GlyphDraw is introduced, a general learning framework aiming at endowing image generation models with the capacity to generate images embedded with coherent text within images, and is the first work in the field of image synthesis to address the generation of Chinese characters.","score":1},{"url":"https://www.semanticscholar.org/paper/4d94dcc6c9c261c8edcd0f3c5a1318a98a45b79d","title":"HRS-Bench: Holistic, Reliable and Scalable Benchmark for Text-to-Image Models","venue":"arXiv.org","year":2023,"referenceCount":73,"citationCount":0,"influentialCitationCount":0,"publicationDate":"11/04/2023","authors":"Eslam Mohamed Bakr,Pengzhan Sun,Xiaoqian Shen,Faizan Farooq Khan,Li Erran Li,Mohamed Elhoseiny","id":"4d94dcc6c9c261c8edcd0f3c5a1318a98a45b79d","summary":"HRS-Bench is introduced, a concrete evaluation benchmark for T2I models that is Holistic, Reliable, and Scalable and measures 13 skills that can be categorized into five major categories: accuracy, robustness, generalization, fairness, and bias.","score":1},{"url":"https://www.semanticscholar.org/paper/f12ebcc9a0a7296cc6c85b243a003f7205c68b3d","title":"What does CLIP know about a red circle? Visual prompt engineering for VLMs","venue":"arXiv.org","year":2023,"referenceCount":63,"citationCount":1,"influentialCitationCount":0,"publicationDate":"13/04/2023","authors":"Aleksandar Shtedritski,C. Rupprecht,A. Vedaldi","id":"f12ebcc9a0a7296cc6c85b243a003f7205c68b3d","summary":"This work explores the idea of visual prompt engineering for solving computer vision tasks beyond classification by editing in image space instead of text, and discovers an emergent ability of CLIP, where, by simply drawing a red circle around an object, it can direct the model's attention to that region, while also maintaining global information.","score":1},{"url":"https://www.semanticscholar.org/paper/f603b9dc81dfea56d437e967b724636d4d72d000","title":"LLM as A Robotic Brain: Unifying Egocentric Memory and Control","venue":"arXiv.org","year":2023,"referenceCount":24,"citationCount":1,"influentialCitationCount":0,"publicationDate":"19/04/2023","authors":"Jinjie Mai,Jun Chen,Bing-chuan Li,Guocheng Qian,Mohamed Elhoseiny,Bernard Ghanem","id":"f603b9dc81dfea56d437e967b724636d4d72d000","summary":"A novel and generalizable framework called LLM-Brain: using Large-scale Language Model as a robotic brain to unify egocentric memory and control, utilizing a zero-shot learning approach.","score":1},{"url":"https://www.semanticscholar.org/paper/8fce3142bc144bdc08bf0cab1db908c7ad3f8454","title":"Contrastive Language, Action, and State Pre-training for Robot Learning","venue":"arXiv.org","year":2023,"referenceCount":30,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/04/2023","authors":"Krishan Rana,Andrew Melnik,N. Sunderhauf","id":"8fce3142bc144bdc08bf0cab1db908c7ad3f8454","summary":"The method, Contrastive Language, Action, and State Pre-training (CLASP), extends the CLIP formulation by incorporating distributional learning, capturing the inherent complexities and one-to-many relationships in behaviour-text alignment.","score":1},{"url":"https://www.semanticscholar.org/paper/598b3961f767c1ad40cbb393afd936de4e30d578","title":"SATIN: A Multi-Task Metadataset for Classifying Satellite Imagery using Vision-Language Models","venue":"arXiv.org","year":2023,"referenceCount":86,"citationCount":0,"influentialCitationCount":0,"publicationDate":"23/04/2023","authors":"J. Roberts,K. Han,Samuel Albanie","id":"598b3961f767c1ad40cbb393afd936de4e30d578","summary":"This work introduces SATellite ImageNet (SATIN), a metadataset curated from 27 existing remotely sensed datasets, and comprehensively evaluates the zero-shot transfer classification capabilities of a broad range of vision-language (VL) models on SATIN.","score":1},{"url":"https://www.semanticscholar.org/paper/59dfa986cc7468561d2c19cdb43f816406ea30d8","title":"TR0N: Translator Networks for 0-Shot Plug-and-Play Conditional Generation","venue":"arXiv.org","year":2023,"referenceCount":85,"citationCount":0,"influentialCitationCount":0,"publicationDate":"26/04/2023","authors":"Zhao-Qian Liu,Noël Vouitsis,S. Gorti,Jimmy Ba,G. Loaiza-Ganem","id":"59dfa986cc7468561d2c19cdb43f816406ea30d8","summary":"TR0N is proposed, a highly general framework to turn pre-trained unconditional generative models, such as GANs and VAEs, into conditional models, and shows how to turn unconditional models into class-conditional ones with the help of a classifier, and also into text-to-image models by leveraging CLIP.","score":1},{"url":"https://www.semanticscholar.org/paper/994a1ce6677b496bd3c0c63aceafc6556005e994","title":"GLIGEN: Open-Set Grounded Text-to-Image Generation","venue":"arXiv.org","year":2023,"referenceCount":79,"citationCount":21,"influentialCitationCount":3,"publicationDate":"17/01/2023","authors":"Yuheng Li,Haotian Liu,Qingyang Wu,Fangzhou Mu,Jianwei Yang,Jianfeng Gao,Chunyuan Li,Yong Jae Lee","id":"994a1ce6677b496bd3c0c63aceafc6556005e994","summary":"A novel approach that builds upon and extends the functionality of existing pre-trained text-to-image diffusion models by enabling them to also be conditioned on grounding inputs, and achieves open-world grounded text2img generation with caption and bounding box condition inputs.","score":1},{"url":"https://www.semanticscholar.org/paper/33827bb0bb8188817083be024614f82bec002c42","title":"A Simple Framework for Open-Vocabulary Segmentation and Detection","venue":"arXiv.org","year":2023,"referenceCount":62,"citationCount":2,"influentialCitationCount":0,"publicationDate":"14/03/2023","authors":"Hao Zhang,Feng Li,Xueyan Zou,Siyi Liu,Chun-yue Li,Jianfeng Gao,Jianwei Yang,Lei Zhang","id":"33827bb0bb8188817083be024614f82bec002c42","summary":"OpenSeeD is the first to explore the potential of joint training on segmentation and detection, and hope it can be received as a strong baseline for developing a single model for both tasks in open world.","score":1},{"url":"https://www.semanticscholar.org/paper/f8e37aa69b3c9b743055e648851c530b18dc54d2","title":"Neural Implicit Vision-Language Feature Fields","venue":"arXiv.org","year":2023,"referenceCount":51,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/03/2023","authors":"Kenneth Blomqvist,Francesco Milano,Jen Jen Chung,Lionel Ott,R. Siegwart","id":"f8e37aa69b3c9b743055e648851c530b18dc54d2","summary":"This work presents a zero-shot volumetric open-vocabulary semantic scene segmentation method that builds on the insight that it can fuse image features from a vision-language model into a neural implicit representation and shows that the resulting feature field can be segmented into different classes by assigning points to natural language text prompts.","score":1},{"url":"https://www.semanticscholar.org/paper/0931888d5cd7c26427cc116af2ac33863552da27","title":"SAM.MD: Zero-shot medical image segmentation capabilities of the Segment Anything Model","venue":"arXiv.org","year":2023,"referenceCount":8,"citationCount":7,"influentialCitationCount":0,"publicationDate":"10/04/2023","authors":"Saikat Roy,T. Wald,Gregor Koehler,Maximilian R. Rokuss,Nico Disch,Julius Holzschuh,David Zimmerer,K. Maier-Hein","id":"0931888d5cd7c26427cc116af2ac33863552da27","summary":"It is shown that SAM generalizes well to CT data, making it a potential catalyst for the advancement of semi-automatic segmentation tools for clinicians, and can serve as a highly potent starting point for further adaptations of such models to the intricacies of the medical domain.","score":1},{"url":"https://www.semanticscholar.org/paper/9570a3abed7c339ea2fa8d89ad1ee0f459e42fc1","title":"Transformer-Based Visual Segmentation: A Survey","venue":"arXiv.org","year":2023,"referenceCount":343,"citationCount":2,"influentialCitationCount":0,"publicationDate":"19/04/2023","authors":"Xiangtai Li,Henghui Ding,Wenwei Zhang,Haobo Yuan,Jiangmiao Pang,Guangliang Cheng,Kai Chen,Ziwei Liu,Chen Change Loy","id":"9570a3abed7c339ea2fa8d89ad1ee0f459e42fc1","summary":"This survey provides a thorough overview of transformer-based visual segmentation, summarizing recent advancements and summarizes a meta-architecture that unifies all recent transformer- based approaches.","score":1},{"url":"https://www.semanticscholar.org/paper/26a2a15c16c78f586169e4768720187c1ef14f8a","title":"UP OP : U NIFIED AND P ROGRESSIVE P RUNING FOR C OMPRESSING V ISION -L ANGUAGE T RANSFORMERS","venue":"","year":null,"referenceCount":0,"citationCount":0,"influentialCitationCount":0,"publicationDate":null,"authors":"","id":"26a2a15c16c78f586169e4768720187c1ef14f8a","summary":"The Unified and Progressive Pruning (UPop) is proposed that compresses vison-language Transformers via pruning and enables zero-cost subnetwork selection after searching countless multimodal subnetworks, and the searched subnetwork can be used without any retraining.","score":1},{"url":"https://www.semanticscholar.org/paper/0d0269f8533a33c3c310fd0a59815aa16a0c47ff","title":"Perception Test : A Diagnostic Benchmark for Multimodal Models","venue":"","year":2022,"referenceCount":39,"citationCount":2,"influentialCitationCount":0,"publicationDate":2022,"authors":"Viorica Patraucean,Lucas Smaira,Ankush Gupta,Adrià Recasens Continente,L. Markeeva,Dylan,Banarse,Mateusz Malinowski,Yezhou Yang,Carl Doersch,Tatiana Matejovicova,Yury Sulsky,Antoine,Miech,Skanda Koppula,A. Fréchette,H. Klimczak,R. Koster,Junlin Zhang,Stephanie,Winkler,Y. Aytar,Simon Osindero,D. Damen,Andrew Zisserman,João Carreira","id":"0d0269f8533a33c3c310fd0a59815aa16a0c47ff","summary":"The Perception Test introduces real-world videos designed to show perceptually interesting situations and defines multiple tasks that require understanding of memory, abstract patterns, physics, and semantics – across visual, audio, and text modalities.","score":1},{"url":"https://www.semanticscholar.org/paper/363270facc8a3ff229a2688f29f16800d45092ff","title":"A Unified View of Masked Image Modeling","venue":"","year":2022,"referenceCount":57,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Rahul Desai","id":"363270facc8a3ff229a2688f29f16800d45092ff","summary":"Under the unified view, a simple yet effective method, termed as MaskDistill, which reconstructs normalized semantic features from teacher models at the masked positions, conditioning on corrupted input images, achieves comparable or superior performance than state-of-the-art methods.","score":1},{"url":"https://www.semanticscholar.org/paper/660ddea322b193c7428f2a3149ebe3d24ff9d88d","title":"Image-and-Language Understanding from Pixels Only","venue":"arXiv.org","year":2022,"referenceCount":77,"citationCount":5,"influentialCitationCount":1,"publicationDate":2022,"authors":"M. Tschannen,Basil Mustafa,N. Houlsby","id":"660ddea322b193c7428f2a3149ebe3d24ff9d88d","summary":"This work explores an additional uniﬁcation: the use of a pure pixel-based model to perform image, text, and multimodal tasks and exploits the fact that CLIPPO does not require a tokenizer to show that it can achieve strong performance on multilingual multi-modal retrieval without modi ﬁcations.","score":1},{"url":"https://www.semanticscholar.org/paper/3c9ba25baca64151af4e9d50c7947de28eb2a599","title":"Survey of Hallucination in Natural Language Generation","venue":"ACM Computing Surveys","year":2022,"referenceCount":250,"citationCount":127,"influentialCitationCount":11,"publicationDate":"08/02/2022","authors":"Ziwei Ji,Nayeon Lee,Rita Frieske,Tiezheng Yu,D. Su,Yan Xu,Etsuko Ishii,Yejin Bang,Wenliang Dai,Andrea Madotto,Pascale Fung","id":"3c9ba25baca64151af4e9d50c7947de28eb2a599","summary":"A broad overview of the research progress and challenges in the hallucination problem in NLG is provided, including task-specific research progress on hallucinations in the following downstream tasks, namely abstractive summarization, dialogue generation, generative question answering, data-to-text generation, and machine translation.","score":1},{"url":"https://www.semanticscholar.org/paper/fa717a2e31f0cef4e26921f3b147a98644d2e64c","title":"Focal Modulation Networks","venue":"Neural Information Processing Systems","year":2022,"referenceCount":118,"citationCount":29,"influentialCitationCount":6,"publicationDate":"22/03/2022","authors":"Jianwei Yang,Chunyuan Li,Jianfeng Gao","id":"fa717a2e31f0cef4e26921f3b147a98644d2e64c","summary":"Focal modulation networks (FocalNets in short), where self-attention is completely replaced by a focal modulation mechanism for modeling token interactions in vision, exhibit clear superiority on the tasks of image classification, object detection, and segmentation.","score":1},{"url":"https://www.semanticscholar.org/paper/c26bb68806a992bf4fc85b5639e1657a445c4781","title":"On the Representation Collapse of Sparse Mixture of Experts","venue":"Neural Information Processing Systems","year":2022,"referenceCount":49,"citationCount":13,"influentialCitationCount":0,"publicationDate":"20/04/2022","authors":"Zewen Chi,Li Dong,Shaohan Huang,Damai Dai,Shuming Ma,Barun Patra,Saksham Singhal,Payal Bajaj,Xia Song,Furu Wei","id":"c26bb68806a992bf4fc85b5639e1657a445c4781","summary":"This work proposes to estimate the routing scores between tokens and experts on a low-dimensional hypersphere and achieves more consistent routing than the baseline mixture-of-experts methods.","score":1},{"url":"https://www.semanticscholar.org/paper/5598c4ece8ffc69a7eb584d16f6de6629044e76a","title":"Vision Transformer Adapter for Dense Predictions","venue":"arXiv.org","year":2022,"referenceCount":107,"citationCount":66,"influentialCitationCount":11,"publicationDate":"17/05/2022","authors":"Zhe Chen,Yuchen Duan,Wenhai Wang,Junjun He,Tong Lu,Jifeng Dai,Y. Qiao","id":"5598c4ece8ffc69a7eb584d16f6de6629044e76a","summary":"The ViT-Adapter is proposed, which allows plain ViT to achieve comparable performance to vision-specific transformers and facilitate future research and is verified on multiple dense prediction tasks, including object detection, instance segmentation, and semantic segmentation.","score":1},{"url":"https://www.semanticscholar.org/paper/53ae1072fd04080e4fc2c9205ebcbc2683d7264c","title":"Sparse Mixture-of-Experts are Domain Generalizable Learners","venue":"","year":2022,"referenceCount":96,"citationCount":1,"influentialCitationCount":0,"publicationDate":"08/06/2022","authors":"Bo Li,Yifei Shen,Jingkang Yang,Yezhen Wang,Jiawei Ren,Tong Che,Jun Zhang,Ziwei Liu","id":"53ae1072fd04080e4fc2c9205ebcbc2683d7264c","summary":"A formal framework to characterize a network's robustness to distribution shifts by studying its architecture's alignment with the correlations in the dataset is developed and guided to propose a novel DG model built upon vision transformers, namely Generalizable Mixture-of-Experts (GMoE).","score":1},{"url":"https://www.semanticscholar.org/paper/e617e103269488c0dae861066ccbacc0375a0efc","title":"Bridge-Tower: Building Bridges Between Encoders in Vision-Language Representation Learning","venue":"arXiv.org","year":2022,"referenceCount":109,"citationCount":3,"influentialCitationCount":1,"publicationDate":"17/06/2022","authors":"Xiao Xu,Chenfei Wu,Shachar Rosenman,Vasudev Lal,Nan Duan","id":"e617e103269488c0dae861066ccbacc0375a0efc","summary":"This paper proposes BridgeTower, which introduces multiple bridge layers that build a connection between the top layers of uni- modal encoders and each layer of the cross-modal encoder, which enables effective bottom-up cross-Modal alignment and fusion between visual and textual representations of different semantic levels of pre-trained uni -modal Encoder.","score":1},{"url":"https://www.semanticscholar.org/paper/8b5eab31e1c5689312fff3181a75bfbf5c13e51c","title":"Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks","venue":"arXiv.org","year":2022,"referenceCount":125,"citationCount":61,"influentialCitationCount":11,"publicationDate":"17/06/2022","authors":"Jiasen Lu,Christopher Clark,Rowan Zellers,Roozbeh Mottaghi,Aniruddha Kembhavi","id":"8b5eab31e1c5689312fff3181a75bfbf5c13e51c","summary":"Unified-IO is the first model capable of performing all 7 tasks on the GRIT benchmark and produces strong results across 16 diverse benchmarks like NYUv2-Depth, ImageNet, VQA2.0, OK-VQA, Swig, VizWizGround, BoolQ, and SciTail, with no task-specific fine-tuning.","score":1},{"url":"https://www.semanticscholar.org/paper/620369d6ed3ed68c3e4374d6ddf282e0b036d2f8","title":"Masked Vision and Language Modeling for Multi-modal Representation Learning","venue":"arXiv.org","year":2022,"referenceCount":73,"citationCount":8,"influentialCitationCount":1,"publicationDate":"03/08/2022","authors":"Gukyeong Kwon,Zhaowei Cai,Avinash Ravichandran,Erhan Bas,Rahul Bhotika,S. Soatto","id":"620369d6ed3ed68c3e4374d6ddf282e0b036d2f8","summary":"The proposed joint masked vision and language modeling, where the masked signal of one modality is reconstructed with the help from another modality, achieves state-of-the-art performance in the regime of millions of pre-training data.","score":1},{"url":"https://www.semanticscholar.org/paper/599be9043ef3571f65758cf36e184c9dc1781baf","title":"BEiT v2: Masked Image Modeling with Vector-Quantized Visual Tokenizers","venue":"arXiv.org","year":2022,"referenceCount":32,"citationCount":64,"influentialCitationCount":12,"publicationDate":"12/08/2022","authors":"Zhiliang Peng,Li Dong,Hangbo Bao,Qixiang Ye,Furu Wei","id":"599be9043ef3571f65758cf36e184c9dc1781baf","summary":"This work proposes to use a semantic-rich visual tokenizer as the reconstruction target for masked prediction, providing a systematic way to promote MIM from pixel-level to semantic-level, and proposes vector-quantized knowledge distillation to train the tokenizer, which discretizes a continuous semantic space to compact codes.","score":1},{"url":"https://www.semanticscholar.org/paper/29ac542838974c75a3ac40e5855ec7d346ea87ee","title":"Design of the topology for contrastive visual-textual alignment","venue":"arXiv.org","year":2022,"referenceCount":46,"citationCount":0,"influentialCitationCount":0,"publicationDate":"05/09/2022","authors":"Zhun Sun","id":"29ac542838974c75a3ac40e5855ec7d346ea87ee","summary":"This work discusses the desired properties of the topology and its endowed distance function for the embedding vectors of feature representations from the view of optimization and proposes a rather simple solution to improve the aforementioned problem.","score":1},{"url":"https://www.semanticscholar.org/paper/0afc96eca8b94d19a4c98fdd78e5fd9c68f6859a","title":"Statistical Foundation Behind Machine Learning and Its Impact on Computer Vision","venue":"arXiv.org","year":2022,"referenceCount":68,"citationCount":1,"influentialCitationCount":0,"publicationDate":"06/09/2022","authors":"Lei Zhang,H. Shum","id":"0afc96eca8b94d19a4c98fdd78e5fd9c68f6859a","summary":"The principle of uniform convergence in statistical learning is revisited, how it acts as the foundation behind machine learning is discussed, and it is argued that more fundamental research is needed on robustness, interpretability, and reasoning capabilities of machine learning by incorporating structure and knowledge.","score":1},{"url":"https://www.semanticscholar.org/paper/9fc5878d49c41beb12b62032b0afe9a8501fe9db","title":"PaLI: A Jointly-Scaled Multilingual Language-Image Model","venue":"arXiv.org","year":2022,"referenceCount":92,"citationCount":77,"influentialCitationCount":17,"publicationDate":"14/09/2022","authors":"Xi Chen,Xiao Wang,Soravit Changpinyo,A. Piergiovanni,Piotr Padlewski,Daniel M. Salz,Sebastian Goodman,Adam Grycner,Basil Mustafa,L. Beyer,Alexander Kolesnikov,J. Puigcerver,Nan Ding,Keran Rong,Hassan Akbari,Gaurav Mishra,Linting Xue,Ashish V. Thapliyal,James Bradbury,Weicheng Kuo,Mojtaba Seyedhosseini,Chao Jia,Burcu Karagol Ayan,C. Riquelme,A. Steiner,A. Angelova,Xiaohua Zhai,N. Houlsby,Radu Soricut","id":"9fc5878d49c41beb12b62032b0afe9a8501fe9db","summary":"The largest ViT to date is trained to quantify the benefits from even larger-capacity vision models, and PaLI achieves state-of-the-art in multiple vision and language tasks, while retaining a simple, modular, and scalable design.","score":1},{"url":"https://www.semanticscholar.org/paper/fdb86bd47092e777b6a75c597ec9982df37a8da8","title":"Correlation Information Bottleneck: Towards Adapting Pretrained Multimodal Models for Robust Visual Question Answering","venue":"","year":2022,"referenceCount":93,"citationCount":1,"influentialCitationCount":0,"publicationDate":"14/09/2022","authors":"Jingjing Jiang,Zi-yi Liu,Nanning Zheng","id":"fdb86bd47092e777b6a75c597ec9982df37a8da8","summary":"This paper proposes the Correlation Information Bottleneck (CIB) principle, which seeks a tradeoff between representation compression and redundancy by minimizing the mutual information (MI) between inputs and internal representations while maximizing the MI between outputs and the representations.","score":1},{"url":"https://www.semanticscholar.org/paper/9b9fb973e5d3b413baa90648d9eb0743bd889747","title":"Spotlight: Mobile UI Understanding using Vision-Language Models with a Focus","venue":"arXiv.org","year":2022,"referenceCount":32,"citationCount":0,"influentialCitationCount":0,"publicationDate":"29/09/2022","authors":"Gang Li,Yang Li","id":"9b9fb973e5d3b413baa90648d9eb0743bd889747","summary":"Spotlight is proposed, a vision-language model that only takes the screenshot of the UI and a region of interest on the screen -- the focus -- as the input and is easily scalable and capable of performing a range of UI modeling tasks.","score":1},{"url":"https://www.semanticscholar.org/paper/13d78bde4dc7059ab941871048ffa91d556584c8","title":"Where Should I Spend My FLOPS? Efficiency Evaluations of Visual Pre-training Methods","venue":"arXiv.org","year":2022,"referenceCount":32,"citationCount":4,"influentialCitationCount":0,"publicationDate":"30/09/2022","authors":"Skanda Koppula,Yazhe Li,Evan Shelhamer,Andrew Jaegle,Nikhil Parthasarathy,R. Arandjelović,João Carreira,Olivier J. H'enaff","id":"13d78bde4dc7059ab941871048ffa91d556584c8","summary":"The results call into question the commonly-held assumption that self-supervised methods inherently scale to large, uncurated data.","score":1},{"url":"https://www.semanticscholar.org/paper/836ca61c0226fd5f763335ad4c13acc784251343","title":"Towards a Unified View on Visual Parameter-Efficient Transfer Learning","venue":"arXiv.org","year":2022,"referenceCount":57,"citationCount":3,"influentialCitationCount":0,"publicationDate":"03/10/2022","authors":"Bruce X. B. Yu,Jianlong Chang,Lin Liu,Qi Tian,Changan Chen","id":"836ca61c0226fd5f763335ad4c13acc784251343","summary":"This work proposes a framework with a unified view of PETL called visual- PETL (V-PETL) to investigate the effects of different PETL techniques, data scales of downstream domains, positions of trainable parameters, and other aspects affecting the trade-off, and proposes a new variation of the prefix-tuning module called parallel attention (PATT) for vision downstream tasks.","score":1},{"url":"https://www.semanticscholar.org/paper/10667c1ae4b49808772b5a377c5b52196701267f","title":"When and why vision-language models behave like bags-of-words, and what to do about it?","venue":"arXiv.org","year":2022,"referenceCount":61,"citationCount":13,"influentialCitationCount":3,"publicationDate":"04/10/2022","authors":"Mert Yuksekgonul,Federico Bianchi,Pratyusha Kalluri,Dan Jurafsky,James Y. Zou","id":"10667c1ae4b49808772b5a377c5b52196701267f","summary":"It is demonstrated that it is possible to perform well on retrieval over existing datasets without using the composition and order information, and it is hypothesize that this can explain why the models do not need to learn to represent compositional information.","score":1},{"url":"https://www.semanticscholar.org/paper/8d6da4ea99898b208d93e7cba4d0ab0b8e160002","title":"W HEN AND WHY V ISION -L ANGUAGE M ODELS BE HAVE LIKE B AGS - OF -W ORDS , AND WHAT TO DO ABOUT IT ?","venue":"","year":2022,"referenceCount":46,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Mert Yuksekgonul,Federico Bianchi,Pratyusha Kalluri,Dan Jurafsky,James Y. Zou","id":"8d6da4ea99898b208d93e7cba4d0ab0b8e160002","summary":"The Attribution, Relation, and Order (ARO) benchmark is created to systematically evaluate the ability of VLMs to understand different types of relationships, attributes, and order information.","score":1},{"url":"https://www.semanticscholar.org/paper/d68c8db0e1b8b7f1e6c44393e0a425daa44a16c7","title":"VIMA: General Robot Manipulation with Multimodal Prompts","venue":"arXiv.org","year":2022,"referenceCount":125,"citationCount":28,"influentialCitationCount":5,"publicationDate":"06/10/2022","authors":"Yunfan Jiang,Agrim Gupta,Zichen Zhang,Guanzhi Wang,Yongqiang Dou,Yanjun Chen,Li Fei-Fei,Anima Anandkumar,Yuke Zhu,Linxi (Jim) Fan","id":"d68c8db0e1b8b7f1e6c44393e0a425daa44a16c7","summary":"This work designs a transformer-based generalist robot agent, VIMA, that processes these prompts and outputs motor actions autoregressively and achieves strong scalability in both model capacity and data size.","score":1},{"url":"https://www.semanticscholar.org/paper/29c2d3d77b6d6f24f4356d5ba20c1a6ab4229c76","title":"Open-Vocabulary Semantic Segmentation with Mask-adapted CLIP","venue":"arXiv.org","year":2022,"referenceCount":49,"citationCount":21,"influentialCitationCount":5,"publicationDate":"09/10/2022","authors":"Feng Liang,Bichen Wu,Xiaoliang Dai,Kunpeng Li,Yinan Zhao,Hang Zhang,Peizhao Zhang,Péter Vajda,D. Marculescu","id":"29c2d3d77b6d6f24f4356d5ba20c1a6ab4229c76","summary":"For the first time, open-vocabulary generalist models match the performance of supervised specialist models in 2017 without dataset-specific adaptations.","score":1},{"url":"https://www.semanticscholar.org/paper/a7336f6afd4eb817e66ca3eeb9c0a89ffacd53ed","title":"VoLTA: Vision-Language Transformer with Weakly-Supervised Local-Feature Alignment","venue":"arXiv.org","year":2022,"referenceCount":100,"citationCount":0,"influentialCitationCount":0,"publicationDate":"09/10/2022","authors":"Shraman Pramanick,Li Jing,Sayan Nag,Jiachen Zhu,Hardik Shah,Yann LeCun,Ramalingam Chellappa","id":"a7336f6afd4eb817e66ca3eeb9c0a89ffacd53ed","summary":"This work proposes VoLTA (Vision-Language Transformer with weakly-supervised local-feature Alignment), a new VLP paradigm that only utilizes image-caption data but achieves fine-grained region-level image understanding, eliminating the use of expensive box annotations.","score":1},{"url":"https://www.semanticscholar.org/paper/6540916e3ebaeaccefeaa303ba94c50bd581ff2a","title":"Like a bilingual baby: The advantage of visually grounding a bilingual language model","venue":"arXiv.org","year":2022,"referenceCount":46,"citationCount":0,"influentialCitationCount":0,"publicationDate":"11/10/2022","authors":"Khai-Nguyen Nguyen,Zixin Tang,A. Mali,M. Kelly","id":"6540916e3ebaeaccefeaa303ba94c50bd581ff2a","summary":"An LSTM language model is trained on images and captions in English and Spanish from MS-COCO-ES and it is found that the visual grounding improves the model's understanding of semantic similarity both within and across languages and improves perplexity.","score":1},{"url":"https://www.semanticscholar.org/paper/07099fe26ee8850c9ccba6fe2ee139d67289b67c","title":"Foundation Transformers","venue":"arXiv.org","year":2022,"referenceCount":38,"citationCount":0,"influentialCitationCount":0,"publicationDate":"12/10/2022","authors":"Hongyu Wang,Shuming Ma,Shaohan Huang,Li Dong,Wenhui Wang,Zhiliang Peng,Yu Wu,Payal Bajaj,Saksham Singhal,Alon Benhaim,Barun Patra,Zhun Liu,Vishrav Chaudhary,Xia Song,Furu Wei","id":"07099fe26ee8850c9ccba6fe2ee139d67289b67c","summary":"This work proposes Sub-LayerNorm for good expressivity, and the initialization strategy theoretically derived from DeepNet for stable scaling up, and introduces a Transformer variant, named Magneto, to fulfill the goal of true general-purpose modeling.","score":1},{"url":"https://www.semanticscholar.org/paper/eba51c023f3ae9eeca783893b973db60e7a99a6c","title":"A Unified View of Masked Image Modeling","venue":"arXiv.org","year":2022,"referenceCount":57,"citationCount":8,"influentialCitationCount":2,"publicationDate":"19/10/2022","authors":"Zhiliang Peng,Li Dong,Hangbo Bao,Qixiang Ye,Furu Wei","id":"eba51c023f3ae9eeca783893b973db60e7a99a6c","summary":"A simple yet effective method, termed as MaskDistill, which reconstructs normalized semantic features from teacher models at the masked positions, conditioning on corrupted input images, achieves comparable or superior performance than state-of-the-art methods.","score":1},{"url":"https://www.semanticscholar.org/paper/aeaf6966d460f28db97609e9baa45276395d05d5","title":"Less is More: Learning Simplicity in Datacenter Scheduling","venue":"International Green and Sustainable Computing Conference","year":2022,"referenceCount":14,"citationCount":0,"influentialCitationCount":0,"publicationDate":"24/10/2022","authors":"Wenkai Guan,Cristinel Ababei","id":"aeaf6966d460f28db97609e9baa45276395d05d5","summary":"A new scheduling algorithm, Qin2, for heterogeneous datacenters, which uses an efficient automatic feature selection technique and the integration of simple and training-efficient DNN models into a scheduler, which is deployed on a real cluster of heterogeneous nodes.","score":1},{"url":"https://www.semanticscholar.org/paper/30477855d76058a9b542cabea3058aad1a837d51","title":"A Case for Business Process-Specific Foundation Models","venue":"arXiv.org","year":2022,"referenceCount":51,"citationCount":1,"influentialCitationCount":0,"publicationDate":"26/10/2022","authors":"Yara Rizk,P. Venkateswaran,Vatche Isahagian,Vinod Muthusamy","id":"30477855d76058a9b542cabea3058aad1a837d51","summary":"It is argued that business process data representations have unique characteristics that warrant the development of a new class of foundation models to handle tasks like process mining, optimization, and decision making.","score":1},{"url":"https://www.semanticscholar.org/paper/9a6d83c836ce6389b526b941d971eee775aa573e","title":"ERNIE-ViLG 2.0: Improving Text-to-Image Diffusion Model with Knowledge-Enhanced Mixture-of-Denoising-Experts","venue":"arXiv.org","year":2022,"referenceCount":47,"citationCount":18,"influentialCitationCount":0,"publicationDate":"27/10/2022","authors":"Zhidan Feng,Zhenyu Zhang,Xintong Yu,Yewei Fang,Lanxin Li,Xuyi Chen,Yuxiang Lu,Jiaxiang Liu,Weichong Yin,Shi Feng,Yu Sun,Hao Tian,Hua Wu,Haifeng Wang","id":"9a6d83c836ce6389b526b941d971eee775aa573e","summary":"This paper proposes ERNIE-ViLG 2.0, a large-scale Chinese text-to-image diffusion model, to progressively upgrade the quality of generated images by incorporating fine-grained textual and visual knowledge of key elements in the scene, and utilizing different denoising experts at differentDenoising stages.","score":1},{"url":"https://www.semanticscholar.org/paper/c3d38dcba5b954d7b9919eb3f6f90afd095f1801","title":"Behavioral Intention Prediction in Driving Scenes: A Survey","venue":"arXiv.org","year":2022,"referenceCount":244,"citationCount":5,"influentialCitationCount":0,"publicationDate":"01/11/2022","authors":"Jianwu Fang,Fan Wang,Peining Shen,Zhedong Zheng,Jianru Xue,Tat-Seng Chua","id":"c3d38dcba5b954d7b9919eb3f6f90afd095f1801","summary":"This work focuses on BIP-conditioned prediction tasks, including trajectory prediction, behavior prediction, and accident prediction and explores the differences among various works in this branch of BIP.","score":1},{"url":"https://www.semanticscholar.org/paper/82a867d6d699231bf4de20dcac8efb293d11e7df","title":"State-of-the-art Models for Object Detection in Various Fields of Application","venue":"arXiv.org","year":2022,"referenceCount":29,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/11/2022","authors":"S. A. G. Naqvi,Syed Bazil Ali","id":"82a867d6d699231bf4de20dcac8efb293d11e7df","summary":"A comprehensive overview of a variety of both generic and specific object detection models, enlisting comparative results like inference time and average precision of box (AP) fixed at different Intersection Over Union (IoUs) and for different sized objects.","score":1},{"url":"https://www.semanticscholar.org/paper/3c2b12824b0027edb49b68300cbeab02cfc49ca8","title":"Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese","venue":"arXiv.org","year":2022,"referenceCount":79,"citationCount":3,"influentialCitationCount":1,"publicationDate":"02/11/2022","authors":"An Yang,Junshu Pan,Junyang Lin,Rui Men,Yichang Zhang,Jingren Zhou,Chang Zhou","id":"3c2b12824b0027edb49b68300cbeab02cfc49ca8","summary":"This work constructs a large-scale dataset of image-text pairs in Chinese, where most data is retrieved from publicly available datasets, and proposes a two-stage pretraining method, where the model is first trained with the image encoder frozen and then trained with all parameters being optimized to achieve enhanced model performance.","score":1},{"url":"https://www.semanticscholar.org/paper/3526c4f136b70b0b8c050a5e2e2926103da1c871","title":"Semantic Segmentation Algorithms for Ground AGV and UAV Medical Transport Scenes","venue":"CME","year":2022,"referenceCount":14,"citationCount":0,"influentialCitationCount":0,"publicationDate":"04/11/2022","authors":"Haosen Wang,Tiankai Chen,Shifeng Wang,Bo Lu","id":"3526c4f136b70b0b8c050a5e2e2926103da1c871","summary":"This paper compares other traditional semantic segmentation networks, and at the same time considers the characteristics of all-weather, all-terrain, and complex transportation of materials in medical transportation, and proposes SSMMTNet(Semantic segmentation of medical material transportation Net).","score":1},{"url":"https://www.semanticscholar.org/paper/58a18a0937fc199e87fbd455af3a53b157462217","title":"Group DETR v2: Strong Object Detector with Encoder-Decoder Pretraining","venue":"arXiv.org","year":2022,"referenceCount":30,"citationCount":11,"influentialCitationCount":2,"publicationDate":"07/11/2022","authors":"Qiang Chen,Jian Wang,Chuchu Han,Shangang Zhang,Zexian Li,Xiaokang Chen,Jiahui Chen,Xiaodi Wang,Shumin Han,Gang Zhang,Haocheng Feng,Kun Yao,Junyu Han,Errui Ding,Jingdong Wang","id":"58a18a0937fc199e87fbd455af3a53b157462217","summary":"Group DETR v2 achieves $\\textbf{64.5}$ mAP on COCO test-dev, and establishes a new SoTA on the C OCO leaderboard.","score":1},{"url":"https://www.semanticscholar.org/paper/c90a33f1f0049d524e9b5b3174d35611fd9a8096","title":"Pretraining in Deep Reinforcement Learning: A Survey","venue":"arXiv.org","year":2022,"referenceCount":213,"citationCount":2,"influentialCitationCount":0,"publicationDate":"08/11/2022","authors":"Zhihui Xie,Zichuan Lin,Junyou Li,Shuai Li,Deheng Ye","id":"c90a33f1f0049d524e9b5b3174d35611fd9a8096","summary":"This survey seeks to systematically review existing works in pretraining for deep reinforcement learning, provide a taxonomy of these methods, discuss each sub-field, and bring attention to open problems and future directions.","score":1},{"url":"https://www.semanticscholar.org/paper/b839b60ffcdedf3f0dfa43da3eefe843307679f3","title":"Towards Reasoning-Aware Explainable VQA","venue":"arXiv.org","year":2022,"referenceCount":23,"citationCount":0,"influentialCitationCount":0,"publicationDate":"09/11/2022","authors":"Rakesh Vaideeswaran,Feng Gao,Abhinav Mathur,G. Thattai","id":"b839b60ffcdedf3f0dfa43da3eefe843307679f3","summary":"This paper investigates two network architectures, including Long Short-Term Memory (LSTM) and Transformer decoder, as the explanation generator, and generates human-readable textual explanations while maintaining SOTA VQA accuracy on the GQA-REX and V QA-E datasets.","score":1},{"url":"https://www.semanticscholar.org/paper/6a993404e07687b7edb7fb9a05092213a9419859","title":"OneFormer: One Transformer to Rule Universal Image Segmentation","venue":"arXiv.org","year":2022,"referenceCount":62,"citationCount":13,"influentialCitationCount":1,"publicationDate":"10/11/2022","authors":"Jitesh Jain,Jiacheng Li,M. Chiu,Ali Hassani,Nikita Orlov,H. Shi","id":"6a993404e07687b7edb7fb9a05092213a9419859","summary":null,"score":1},{"url":"https://www.semanticscholar.org/paper/26c80bd65baa90f5b18157de4951f4eb0b62ab69","title":"InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions","venue":"arXiv.org","year":2022,"referenceCount":106,"citationCount":26,"influentialCitationCount":4,"publicationDate":"10/11/2022","authors":"Wenhai Wang,Jifeng Dai,Zhe Chen,Zhenhang Huang,Zhiqi Li,Xizhou Zhu,Xiao-hua Hu,Tong Lu,Lewei Lu,Hongsheng Li,Xiaogang Wang,Y. Qiao","id":"26c80bd65baa90f5b18157de4951f4eb0b62ab69","summary":"This work presents a new large-scale CNN-based foundation model, termed InternImage, which can obtain the gain from increasing parameters and training data like ViTs, and reduces the strict inductive bias of traditional CNNs and makes it possible to learn stronger and more robust patterns with large- scale parameters from massive datalike ViTs.","score":1},{"url":"https://www.semanticscholar.org/paper/db3b99c407ff8a06bdc96151dfae1328fadfb858","title":"Grafting Pre-trained Models for Multimodal Headline Generation","venue":"Conference on Empirical Methods in Natural Language Processing","year":2022,"referenceCount":43,"citationCount":0,"influentialCitationCount":0,"publicationDate":"14/11/2022","authors":"Lingfeng Qiao,Chen Wu,Ye Liu,Haoyuan Peng,Di Yin,Bo Ren","id":"db3b99c407ff8a06bdc96151dfae1328fadfb858","summary":"This paper proposes a novel approach to graft the video encoder from the pre- trained video-language model on the generative pre-trained language model, and presents a consensus fusion mechanism for the integration of different components, via inter/intra modality relation.","score":1},{"url":"https://www.semanticscholar.org/paper/78281482c1fdad8e167bab39cc9955c73d58ae8f","title":"EVA: Exploring the Limits of Masked Visual Representation Learning at Scale","venue":"arXiv.org","year":2022,"referenceCount":125,"citationCount":27,"influentialCitationCount":4,"publicationDate":"14/11/2022","authors":"Yuxin Fang,Wen Wang,Binhui Xie,Quan-Sen Sun,Ledell Yu Wu,Xinggang Wang,Tiejun Huang,Xinlong Wang,Yue Cao","id":"78281482c1fdad8e167bab39cc9955c73d58ae8f","summary":"Evaluating the vision tower of a giant CLIP from EVA can greatly stabilize the training and outperform the training from scratch counterpart with much fewer samples and less compute, providing a new direction for scaling up and accelerating the costly training of multi-modal foundation models.","score":1},{"url":"https://www.semanticscholar.org/paper/30a3731f09e7a391e79a28fa736fa6bdd8331866","title":"Uni-Perceiver v2: A Generalist Model for Large-Scale Vision and Vision-Language Tasks","venue":"arXiv.org","year":2022,"referenceCount":42,"citationCount":4,"influentialCitationCount":0,"publicationDate":"17/11/2022","authors":"Hao Li,Jinguo Zhu,Xiaohu Jiang,Xizhou Zhu,Hongsheng Li,Chun Yuan,Xiaohua Wang,Y. Qiao,Xiaogang Wang,Wenhai Wang,Jifeng Dai","id":"30a3731f09e7a391e79a28fa736fa6bdd8331866","summary":"This paper proposes Uni-Perceiver v2, which is the first generalist model capable of handling major large-scale vision and vision-language tasks with competitive performance and proposes an improved optimizer to ensure stable multi-task learning with an unmixed sampling strategy, helpful for tasks requiring large batch-size training.","score":1},{"url":"https://www.semanticscholar.org/paper/96282f6456b10da5acfb8268632ef86645f4b339","title":"Cross-Modal Adapter for Text-Video Retrieval","venue":"arXiv.org","year":2022,"referenceCount":57,"citationCount":1,"influentialCitationCount":0,"publicationDate":"17/11/2022","authors":"Haojun Jiang,Jianke Zhang,Rui Huang,Chunjiang Ge,Z. Ni,Jiwen Lu,Jie Zhou,S. Song,Gao Huang","id":"96282f6456b10da5acfb8268632ef86645f4b339","summary":"This work presents a novel Cross-Modal Adapter for parameter-efficient fine-tuning, Inspired by adapter-based methods, and adjusts the pre-trained model with a few parameterization layers, designed for the multi-modal domain.","score":1},{"url":"https://www.semanticscholar.org/paper/b2286da2b293b50644e5dc8ddd75eb8651e8b257","title":"UniFormerV2: Spatiotemporal Learning by Arming Image ViTs with Video UniFormer","venue":"arXiv.org","year":2022,"referenceCount":91,"citationCount":4,"influentialCitationCount":1,"publicationDate":"17/11/2022","authors":"Kunchang Li,Yali Wang,Yinan He,Yizhuo Li,Yi Wang,Limin Wang,Y. Qiao","id":"b2286da2b293b50644e5dc8ddd75eb8651e8b257","summary":"This work proposes a generic paradigm to build a powerful family of video networks, by arming the pretrained ViTs with efficient UniFormer designs, which contains brand-new local and global relation aggregators, which allow for preferable accuracy-computation balance by seamlessly integrating advantages from both ViTs and UniFormer.","score":1},{"url":"https://www.semanticscholar.org/paper/ee96ec926f06ff2f3ce3d131cffcbfe63af39f0c","title":"Towards All-in-one Pre-training via Maximizing Multi-modal Mutual Information","venue":"arXiv.org","year":2022,"referenceCount":98,"citationCount":3,"influentialCitationCount":0,"publicationDate":"17/11/2022","authors":"Weijie Su,Xizhou Zhu,Chenxin Tao,Lewei Lu,Bin Li,Gao Huang,Y. Qiao,Xiaogang Wang,Jie Zhou,Jifeng Dai","id":"ee96ec926f06ff2f3ce3d131cffcbfe63af39f0c","summary":"This paper proposes an all-in-one single-stage pre-training approach, named Maximizing Multi-modal Mutual Information Pre- training (M3I Pre-training), which achieves better performance than previous pre- training methods on various vision benchmarks, including ImageNet classification, COCO object detection, LVIS long-tailed object detection and ADE20k semantic segmentation.","score":1},{"url":"https://www.semanticscholar.org/paper/33ef78737ba57ecc1ff98c22369a8e17ed90eb98","title":"Synthesizing Coherent Story with Auto-Regressive Latent Diffusion Models","venue":"arXiv.org","year":2022,"referenceCount":44,"citationCount":5,"influentialCitationCount":1,"publicationDate":"20/11/2022","authors":"Xichen Pan,Pengda Qin,Yuhong Li,Hui Xue,Wenhu Chen","id":"33ef78737ba57ecc1ff98c22369a8e17ed90eb98","summary":"This work proposes AR-LDM, a latent diffusion model auto-regressively conditioned on history captions and generated images that achieves SoTA FID scores on PororoSV, FlintstonesV, and the newly introduced challenging dataset VIST containing natural images.","score":1},{"url":"https://www.semanticscholar.org/paper/66e6d0dd6fe47db69f55bb6f959d672e6e5c6bbd","title":"VATLM: Visual-Audio-Text Pre-Training with Unified Masked Prediction for Speech Representation Learning","venue":"arXiv.org","year":2022,"referenceCount":63,"citationCount":2,"influentialCitationCount":2,"publicationDate":"21/11/2022","authors":"Qiu-shi Zhu,Long Zhou,Zi-Hua Zhang,Shujie Liu,Binxing Jiao,J. Zhang,Lirong Dai,Daxin Jiang,Jinyu Li,Furu Wei","id":"66e6d0dd6fe47db69f55bb6f959d672e6e5c6bbd","summary":"Results show that the proposed VATLM outperforms previous the state-of-the-art models, such as audio-visual pre-trained AV-HuBERT model, and analysis demonstrates that VATLM is capable of aligning different modalities into the same space.","score":1},{"url":"https://www.semanticscholar.org/paper/fd8c1b8741163d8737652fbcd3507bcd7d6225c7","title":"Multitask Vision-Language Prompt Tuning","venue":"arXiv.org","year":2022,"referenceCount":103,"citationCount":3,"influentialCitationCount":0,"publicationDate":"21/11/2022","authors":"Sheng Shen,Shijia Yang,Tianjun Zhang,Bohan Zhai,Joseph Gonzalez,K. Keutzer,Trevor Darrell","id":"fd8c1b8741163d8737652fbcd3507bcd7d6225c7","summary":"This paper demonstrates the effectiveness of learning a single transferable prompt from multiple source tasks to initialize the prompt for each target task and shows many target tasks can benefit each other from sharing prompt vectors and thus can be jointly learned via multitask prompt tuning.","score":1},{"url":"https://www.semanticscholar.org/paper/13c97db0f424449ae384b20b18022a3d7e422455","title":"DETRs with Collaborative Hybrid Assignments Training","venue":"arXiv.org","year":2022,"referenceCount":39,"citationCount":5,"influentialCitationCount":0,"publicationDate":"22/11/2022","authors":"Zhuofan Zong,Guanglu Song,Yu Liu","id":"13c97db0f424449ae384b20b18022a3d7e422455","summary":"This paper presents a novel collaborative hybrid assignments training scheme, namely Co-DETR, to learn more efficient and effective DETR-based detectors from versatile label assignment manners and introduces no additional parameters and computational cost to the original detector while requiring no hand-crafted non-maximum suppression (NMS).","score":1},{"url":"https://www.semanticscholar.org/paper/1c199e3d50349153c0b6200006b02fbe66c27acd","title":"X2-VLM: All-In-One Pre-trained Model For Vision-Language Tasks","venue":"arXiv.org","year":2022,"referenceCount":79,"citationCount":5,"influentialCitationCount":0,"publicationDate":"22/11/2022","authors":"Yan Zeng,Xinsong Zhang,Hang Li,Jiawei Wang,Jipeng Zhang,Wangchunshu Zhou","id":"1c199e3d50349153c0b6200006b02fbe66c27acd","summary":"Results show that X$^2$-VLM performs the best on base and large scale for both image-text and video-text tasks, making a good trade-off between performance and model scale, and it is shown that the modular design of X$1.2-V LM results in high transferability for X$2- VLM to be utilized in any language or domain.","score":1},{"url":"https://www.semanticscholar.org/paper/f64111aa1a5695e9209bca131469b1dc184d91d0","title":"Seeing What You Miss: Vision-Language Pre-training with Semantic Completion Learning","venue":"arXiv.org","year":2022,"referenceCount":52,"citationCount":0,"influentialCitationCount":0,"publicationDate":"24/11/2022","authors":"Yatai Ji,Rong-Cheng Tu,Jie Jiang,Weijie Kong,Chengfei Cai,Wenzhe Zhao,Hongfa Wang,Yujiu Yang,Wei Liu","id":"f64111aa1a5695e9209bca131469b1dc184d91d0","summary":"A novel Semantic Completion Learning (SCL) task, complementary to existing masked modeling tasks, to facilitate global-to-local alignment, and presents a flexible vision encoder, which enables the model to perform image-text and video-text multimodal tasks simultaneously.","score":1},{"url":"https://www.semanticscholar.org/paper/4e6a2d863aeaafed82a8411f01be6e5a9f801b44","title":"SLAN: Self-Locator Aided Network for Cross-Modal Understanding","venue":"arXiv.org","year":2022,"referenceCount":42,"citationCount":0,"influentialCitationCount":0,"publicationDate":"28/11/2022","authors":"Jiang-Tian Zhai,Qi Zhang,Tong Wu,Xinghan Chen,Jiangjiang Liu,Bo Ren,Ming-Ming Cheng","id":"4e6a2d863aeaafed82a8411f01be6e5a9f801b44","summary":"Self-Locator Aided Network (SLAN) is proposed for cross-modal understanding tasks without any extra gold data and demonstrates strong zero-shot and fine-tuned transferability to two localization tasks.","score":1},{"url":"https://www.semanticscholar.org/paper/3dc7cb6c14cec8b02a150bfb8ce95e8e3e8a01f2","title":"Synaptic Dynamics Realize First-order Adaptive Learning and Weight Symmetry","venue":"arXiv.org","year":2022,"referenceCount":29,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/12/2022","authors":"Yukun Yang,Peng Li","id":"3dc7cb6c14cec8b02a150bfb8ce95e8e3e8a01f2","summary":"A realization of the Adam optimizer is demonstrated using biologically-plausible mechanisms in synapses, and a new approach, inspired by the predisposition property of synapses observed in neuroscience, to circumvent the biological implausibility of the weight transport problem in backpropagation (BP).","score":1},{"url":"https://www.semanticscholar.org/paper/91694fc5f0bae350157f4fc565d0207ae12f7eb9","title":"FusionBrain: Research Project in Multimodal and Multitask Learning","venue":"Doklady. Mathematics","year":2022,"referenceCount":3,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/12/2022","authors":"Dimitar I. Dimitrov,A. V. Kuznetsov,A. A. Mal’tseva,E. F. Goncharova","id":"91694fc5f0bae350157f4fc565d0207ae12f7eb9","summary":"The general purpose and idea of the project is to learn to create models that can effectively extract additional important knowledge from a large number of data modalities and training tasks and, as a result, can better solve other tasks.","score":1},{"url":"https://www.semanticscholar.org/paper/1db3db7e00fd53290f0c0d07f22937ed5fedfabf","title":"Masked Contrastive Pre-Training for Efficient Video-Text Retrieval","venue":"arXiv.org","year":2022,"referenceCount":63,"citationCount":2,"influentialCitationCount":0,"publicationDate":"02/12/2022","authors":"Fangxun Shu,Biaolong Chen,Yue Liao,Ke Gao,Shuwen Xiao,Wenyu Sun,Xiaobo Li,Yousong Zhu,Jinqiao Wang,Si Liu","id":"1db3db7e00fd53290f0c0d07f22937ed5fedfabf","summary":"The MAC aims to reduce video representation's spatial and temporal redundancy in the VidLP model by a mask sampling mechanism to improve pre-training efficiency and achieves state-of-the-art results on various video-text retrieval datasets, including MSR-VTT, DiDeMo, and ActivityNet.","score":1},{"url":"https://www.semanticscholar.org/paper/040fdeabc5e934f13eb7b05c1907568b7d7efe81","title":"Knowledge Helps Pretrained Model: An Ensemble Model of Knowledge Model and CLIP for Zero-shot Image Recognition","venue":"2022 4th International Conference on Frontiers Technology of Information and Computer (ICFTIC)","year":2022,"referenceCount":21,"citationCount":0,"influentialCitationCount":0,"publicationDate":"02/12/2022","authors":"Bo Xu","id":"040fdeabc5e934f13eb7b05c1907568b7d7efe81","summary":"An ensemble method Knowledge and Pretrained Ensemble (KPE) model that utilize the strength of both of the knowledge graph and large scale pretrained model for zero-shot image classification task is proposed.","score":1},{"url":"https://www.semanticscholar.org/paper/36306b2de6e9b2b11d53b029e754b03977de6072","title":"Compound Tokens: Channel Fusion for Vision-Language Representation Learning","venue":"arXiv.org","year":2022,"referenceCount":59,"citationCount":1,"influentialCitationCount":0,"publicationDate":"02/12/2022","authors":"Maxwell Mbabilla Aladago,A. Piergiovanni","id":"36306b2de6e9b2b11d53b029e754b03977de6072","summary":"The effectiveness of compound tokens is demonstrated using an encoder-decoder vision-language model trained end-to-end in the open-vocabulary setting and achieves highly competitive performance across a range of question answering tasks including GQA, VQA2.0, and SNLI-VE.","score":1},{"url":"https://www.semanticscholar.org/paper/17066da1e298a997c123f551bf0515daccc2b7b5","title":"Unifying Vision, Text, and Layout for Universal Document Processing","venue":"arXiv.org","year":2022,"referenceCount":57,"citationCount":2,"influentialCitationCount":0,"publicationDate":"05/12/2022","authors":"Zineng Tang,Ziyi Yang,Guoxin Wang,Yuwei Fang,Yang Liu,Chenguang Zhu,Michael Zeng,Chao-Yue Zhang,Mohit Bansal","id":"17066da1e298a997c123f551bf0515daccc2b7b5","summary":"This work proposes Universal Document Processing, a foundation Document AI model which unifies text, image, and layout modalities together with varied task formats, including document understanding and generation, and is the first time in the field of document AI that one model simultaneously achieves high-quality neural document editing and content customization.","score":1},{"url":"https://www.semanticscholar.org/paper/325d8e9501af05e594bd668b6cd6d43ed42c8b4d","title":"InternVideo: General Video Foundation Models via Generative and Discriminative Learning","venue":"arXiv.org","year":2022,"referenceCount":110,"citationCount":12,"influentialCitationCount":4,"publicationDate":"06/12/2022","authors":"Yi Wang,Kunchang Li,Yizhuo Li,Yinan He,Bingkun Huang,Zhiyu Zhao,Hongjie Zhang,Jilan Xu,Yi Liu,Zun Wang,Sen Xing,Guo Chen,Junting Pan,Jiashuo Yu,Yali Wang,Limin Wang,Yu Qiao","id":"325d8e9501af05e594bd668b6cd6d43ed42c8b4d","summary":"Without bells and whistles, InternVideo achieves state-of-the-art performance on 39 video datasets from extensive tasks including video action recognition/detection, video-language alignment, and open-world video applications.","score":1},{"url":"https://www.semanticscholar.org/paper/458e3d2be80c401fb47e562d9d57012bd63da1c3","title":"Talking Head Generation with Probabilistic Audio-to-Visual Diffusion Priors","venue":"arXiv.org","year":2022,"referenceCount":67,"citationCount":3,"influentialCitationCount":0,"publicationDate":"07/12/2022","authors":"Zhentao Yu,Zixin Yin,Deyu Zhou,Duomin Wang,Finn Wong,Baoyuan Wang","id":"458e3d2be80c401fb47e562d9d57012bd63da1c3","summary":"A simple and novel framework for one-shot audio-driven talking head generation that probabilistically sample all the holistic lip-irrelevant facial motions to semantically match the input audio while still maintaining both the photo-realism of audio-lip synchronization and the overall naturalness.","score":1},{"url":"https://www.semanticscholar.org/paper/933b37b21e9d61139660088adb032ff3fdf56d86","title":"Learning Video Representations from Large Language Models","venue":"arXiv.org","year":2022,"referenceCount":86,"citationCount":2,"influentialCitationCount":1,"publicationDate":"08/12/2022","authors":"Yue Zhao,Ishan Misra,Philipp Krahenbuhl,Rohit Girdhar","id":"933b37b21e9d61139660088adb032ff3fdf56d86","summary":"LaViLa, a new approach to learning video-language representations by leveraging Large Language Models to be conditioned on visual input, and finetune them to create automatic video narrators, offers a number of advantages, including dense coverage of long videos, better temporal synchronization of the visual information and text, and much higher diversity of text.","score":1},{"url":"https://www.semanticscholar.org/paper/d232d97761490828f20e9b77d2c91a135a7270ee","title":"OFASys: A Multi-Modal Multi-Task Learning System for Building Generalist Models","venue":"arXiv.org","year":2022,"referenceCount":113,"citationCount":1,"influentialCitationCount":0,"publicationDate":"08/12/2022","authors":"Jinze Bai,Rui Men,Han Yang,Xuancheng Ren,K. Dang,Yichang Zhang,Xiaohuan Zhou,Peng Wang,Sinan Tan,An Yang,Zeyu Cui,Yu Han,Shuai Bai,Wenhang Ge,Jianxin Ma,Junyang Lin,Jingren Zhou,Chang Zhou","id":"d232d97761490828f20e9b77d2c91a135a7270ee","summary":"A generalist model learning system built on top of a declarative task interface named OFASys, which develops a first-in-kind, single model, OFA+, that can handle text, image, speech, video, and motion data and demonstrates the performance reliability of multi-modal task-scaling provided byOFASys.","score":1},{"url":"https://www.semanticscholar.org/paper/2096246df0649fbbd9adc7895d2a50beeb46b6b7","title":"Structured Vision-Language Pretraining for Computational Cooking","venue":"arXiv.org","year":2022,"referenceCount":83,"citationCount":1,"influentialCitationCount":0,"publicationDate":2022,"authors":"Mustafa Shukor,Nicolas Thome,M. Cord","id":"2096246df0649fbbd9adc7895d2a50beeb46b6b7","summary":"VLPCook outperforms current SoTA by a signiﬁcant margin (+3.3 Recall@1 absolute improvement) on the task of Cross-Modal Food Retrieval on the large Recipe1M dataset.","score":1},{"url":"https://www.semanticscholar.org/paper/8ca316a10a2749e4c6bf3d0284e8cce2f56a4543","title":"Open Vocabulary Semantic Segmentation with Patch Aligned Contrastive Learning","venue":"arXiv.org","year":2022,"referenceCount":61,"citationCount":5,"influentialCitationCount":0,"publicationDate":"09/12/2022","authors":"Jishnu Mukhoti,Tsung-Yu Lin,Omid Poursaeed,Rui Wang,Ashish Shah,Philip H. S. Torr,S. Lim","id":"8ca316a10a2749e4c6bf3d0284e8cce2f56a4543","summary":"Patch Aligned Contrastive Learning (PACL), a modified compatibility function for CLIP's contrastive loss, is introduced, intending to train an alignment between the patch tokens of the vision encoder and the CLS token of the text encoder, to transfer seamlessly to the task of open vocabulary semantic segmentation.","score":1},{"url":"https://www.semanticscholar.org/paper/f5c853861fcde704a7100e24e963c5262e625229","title":"Video-Text Modeling with Zero-Shot Transfer from Contrastive Captioners","venue":"arXiv.org","year":2022,"referenceCount":76,"citationCount":5,"influentialCitationCount":3,"publicationDate":"09/12/2022","authors":"Shen Yan,Tao Zhu,Zirui Wang,Yuan Cao,Mi Zhang,Soham Ghosh,Yonghui Wu,Jiahui Yu","id":"f5c853861fcde704a7100e24e963c5262e625229","summary":"This work presents VideoCoCa, a maximally reuses a pretrained image-text contrastive captioner (CoCa) model and adapt it to video-text tasks with minimal extra training, yielding state-of-the-art results on zero-shot video classification and zero- shot text-to-video retrieval.","score":1},{"url":"https://www.semanticscholar.org/paper/c8dbf43fc20160814b9506de32be86ada91fa725","title":"VindLU: A Recipe for Effective Video-and-Language Pretraining","venue":"arXiv.org","year":2022,"referenceCount":94,"citationCount":5,"influentialCitationCount":0,"publicationDate":"09/12/2022","authors":"Feng Cheng,Xizi Wang,Jie Lei,David J. Crandall,Mohit Bansal,Gedas Bertasius","id":"c8dbf43fc20160814b9506de32be86ada91fa725","summary":"A thorough empirical study demystifying the most important factors in the VidL model design and develops a step-by-step recipe, dubbed VindLU, for effective VidL pretraining, which achieves comparable or better than state-of-the-art results on several VidL tasks without relying on external CLIP pretraining.","score":1},{"url":"https://www.semanticscholar.org/paper/3e8251f259dc529b3aa2366fc68c1516b202cfb9","title":"REVEAL: Retrieval-Augmented Visual-Language Pre-Training with Multi-Source Multimodal Knowledge Memory","venue":"arXiv.org","year":2022,"referenceCount":52,"citationCount":2,"influentialCitationCount":0,"publicationDate":"10/12/2022","authors":"Ziniu Hu,Ahmet Iscen,Chen Sun,Zirui Wang,Kai-Wei Chang,Yizhou Sun,C. Schmid,David A. Ross,A. Fathi","id":"3e8251f259dc529b3aa2366fc68c1516b202cfb9","summary":"An end-to-end Retrieval-Augmented Visual Language Model that learns to encode world knowledge into a large-scale memory, and to retrieve from it to answer knowledge-intensive queries, which is shown to result in significant gains on visual question answering and image captioning.","score":1},{"url":"https://www.semanticscholar.org/paper/fe34137e5cc07235eae65ce53a54cd226b9f8b23","title":"MAGVIT: Masked Generative Video Transformer","venue":"arXiv.org","year":2022,"referenceCount":82,"citationCount":6,"influentialCitationCount":1,"publicationDate":"10/12/2022","authors":"Lijun Yu,Yong Cheng,Kihyuk Sohn,José Lezama,Han Zhang,Huiwen Chang,A. Hauptmann,Ming-Hsuan Yang,Yuan Hao,Irfan Essa,Lu Jiang","id":"fe34137e5cc07235eae65ce53a54cd226b9f8b23","summary":"A 3D tokenizer to quantize a video into spatial-temporal visual tokens and propose an embedding method for masked video token modeling to facilitate multi-task learning are introduced.","score":1},{"url":"https://www.semanticscholar.org/paper/ec8f75e22ffbb5ad7e2f9cfc20a7780eed45715b","title":"CLIPPO: Image-and-Language Understanding from Pixels Only","venue":"","year":2022,"referenceCount":84,"citationCount":0,"influentialCitationCount":0,"publicationDate":"15/12/2022","authors":"M. Tschannen,Basil Mustafa,N. Houlsby","id":"ec8f75e22ffbb5ad7e2f9cfc20a7780eed45715b","summary":"The fact that CLIPPO does not require a tokenizer is exploited to show that it can achieve strong performance on multilingual multimodal retrieval without modifications, and it can obtain good accuracy in visual question answering, simply by rendering the question and image together.","score":1},{"url":"https://www.semanticscholar.org/paper/1b31dbf44e68b698120552366df03e6e35a1e428","title":"Objaverse: A Universe of Annotated 3D Objects","venue":"arXiv.org","year":2022,"referenceCount":86,"citationCount":8,"influentialCitationCount":3,"publicationDate":"15/12/2022","authors":"Matt Deitke,Dustin Schwenk,Jordi Salvador,Luca Weihs,Oscar Michel,Eli VanderBilt,Ludwig Schmidt,Kiana Ehsani,Aniruddha Kembhavi,Ali Farhadi","id":"1b31dbf44e68b698120552366df03e6e35a1e428","summary":"The large potential of Objaverse is demonstrated via four diverse applications: training generative 3D models, improving tail category segmentation on the LVIS benchmark, training open-vocabulary object-navigation models for Embodied AI, and creating a new benchmark for robustness analysis of vision models.","score":1},{"url":"https://www.semanticscholar.org/paper/30279ffe74bc5eccbb37bf7082056e7065727bc4","title":"On Human Visual Contrast Sensitivity and Machine Vision Robustness: A Comparative Study","venue":"arXiv.org","year":2022,"referenceCount":29,"citationCount":0,"influentialCitationCount":0,"publicationDate":"16/12/2022","authors":"Ming-Chang Chiu,Yingfei Wang,Derrick Eui Gyu Kim,Pin-Yu Chen,Xuezhe Ma","id":"30279ffe74bc5eccbb37bf7082056e7065727bc4","summary":"Drawing inspirations from ophthalmology and the robustness literature, this work analogize contrast sensitivity from the human visual aspect to machine vision and complement the current robustness study using corrupted images with the authors' CIFAR-CoCo datasets.","score":1},{"url":"https://www.semanticscholar.org/paper/22471140ae31b15dd55241e4be0c8bb851961ddc","title":"Autoencoders as Cross-Modal Teachers: Can Pretrained 2D Image Transformers Help 3D Representation Learning?","venue":"arXiv.org","year":2022,"referenceCount":121,"citationCount":5,"influentialCitationCount":1,"publicationDate":"16/12/2022","authors":"Runpei Dong,Zekun Qi,Linfeng Zhang,Junbo Zhang,Jian‐Yuan Sun,Zheng Ge,Li Yi,Kaisheng Ma","id":"22471140ae31b15dd55241e4be0c8bb851961ddc","summary":"This paper revisits masked modeling in a unified fashion of knowledge distillation, and shows that foundational Transformers pretrained with 2D images or natural languages can help self-supervised 3D representation learning through training Autoencoders as Cross-Modal Teachers (ACT).","score":1},{"url":"https://www.semanticscholar.org/paper/89a1dbbfd4c96d90b769f5d3427bd970b082898e","title":"BEATs: Audio Pre-Training with Acoustic Tokenizers","venue":"arXiv.org","year":2022,"referenceCount":66,"citationCount":3,"influentialCitationCount":0,"publicationDate":"18/12/2022","authors":"Sanyuan Chen,Yu Wu,Chengyi Wang,Shujie Liu,Daniel C. Tompkins,Zhuo Chen,Furu Wei","id":"89a1dbbfd4c96d90b769f5d3427bd970b082898e","summary":"BEATs, an iterative audio pre-training framework to learn Bidirectional Encoder representation from Audio Transformers, where an acoustic tokenizer and an audio SSL model are optimized by iterations, demonstrates that acoustic tokenizers can generate discrete labels with rich audio semantics and audio SSL models achieve state-of-the-art results across various audio classification benchmarks.","score":1},{"url":"https://www.semanticscholar.org/paper/fbed623ca22abaa493081f7d97be51b1c317d437","title":"Transferring General Multimodal Pretrained Models to Text Recognition","venue":"arXiv.org","year":2022,"referenceCount":38,"citationCount":0,"influentialCitationCount":0,"publicationDate":"19/12/2022","authors":"Junyang Lin,Xuancheng Ren,Yichang Zhang,Gao Liu,Peng Wang,An Yang,Chang Zhou","id":"fbed623ca22abaa493081f7d97be51b1c317d437","summary":"This paper recast text recognition as image captioning and directly transfer a unified vision-language pretrained model to the end task, resulting in a new method, OFA-OCR, to transfer multimodal pretrained models to text recognition.","score":1},{"url":"https://www.semanticscholar.org/paper/4c8655f2618b26317fee53190eb1efcddcdfd12b","title":"Position-guided Text Prompt for Vision-Language Pre-training","venue":"arXiv.org","year":2022,"referenceCount":49,"citationCount":0,"influentialCitationCount":0,"publicationDate":"19/12/2022","authors":"Alex Wang,Pan Zhou,Mike Zheng Shou,Shuicheng Yan","id":"4c8655f2618b26317fee53190eb1efcddcdfd12b","summary":"A novel Position-guided Text Prompt (PTP) paradigm is proposed to enhance the visual grounding ability of cross-modal models trained with VLP and achieves comparable results with object-detector based methods, and much faster inference speed since PTP discards its object detector for inference while the later cannot.","score":1},{"url":"https://www.semanticscholar.org/paper/9575afb5702bc33d7df14c48feeee5901ea00369","title":"A Length-Extrapolatable Transformer","venue":"arXiv.org","year":2022,"referenceCount":29,"citationCount":1,"influentialCitationCount":0,"publicationDate":"20/12/2022","authors":"Yutao Sun,Li Dong,Barun Patra,Shuming Ma,Shaohan Huang,Alon Benhaim,Vishrav Chaudhary,Xia Song,Furu Wei","id":"9575afb5702bc33d7df14c48feeee5901ea00369","summary":"This paper introduces a relative position embedding to explicitly maximize attention resolution in Transformers and uses blockwise causal attention during inference for better resolution.","score":1},{"url":"https://www.semanticscholar.org/paper/d2aa89bbfa5eb972626f189cd7454f7d6d0af7c3","title":"MultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning","venue":"arXiv.org","year":2022,"referenceCount":48,"citationCount":5,"influentialCitationCount":0,"publicationDate":"21/12/2022","authors":"Zhiyang Xu,Ying Shen,Lifu Huang","id":"d2aa89bbfa5eb972626f189cd7454f7d6d0af7c3","summary":"This work introduces MultiInstruct, the first multimodal instruction tuning benchmark dataset that consists of 47 diverse multimodAL tasks covering 11 broad categories, and designs a new evaluation metric: Sensitivity, to evaluate how sensitive the model is to the variety of instructions.","score":1},{"url":"https://www.semanticscholar.org/paper/007323e9a19faa7be415eb2122dd331b11a54989","title":"Reversible Column Networks","venue":"arXiv.org","year":2022,"referenceCount":80,"citationCount":2,"influentialCitationCount":0,"publicationDate":"22/12/2022","authors":"Y. Cai,Yi Zhou,Qi Han,Jia-Ying Sun,Xiangwen Kong,Jun Yu Li,Xiangyu Zhang","id":"007323e9a19faa7be415eb2122dd331b11a54989","summary":"This work proposes a new neural network design paradigm Reversible Column Network, which is composed of multiple copies of subnetworks, named columns respectively, between which multi-level reversible connections are employed and is demonstrated to improve the performances in both computer vision and NLP tasks.","score":1},{"url":"https://www.semanticscholar.org/paper/ab972a92dd5ac31f8b8b026a64707bfeb3149397","title":"Do DALL-E and Flamingo Understand Each Other?","venue":"arXiv.org","year":2022,"referenceCount":49,"citationCount":3,"influentialCitationCount":0,"publicationDate":"23/12/2022","authors":"Hang Li,Jindong Gu,Rajat Koner,Sahand Sharifzadeh,Volker Tresp","id":"ab972a92dd5ac31f8b8b026a64707bfeb3149397","summary":"It is argued that the best text or caption for a given image is the text which would generate the image which is the most similar to that image, and the best image for agiven text is the image that results in the caption which is best aligned with the original text.","score":1},{"url":"https://www.semanticscholar.org/paper/52b79321a4862d44db09065e4b40021e2ec1eb0c","title":"On Realization of Intelligent Decision-Making in the Real World: A Foundation Decision Model Perspective","venue":"arXiv.org","year":2022,"referenceCount":95,"citationCount":0,"influentialCitationCount":0,"publicationDate":"24/12/2022","authors":"Ying Wen,Ziyu Wan,M. Zhou,Shufang Hou,Zhe Cao,Chenyang Le,Jingxiao Chen,Zheng Tian,Weinan Zhang,J. Wang","id":"52b79321a4862d44db09065e4b40021e2ec1eb0c","summary":"It is argued that a foundation decision model (FDM) can be established by formulating various decision-making tasks as a sequence decoding task using the Transformer architecture; this would be a promising solution to advance the applications of IDM in more complex real world tasks.","score":1},{"url":"https://www.semanticscholar.org/paper/14c840a7faa15a7e42e5664b5e896878d91dd8ae","title":"Self Supervision Does Not Help Natural Language Supervision at Scale","venue":"arXiv.org","year":2023,"referenceCount":92,"citationCount":1,"influentialCitationCount":0,"publicationDate":2023,"authors":"F.R.T. Weers,Vaishaal Shankar,Angelos Katharopoulos,Yinfei Yang,Tom Gunter","id":"14c840a7faa15a7e42e5664b5e896878d91dd8ae","summary":"This work finds that a combination of two state of the art approaches: masked auto-encoders, MAE and contrastive language image pre-training, CLIP provides a beneﬁt over CLIP when trained on a corpus of 11.3M image-text pairs, but lit-tle to no beneﷁt (as evaluated on a suite of common vision tasks) over ClIP when training on a large corpus of 1.4B images.","score":1},{"url":"https://www.semanticscholar.org/paper/497a1accfd0be6cad1be4f2b6fa88078dae7414a","title":"Quant 4.0: Engineering Quantitative Investment with Automated, Explainable and Knowledge-driven Artificial Intelligence","venue":"arXiv.org","year":2022,"referenceCount":341,"citationCount":0,"influentialCitationCount":0,"publicationDate":"13/12/2022","authors":"Jian Guo,Sai Wang,L. Ni,H. Shum","id":"497a1accfd0be6cad1be4f2b6fa88078dae7414a","summary":"This paper introduces Quant 4.0 and provides an engineering perspective for next-generation quant and proposes ten challenging research problems for quant technology, and discusses potential solutions, research directions, and future trends.","score":1},{"url":"https://www.semanticscholar.org/paper/6f6c19b44c1e82e24d2b34683669e93277539021","title":"Disjoint Masking with Joint Distillation for Efficient Masked Image Modeling","venue":"arXiv.org","year":2022,"referenceCount":64,"citationCount":0,"influentialCitationCount":0,"publicationDate":"31/12/2022","authors":"Xin Ma,Chang-Shu Liu,Chunyu Xie,Long Ye,Yafeng Deng,Xiang Ji","id":"6f6c19b44c1e82e24d2b34683669e93277539021","summary":"A conceptually simple yet learning-efficient MIM training scheme, termed Disjoint Masking with Joint Distillation (DMJD), which improves the linear probing classification accuracy over ConvMAE by 5.8% and presents superior generalization compared with state-of-theart SSL methods.","score":1},{"url":"https://www.semanticscholar.org/paper/aa5645b4896acb72aa4893d174af765d962aa708","title":"Policy Pre-training for Autonomous Driving via Self-supervised Geometric Modeling","venue":"","year":2023,"referenceCount":61,"citationCount":3,"influentialCitationCount":0,"publicationDate":"03/01/2023","authors":"Peng Wu,Li Chen,Hongyang Li,Xiaosong Jia,Junchi Yan,Y. Qiao","id":"aa5645b4896acb72aa4893d174af765d962aa708","summary":"PPGeo (Policy Pre-training via Geometric modeling), an intuitive and straightforward fully self-supervised framework curated for the policy pretraining in visuomotor driving, which aims at learning policy representations as a powerful abstraction by modeling 3D geometric scenes on large-scale unlabeled and uncalibrated YouTube driving videos.","score":1},{"url":"https://www.semanticscholar.org/paper/397156af1b4fc1f1c3bbbe5c2dbc698ef0b9b6ec","title":"Policy Pre-training for End-to-end Autonomous Driving via Self-supervised Geometric Modeling","venue":"arXiv.org","year":2023,"referenceCount":59,"citationCount":0,"influentialCitationCount":0,"publicationDate":2023,"authors":"Peng Wu,Li Chen,Hongyang Li,Xiaosong Jia,Junchi Yan,Y. Qiao","id":"397156af1b4fc1f1c3bbbe5c2dbc698ef0b9b6ec","summary":"PPGeo (Policy Pre-training via Geometric modeling), an intuitive and straightforward fully self-supervised framework curated for the policy pre-training in visuomotor driving, aimed at learning policy representations as a powerful abstraction by modeling 3D geometric scenes on large-scale unlabeled and uncalibrated YouTube driving videos.","score":1},{"url":"https://www.semanticscholar.org/paper/e0b63fd4dd74239a7eb1b75e0108ca55bcad782d","title":"All in Tokens: Unifying Output Space of Visual Tasks via Soft Token","venue":"arXiv.org","year":2023,"referenceCount":54,"citationCount":3,"influentialCitationCount":1,"publicationDate":"05/01/2023","authors":"Jia Ning,Chen Li,Zheng Zhang,Zigang Geng,Qi Dai,Kun He,Han Hu","id":"e0b63fd4dd74239a7eb1b75e0108ca55bcad782d","summary":"A single unified model is demonstrated that simultaneously handles two typical visual tasks of instance segmentation and depth estimation, which have discrete/fixed-length and continuous/varied-length outputs, respectively.","score":1},{"url":"https://www.semanticscholar.org/paper/d16ac1cc0036ffda0d44383304df8bd4f8e38c95","title":"Vision Transformers Are Good Mask Auto-Labelers","venue":"arXiv.org","year":2023,"referenceCount":63,"citationCount":4,"influentialCitationCount":0,"publicationDate":"10/01/2023","authors":"Shiyi Lan,Xitong Yang,Zhiding Yu,Zuxuan Wu,J. Álvarez,Anima Anandkumar","id":"d16ac1cc0036ffda0d44383304df8bd4f8e38c95","summary":"Qualitative results indicate that masks produced by MAL are, in some cases, even better than human annotations and significantly reduces the gap between auto-labeling and human annotation regarding mask quality.","score":1},{"url":"https://www.semanticscholar.org/paper/701a9882884a473faa92324ea6c1ff6c9dacc3ce","title":"Toward Building General Foundation Models for Language, Vision, and Vision-Language Understanding Tasks","venue":"arXiv.org","year":2023,"referenceCount":83,"citationCount":0,"influentialCitationCount":0,"publicationDate":"12/01/2023","authors":"Xinsong Zhang,Yan Zeng,Jipeng Zhang,Hang Li","id":"701a9882884a473faa92324ea6c1ff6c9dacc3ce","summary":"Extensive experiments on benchmark datasets show that X-FM can significantly outperform existing general foundation models and perform better than or comparable to existing foundation models specifically for language, vision, or vision-language understanding.","score":1},{"url":"https://www.semanticscholar.org/paper/9b5a11d9bb3790dbbb02725231b290f67579469a","title":"A Survey of Self-Supervised Learning from Multiple Perspectives: Algorithms, Theory, Applications and Future Trends","venue":"arXiv.org","year":2023,"referenceCount":345,"citationCount":2,"influentialCitationCount":0,"publicationDate":"13/01/2023","authors":"Jie Gui,Tuo Chen,Qiong Cao,Zhe Sun,Haowen Luo,Dacheng Tao","id":"9b5a11d9bb3790dbbb02725231b290f67579469a","summary":"This paper attempts to provide a review of the various SSL methods from the perspectives of algorithms, theory, applications, three main trends, and open questions.","score":1},{"url":"https://www.semanticscholar.org/paper/b759f3fcf2459013c710bc0b000c46c8e70f9bf8","title":"PRUDEX-Compass: Towards Systematic Evaluation of Reinforcement Learning in Financial Markets","venue":"arXiv.org","year":2023,"referenceCount":103,"citationCount":1,"influentialCitationCount":0,"publicationDate":"14/01/2023","authors":"Shuo Sun,Molei Qin,Xinrun Wang,Bo An","id":"b759f3fcf2459013c710bc0b000c46c8e70f9bf8","summary":"","score":1},{"url":"https://www.semanticscholar.org/paper/46afac2f89e0f5ff1c250cdf280cf2669fefbe7d","title":"Masked Autoencoding Does Not Help Natural Language Supervision at Scale","venue":"","year":2023,"referenceCount":95,"citationCount":0,"influentialCitationCount":0,"publicationDate":"19/01/2023","authors":"F.R.T. Weers,Vaishaal Shankar,Angelos Katharopoulos,Yinfei Yang,Tom Gunter","id":"46afac2f89e0f5ff1c250cdf280cf2669fefbe7d","summary":"This work finds that a combination of two state of the art approaches: masked auto-encoders, MAE and contrastive language image pre-training, CLIP provides a benefit over CLIP when trained on a corpus of 11.3M image-text pairs, but little to no benefit over ClIP when training on a large corpus of 1.4B images.","score":1},{"url":"https://www.semanticscholar.org/paper/f10d25c3a19d5bf3544dc56076d90a02ed99eae9","title":"ClimaX: A foundation model for weather and climate","venue":"arXiv.org","year":2023,"referenceCount":113,"citationCount":6,"influentialCitationCount":0,"publicationDate":"24/01/2023","authors":"Tung Nguyen,Johannes Brandstetter,Ashish Kapoor,Jayesh K. Gupta,Aditya Grover","id":"f10d25c3a19d5bf3544dc56076d90a02ed99eae9","summary":"ClimaX is developed and demonstrated, a flexible and generalizable deep learning model for weather and climate science that can be trained using heterogeneous datasets spanning different variables, spatio-temporal coverage, and physical groundings and results in superior performance on benchmarks for weather forecasting and climate projections, even when pretrained at lower resolutions and compute budgets.","score":1},{"url":"https://www.semanticscholar.org/paper/81620597ffafb4368cf0fe4fab7b7cd4506e09cd","title":"Advancing Radiograph Representation Learning with Masked Record Modeling","venue":"arXiv.org","year":2023,"referenceCount":45,"citationCount":1,"influentialCitationCount":0,"publicationDate":"30/01/2023","authors":"Hong-Yu Zhou,Chenyu Lian,Lian-cheng Wang,Yizhou Yu","id":"81620597ffafb4368cf0fe4fab7b7cd4506e09cd","summary":"It is found that MRM offers superior performance in label-efficient fine-tuning and surpasses self- and report-supervised pre-training in identifying the pneumonia type and the pneumothorax area, sometimes by large margins.","score":1},{"url":"https://www.semanticscholar.org/paper/600d3ad507c581cd143f363d8d55042da53ef142","title":"UPop: Unified and Progressive Pruning for Compressing Vision-Language Transformers","venue":"arXiv.org","year":2023,"referenceCount":44,"citationCount":0,"influentialCitationCount":0,"publicationDate":"31/01/2023","authors":"Dachuan Shi,Chaofan Tao,Ying Jin,Zhendong Yang,Chun Yuan,Jiaqi Wang","id":"600d3ad507c581cd143f363d8d55042da53ef142","summary":"Experiments on multiple generative and discriminative vision-language tasks, including Visual Reasoning, Image Caption, Visual Question Answer, Image-Text Retrieval, Text-Image Retrival, and Image Classiﬁcation, demonstrate the effectiveness and versatility of the proposed UPop framework.","score":1}]}
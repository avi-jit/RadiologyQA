{"papers":[{"url":"https://www.semanticscholar.org/paper/93b6b79b4ef6c345f31722ce7c829385c6dce0d6","title":"Slake: A Semantically-Labeled Knowledge-Enhanced Dataset For Medical Visual Question Answering","venue":"IEEE International Symposium on Biomedical Imaging","year":2021,"referenceCount":15,"citationCount":33,"influentialCitationCount":8,"publicationDate":"18/02/2021","authors":"Bo Liu,Li-Ming Zhan,Li Xu,Lin Ma,Y. Yang,Xiao-Ming Wu","citations":[{"paperId":"bf40c9e7832e1b2887cbf5798455f91705ea11ba","title":"Masked Vision and Language Pre-training with Unimodal and Multimodal Contrastive Losses for Medical Visual Question Answering"},{"paperId":"e0e9ba0c01d441e1fdcb8628d3f743d387b0b017","title":"UIT-Saviors at MEDVQA-GI 2023: Improving Multimodal Learning with Image Enhancement for Gastrointestinal Visual Question Answering"},{"paperId":"ebedc4d7a2356090904baba4104ef0832bc236df","title":"A Survey on Multimodal Large Language Models"},{"paperId":"d82e057236b7c51dcf367ed7ad79d053bdc80d81","title":"ECG-QA: A Comprehensive Question Answering Dataset Combined With Electrocardiogram"},{"paperId":"238c33aa23774562cb45918c4917565f1b725044","title":"Multi-modal Pre-training for Medical Vision-language Understanding and Generation: An Empirical Study with A New Benchmark"},{"paperId":"31a7d8c4a5ab6bab522494b57270249105c8748e","title":"BiomedGPT: A Unified and Generalist Biomedical Generative Pre-trained Transformer for Vision, Language, and Multimodal Tasks"},{"paperId":"f646d3056ca02daa99820917b3ba48a43a0022e2","title":"SUR-adapter: Enhancing Text-to-Image Pre-trained Diffusion Models with Large Language Models"},{"paperId":"ac4d13b6a4f9fb67337099f4602135a0351f5c99","title":"Towards Medical Artificial General Intelligence via Knowledge-Enhanced Multimodal Pretraining"},{"paperId":"f7ea746cd2cc25628a7a553ac27d228198be42cb","title":"Pre-trained multilevel fuse network based on vision-conditioned reasoning and bilinear attentions for medical image visual question answering"},{"paperId":"3d45e69557f0c6a54ec698304c2e27ec29bc1c2b","title":"PMC-CLIP: Contrastive Language-Image Pre-training using Biomedical Documents"},{"paperId":"4d4a96708fc67403176bb2b891b564af7a20c148","title":"RAMM: Retrieval-augmented Biomedical Visual Question Answering with Multi-modal Pre-training"},{"paperId":"8200be2e8b9af243ee72a9d919a4f7fbe82a17d2","title":"Medical knowledge-based network for Patient-oriented Visual Question Answering"},{"paperId":"da9579539385daedd33a0de0f814e2977ad0d1f5","title":"Towards Unifying Medical Vision-and-Language Pre-training via Soft Prompts"},{"paperId":"940f303c2530a52c5fd3c52c9c64ceea4b53ab05","title":"Diversity Learning Based on Multi-Latent Space for Medical Image Visual Question Generation"},{"paperId":"2580d3fc39fed3989f10665559a955b847b7eb7f","title":"Medical Visual Question Answering via Conditional Reasoning and Contrastive Learning"},{"paperId":"2ea26b243171e37ef20af269942ffde414f9f8cc","title":"UnICLAM: Contrastive Representation Learning with Adversarial Masking for Unified and Interpretable Medical Vision Question Answering"},{"paperId":"5942335fdd35d1651aaabd7af4db129a29ed2a85","title":"How Well Apply Multimodal Mixup and Simple MLPs Backbone to Medical Visual Question Answering?"},{"paperId":"560e0114a023bdfd99eb60eb4d9d555a348600a0","title":"PiggyBack: Pretrained Visual Question Answering Environment for Backing up Non-deep Learning Professionals"},{"paperId":"170667a96f04adf3b3b83526f75fe8d1063e0f7a","title":"Self-supervised vision-language pretraining for Medical visual question answering"},{"paperId":"6ae700c89a9a9a3da7e55dd51c4710b5ed8c8d4e","title":"Caption-Aware Medical VQA via Semantic Focusing and Progressive Cross-Modality Comprehension"},{"paperId":"28ff0816f19a5e3e37eac5569de41872fd262f0a","title":"Align, Reason and Learn: Enhancing Medical Vision-and-Language Pre-training with Knowledge"},{"paperId":"0cbd644254462341a897d4bfa0134637662c3ab5","title":"A Transformer-based Medical Visual Question Answering Model"},{"paperId":"ef2edea434e487f288d4eed6f9b1dc480b917211","title":"Adversarial Learning to Improve Question Image Embedding in Medical Visual Question Answering"},{"paperId":"e9480d62e216f77d5556b7eda769daa4c92d004d","title":"VQAMix: Conditional Triplet Mixup for Medical Visual Question Answering"},{"paperId":"22f2f53e7474af620268318ac3ff4bb5a4fe3ab4","title":"MED-GPVS: A Deep Learning-Based Joint Biomedical Image Classification and Visual Question Answering System for Precision e-Health"},{"paperId":"ab2ba04580edb4340a896b37543e77fdc2ec6bbf","title":"Hybrid deep learning model for answering visual medical questions"},{"paperId":"e678898301a66faab85dfa4c84e51118e434b8f2","title":"Vision-Language Transformer for Interpretable Pathology Visual Question Answering"},{"paperId":"4ef3d9e492479e28fa57d107e52acc6a0c803de2","title":"Does CLIP Benefit Visual Question Answering in the Medical Domain as Much as it Does in the General Domain?"},{"paperId":"e34b699cef0a711a8cb9c39ecea20ac2df1578f5","title":"Medical Visual Question Answering: A Survey"},{"paperId":"3c83f80f06633ff4598d33c2959f8e4cdcad3e93","title":"Cross-Modal Self-Attention with Multi-Task Pre-Training for Medical Visual Question Answering"},{"paperId":"ba067bb339ce3d28c0f084a029485374b3773aba","title":"Reducing Knowledge Noise for Improved Semantic Analysis in Biomedical Natural Language Processing Applications"},{"paperId":"99267305914a44ad6d626b7a6406fd0079b5508d","title":"Incorporating Medical Knowledge to Transformer-based Language Models for Medical Dialogue Generation"},{"paperId":"357fc385caf2e5b9898c9140fa3ac9955e6bb3c6","title":"TeamS at VQA-Med 2021: BBN-Orchestra for Long-tailed Medical Visual Question Answering"}],"references":[{"paperId":"b8d5b853f2212cbb48a43f1edec9b96d76d388ec","title":"Medical Visual Question Answering via Conditional Reasoning"},{"paperId":"33301b25a297b701bdc287e985c006375cb7bb21","title":"Overcoming Data Limitation in Medical Visual Question Answering"},{"paperId":"4654aa505e5bcdb089d0df202cd7ceabc9d2d41f","title":"A large annotated medical image dataset for the development and evaluation of segmentation algorithms"},{"paperId":"18f9a6045ba01cb079c4fa49a630d71bbd27cd92","title":"A dataset of clinically generated visual questions and answers about radiology images"},{"paperId":"05e882679d61f4c64a68ebe21826251a39f87e98","title":"ChestX-Ray8: Hospital-Scale Chest X-Ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases"},{"paperId":"03eb382e04cca8cca743f7799070869954f1402a","title":"CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning"},{"paperId":"b60630911d7746fba06de7c34abe98c9a61c6bcc","title":"FVQA: Fact-Based Visual Question Answering"},{"paperId":"8e759195eb4b4f0f480a8a2cf1c629bfd881d4e5","title":"Analyzing the Behavior of Visual Question Answering Models"},{"paperId":"5fa973b8d284145bf0ced9acf2913a74674260f6","title":"Yin and Yang: Balancing and Answering Binary Visual Questions"},{"paperId":"2c1890864c1c2b750f48316dc8b650ba4772adc5","title":"Stacked Attention Networks for Image Question Answering"},{"paperId":"97ad70a9fa3f99adf18030e5e38ebe3d90daa2db","title":"VQA: Visual Question Answering"},{"paperId":"eb42cf88027de515750f230b23b1a057dc782108","title":"Very Deep Convolutional Networks for Large-Scale Image Recognition"},{"paperId":"2582ab7c70c9e7fcb84545944eba8f3a7f253248","title":"Translating Embeddings for Modeling Multi-relational Data"},{"paperId":"038b582cccb00c54589c5563d9a00ee28dad83b0","title":"User-guided 3D active contour segmentation of anatomical structures: Significantly improved efficiency and reliability"},{"paperId":null,"title":"CHAOS - Combined (CT- MR) Healthy Abdominal Organ Segmentation Challenge Data"}],"id":"93b6b79b4ef6c345f31722ce7c829385c6dce0d6","summary":"A large bilingual dataset, SLAKE, with comprehensive semantic labels annotated by experienced physicians and a new structural medical knowledge base for Med-VQA is presented, which includes richer modalities and covers more human body parts than the currently available dataset."},{"url":"https://www.semanticscholar.org/paper/4d4a96708fc67403176bb2b891b564af7a20c148","title":"RAMM: Retrieval-augmented Biomedical Visual Question Answering with Multi-modal Pre-training","venue":"arXiv.org","year":2023,"referenceCount":46,"citationCount":2,"influentialCitationCount":0,"publicationDate":"01/03/2023","authors":"Zheng Yuan,Qiao Jin,Chuanqi Tan,Zhengyun Zhao,Hongyi Yuan,Fei Huang,Songfang Huang","citations":[{"paperId":"effc7842011fac2b9eefceec58f2a730d4d54c02","title":"LADER: Log-Augmented DEnse Retrieval for Biomedical Literature Search"},{"paperId":"052a5e2bcc999810ee6f1eedcf758c528e4f125f","title":"Retrieving Multimodal Information for Augmented Generation: A Survey"}],"references":[{"paperId":"81adb80e390f25d4a2d764b1063eeaa2c334d441","title":"Multi-modal Masked Autoencoders for Medical Vision-and-Language Pre-training"},{"paperId":"28ff0816f19a5e3e37eac5569de41872fd262f0a","title":"Align, Reason and Learn: Enhancing Medical Vision-and-Language Pre-training with Knowledge"},{"paperId":"02251886950770e82b3d68564d60cdfe15e73199","title":"Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks"},{"paperId":"0d975a8bbc1e0495cb95df8666d42111b546ab34","title":"Retrieval-Augmented Transformer for Image Captioning"},{"paperId":"ea3920611353d4ce21d98ecd21b4b3e900d5a954","title":"mPLUG: Effective and Efficient Vision-Language Learning by Cross-modal Skip-connections"},{"paperId":"0db5207510819b9956849eb84bfe8703f8f3688d","title":"BioBART: Pretraining and Evaluation of A Biomedical Generative Language Model"},{"paperId":"15115f67452f3305b69e6886cee98ac466d42cd5","title":"Retrieval Augmented Classification for Long-Tail Visual Recognition"},{"paperId":"4ef3d9e492479e28fa57d107e52acc6a0c803de2","title":"Does CLIP Benefit Visual Question Answering in the Medical Domain as Much as it Does in the General Domain?"},{"paperId":"94ff111c4d81bd03f159321728ceec8b4711c89d","title":"An Empirical Study of Training End-to-End Vision-and-Language Transformers"},{"paperId":"b82c5f9efdb2ae56baa084ca41aeddd8a665c1d1","title":"Align before Fuse: Vision and Language Representation Learning with Momentum Distillation"},{"paperId":"520bd2331cca8d5a9c032c186a2a0f7704ead6ff","title":"R-Drop: Regularized Dropout for Neural Networks"},{"paperId":"d9317660e2a538d9c018028956fd114d55330f82","title":"Multi-Modal Understanding and Generation for Medical Images and Text via Vision-Language Pre-Training"},{"paperId":"5bd42c29a5ba8a6c39547db89023d879e98a6b32","title":"Multiple Meta-model Quantifying for Medical Visual Question Answering"},{"paperId":"3c83f80f06633ff4598d33c2959f8e4cdcad3e93","title":"Cross-Modal Self-Attention with Multi-Task Pre-Training for Medical Visual Question Answering"},{"paperId":"58fe64beb45b18f63cbc001849a0dee3e4e60482","title":"Improving Biomedical Pretrained Language Models with Knowledge"},{"paperId":"17c2bb358169541f2d0a769f80779f46d1cd3d37","title":"MMBERT: Multimodal BERT Pretraining for Improved Medical VQA"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"93b6b79b4ef6c345f31722ce7c829385c6dce0d6","title":"Slake: A Semantically-Labeled Knowledge-Enhanced Dataset For Medical Visual Question Answering"},{"paperId":"a627232a97a7a63f8399d157f0b022eb1ccd547c","title":"Biomedical Question Answering: A Survey of Approaches and Challenges"},{"paperId":"b8d5b853f2212cbb48a43f1edec9b96d76d388ec","title":"Medical Visual Question Answering via Conditional Reasoning"},{"paperId":"54523ff961a1ac57a86696ef9a53b3a630b482c0","title":"Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing"},{"paperId":"ea1f95989f808f409a3cd29b128000c04036c224","title":"Retrieval Augmented Language Model Pre-Training"},{"paperId":"58ed1fbaabe027345f7bb3a6312d41c5aac63e22","title":"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"},{"paperId":"d1f407b16fb8d99487baee37ed0805676c58e7ac","title":"MIMIC-CXR, a de-identified publicly available database of chest radiographs with free-text reports"},{"paperId":"87f6a7c014ce206ac5b57299c07e10667d194b39","title":"Randaugment: Practical automated data augmentation with a reduced search space"},{"paperId":"33301b25a297b701bdc287e985c006375cb7bb21","title":"Overcoming Data Limitation in Medical Visual Question Answering"},{"paperId":"a81874b4a651a740fffbfc47ef96515e8c7f782f","title":"Latent Retrieval for Weakly Supervised Open Domain Question Answering"},{"paperId":"156d217b0a911af97fa1b5a71dc909ccef7a8028","title":"SciBERT: A Pretrained Language Model for Scientific Text"},{"paperId":"1e43c7084bdcb6b3102afaf301cce10faead2702","title":"BioBERT: a pre-trained biomedical language representation model for biomedical text mining"},{"paperId":"18f9a6045ba01cb079c4fa49a630d71bbd27cd92","title":"A dataset of clinically generated visual questions and answers about radiology images"},{"paperId":"a564fabf130ff6e2742cfba90c7a4018937d764d","title":"Radiology Objects in COntext (ROCO): A Multimodal Image Dataset"},{"paperId":"a5d10341717c0519cf63151b496a6d2ed67aa05f","title":"Bilinear Attention Networks"},{"paperId":"d07284a6811f1b2745d91bdb06b040b57f226882","title":"Decoupled Weight Decay Regularization"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"2cbb8de53759e75411bc528518947a3094fbce3a","title":"Billion-Scale Similarity Search with GPUs"},{"paperId":"2c1890864c1c2b750f48316dc8b650ba4772adc5","title":"Stacked Attention Networks for Image Question Answering"},{"paperId":"47ced790a563344efae66588b5fb7fe6cca29ed3","title":"The Probabilistic Relevance Framework: BM25 and Beyond"},{"paperId":"769eae977c287f7696ad8fd4cc568785fdbe1779","title":"PMC-Patients: A Large-scale Dataset of Patient Notes and Relations Extracted from Case Reports in PubMed Central"},{"paperId":"02c7c78fa8585b4f37420b9e0acbaf77d70108a8","title":"ViLMedic: a framework for research at the intersection of vision and language in medical AI"},{"paperId":"c8b25fab5608c3e033d34b4483ec47e68ba109b7","title":"Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"},{"paperId":"eae760ee19c7d0c4e2b39adca209abcbc59e0a37","title":"Overview of the ImageCLEF 2021: Multimedia Retrieval in Medical, Nature, Internet and Social Media Applications"},{"paperId":"519799a9e2a72b808d81c6bcba5de263503de053","title":"Contrastive Pre-training and Representation Distillation for Medical Visual Question Answering Based on Radiology Images"},{"paperId":"13e7212d5af59137ad770f42712d762247ebd3ed","title":"SYSU-HCP at VQA-Med 2021: A Data-centric Model with Efficient Training Methodology for Medical Visual Question Answering"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"9eeeb23546d3d2bbc73959bffc6819f2335f3c83","title":"VQA-Med: Overview of the Medical Visual Question Answering Task at ImageCLEF 2019"},{"paperId":null,"title":"Multimodal factorized bilinear pooling with co-attention learning for visual question answering"}],"id":"4d4a96708fc67403176bb2b891b564af7a20c148","summary":"This paper collects a new biomedical dataset named PMCPM which offers patient-based image-text pairs containing diverse patient situations from PubMed and proposes a retrieval-augmented pretrain-and-finetune paradigm named RAMM for biomedical VQA to overcome the data limitation issue."},{"url":"https://www.semanticscholar.org/paper/18f9a6045ba01cb079c4fa49a630d71bbd27cd92","title":"A dataset of clinically generated visual questions and answers about radiology images","venue":"Scientific Data","year":2018,"referenceCount":24,"citationCount":100,"influentialCitationCount":31,"publicationDate":"20/11/2018","authors":"J. Lau,Soumya Gayen,Asma Ben Abacha,Dina Demner-Fushman","citations":[{"paperId":"9b6779fe8805abae18dcafcdf80f45109d61a7d7","title":"Medical visual question answering with symmetric interaction attention and cross-modal gating"},{"paperId":"bf40c9e7832e1b2887cbf5798455f91705ea11ba","title":"Masked Vision and Language Pre-training with Unimodal and Multimodal Contrastive Losses for Medical Visual Question Answering"},{"paperId":"e0e9ba0c01d441e1fdcb8628d3f743d387b0b017","title":"UIT-Saviors at MEDVQA-GI 2023: Improving Multimodal Learning with Image Enhancement for Gastrointestinal Visual Question Answering"},{"paperId":"ebedc4d7a2356090904baba4104ef0832bc236df","title":"A Survey on Multimodal Large Language Models"},{"paperId":"238c33aa23774562cb45918c4917565f1b725044","title":"Multi-modal Pre-training for Medical Vision-language Understanding and Generation: An Empirical Study with A New Benchmark"},{"paperId":"f22d71c7ce9720ba1f717a4f1181488200e78198","title":"LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day"},{"paperId":"a563f823a887b75dd61adc96c556a8bd83c6e4c3","title":"HaVQA: A Dataset for Visual Question Answering and Multimodal Research in Hausa Language"},{"paperId":"31a7d8c4a5ab6bab522494b57270249105c8748e","title":"BiomedGPT: A Unified and Generalist Biomedical Generative Pre-trained Transformer for Vision, Language, and Multimodal Tasks"},{"paperId":"f4793adffd6f67ffcb93ccfc5672ab301b8a2b96","title":"PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering"},{"paperId":"fcb8795dd9b98784ffcd355a7ad1250ec1b9760f","title":"Multi-task Paired Masking with Alignment Modeling for Medical Vision-Language Pre-training"},{"paperId":"c5bcc78ae708b29edb03481e12213eca53c28963","title":"A multi-modal model based on transformers for medical visual question answering"},{"paperId":"ac4d13b6a4f9fb67337099f4602135a0351f5c99","title":"Towards Medical Artificial General Intelligence via Knowledge-Enhanced Multimodal Pretraining"},{"paperId":"2f9b344158e40d4af8391fc7e79d400bebba39ce","title":"Assertiveness-based Agent Communication for a Personalized Medicine on Medical Imaging Diagnosis"},{"paperId":"2587cb7b1c02fde76e3c23c13f1bd40d6a199c57","title":"Q2ATransformer: Improving Medical VQA via an Answer Querying Decoder"},{"paperId":"46a0f57d5376ad1b9fb94b894931a5419d67dbf5","title":"A reinforcement learning approach for VQA validation: An application to diabetic macular edema grading"},{"paperId":"f7ea746cd2cc25628a7a553ac27d228198be42cb","title":"Pre-trained multilevel fuse network based on vision-conditioned reasoning and bilinear attentions for medical image visual question answering"},{"paperId":"9e8936a131ee0c765a6749d93bc58ad9522b7d6d","title":"Logical Implications for Visual Question Answering Consistency"},{"paperId":"3d45e69557f0c6a54ec698304c2e27ec29bc1c2b","title":"PMC-CLIP: Contrastive Language-Image Pre-training using Biomedical Documents"},{"paperId":"17ca48ad1b944c897863f04ba9ffa72674dce1ce","title":"Parallel multi-head attention and term-weighted question embedding for medical visual question answering"},{"paperId":"d8da72e7857cc1a0d3505e6c8a746eac815901b2","title":"Large-Scale Domain-Specific Pretraining for Biomedical Vision-Language Processing"},{"paperId":"20fb06a4aa4010470d388098618af5d1bea224ad","title":"Vision–Language Model for Visual Question Answering in Medical Imagery"},{"paperId":"8200be2e8b9af243ee72a9d919a4f7fbe82a17d2","title":"Medical knowledge-based network for Patient-oriented Visual Question Answering"},{"paperId":"4d4a96708fc67403176bb2b891b564af7a20c148","title":"RAMM: Retrieval-augmented Biomedical Visual Question Answering with Multi-modal Pre-training"},{"paperId":"ffeeb60b76d18e1e36dee0f87c95bab1bc65aa79","title":"Medical visual question answering using joint self-supervised learning"},{"paperId":"9259c41695c4451f1ca3e6bdc9829623b43f9a69","title":"Interpretable Medical Image Visual Question Answering via Multi-Modal Relationship Graph Learning"},{"paperId":"da9579539385daedd33a0de0f814e2977ad0d1f5","title":"Towards Unifying Medical Vision-and-Language Pre-training via Soft Prompts"},{"paperId":"f4b9ec310ef9aff69066e3e482cc113c8ae8d9dd","title":"PathNarratives: Data annotation for pathological human-AI collaborative diagnosis"},{"paperId":"940f303c2530a52c5fd3c52c9c64ceea4b53ab05","title":"Diversity Learning Based on Multi-Latent Space for Medical Image Visual Question Generation"},{"paperId":"e0b4ca7bffb64b4bbd95c9f5ee7a610e35fe95d8","title":"A comprehensive interpretation for medical VQA: Datasets, techniques, and challenges"},{"paperId":"2580d3fc39fed3989f10665559a955b847b7eb7f","title":"Medical Visual Question Answering via Conditional Reasoning and Contrastive Learning"},{"paperId":"2ea26b243171e37ef20af269942ffde414f9f8cc","title":"UnICLAM: Contrastive Representation Learning with Adversarial Masking for Unified and Interpretable Medical Vision Question Answering"},{"paperId":"83caa8f9ec66a9ebae68d0e963ac7ca2396c94c2","title":"Is Unimodal Bias Always Bad for Visual Question Answering? A Medical Domain Study with Dynamic Attention"},{"paperId":"5942335fdd35d1651aaabd7af4db129a29ed2a85","title":"How Well Apply Multimodal Mixup and Simple MLPs Backbone to Medical Visual Question Answering?"},{"paperId":"56d8d9fff399f798da97a69e891de4eeb4568d4f","title":"MHKD-MVQA: Multimodal Hierarchical Knowledge Distillation for Medical Visual Question Answering"},{"paperId":"b60711d89c34d8902d4b2768f01770473cf0adfc","title":"Medical image enhancement strategy based on morphologically processing of residuals using a special kernel"},{"paperId":"170667a96f04adf3b3b83526f75fe8d1063e0f7a","title":"Self-supervised vision-language pretraining for Medical visual question answering"},{"paperId":"9d79f3601b0d73a2b64784cad2738c0fcd030824","title":"MF2-MVQA: A Multi-stage Feature Fusion method for Medical Visual Question Answering"},{"paperId":"6ae700c89a9a9a3da7e55dd51c4710b5ed8c8d4e","title":"Caption-Aware Medical VQA via Semantic Focusing and Progressive Cross-Modality Comprehension"},{"paperId":"d4d07180764fc30cf31261ddd072175a4daee10b","title":"A Dual-Attention Learning Network with Word and Sentence Embedding for Medical Visual Question Answering"},{"paperId":"8f93076f8e060eec0c058edb3de05f62886fffdf","title":"RepsNet: Combining Vision with Language for Automated Medical Reports"},{"paperId":"81adb80e390f25d4a2d764b1063eeaa2c334d441","title":"Multi-modal Masked Autoencoders for Medical Vision-and-Language Pre-training"},{"paperId":"28ff0816f19a5e3e37eac5569de41872fd262f0a","title":"Align, Reason and Learn: Enhancing Medical Vision-and-Language Pre-training with Knowledge"},{"paperId":"8cee548aa6a8d31dcac830695e4b72960ff45ecb","title":"MMCN: Multi-Modal Co-attention Network for Medical Visual Question Answering"},{"paperId":"0cbd644254462341a897d4bfa0134637662c3ab5","title":"A Transformer-based Medical Visual Question Answering Model"},{"paperId":"0976f5e6e7c0481f5f44c981d9f676e9ea7fa4d0","title":"AMAM: An Attention-based Multimodal Alignment Model for Medical Visual Question Answering"},{"paperId":"ef7dd87e8bfd11878e88ec3f0795ebda8aaf1690","title":"A Bi-level representation learning model for medical visual question answering"},{"paperId":"ef2edea434e487f288d4eed6f9b1dc480b917211","title":"Adversarial Learning to Improve Question Image Embedding in Medical Visual Question Answering"},{"paperId":"2ac3bacbbee520b701707ebcf7b9ca7a3f233129","title":"Medical visual question answering via corresponding feature fusion combined with semantic attention."},{"paperId":"67f992f43cc777a3e1aedc14cf3a11582ccfa570","title":"OVQA: A Clinically Generated Visual Question Answering Dataset"},{"paperId":"6994c3cfd50e36f0d72dfa81807f98b639215b94","title":"OVQA"},{"paperId":"c98eafbef6fa40010fc3b78d96a04c41699e2c1b","title":"EBMs vs. CL: Exploring Self-Supervised Visual Pretraining for Visual Question Answering"},{"paperId":"6e7763ec04906726377953cc85f31a1a0c889001","title":"Anomaly Matters: An Anomaly-Oriented Model for Medical Visual Question Answering"},{"paperId":"e9480d62e216f77d5556b7eda769daa4c92d004d","title":"VQAMix: Conditional Triplet Mixup for Medical Visual Question Answering"},{"paperId":"d1f25ff0b282486acf6ae225e5fd18a82673eb35","title":"Multi-Modal Alignment of Visual Question Answering Based on Multi-Hop Attention Mechanism"},{"paperId":"8c9a9a1bbba2a3e3bab34bce533b3b2acfda32b0","title":"Medical visual question answering based on question-type reasoning and semantic space constraint"},{"paperId":"38cbcaab9387c9c08df2f89fe93792c3dfe46a01","title":"BreastScreening-AI: Evaluating medical intelligent agents for human-AI interactions"},{"paperId":"8a49d48f1ac243cf7ea92f7000cb8759eda37be0","title":"Barlow constrained optimization for Visual Question Answering"},{"paperId":"4ef3d9e492479e28fa57d107e52acc6a0c803de2","title":"Does CLIP Benefit Visual Question Answering in the Medical Domain as Much as it Does in the General Domain?"},{"paperId":"e34b699cef0a711a8cb9c39ecea20ac2df1578f5","title":"Medical Visual Question Answering: A Survey"},{"paperId":"39528ef1de5a6c1b4fba44071591e9f12167769c","title":"MVQAS: A Medical Visual Question Answering System"},{"paperId":"933242d263859a12c058979163f58035047d78b5","title":"Fine-grained Hand Gesture Recognition in Multi-viewpoint Hand Hygiene"},{"paperId":"ecf3163157d477d1a2188a3f8cf75c697a303708","title":"Goal-Driven Visual Question Generation from Radiology Images"},{"paperId":"e4f99837e02e7fbcccec1bf15cececacaaabbe32","title":"MuVAM: A Multi-View Attention-based Model for Medical Visual Question Answering"},{"paperId":"3795b18bb223ee70c6c4347ea371b28fffb671e8","title":"Automatic Generation of Structured Radiology Reports for Volumetric Computed Tomography Images Using Question-Specific Deep Feature Extraction and Learning"},{"paperId":"5bd42c29a5ba8a6c39547db89023d879e98a6b32","title":"Multiple Meta-model Quantifying for Medical Visual Question Answering"},{"paperId":"3c83f80f06633ff4598d33c2959f8e4cdcad3e93","title":"Cross-Modal Self-Attention with Multi-Task Pre-Training for Medical Visual Question Answering"},{"paperId":"17c2bb358169541f2d0a769f80779f46d1cd3d37","title":"MMBERT: Multimodal BERT Pretraining for Improved Medical VQA"},{"paperId":"a4d21d620a6cb7a8e6f06b996463172478562a0a","title":"Visual Question Answering using Data Mining Techniques for Skeletal Scintigraphy in medical domain - VQADMSS"},{"paperId":"93b6b79b4ef6c345f31722ce7c829385c6dce0d6","title":"Slake: A Semantically-Labeled Knowledge-Enhanced Dataset For Medical Visual Question Answering"},{"paperId":"a627232a97a7a63f8399d157f0b022eb1ccd547c","title":"Biomedical Question Answering: A Survey of Approaches and Challenges"},{"paperId":"b8d5b853f2212cbb48a43f1edec9b96d76d388ec","title":"Medical Visual Question Answering via Conditional Reasoning"},{"paperId":"72e0dccf59f126a64f970fe9f4712b3221a3be8c","title":"Pathological Visual Question Answering"},{"paperId":"9fe3eeafbe022de014aeb54d0b55502e2a2e46fe","title":"Hierarchical Deep Multi-modal Network for Medical Visual Question Answering"},{"paperId":"ff554f6228cf1f939a0e9e44ada06ef9cd28be15","title":"A Comparison of Pre-trained Vision-and-Language Models for Multimodal Representation Learning across Medical Images and Reports"},{"paperId":"d77a71c94e688d92a3fa10fb7f7feda2c306b9dc","title":"Visual Question Generation from Radiology Images"},{"paperId":"6609489a0f800a9ef411efdcfca4c014c4e86aa8","title":"Towards Visual Dialog for Radiology"},{"paperId":"30f86e15b4fd7936b9812d476976a6ff579b9036","title":"Toward General Scene Graph: Integration of Visual Semantic Knowledge with Entity Synset Alignment"},{"paperId":"ed6ce80789889c0fd56c8117f85079c1c31fe426","title":"CGMVQA: A New Classification and Generative Model for Medical Visual Question Answering"},{"paperId":"fc0b46a0f3720e6c29c1a913aaa3de4a0699f713","title":"PathVQA: 30000+ Questions for Medical Visual Question Answering"},{"paperId":"33301b25a297b701bdc287e985c006375cb7bb21","title":"Overcoming Data Limitation in Medical Visual Question Answering"},{"paperId":"f59ae732612ce8c42035adfb47bd5739c6288ad6","title":"Answering Questions about Data Visualizations using Efficient Bimodal Fusion"},{"paperId":"46699dd04d40efd34ae9088f945f672e50f9ec62","title":"Concept-Centric Visual Turing Tests for Method Validation"},{"paperId":"9ae30ac3609a90b487df3beec10eeface021c7c5","title":"Machine Learning in Computer Vision: A Review"},{"paperId":"61c0b6a5e7aea48a1376b61a4a737137d602b242","title":"PubMedCLIP: How Much Does CLIP Benefit Visual Question Answering in the Medical Domain?"},{"paperId":"b047b3b7d76b79958e23b0fcab985be22b1ce42d","title":"Alternating Cross-attention Vision-Language Model for Efficient Learning with Medical Image and Report without Curation"},{"paperId":"79478a2ac67b9fdbeadcde13faa2d84eb239e080","title":"Vision-Language Pretraining Enables Radiographs and Reports to be Learned without Curation"},{"paperId":"fae23fc97a31bf66563dd033faf311eaaaa05911","title":"Exploratory analysis of different metaheuristic optimization methods for medical image enhancement"},{"paperId":"2441230bd2f3cca924d597b3044ad63aaff269ec","title":"Self-supervised Co-learning of Uncurated Images and Reports Enables Oversight AI in Radiology"},{"paperId":"1f96539c083d60fa83f7548bc6996cdede1026ee","title":"Biomedical Question Answering: A Comprehensive Review"},{"paperId":"2551990a1ccdffb1a4d1d9040b2d493ba6d26dd1","title":"Towards Visual Question Answering on Pathology Images"},{"paperId":"411c7a1fb951a1420013c0af56f9d142565112aa","title":"Chabbiimen at VQA-Med 2021: Visual Generation of Relevant Natural Language Questions from Radiology Images for Anomaly Detection"},{"paperId":"357fc385caf2e5b9898c9140fa3ac9955e6bb3c6","title":"TeamS at VQA-Med 2021: BBN-Orchestra for Long-tailed Medical Visual Question Answering"},{"paperId":"31acfba3a19f780a3239925ff12a7a4047d6a705","title":"MLEC-QA: A Chinese Multi-Choice Biomedical Question Answering Dataset"},{"paperId":"5c01315f0840d8bead978cd9a9cb4c21f0400805","title":"The Inception Team at VQA-Med 2020: Pretrained VGG with Data Augmentation for Medical VQA and VQG"},{"paperId":"39dbb2e49fb33351044a9b8c152a173b31f4c405","title":"Overview of the VQA-Med Task at ImageCLEF 2021: Visual Question Answering and Generation in the Medical Domain"},{"paperId":"cc71a905ca132999a158857823606cd979b9080e","title":"Visual Dialog for Radiology: Data Curation and FirstSteps"},{"paperId":"b7c9e854c1e9b964c8abe0cb80a744e2739ddf40","title":"Tlemcen University at ImageCLEF 2019 Visual Question Answering Task"},{"paperId":"1526501f1939311106f72c128a189bbb6487ca6a","title":"SMAC: An Interpretable Reasoning Network for Visual Question Answering"},{"paperId":"9eeeb23546d3d2bbc73959bffc6819f2335f3c83","title":"VQA-Med: Overview of the Medical Visual Question Answering Task at ImageCLEF 2019"},{"paperId":"4634bf44a0c994e2bed89686225f8cef601a0224","title":"NLM at ImageCLEF 2018 Visual Question Answering in the Medical Domain"}],"references":[{"paperId":"2fdb8da25be54bbba1afcf05294fbede5ccdff37","title":"Visual Question Answering in Radiology (VQA-RAD)"},{"paperId":"a9e19e8ab24071a085d1273b9f9d49aa0e4ba48c","title":"VizWiz Grand Challenge: Answering Visual Questions from Blind People"},{"paperId":"7e4b638e028498e900747b600f46cd723f1f231e","title":"Data Augmentation for Visual Question Answering"},{"paperId":"915b5b12f9bdebc321e970ecd713458c3479d70e","title":"An Analysis of Visual Question Answering Algorithms"},{"paperId":"2abde28f75a9135c8ed7c50ea16b7b9e49da0c09","title":"A survey on deep learning in medical image analysis"},{"paperId":"03eb382e04cca8cca743f7799070869954f1402a","title":"CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning"},{"paperId":"7e232313a59d735ef7c8a9f4cc7bc980a29deb5e","title":"Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering"},{"paperId":"fddc15480d086629b960be5bff96232f967f2252","title":"Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding"},{"paperId":"0460d3497490fa8332c5ff2ecdab88fb7dff4755","title":"Learning to Read Chest X-Rays: Recurrent Neural Cascade Model for Automated Image Annotation"},{"paperId":"afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d","title":"Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations"},{"paperId":"2c03df8b48bf3fa39054345bafabfeff15bfd11d","title":"Deep Residual Learning for Image Recognition"},{"paperId":"def584565d05d6a8ba94de6621adab9e301d375d","title":"Visual7W: Grounded Question Answering in Images"},{"paperId":"0ac8f1a3c679b90d22c1f840cdc8d61ffef750ac","title":"Deep Compositional Question Answering with Neural Module Networks"},{"paperId":"2c1890864c1c2b750f48316dc8b650ba4772adc5","title":"Stacked Attention Networks for Image Question Answering"},{"paperId":"9b17b9c40ea8bb8904b782e91627c1f022a5574f","title":"Learning Knowledge Bases for Multimedia in 2015"},{"paperId":"580062407427236ced45253a2ff7df2e147a81e2","title":"The Multimodal Brain Tumor Image Segmentation Benchmark (BRATS)"},{"paperId":"2fcd5cff2b4743ea640c4af68bf4143f4a2cccb1","title":"Are You Talking to a Machine? Dataset and Methods for Multilingual Image Question"},{"paperId":"97ad70a9fa3f99adf18030e5e38ebe3d90daa2db","title":"VQA: Visual Question Answering"},{"paperId":"eb42cf88027de515750f230b23b1a057dc782108","title":"Very Deep Convolutional Networks for Large-Scale Image Recognition"},{"paperId":"71b7178df5d2b112d07e45038cb5637208659ff7","title":"Microsoft COCO: Common Objects in Context"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":"589951bd421e2b701225fe6626fe980d94ad2770","title":"Overview of ImageCLEF 2018 Medical Domain Visual Question Answering Task"},{"paperId":null,"title":"Data Citations"}],"id":"18f9a6045ba01cb079c4fa49a630d71bbd27cd92","summary":"This work introduces VQA-RAD, the first manually constructed dataset where clinicians asked naturally occurring questions about radiology images and provided reference answers and demonstrates the rich quality of this dataset over other automatically constructed ones."},{"url":"https://www.semanticscholar.org/paper/1e43c7084bdcb6b3102afaf301cce10faead2702","title":"BioBERT: a pre-trained biomedical language representation model for biomedical text mining","venue":"Bioinform.","year":2019,"referenceCount":45,"citationCount":3239,"influentialCitationCount":536,"publicationDate":"25/01/2019","authors":"Jinhyuk Lee,Wonjin Yoon,Sungdong Kim,Donghyeon Kim,Sunkyu Kim,Chan Ho So,Jaewoo Kang","citations":[{"paperId":"bffbfd88a9c4465d56d2535683f5ca1cbff26588","title":"Disambiguation of medical abbreviations for knowledge organization"},{"paperId":"735035035a8b81cffb7b982d88d5ff753517fbce","title":"SPS-LCNN: A Significant Point Sampling-based Lightweight Convolutional Neural Network for point cloud processing"},{"paperId":"9812785093330fb6882e243e1c5051a3be1f9ee6","title":"Weak-PMLC: A large-scale framework for multi-label policy classification based on extremely weak supervision"},{"paperId":"ed80b4a0337944755d9c6c3d9a16f84b396a84c5","title":"A Survey on Relation Extraction"},{"paperId":"68469d83ebd14069e8173e5bf830ed925d13bea2","title":"Control, coordination, and adaptation functions in construction contracts: A machine-coding model"},{"paperId":"9f4017de7deded49c032a83d7844efcd9ea1aa21","title":"Embroid: Unsupervised Prediction Smoothing Can Improve Few-Shot Classification"},{"paperId":"d0edff5402edeab721b7681643e1ff7c2354de4a","title":"Leveraging pre-trained language models for mining microbiome-disease relationships"},{"paperId":"d8d095d6a94d747f6f41534fc27b38b07dc9c6de","title":"Improving Pre-trained Language Models' Generalization"},{"paperId":"42555e01ba20ea0ef970ff7d5d6266992e50e233","title":"Investigating drug translational research using PubMed articles"},{"paperId":"d210cb3f64ab2f5fa106fac60f2f5a94387a3b29","title":"RegEMR: a natural language processing system to automatically identify premature ovarian decline from Chinese electronic medical records"},{"paperId":"19b5d041491de67bfb37531309644b5cc8b36dae","title":"Few-shot Named Entity Recognition: definition, taxonomy and research directions"},{"paperId":"91c5e43f6ed8d13057eaead21c4f6dd0ed8a1112","title":"Multimodal Machine Learning for Extraction of Theorems and Proofs in the Scientific Literature"},{"paperId":"5b97c7ba8dc762d917af5a486ca76e352c8f2a33","title":"Assessment of the E3C corpus for the recognition of disorders in clinical texts"},{"paperId":"94ce1d5924e05e8d75e43ce70044293ddcef850a","title":"Large language models in medicine."},{"paperId":"5d7bef7691811a080a38b7852f2bed942a42797e","title":"Improved prediction of drug-induced liver injury literature using natural language processing and machine learning methods"},{"paperId":"97f06732672462ecaeea2e9006a9bb14d15d6c33","title":"Generalizable prediction of potential miRNA-disease associations based on heterogeneous graph learning"},{"paperId":"29954a8c7e43b9d96bf968298c8dbd37aedbb887","title":"Leveraging artificial intelligence in the fight against infectious diseases"},{"paperId":"4b84cb46a704cc3a978758d8bf09fff25ed71a5a","title":"Inferring cancer disease response from radiology reports using large language models with data augmentation and prompting."},{"paperId":"2003dcf311ccab2426818113964e59ca0cfde2c9","title":"FDAPT: Federated Domain-adaptive Pre-training for Language Models"},{"paperId":"313e928295fcf9a33186155c53a9acda98e4f945","title":"Artificial Intelligence in Orthopaedic Surgery: Can a Large Language Model \"Write\" a Believable Orthopaedic Journal Article?"},{"paperId":"7e2e093009d63e3cd1f3152fa11d8a75bf937c31","title":"Generalizability of machine learning methods in detecting adverse drug events from clinical narratives in electronic medical records"},{"paperId":"9bf8cb128b5e5a13122152bb34617a393520d02c","title":"Advancements in Scientific Controllable Text Generation Methods"},{"paperId":"fbd748bdd6102c1b945c3d58e92469802dfec244","title":"Parameter-Efficient Fine-Tuning of LLaMA for the Clinical Domain"},{"paperId":"e0e9ba0c01d441e1fdcb8628d3f743d387b0b017","title":"UIT-Saviors at MEDVQA-GI 2023: Improving Multimodal Learning with Image Enhancement for Gastrointestinal Visual Question Answering"},{"paperId":"a2d765c0563aaf802cf3b70bb69fe6361bb9e316","title":"DeepOnto: A Python Package for Ontology Engineering with Deep Learning"},{"paperId":"0734964fbeaab1da5b992fd5e88a6fe1b8c6e97a","title":"Enhancing multiple-choice question answering through sequential fine-tuning and Curriculum Learning strategies"},{"paperId":"a59c8bdd283e5a49329d5c46c4fbcf14d1a470f5","title":"ODD: A Benchmark Dataset for the NLP-based Opioid Related Aberrant Behavior Detection"},{"paperId":"1e3ef48abeef882e12f9553a1baf8944f3782c88","title":"Several Categories of Large Language Models (LLMs): A Short Survey"},{"paperId":"1b8cdff9e5eefc614cd8940c386b26ce215d32b6","title":"DeBEIR: A Python Package for Dense Bi-Encoder\nInformation Retrieval"},{"paperId":"9ecf184dd657640cbd1c4cc0f3c801ebd9d53162","title":"ReactIE: Enhancing Chemical Reaction Extraction with Weak Supervision"},{"paperId":"fd42031baa3fe8690a00767c2fdf52dbcf945713","title":"Exploring the In-context Learning Ability of Large Language Model for Biomedical Concept Linking"},{"paperId":"afa697ec0f3eac8467ec7d0c4c807d782945b0ed","title":"T4SEpp: a pipeline integrated with protein language models effectively predicting bacterial type IV secreted effectors"},{"paperId":"07985ad1d563d668a8871391a940e8cc9ba205c5","title":"BioCPT: Contrastive Pre-trained Transformers with Large-scale PubMed Search Logs for Zero-shot Biomedical Information Retrieval"},{"paperId":"e9388e699a7ef4d12ae425e341ff610c67cbf64b","title":"How far is Language Model from 100% Few-shot Named Entity Recognition in Medical Domain"},{"paperId":"316a845b449b64bed6f2aeab94045e5d77c25a8b","title":"Biomedical extractive question answering based on dynamic routing and answer voting"},{"paperId":"ec23e0a536e0c2d09cfb115e11842fc4575043a2","title":"A social media event detection framework based on transformers and swarm optimization for public notification of crises and emergency management"},{"paperId":"3ec26a84129f1dfd758563d116a4c2b8976732e2","title":"Counterfactual can be strong in medical question and answering"},{"paperId":"d935f18e9942bc6ecaefad2c739a5c4a83f3b121","title":"SenRev: Measurement of Personal Information Disclosure in Online Health Communities"},{"paperId":"cb3fc8d828f5c7628e5bc5f7427dc567a4043788","title":"ShortMail: An email summarizer system"},{"paperId":"7d9fbd1ce90c2e6b64ba054c9f78cc5c45756cac","title":"Self-prediction of relations in GO facilitates its quality auditing."},{"paperId":"d798b65bb57f54c62e6f54b074c580fb7bc360d3","title":"RDKG-115: Assisting drug repurposing and discovery for rare diseases by trimodal knowledge graph embedding"},{"paperId":"5e791fa8bd5ebe8b31957345112cb03ceec23b9e","title":"Automated Recognition of Visual Acuity Measurements in Ophthalmology Clinical Notes using Deep Learning"},{"paperId":"ebb3d299213bae89b5d302cc3dfc36573ec83956","title":"SummQA at MEDIQA-Chat 2023: In-Context Learning with GPT-4 for Medical Summarization"},{"paperId":"b44865672b3896e249b81a39cbe850286f8140c0","title":"Transformers in Healthcare: A Survey"},{"paperId":"30f36f68265823c7f9945f902451fe0b1fac790b","title":"Biomedical Language Models are Robust to Sub-optimal Tokenization"},{"paperId":"ee29656917619452360fe5dd848251a5081944bf","title":"Machine learning for potion development at Hogwarts"},{"paperId":"439c2a5c4883b421ca316617b1306583cc1d706c","title":"Automated Extraction and Visualization of Metabolic Networks from Biomedical Literature Using a Large Language Model"},{"paperId":"4f20a597685bb1554c318d0b75cf71f608708166","title":"Knowledge and skills extraction from the job requirements texts"},{"paperId":"c44c15d0cb3c1da74aa8768fb12a8cc49e9ba6b1","title":"Is ChatGPT a Biomedical Expert? - Exploring the Zero-Shot Performance of Current GPT Models in Biomedical Tasks"},{"paperId":"ce6c1514a75619996e9fd686e53ff1f480b87d51","title":"DMNER: Biomedical Entity Recognition by Detection and Matching"},{"paperId":"46c7abb4572c51fdeae692fda91b22376cecd3ac","title":"CamemBERT-bio: a Tasty French Language Model Better for your Health"},{"paperId":"e6c1422edf59b77da143c44e29f0020352e85d65","title":"Applications of cutting-edge artificial intelligence technologies in biomedical literature and document mining"},{"paperId":"920953238e203dbfc507ec6a1d103ff49ddabf09","title":"Multimodal Deep Learning Methods on Image and Textual Data to Predict Radiotherapy Structure Names"},{"paperId":"0c4a1d3bb96df512037c6f0ae7630aa696e09180","title":"Vocabulary Matters: An Annotation Pipeline and Two Deep Learning Algorithms for Enzyme Named Entity Recognition"},{"paperId":"ba95eca1bc4b99c1b95246d09efd8bf46de2a6e9","title":"Stress Testing BERT Anaphora Resolution Models for Reaction Extraction in Chemical Patents"},{"paperId":"e30d993d79cb5680a5a7a7fbff377304326c1397","title":"Identifying Patient Populations in Texts Describing Drug Approvals Through Deep Learning-Based Information Extraction: Development of a Natural Language Processing Algorithm."},{"paperId":"aed522d7ea6a05349d0f6e5365b9284d463b8a52","title":"Explainable online health information truthfulness in Consumer Health Search"},{"paperId":"e96d3f85aa56f027e028189346e043e346f3acea","title":"LMFlow: An Extensible Toolkit for Finetuning and Inference of Large Foundation Models"},{"paperId":"1d633934bb4164c48f1c29bf2632492fe31b629b","title":"Bidirectional End-to-End Learning of Retriever-Reader Paradigm for Entity Linking"},{"paperId":"c7288139fa83a54c6bbc3680535256371678ff1e","title":"Exploring New Frontiers in Agricultural NLP: Investigating the Potential of Large Language Models for Food Applications"},{"paperId":"4ca6b4cd66c418478461f6e741b5195dd9aae763","title":"Provision and Characterization of a Corpus for Pharmaceutical, Biomedical Named Entity Recognition for Pharmacovigilance: Evaluation of Language Registers and Training Data Sufficiency"},{"paperId":"e623fc12e03d599de94d6384601723416bedd41c","title":"Enhancing Documents with Multidimensional Relevance Statements in Cross-encoder Re-ranking"},{"paperId":"a6418c9f0135a4d1489aa4cf9e67564c57d14706","title":"BioREx: Improving Biomedical Relation Extraction by Leveraging Heterogeneous Datasets"},{"paperId":"b31911d6028e392755f20d2bd246fd640c19b407","title":"EMSAssist: An End-to-End Mobile Voice Assistant at the Edge for Emergency Medical Services"},{"paperId":"09494bb14ca2d09729ca1892fcc27505706b9239","title":"A Joint Entity and Relation Extraction Model based on Efficient Sampling and Explicit Interaction"},{"paperId":"8453a04c5a7b41675fd4c6748ce9bc3ae16ecdb9","title":"Automatic Extraction of Comprehensive Drug Safety Information from Adverse Drug Event Narratives in the Korea Adverse Event Reporting System Using Natural Language Processing Techniques"},{"paperId":"42248d44329626f6e2714bdce32089b99276b9d6","title":"SCALE: Scaling up the Complexity for Advanced Language Model Evaluation"},{"paperId":"8772847b9b87a7babd3aa0b5fe5bf55f8c9e028b","title":"Opportunities and Challenges for ChatGPT and Large Language Models in Biomedicine and Health"},{"paperId":"ace34d08c8600ddd364d09347295f4529d662a30","title":"Acute stroke CDS: automatic retrieval of thrombolysis contraindications from unstructured clinical letters"},{"paperId":"be26d2b940eac33a9989d1a6bf2316934c6e1837","title":"Building a Corpus for Biomedical Relation Extraction of Species Mentions"},{"paperId":"405c420a5c5753d0d7168e67809b67096c862a0a","title":"Large Language Models and Medical Education: Preparing for a Rapid Transformation in How Trainees Will Learn to Be Doctors"},{"paperId":"9f3778d51275a912a46bd05ace20b101c3f8f0d7","title":"A detailed library perspective on nearly unsupervised information extraction workflows in digital libraries"},{"paperId":"86804135859c64b37b0dac7e41fe4b9a0e8b6023","title":"Contextualized medication event extraction with striding NER and multi-turn QA."},{"paperId":"1bfe35520140dbd1a21508ed3ff814e7ae218464","title":"Weakly supervised information extraction from inscrutable handwritten document images"},{"paperId":"b17e827549b75376a2e56e4324b28e96bb13e576","title":"EriBERTa: A Bilingual Pre-Trained Language Model for Clinical Natural Language Processing"},{"paperId":"6294f078e79828cac21e717813e8f3d02b18a97c","title":"The importance of resource awareness in artificial intelligence for healthcare"},{"paperId":"09734bb2b7da3f7dfc0eb1c093a949e855794d6a","title":"MedKPL: a heterogeneous knowledge enhanced prompt learning framework for transferable diagnosis."},{"paperId":"116c19f5cdf2bf7884fd25ff2a7683ede6eaaa8a","title":"QUERT: Continual Pre-training of Language Model for Query Understanding in Travel Domain Search"},{"paperId":"a866be3528023e70ba4a6e3abf4c1151d3a7eff5","title":"ECGBERT: Understanding Hidden Language of ECGs with Self-Supervised Representation Learning"},{"paperId":"b239648a07d44d71185e48ab79bf5a2bc347e52f","title":"Medical Data Augmentation via ChatGPT: A Case Study on Medication Identification and Medication Event Classification"},{"paperId":"1da7013f004bc2107d58d094cb9e868152b255c9","title":"Using BERT models for breast cancer diagnosis from Turkish radiology reports"},{"paperId":"17e83563a7b7be397240b1996861e5858fdb8dfc","title":"$FPDM$: Domain-Specific Fast Pre-training Technique using Document-Level Metadata"},{"paperId":"542308ad5c1c0b5ad88e76e6c8d941a6d08ccd01","title":"Interpretable Medical Diagnostics with Structured Data Extraction by Large Language Models"},{"paperId":"bb17c5fb339e758feeed1bd080bf49ba1f097900","title":"Comprehensive evaluation of deep and graph learning on drug-drug interactions prediction"},{"paperId":"4ef3562c52ff0e1d1ffc978c4715edff5a586cda","title":"Advancing Italian Biomedical Information Extraction with Large Language Models: Methodological Insights and Multicenter Practical Application"},{"paperId":"db1aa71314016e12e115fbe449a688f523e52e77","title":"CUED at ProbSum 2023: Hierarchical Ensemble of Summarization Models"},{"paperId":"9618aa98729670f74418d2087f5e47ab137856b4","title":"Mixture-of-Domain-Adapters: Decoupling and Injecting Domain Knowledge to Pre-trained Language Models’ Memories"},{"paperId":"9821928eee1ed1f36a2f5f935a5d31a71eede6b7","title":"Leveraging Knowledge Graph Embeddings to Enhance Contextual Representations for Relation Extraction"},{"paperId":"a381e0323f53022382a09422ad100e43272847e5","title":"Evaluation of ChatGPT on Biomedical Tasks: A Zero-Shot Comparison with Fine-Tuned Generative Transformers"},{"paperId":"0bd2602df71e89c8961562175c8759e625e99389","title":"ModuleFormer: Learning Modular Large Language Models From Uncurated Data"},{"paperId":"9b11f5e8b40b109cb774e29e5cf5a5baa8beeed8","title":"Generative Text-Guided 3D Vision-Language Pretraining for Unified Medical Image Segmentation"},{"paperId":"a7f55cf136f6a24eaa0772006de92979acfc3a58","title":"BioBLP: A Modular Framework for Learning on Multimodal Biomedical Knowledge Graphs"},{"paperId":"db675736b5f8169bb7da1c2d0bba2f93e8fe2d3a","title":"PyTrial: A Comprehensive Platform for Artificial Intelligence for Drug Development"},{"paperId":"3846d296a938dcc0a92784da76f9ef90d9de2e29","title":"Incorporating domain knowledge for biomedical text analysis into deep learning: A survey."},{"paperId":"f63a02601c7c3fdabcfff118d98e815697c42e0f","title":"shs-nlp at RadSum23: Domain-Adaptive Pre-training of Instruction-tuned LLMs for Radiology Report Impression Generation"},{"paperId":"264fd0af8b0906e3deb5df7f0ec0a7845b8ba213","title":"CoSiNES: Contrastive Siamese Network for Entity Standardization"},{"paperId":"4791aacf07f8ed425003d32ad3138416ffa44ae2","title":"Meta Learning for Domain Agnostic Soft Prompt"},{"paperId":"3f18227b9b23115253e582610b41d864264fe7f7","title":"RadLing: Towards Efficient Radiology Report Understanding"},{"paperId":"93856a907207c770a68f5c91c67854b366b3c1f7","title":"Impact of translation on biomedical information extraction from real-life clinical notes"},{"paperId":"4b78ad9179428a746908d05313a160d220e97750","title":"A Comprehensive Survey on Deep Learning for Relation Extraction: Recent Advances and New Frontiers"},{"paperId":"362d4e00506f9bb39d42185a0b128f8602e139a8","title":"Utilizing ChatGPT to Enhance Clinical Trial Enrollment"},{"paperId":"b4f0059c9a706f963e0ab6d82163a7762dafb176","title":"ACI-BENCH: a Novel Ambient Clinical Intelligence Dataset for Benchmarking Automatic Visit Note Generation"},{"paperId":"80cee5037d01470edc7fbd20c564f2e1fc2c6b85","title":"MultiLegalPile: A 689GB Multilingual Legal Corpus"},{"paperId":"edb428f00899810457892faaecdbcfbd04a4b42f","title":"Contextual Representation in NLP to Improve Success in Accident Classification of Mine Safety Narratives"},{"paperId":"4a4511d367113da3d3febf813bcb857023665533","title":"An analysis of entity normalization evaluation biases in specialized domains"},{"paperId":"120c3c98818ce29dbb9847f221050b2a2a82d4ed","title":"Extensive Evaluation of Transformer-based Architectures for Adverse Drug Events Extraction"},{"paperId":"ac9d0362acd4cb37a0226cc29cedde336a740bb3","title":"EMS-BERT: A Pre-Trained Language Representation Model for the Emergency Medical Services (EMS) Domain"},{"paperId":"2c26efa3a0b9dccd4f49edf3abd6515ee0228ad0","title":"MuLan-Methyl - Multiple Transformer-based Language Models for Accurate DNA Methylation Prediction"},{"paperId":"1681b374734559fe476aed69afaa2887f3576ad9","title":"BERT-based natural language processing analysis of French CT reports: Application to the measurement of the positivity rate for pulmonary embolism"},{"paperId":"08d5d3f67cb783ebe6fcb4274116335607a4b3ba","title":"Performance Comparison of Transformer-Based Models on Twitter Health Mention Classification"},{"paperId":"ea537354fc2e2e9c53253aad1dc752f7bf715805","title":"CAISA at SemEval-2023 Task 8: Counterfactual Data Augmentation for Mitigating Class Imbalance in Causal Claim Identification"},{"paperId":"f22d71c7ce9720ba1f717a4f1181488200e78198","title":"LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day"},{"paperId":"ea4cf43b0c4990a2a262cf86c62e3a522e60861f","title":"MTMG:A multi-task model with multi-granularity information for drug-drug interaction extraction"},{"paperId":"dd6ed104420b314ac3b4f3556f35fd07274bf453","title":"Quality of word and concept embeddings in targetted biomedical domains"},{"paperId":"6b38963d45e15a83244aafec08438bb9bf8f3831","title":"Serial KinderMiner (SKiM) Discovers and Annotates Biomedical Knowledge Using Co-Occurrence and Transformer Models"},{"paperId":"c0f1ca1ed0f031ffaaf91a00eb16049917d395af","title":"Probing the EHR for Standardized Nursing Data"},{"paperId":"b5f437115d6c763218f186962811185613d64908","title":"Large-scale neural biomedical entity linking with layer overwriting."},{"paperId":"ee28be89eea46d2d46ea39efc670f08089241bf2","title":"Pre-Training MLM Using Bert for the Albanian Language"},{"paperId":"cb97b1570591835e15ed94c525f847cf53c633f6","title":"FindZebra online search delving into rare disease case reports using natural language processing"},{"paperId":"858be2842122005b5a1aabce844676423ab6ca01","title":"CardioBERTpt: Transformer-based Models for Cardiology Language Representation in Portuguese"},{"paperId":"6e69fe3f69249dffc90ef2336b842ca7f4724c82","title":"MedNgage: A Dataset for Understanding Engagement in Patient-Nurse Conversations"},{"paperId":"8b5d8d852a0b924fe285ed70c3add4a8ff14713c","title":"Shuo Wen Jie Zi: Rethinking Dictionaries and Glyphs for Chinese Language Pre-training"},{"paperId":"96f006da556061e74751a598c5ff185999efa240","title":"DyGen: Learning from Noisy Labels via Dynamics-Enhanced Generative Modeling"},{"paperId":"d78fdf702c45b937780f60224505da0ed3085ce4","title":"W-procer: Weighted Prototypical Contrastive Learning for Medical Few-Shot Named Entity Recognition"},{"paperId":"d3060876d9ad4e4e50e1c88a8c04186df00f24e2","title":"A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets"},{"paperId":"55971443f3ca7f26056682ecb3da4491a1e4810d","title":"Understanding Breast Cancer Survival: Using Causality and Language Models on Multi-omics Data"},{"paperId":"b7ef3c5d0da6e609f1e26b890389b3f1989e777a","title":"An Investigation into the Effects of Pre-training Data Distributions for Pathology Report Classification"},{"paperId":"04b62536270e27efe280f904f7e27e394f6d0192","title":"Complementary and Integrative Health Lexicon (CIHLex) and Entity Recognition in the Literature"},{"paperId":"2def8709b2a75ed533663ec7676601ceebdd23c3","title":"Constructing a disease database and using natural language processing to capture and standardize free text clinical information"},{"paperId":"31a7d8c4a5ab6bab522494b57270249105c8748e","title":"BiomedGPT: A Unified and Generalist Biomedical Generative Pre-trained Transformer for Vision, Language, and Multimodal Tasks"},{"paperId":"0133c1128f2036ecb6b65ab15c562b71bf4f18a0","title":"Scientific Fact-Checking: A Survey of Resources and Approaches"},{"paperId":"f45bee9da1655320b7fc290d2abc20903bd12545","title":"Leveraging Domain Knowledge for Inclusive and Bias-aware Humanitarian Response Entry Classification"},{"paperId":"51b169701290cd129e0781fc9f3a9918604c89b5","title":"Improving accuracy of GPT-3/4 results on biomedical data using a retrieval-augmented language model"},{"paperId":"d8eb6b0535ae6c96921bfc2c08318902c08a3e63","title":"Efficient Document Embeddings via Self-Contrastive Bregman Divergence Learning"},{"paperId":"eda1a6780c32201827ff0bcd3ed42649f624f2f7","title":"The landscape of biomedical research"},{"paperId":"a96ead1e2905bca910beae79c1f8e79365e654a4","title":"Neural Natural Language Processing for Long Texts: A Survey of the State-of-the-Art"},{"paperId":"0fbf7ea1a3bd1754ed9aa12ed25906b731ece589","title":"Training Data Extraction From Pre-trained Language Models: A Survey"},{"paperId":"06091944b864d6dc473cab63321a95fb9c4067cc","title":"ChatCAD+: Towards a Universal and Reliable Interactive CAD using LLMs"},{"paperId":"f31b89d216f3c60773e11228fd0b60c37ccfefdf","title":"Ensemble of deep learning language models to support the creation of living systematic reviews for the COVID-19 literature"},{"paperId":"30fd5cad61b20dd39a49fb50c3dbc300d146c049","title":"Injecting Knowledge into Biomedical Pre-trained Models via Polymorphism and Synonymous Substitution"},{"paperId":"a05a0e85d22a3a9dd911c5612f850203a9fac7eb","title":"Lawyer LLaMA Technical Report"},{"paperId":"180a5bfb5459538127f90db0a44445353175b052","title":"A publication-wide association study (PWAS), historical language models to prioritise novel therapeutic drug targets"},{"paperId":"29bf34941a5f8c33f2262356cb18ffe4f555fbc5","title":"A-BBL: A Risk Prediction Model for Patient Readmission based on Electronic Medical Records"},{"paperId":"3278db202e10c878219f51cc7eda1859c6956aaf","title":"Difference-Masking: Choosing What to Mask in Continued Pretraining"},{"paperId":"ca4c88f57a1914024ccfd2e98d59e343c340fb01","title":"Challenges to sharing sample metadata in computational genomics"},{"paperId":"ee2db2936524122cdeb4755dc4a8f12933f422af","title":"Pre-training Multi-task Contrastive Learning Models for Scientific Literature Understanding"},{"paperId":"01a2f5d697d7009cff153773612cf5a6f2c48390","title":"BAND: Biomedical Alert News Dataset"},{"paperId":"3351c60442e8bd9e109fe2d9cd7fbf806a42dd33","title":"Partial Annotation Learning for Biomedical Entity Recognition"},{"paperId":"1e4c49c9c93678dec95326ce25715fd2a1e64192","title":"Farewell to Aimless Large-scale Pretraining: Influential Subset Selection for Language Model"},{"paperId":"26c709e11c528bdbc2d82965867160d6918c3cbb","title":"Biomedical Named Entity Recognition via Dictionary-based Synonym Generalization"},{"paperId":"9bd30b7626fb4c229abe41c2c0d19c90dd87e168","title":"Drug–disease association prediction with literature based multi-feature fusion"},{"paperId":"d62ddd64f841cd5c263003a426adb3909b6311bc","title":"Evaluating Prompt-based Question Answering for Object Prediction in the Open Research Knowledge Graph"},{"paperId":"74b94891f8f7ac8d73d9df817b6720e1cb792bcc","title":"Enhancing Small Medical Learners with Privacy-preserving Contextual Prompting"},{"paperId":"de24a9888816de03dfabd9215ceb3443981b8e7e","title":"TADA: Efficient Task-Agnostic Domain Adaptation for Transformers"},{"paperId":"508730e9b10bf7fb048677248b53d82144d63666","title":"ARCH: Large-scale Knowledge Graph via Aggregated Narrative Codified Health Records Analysis"},{"paperId":"fe090a804a6d229034823dde035e4c0655b6665c","title":"Understanding the Effect of Data Augmentation on Knowledge Distillation"},{"paperId":"21c3343148290cf3f3d7739ca6d6f191fc66f613","title":"Gene Set Summarization using Large Language Models"},{"paperId":"4eeade5b60ad6495dfe9f4fb4cd6183a861520af","title":"EduNER: a Chinese named entity recognition dataset for education research"},{"paperId":"c07618042c9ad4ae4b296cc307f21d6b28d3dcdd","title":"ESCOXLM-R: Multilingual Taxonomy-driven Pre-training for the Job Market Domain"},{"paperId":"2beff5fc095901a967ee15eb637e2b97b22eb8e4","title":"AnyPredict: Foundation Model for Tabular Prediction"},{"paperId":"eed71f3015fd46e4df5b681dcd78bc173e0eeaa5","title":"Appraising the Potential Uses and Harms of LLMs for Medical Systematic Reviews"},{"paperId":"6783b17fe4328f48403f57009a73f784de09f645","title":"XuanYuan 2.0: A Large Chinese Financial Chat Model with Hundreds of Billions Parameters"},{"paperId":"6026196f25a1ec5a7af3ff055ecefa49bfe1e710","title":"Plug-and-Play Medical Dialogue System"},{"paperId":"1cce4b2f81106c6e457c6650a5bda4bc742cf88b","title":"Decouple knowledge from paramters for plug-and-play language modeling"},{"paperId":"5ecfadb0211dd89a81d066a35a0e2312e991bc4c","title":"BioAug: Conditional Generation based Data Augmentation for Low-Resource Biomedical NER"},{"paperId":"cb5f53cd7d43ac6f48abf4dde6fc58e53a083b0e","title":"Accelerating the integration of ChatGPT and other large‐scale AI models into biomedical research and healthcare"},{"paperId":"cd72278471f49a3e667d6eab56ba9b538b42403b","title":"A domain semantics-enhanced relation extraction model for identifying the railway safety risk"},{"paperId":"a03b0ca43b5b687a6c38789157c3b803c9e02694","title":"Echoes of Biases: How Stigmatizing Language Affects AI Performance"},{"paperId":"80e972c82df37c60970552fb262c68cc24114964","title":"Review on Query focused Multi-Document Summarization (QMDS) with Comparative Analysis"},{"paperId":"918994424dd6b45005c72563dcda33183552cde1","title":"Question-Answering System Extracts Information on Injection Drug Use from Clinical Progress Notes"},{"paperId":"fded78529a6c853bb97ce7755000ff217756bd0d","title":"From language models to large-scale food and biomedical knowledge graphs"},{"paperId":"dff0efaa388f546249976e8f5c6cee8eb4f12633","title":"An answer recommendation framework for an online cancer community forum"},{"paperId":"049288e68caeadf7842df6977e140b47a8a2f89d","title":"MatSci-NLP: Evaluating Scientific Language Models on Materials Science Language Tasks Using Text-to-Schema Modeling"},{"paperId":"2e4e00164e5fb1a6786cc45d39ce95063dca2e70","title":"Deployment of machine learning algorithms to predict sepsis: systematic review and application of the SALIENT clinical AI implementation framework"},{"paperId":"b5ab93517516a4758f866e7cac03ca378296c124","title":"INGENIOUS: Using Informative Data Subsets for Efficient Pre-Training of Large Language Models"},{"paperId":"39831c8c222a8d78fff4d67e7e56f5eeb90fdd7f","title":"Generative Pre-trained Transformer: A Comprehensive Review on Enabling Technologies, Potential Applications, Emerging Challenges, and Future Directions"},{"paperId":"fc19a3b7364cc9ee1092df99cb74426843d92af1","title":"Large language models in biomedical natural language processing: benchmarks, baselines, and recommendations"},{"paperId":"4c4722d3767dae6bc00b7de3e3fa160caaffe483","title":"Privacy-Preserving Prompt Tuning for Large Language Model Services"},{"paperId":"0152a75cfe2cee2b7510118635c75369dd9be690","title":"Representation Learning for Person or Entity-centric Knowledge Graphs: An application in Healthcare"},{"paperId":"ce7d9f1fab20e8121a6a86107eccbf0064405003","title":"Large language models for oncological applications."},{"paperId":"7445ca3b53cf597b1c81b347b3e954e70b71dee9","title":"GersteinLab at MEDIQA-Chat 2023: Clinical Note Summarization from Doctor-Patient Conversations through Fine-tuning and In-context Learning"},{"paperId":"bf09ea12856f2e055d24a1e41612c2a70e4e24f2","title":"Enhancing Knowledge Management in Healthcare: An Embedding Fusion Approach to Business Rule Representation"},{"paperId":"aa95fa8f677b07d32446de78dca300aae13d0a36","title":"MIReAD: Simple Method for Learning High-quality Representations from Scientific Documents"},{"paperId":"9e005b62ac48fae400028bcc00378d9bb1c04a7b","title":"NER-to-MRC: Named-Entity Recognition Completely Solving as Machine Reading Comprehension"},{"paperId":"32c2af20ae3581cb8d3dafd225d943ef0757c217","title":"SANTA: Separate Strategies for Inaccurate and Incomplete Annotation Noise in Distantly-Supervised Named Entity Recognition"},{"paperId":"76c26e1a30d665c62fe78b7e9c31ed8358915dcc","title":"From Zero to Hero: Harnessing Transformers for Biomedical Named Entity Recognition in Zero- and Few-shot Contexts"},{"paperId":"35d2276749c2c31290d2ff410a305112e742da71","title":"Low-Resource Multi-Granularity Academic Function Recognition Based on Multiple Prompt Knowledge"},{"paperId":"1ced16c3861cd4a9b88eee7330df1917f55b3a00","title":"NLI4CT: Multi-Evidence Natural Language Inference for Clinical Trial Reports"},{"paperId":"42b6c3f3ab62311d503e6d30233aa96beac689fe","title":"Harnessing the Power of BERT in the Turkish Clinical Domain: Pretraining Approaches for Limited Data Scenarios"},{"paperId":"f11ea0fe307ce40fee1f18dfc3eef946a1c7a769","title":"SemEval-2023 Task 7: Multi-Evidence Natural Language Inference for Clinical Trial Data"},{"paperId":"75b04f49dd5685fc639c4511a4793a8d7b416abb","title":"Learning Missing Modal Electronic Health Records with Unified Multi-modal Data Embedding and Modality-Aware Attention"},{"paperId":"452fa2baafdfedaa93c6cbe9a28d3927864b185d","title":"SnorkelPlus: A Novel Approach for Identifying Relationships Among Biomedical Entities Within Abstracts"},{"paperId":"ae15f13135748fae111c8cfdeb18b35eb9239cc6","title":"Creating an Ignorance-Base: Exploring Known Unknowns in the Scientific Literature"},{"paperId":"fcd1d26d443a982ea79e1351bfaf791209e7c74d","title":"Europe PMC Annotated Full-text Corpus for Gene/Proteins, Diseases and Organisms"},{"paperId":"c3b516c152c02bee1896ba6205d71f852ec9b236","title":"Heart disease risk factors detection from electronic health records using advanced NLP and deep learning techniques"},{"paperId":"60d309178cd324de7980008d768707ba1682b8ea","title":"Siamese BERT Architecture Model with attention mechanism for Textual Semantic Similarity"},{"paperId":"1c8703b93f28db2c748627e176f280b138354c12","title":"Knowledge-graph-enabled biomedical entity linking: a survey"},{"paperId":"912ef861b37cfd95d631daefb57a21832efb8380","title":"Biomedical Relation Extraction Using Dependency Graph and Decoder-Enhanced Transformer Model"},{"paperId":"df8740034b68e4250d0ceefa9fcbdf42c83af25d","title":"A Comparative Study of Cross-Sentence Features for Named Entity Recognition"},{"paperId":"8ff8b3173c6b684d361209d7493568c3588d3daa","title":"Affect Analysis in Arabic Text: Further Pre-Training Language Models for Sentiment and Emotion"},{"paperId":"1227c8f40b62153c7068e4a134711f6ebb402e64","title":"Exploring the knowledge certainty shift: Metaknowledge analysis on drugs via assertion uncertainty burstiness"},{"paperId":"d2685ecfb612493b44b6eaecb5533e8da7b4b7c3","title":"Lessons learned to enable question answering on knowledge graphs extracted from scientific publications: A case study on the coronavirus literature"},{"paperId":"e5d51e257299cc3996b4dc10a3ca55f4934a38df","title":"Deep learning to refine the identification of high-quality clinical research articles from the biomedical literature: Performance evaluation"},{"paperId":"e4d6df552ec096000e459cecfe936ab4d6781c3b","title":"Enhancing early autism prediction based on electronic records using clinical narratives."},{"paperId":"69e3769724b91aa19f852786c04a9d079097ae64","title":"Active learning with feature matching for clinical named entity recognition"},{"paperId":"033cd044b41868c4d1713d917b6ff73f919783c5","title":"KEBLM: Knowledge-Enhanced Biomedical Language Models."},{"paperId":"cd5a1c434fc47892f68a4d0f58d88dc3b6218e05","title":"FooDis: A food-disease relation mining pipeline"},{"paperId":"6486f0b6e443cb864639d4a85277d71cf69f78e0","title":"Embracing Large Language Models for Medical Applications: Opportunities and Challenges"},{"paperId":"618f9c581f6003a79eff5ca2410e3be70bf4abd1","title":"Towards electronic health record-based medical knowledge graph construction, completion, and applications: A literature study."},{"paperId":"662a0ffbbb108e6d51882c788a949e73ff56fe7f","title":"A co-adaptive duality-aware framework for biomedical relation extraction"},{"paperId":"b9f7947227210e44012f806c28510dbf761dacc0","title":"A Joint Extraction System Based on Conditional Layer Normalization for Health Monitoring"},{"paperId":"ae495ee34fa97614c79949d528de9dec182f5365","title":"RepresentThemAll: A Universal Learning Representation of Bug Reports"},{"paperId":"e18590ebcd56755fb19d8c1ff3f8ed46a66fe1a8","title":"An integrated explicit and implicit offensive language taxonomy"},{"paperId":"736bfcdf743bb49c28eb604f3481632b70dd7605","title":"MSQ-BioBERT: Ambiguity Resolution to Enhance BioBERT Medical Question-Answering"},{"paperId":"309275a127536f4efca4a5cda1d47ee7bb0368c3","title":"Contextual Response Interpretation for Automated Structured Interviews: A Case Study in Market Research"},{"paperId":"a0dca1c35f698b7b7af91449427ed035d9e4e049","title":"A Review of ChatGPT Applications in Education, Marketing, Software Engineering, and Healthcare: Benefits, Drawbacks, and Research Directions"},{"paperId":"39c486150cc8179e30ac8bab5935c7990167b5c0","title":"BactInt: A domain driven transfer learning approach and a corpus for extracting inter-bacterial interactions from biomedical text"},{"paperId":"37ebfbc27e9a9a9dd772a472c94c2ae664152508","title":"MasonNLP+ at SemEval-2023 Task 8: Extracting Medical Questions, Experiences and Claims from Social Media using Knowledge-Augmented Pre-trained Language Models"},{"paperId":"131c6f328c11706de2c43cd16e0b7c5d5e610b6a","title":"Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond"},{"paperId":"32fdf76e571089ac463e356694a90599b99c6214","title":"Extracting social determinants of health from clinical note text with classification and sequence-to-sequence approaches."},{"paperId":"d15009785d3449d16244b50ef570922e463ba3ba","title":"A Compressed Language Model Embedding Dataset of ICD 10 CM Descriptions"},{"paperId":"12594b6afe01461384d2856d2bf44f1cf8533e3e","title":"ChatGPT and the rise of large language models: the new AI-driven infodemic threat in public health"},{"paperId":"6377f7a529549af725908e8eaee9d24aef9879cc","title":"Sebis at SemEval-2023 Task 7: A Joint System for Natural Language Inference and Evidence Retrieval from Clinical Trial Reports"},{"paperId":"7480bf28ce20a03f8296925ec3eb6e71ee71935b","title":"Information Extraction Network Based on Multi-Granularity Attention and Multi-Scale Self-Learning"},{"paperId":"da226d6e7007b39e7f5f0878894419858e3133cb","title":"Improving Model Transferability for Clinical Note Section Classification Models Using Continued Pretraining"},{"paperId":"dbd1162ad79cf153bad482a5dc63b9430b6e593e","title":"FineEHR: Refine Clinical Note Representations to Improve Mortality Prediction"},{"paperId":"c7f152f0c48e039f63f8c3da1a663b180a6f71f6","title":"Benchmarking ChatGPT-4 on ACR Radiation Oncology In-Training (TXIT) Exam and Red Journal Gray Zone Cases: Potentials and Challenges for AI-Assisted Medical Education and Decision Making in Radiation Oncology"},{"paperId":"89cbb16aea55469e56a7b5051196e14055511a00","title":"DDI-MuG: Multi-aspect graphs for drug-drug interaction extraction"},{"paperId":"286756b2b02d6a7bc49a7ad66686f30831f26c25","title":"Differentiate ChatGPT-generated and Human-written Medical Texts"},{"paperId":"2d31cf90218fbf3ce4b220ab3bdff74e17d1b4f5","title":"Web Interface of NER and RE with BERT for Biomedical Text Mining"},{"paperId":"40ae90005ba612cff567a96b7d0cedeca0d2635a","title":"On the Use of Transformer-Based Models for Intent Detection Using Clustering Algorithms"},{"paperId":"a564daa5ffdd4ea4cbb28b6ea459da9f9f65428d","title":"Faithful AI in Healthcare and Medicine"},{"paperId":"a0ec9f110ea172aa863862929cc0338934ff93c6","title":"Harnessing Biomedical Literature to Calibrate Clinicians’ Trust in AI Decision Support Systems"},{"paperId":"8e4558c878ecdac1091486727634c1ed5c8c38a8","title":"A Systematic survey on automated text generation tools and techniques: application, evaluation, and challenges"},{"paperId":"be545c9ec5e757513988a11cb7024e026616d8b4","title":"Multi-task learning for few-shot biomedical relation extraction"},{"paperId":"3f1ee0a36b970a0f8f118ccb3f6fd4ee4b5de949","title":"A Survey on Biomedical Text Summarization with Pre-trained Language Model"},{"paperId":"258605dc5b00fe66b72091f947642a554e472aee","title":"Exploring the Trade-Offs: Unified Large Language Models vs Local Fine-Tuned Models for Highly-Specific Radiology NLI Task"},{"paperId":"a7f8fd45fbcdd81449cb7a1a6a2b2c18b38f8151","title":"ImpressionGPT: An Iterative Optimizing Framework for Radiology Report Summarization with ChatGPT"},{"paperId":"7e97c05a1374082c49b69c8a19461490d6452efa","title":"EasyNER: A Customizable Easy-to-Use Pipeline for Deep Learning- and Dictionary-based Named Entity Recognition from Medical Text"},{"paperId":"9ec07ad77267af6c304bdf0c2b9c914d296b468b","title":"EDAD: Efficient Domain Adaptive Distillation by Integrating Domain Adaptation and Knowledge Distillation"},{"paperId":"c7705944c22a3d95413bc1a1950521662a25f1b3","title":"Bridging the Gap between Medical Tabular Data and NLP Predictive Models: A Fuzzy-Logic-Based Textualization Approach"},{"paperId":"965e0d4bfe8097baab1947fc23263ae790620e23","title":"AGI for Agriculture"},{"paperId":"90616bb932b345c83b5b70dffc76a75b14805315","title":"From benchmark to bedside: transfer learning from social media to patient-provider text messages for suicide risk prediction"},{"paperId":"30fec23437cf9aaf3e9cb7d0c076483c14893abf","title":"An NLP approach to identify SDoH-related circumstance and suicide crisis from death investigation narratives."},{"paperId":"b8c9bcec47ac62105e32549e77aac979df8ad481","title":"DisGeReExT: a knowledge discovery system for exploration of disease–gene associations through large-scale literature-wide analysis study"},{"paperId":"989c337316e25f8e5dadf3847f8bac36d4ed0e3c","title":"Drug–drug interaction extraction‐based system: An\n natural language processing\n approach"},{"paperId":"89af504f2a9e0aeab794012ead793c61c76e86ba","title":"Are Large Language Models Ready for Healthcare? A Comparative Study on Clinical Language Understanding"},{"paperId":"4ce68478791bd4cfcdf883d75fa31fc1ebc6c7cc","title":"FrenchMedMCQA: A French Multiple-Choice Question Answering Dataset for Medical domain"},{"paperId":"994b335b0e42955df63c3e963de47655f5aa8b8d","title":"How Does Imperfect Automatic Indexing Affect Semantic Search Performance?"},{"paperId":"744cbd5c2e6edff0584104d196ef45128ac12800","title":"Transfer Learning Approach to Multilabel Biomedical Literature Classification using Transformer Models"},{"paperId":"0940155bec999067aba536d80e37f720ce91c4d0","title":"Automatic ICD-10 Code Association: A Challenging Task on French Clinical Texts"},{"paperId":"1774405ae834c0e3c1f7af2b1e8f963fc23bd4a1","title":"Machine learning for synergistic network pharmacology: a comprehensive overview"},{"paperId":"020e473d8c987dcfb03fcfffeb87b17812447031","title":"Evaluation of ChatGPT Family of Models for Biomedical Reasoning and Classification"},{"paperId":"8f4e198467de15fdbb305d0982ff6f15565ab601","title":"To Asymmetry and Beyond: Structured Pruning of Sequence to Sequence Models for Improved Inference Efficiency"},{"paperId":"31f333bcbc99f60ebb7c219c8c65cbe2aff278ff","title":"G2PTL: A Pre-trained Model for Delivery Address and its Applications in Logistics System"},{"paperId":"677529c13cfebc273e13ae036c09658033b0afee","title":"DrBERT: A Robust Pre-trained Model in French for Biomedical and Clinical domains"},{"paperId":"bce55193d9a887ad00774a9134df08cd521a85ae","title":"DoctorGLM: Fine-tuning your Chinese Doctor is not a Herculean Task"},{"paperId":"538680e08812fadc22ac4a7eefa6b40ae9179b28","title":"Identifying Mentions of Pain in Mental Health Records Text: A Natural Language Processing Approach"},{"paperId":"a41552d69ca566bfc5c7f1b7e0e97fe61cadb7af","title":"Robust Identification of Figurative Language in Personal Health Mentions on Twitter"},{"paperId":"e68d738c6c769fc28d790bfad57b9ce804961f18","title":"Modeling semantic business trajectories of territories for multidisciplinary studies through controlled vocabularies"},{"paperId":"304b640c1b2559ac1442ac9be54853ac80ec248c","title":"Multimodal data fusion for cancer biomarker discovery with deep learning"},{"paperId":"2da5b68b89d1d53373d122ac8e6fb2d23668c22f","title":"Toward structuring real-world data: Deep learning for extracting oncology information from clinical text with patient-level supervision"},{"paperId":"80652b8e0c03ebfabb6255f097a9704dcb2b79ff","title":"Disto-TRP: An approach for identifying transient receptor potential (TRP) channels using structural information generated by AlphaFold."},{"paperId":"07bfaf2d713040efe69d0c61bcdfd9870bfbaf5a","title":"A novel self-attention enriching mechanism for biomedical question answering"},{"paperId":"4283021777631cbdc0e9a84218d37d2fe0e9f828","title":"Vision-knowledge fusion model for multi-domain medical report generation"},{"paperId":"1cdce64d3832e465adb1151044153c687f8c819d","title":"Analysis of ‘One in a Million’ primary care consultation conversations using natural language processing"},{"paperId":"b56c53936a436ebb7d9e79a0e0da1760184cd3c5","title":"Artificial Intelligence in Pharmaceutical Sciences"},{"paperId":"3b99f8c18dfe1ba61144cfacaf8c22357455024c","title":"Review: A Roadmap to Use Nonstructured Data to Discover Multitarget Cancer Therapies."},{"paperId":"06c6eca6bf50d10f7e8a9d9a29f9457526a0d7a5","title":"Exploring the Potential of Large Language models in Traditional Korean Medicine: A Foundation Model Approach to Culturally-Adapted Healthcare"},{"paperId":"4be694b101230da39a035c0cf4ebb90aa569879c","title":"Quick Dense Retrievers Consume KALE: Post Training Kullback Leibler Alignment of Embeddings for Asymmetrical dual encoders"},{"paperId":"83edcfbb206ddad38a971d605da09390604248ea","title":"BloombergGPT: A Large Language Model for Finance"},{"paperId":"1ad9a295ea841599383e5ae3e88381438d4a7db3","title":"Evaluation of GPT and BERT-based models on identifying protein-protein interactions in biomedical text"},{"paperId":"1f78eb8e03774209cff65dba4bfd3b95aef77ded","title":"oBERTa: Improving Sparse Transfer Learning via improved initialization, distillation, and pruning regimes"},{"paperId":"06cf8da98925503cd2f4186dc48d8ef454d5a6f6","title":"Methods of extracting biomedical information from patents and scientific publications (on the example of chemical compounds)"},{"paperId":"eaadcb852955ac2b664f7b7cd111a73661aa90df","title":"Biomedical named entity recognition based on fusion multi-features embedding"},{"paperId":"b298615c523282d96339c7b9ef7d840ddad61412","title":"Zero-shot Clinical Entity Recognition using ChatGPT"},{"paperId":"0d7a0b53f65929776b851933d869fa753798bca3","title":"Carolina: a General Corpus of Contemporary Brazilian Portuguese with Provenance, Typology and Versioning Information"},{"paperId":"df6ce09e914f278a95c5c2b6dfaecfaf6e81c8c7","title":"Identifying Reasons for Statin Nonuse in Patients With Diabetes Using Deep Learning of Electronic Health Records"},{"paperId":"0f8f20ef4bc90a2b210bc5be08a7f326a214556f","title":"An Information Extraction Study: Take In Mind the Tokenization!"},{"paperId":"02634d754f4898ffd68623f8ff6f7861e700ef88","title":"Accurate and Reliable Classification of Unstructured Reports on Their Diagnostic Goal Using BERT Models"},{"paperId":"620facb14edcd4897c00e335569932392894778f","title":"A Disease-Prediction Protocol Integrating Triage Priority and BERT-Based Transfer Learning for Intelligent Triage"},{"paperId":"ea2504a6ca0a520af5ea0c96d00fb28cccc5d410","title":"A Biomedical Entity Extraction Pipeline for Oncology Health Records in Portuguese"},{"paperId":"42c2507cf28070785b92342804aed1eba4380400","title":"Bias Amplification in Intersectional Subpopulations for Clinical Phenotyping by Large Language Models"},{"paperId":"3494e10a099ffef4e87f0a84d64af8f1a527b80c","title":"ChatGPT in glioma patient adjuvant therapy decision making: ready to assume the role of a doctor in the tumour board?"},{"paperId":"812b6f4dd78edb52959a660c1ac3cdcf5f8e13c6","title":"Bat4RCT: A suite of benchmark data and baseline methods for text classification of randomized controlled trials"},{"paperId":"2d0b9af28c80cfa5c87c08a249af7393c6b4695f","title":"Enabling Early Health Care Intervention by Detecting Depression in Users of Web-Based Forums using Language Models: Longitudinal Analysis and Evaluation"},{"paperId":"9305fd6d87007c7b90d2e0db579ff40467352969","title":"Natural language processing to automatically extract the presence and severity of esophagitis in notes of patients undergoing radiotherapy"},{"paperId":"ec13360ba5820b228333bc21d12f4871250546e8","title":"Lay Text Summarisation Using Natural Language Processing: A Narrative Literature Review"},{"paperId":"44caf26949b4799e21f7b0754fe61315e8b71542","title":"Compositional Zero-Shot Domain Transfer with Text-to-Text Models"},{"paperId":"3f11e89e7f19e931adf31b91f15302d7539c809d","title":"A Joint Domain-Specific Pre-Training Method Based on Data Enhancement"},{"paperId":"92bf19978212365fd372d99e946ae86f36fb20b0","title":"Deep Learning Based Structural Reliability Finite Element Analysis Surrogate for Hydro-Generator Lower Frame"},{"paperId":"6fdcc152422c64de86c859b53669c0548261ec09","title":"GrapeQA: GRaph Augmentation and Pruning to Enhance Question-Answering"},{"paperId":"e9e52a94f1cd46da9ae47e40ba2981ba87cb92ab","title":"Analyzing the Generalizability of Deep Contextualized Language Representations For Text Classification"},{"paperId":"443d898928eb8e32d2e6f8f287beaa63f5b00eb9","title":"JaCoText: A Pretrained Model for Java Code-Text Generation"},{"paperId":"994e08ac813028601907516aee9c4699234a6b4d","title":"Large AI Models in Health Informatics: Applications, Challenges, and the Future"},{"paperId":"3611674ccd820efc0b59981038bc4161d46b3add","title":"A Systematic Literature Review of the Use of Computational Text Analysis Methods in Intimate Partner Violence Research"},{"paperId":"cff26bda86237d113ed01c812ad8bedd0afbe070","title":"DeID-GPT: Zero-shot Medical Text De-Identification by GPT-4"},{"paperId":"79545d9d30b924df293ee103e46f78aaf3249e51","title":"Leveraging Foundation Models for Clinical Text Analysis"},{"paperId":"9f105fdbe301eb23371f35f697164a19e6c45ed5","title":"OpticalBERT and OpticalTable-SQA: Text- and Table-Based Language Models for the Optical-Materials Domain"},{"paperId":"01caa1fbe78ee2234ade9228c772cbb6d5e47458","title":"Exploring Partial Knowledge Base Inference in Biomedical Entity Linking"},{"paperId":"689e0dfcec660611c1f84490b3055b020b7bd0e1","title":"Public Awareness and Sentiment Analysis of COVID-Related Discussions Using BERT-Based Infoveillance"},{"paperId":"701e61977155143529e44264ccdb8443f07c4660","title":"IK-DDI: a novel framework based on instance position embedding and key external text for DDI extraction"},{"paperId":"0a438980ac42451d6d32dd2ad8ead7b55520408d","title":"A Systematic Review of Transformer-Based Pre-Trained Language Models through Self-Supervised Learning"},{"paperId":"eea77daf238730dd7e3686b33cf31bb771f058ff","title":"B-LBConA: a medical entity disambiguation model based on Bio-LinkBERT and context-aware mechanism"},{"paperId":"0429e9343424ded011eaa46547780c5c17f66fec","title":"A cross-modal deep metric learning model for disease diagnosis based on chest x-ray images"},{"paperId":"171598987de38aeac08ffa338df9e0bbd78d58ca","title":"Applying unsupervised keyphrase methods on concepts extracted from discharge sheets"},{"paperId":"c7ccb92677afc2738868cf95d38af79a1adf7e9d","title":"Potential of natural language processing for metadata extraction from environmental scientific publications"},{"paperId":"7d66ec6870d6773d559df20642bb2f30b106edc0","title":"Input-length-shortening and text generation via attention values"},{"paperId":"8641c70c106c4f7485e613888b91a58e9812a5a7","title":"MEDBERT.de: A Comprehensive German BERT Model for the Medical Domain"},{"paperId":"1fcd70f19c05c37f75bbf856cbb3b8bbef73373a","title":"Contextualized Medication Information Extraction Using Transformer-based Deep Learning Architectures"},{"paperId":"6e96773bac534c87cf0eeaf11c5ba2a596b3380e","title":"Attention is not all you need: the complicated case of ethically using large language models in healthcare and medicine"},{"paperId":"bc1e3ae8cf66322a95e0475cdf79c867e1c9d026","title":"Self-supervised based general laboratory progress pretrained model for cardiovascular event detection"},{"paperId":"9556d7ad415cadb2c2d9b34e5fa5d9c2192a26b7","title":"Generating multiple-choice questions for medical question answering with distractors and cue-masking"},{"paperId":"17ca48ad1b944c897863f04ba9ffa72674dce1ce","title":"Parallel multi-head attention and term-weighted question embedding for medical visual question answering"},{"paperId":"656e8c5f8bced540425c12d854b2911dddefff14","title":"Multimodal Data Integration for Oncology in the Era of Deep Neural Networks: A Review"},{"paperId":"3440687e1fc734baeab1abee4a86c81347d1422a","title":"aeroBERT-Classifier: Classification of Aerospace Requirements Using BERT"},{"paperId":"fa9482793ab5f9ad6f7429476db6e11f502e2440","title":"Math Function Recognition with Fine-Tuning Pre-Trained Models"},{"paperId":"e5174aeda1baa67c17f4ac630ae2e44453954cc3","title":"Personalisation within bounds: A risk taxonomy and policy framework for the alignment of large language models with personalised feedback"},{"paperId":"758e94d65a10783c2a64e478a2103224d295ab13","title":"Enhanced disease-disease association with information enriched disease representation."},{"paperId":"bdf7bf9e81a6c12e22323d0402885b2ba62f623e","title":"Does Synthetic Data Generation of LLMs Help Clinical Text Mining?"},{"paperId":"1475905e3687c21428ef3cad902d465093072fd1","title":"Deep multi-modal intermediate fusion of clinical record and time series data in mortality prediction"},{"paperId":"38311d8f52e4fef86a121dd91923a1df798f79fd","title":"A Study of Text Summarization in the Medical Domain using BERT and its Variants"},{"paperId":"0606bb9a541ce7e57bd78ac680a7df0225ece30c","title":"Can large language models build causal graphs?"},{"paperId":"914f807de0eaf055aded977419d5d22bb6078d90","title":"Document-level Relation Extraction with Cross-sentence Reasoning Graph"},{"paperId":"b388002a68143f94a6efb12ea75a2d18af64da0d","title":"The named entity recognition of vessel power equipment fault using the multi-details embedding model"},{"paperId":"30809168fff23c852867ad359baaebfae532f0a7","title":"Enhancing Activity Prediction Models in Drug Discovery with the Ability to Understand Human Language"},{"paperId":"bd97d3171b5a9bbe9a0aae6c7461cb8f9123a5eb","title":"A Supervised Text Classification System Detects Fontan Patients in Electronic Records with Higher Accuracy than ICD Codes"},{"paperId":"7f193c6302b2eed4779354f6763fa1217b123d05","title":"Rad-Former: Structuring Radiology Reports using Transformers*"},{"paperId":"d8da72e7857cc1a0d3505e6c8a746eac815901b2","title":"Large-Scale Domain-Specific Pretraining for Biomedical Vision-Language Processing"},{"paperId":"883a5338dee175ae61f323202a5aba80b2458e0f","title":"Deep Learning Based Code Generation Methods: A Literature Review"},{"paperId":"31f32b26e6b997f9d5691a8edc6ea86e434865d8","title":"Constructing and analyzing domain-specific language model for financial text mining"},{"paperId":"1419dea18f214029959097c265024e9e9bc598f3","title":"WeakMeSH: Leveraging provenance information for weakly supervised classification of biomedical articles with emerging MeSH descriptors"},{"paperId":"4d4a96708fc67403176bb2b891b564af7a20c148","title":"RAMM: Retrieval-augmented Biomedical Visual Question Answering with Multi-modal Pre-training"},{"paperId":"a68ba8496d7c7125ac470057f7b2e8fb13845e3a","title":"Domain-adapted large language models for classifying nuclear medicine reports"},{"paperId":"57c14d250f231bf56b3c68441aaa36d389281b0d","title":"Assessment of Natural Language Processing of Electronic Health Records to Measure Goals-of-Care Discussions as a Clinical Trial Outcome"},{"paperId":"6c48f1a429e8371ebdac412602629cb6472db1a8","title":"Domain Word Extension Using Curriculum Learning"},{"paperId":"b169f2ce55e14584d2db6f64eeb5ad2702d39d40","title":"Artificial intelligence foundation and pre-trained models: Fundamentals, applications, opportunities, and social impacts"},{"paperId":"9991fe75b5c3d47699d7aa3b44f2a26a9b29eecd","title":"Extraction and analysis of risk factors from Chinese Chemical Accident Reports"},{"paperId":"10916b9df058a076238f6520435d0961419a4308","title":"Knowledge graph assisted end-to-end medical dialog generation"},{"paperId":"eb78b34409a323fb36af93b5c252ee99a9c036b5","title":"Planarized sentence representation for nested named entity recognition"},{"paperId":"430aa6966c15c4a20a4fb2d8383e136b9cb6cde7","title":"Almanac: Retrieval-Augmented Language Models for Clinical Medicine"},{"paperId":"efa9fc7d5b6b244d8ae3a9c3db98418ec39aa7a3","title":"Precision information extraction for rare disease epidemiology at scale"},{"paperId":"34b897c4fb8d2ce64199aec8b29b57316d52e17d","title":"German Medical Named Entity Recognition Model and Data Set Creation Using Machine Translation and Word Alignment: Algorithm Development and Validation"},{"paperId":"d8035d652a26bc96ceb9d0ba89460d11d4850e76","title":"Knowledge grounded medical dialogue generation using augmented graphs"},{"paperId":"8abee896e893dcf230c9e02de2bb435e33ecba76","title":"Survey on the Biomedical Text Summarization Techniques with an Emphasis on Databases, Techniques, Semantic Approaches, Classification Techniques, and Similarity Measures"},{"paperId":"236445f0a3b1e30b2542e5e64616ff6a8af7e3ea","title":"Language Models are Few-shot Learners for Prognostic Prediction"},{"paperId":"7480d1e33e14ff113413de8dc09b7664dad1c0da","title":"Improving Clinical Decision Making with a Two-Stage Recommender System Based on Language Models: A Case Study on MIMIC-III Dataset"},{"paperId":"fefe6c2eb25da9f9f7982b8718f3abc1de2ada03","title":"Modelling Temporal Document Sequences for Clinical ICD Coding"},{"paperId":"114655441607fbf58f5b174f2905a006b3853d91","title":"FiTs: Fine-grained Two-stage Training for Knowledge-aware Question Answering"},{"paperId":"ef959f7212091d0e9aa7502d15ef7d87dd70b902","title":"Identification of Thermophilic Proteins Based on Sequence-Based Bidirectional Representations from Transformer-Embedding Features"},{"paperId":"8ea7603260a648b7df42e749f28e9866aa48612d","title":"Construction of Knowledge Graphs: State and Challenges"},{"paperId":"5c66d3d97746b7438d5b374e0322e79ab9e5ac5e","title":"On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective"},{"paperId":"b48adefb1d6a2da907f602c1d572704f4599792e","title":"S1000: a better taxonomic name corpus for biomedical information extraction"},{"paperId":"460dea7c62ca0fdc27f671f50b76f477942dea12","title":"Improving text mining in plant health domain with GAN and/or pre-trained language model"},{"paperId":"534be0d0603866e71f873d9940e1281a5a3045fb","title":"Boosting classification reliability of NLP transformer models in the long run"},{"paperId":"692f49d243a23c220b40cbd2b6b26b773d9b31c4","title":"Hashtag-Guided Low-Resource Tweet Classification"},{"paperId":"b58c2110655e950c36bc533fa81f143397a5fe2e","title":"Exploring the Limits of Transfer Learning with Unified Model in the Cybersecurity Domain"},{"paperId":"f221280799e5cc9f4e43f2a26754ff802e22a3f0","title":"Question Answering Chatbots for Biomedical Research Using Transformers"},{"paperId":"6b6ea46e57d026c546e45cbe25ea4be55523a6b6","title":"On the Use of Knowledge Transfer Techniques for Biomedical Named Entity Recognition"},{"paperId":"48de1a31cca6631bd73a5d0854acfda5e5195d66","title":"BORD: A Biomedical Ontology based method for concept Recognition using Distant supervision: Application to Phenotypes and Diseases"},{"paperId":"d0c5f901868f6e2cb126fd51b155f631372a9669","title":"Biomedical Text Classification Using Augmented Word Representation Based on Distributional and Relational Contexts"},{"paperId":"99ecb1ffe691f5414d737c4cb8e824f513c0bb31","title":"Uni-Fold MuSSe: De Novo Protein Complex Prediction with Protein Language Models"},{"paperId":"ef91c31d8aab9fe95fec29149e2fe4568ab2fb32","title":"SwitchPrompt: Learning Domain-Specific Gated Soft Prompts for Classification in Low-Resource Domains"},{"paperId":"05dcef911ec4ad6ddc3e3cd866979cd27733147e","title":"Reveal the Unknown: Out-of-Knowledge-Base Mention Discovery with Entity Linking"},{"paperId":"629bc57782bb4326a3eb5f89314e350729c5f417","title":"AdapterSoup: Weight Averaging to Improve Generalization of Pretrained Language Models"},{"paperId":"5ef821267fa68d3231ed8135ff8ec09f25bb1398","title":"ChatCAD: Interactive Computer-Aided Diagnosis on Medical Image using Large Language Models"},{"paperId":"1929c070964bac55a1d57d13bcaae44b28eb97fb","title":"BLIAM: Literature-based Data Synthesis for Synergistic Drug Combination Prediction"},{"paperId":"83c3bef2a3d24c31bccb8cf638dcdea630567089","title":"Expediting Distributed DNN Training With Device Topology-Aware Graph Deployment"},{"paperId":"ce6f2d68b1a4029ff4a838fcf12d5ad1d47f0e68","title":"Multilingual translation for zero-shot biomedical classification using BioTranslator"},{"paperId":"dd4238d23fea6fc8e9232ca0fec4cad728f213ea","title":"Span-based Named Entity Recognition by Generating and Compressing Information"},{"paperId":"597c60202acd32e4ad7e3fc9f1db993fa25290d6","title":"TaughtNet: Learning Multi-Task Biomedical Named Entity Recognition From Single-Task Teachers"},{"paperId":"d7047f54c65fee3c61288b3c490a862e95ae5092","title":"Lightweight Transformers for Clinical Natural Language Processing"},{"paperId":"1e2839669f61fd99c524690e238f6cbe505e5ffd","title":"Real-Time Visual Feedback to Guide Benchmark Creation: A Human-and-Metric-in-the-Loop Workflow"},{"paperId":"292c3ca299362db1435ae8ea6a35929b430bdb17","title":"Zero-Shot Learning for Requirements Classification: An Exploratory Study"},{"paperId":"4562c122c523f7ea2b7c36ee524a47f59d7e74b2","title":"A Biomedical Knowledge Graph for Biomarker Discovery in Cancer"},{"paperId":"9d379e0e77a57bf0c9e33576c465afcedd13ec89","title":"A prefix and attention map discrimination fusion guided attention for biomedical named entity recognition"},{"paperId":"b3cbce144f18ba1bfbaacb17d6284ba8bb4dbb28","title":"A Review on Clinical Named Entity Recognition"},{"paperId":"fed2fab877ba1af72470d3dc061747d0ea9879d0","title":"Deep learning approach to detection of colonoscopic information from unstructured reports"},{"paperId":"2ad818c34b63aa2260542ac619b7098fb7745bac","title":"Machine learning and deep learning in medicine and neuroimaging"},{"paperId":"10c5079d2baa5a287054d9ddd7806a4c16fd7531","title":"UDAPTER - Efficient Domain Adaptation Using Adapters"},{"paperId":"e7dcdfb7734d59b97f825cce8b3105a2d9b14d10","title":"The Effect of Metadata on Scientific Literature Tagging: A Cross-Field Cross-Model Study"},{"paperId":"e3ec55e9e6720194a0ed5d4033d93a941c8a4f99","title":"Continual Pre-training of Language Models"},{"paperId":"2c6a6eb161c04d1f4149b38321b23878d24f2da3","title":"A survey on Transformers for long sentences and its application to medical notes"},{"paperId":"627b6f7687e122b5578f095221f66583850f0ea5","title":"GLADIS: A General and Large Acronym Disambiguation Benchmark"},{"paperId":"3b0424149731d10829015cb4ab6299d18162128e","title":"LIQUID: A Framework for List Question Answering Dataset Generation"},{"paperId":"4cfec8ec51a0ecd7efbc6e6622ae8f930935f714","title":"Bioformer: an efficient transformer language model for biomedical text mining"},{"paperId":"279cc657655eeb4e96a2eaf3d77f708edbf6a313","title":"Construction and evaluation of a domain-specific knowledge graph for knowledge discovery"},{"paperId":"cb1a64200edaa038326a177538b6b0d5ba21558a","title":"Informing clinical assessment by contextualizing post-hoc explanations of risk prediction models in type-2 diabetes"},{"paperId":"c41146620c907535b5f21c1f50f981f4c724f3d5","title":"Improving Protein Function Prediction by Adaptively Fusing Information From Protein Sequences and Biomedical Literature"},{"paperId":"02e6bd66517850b1ff446fc34acb647ac72f3ba9","title":"A role distinguishing Bert model for medical dialogue system in sustainable smart city"},{"paperId":"1755d306187b5b87110ae5e0006dc762942554a4","title":"Deep representation learning of scientific paper reveals its potential scholarly impact"},{"paperId":"47a1263ba21a72790334544f2a11b7c0ee4b5e76","title":"Multimodality Representation Learning: A Survey on Evolution, Pretraining and Its Applications"},{"paperId":"0acd41876736b1563d35588ab76cb4c2266052e9","title":"LFT-Net: Local Feature Transformer Network for Point Clouds Analysis"},{"paperId":"5e3ff15eba3770f479c1e44bc2fea54c4cab1384","title":"Transformer-based language models for mental health issues: A survey"},{"paperId":"3474e4bb497e5b5fc31a4c9757f55cd578678549","title":"Deep learning based classification of multi-label chest X-ray images via dual-weighted metric loss"},{"paperId":"f0b40e3bc7a4ed554e82905e6fb65cd8d3489f44","title":"ADPG: Biomedical entity recognition based on Automatic Dependency Parsing Graph"},{"paperId":"31605129071ea6fca8e22e622de164e1a2ae1549","title":"Neurofuzzy semantic similarity measurement"},{"paperId":"9bedc9b944b2480affea4832e38106ce5eccead5","title":"Classifying literature mentions of biological pathogens as experimentally studied using natural language processing"},{"paperId":"de7fa58930e86217498e6c3ac656366cfb39930a","title":"Supporting SNOMED CT postcoordination with knowledge graph embeddings"},{"paperId":"55a48a021b5927cc569b0fc461ba2e020f5356d6","title":"Large Language Models for Biomedical Causal Graph Construction"},{"paperId":"21b36c8908c6293331c3e80e675ccafd0692c589","title":"ProtST: Multi-Modality Learning of Protein Sequences and Biomedical Texts"},{"paperId":"71dc990592911c454714e6fbe680dadf0cae1e45","title":"Context Matters: A Strategy to Pre-train Language Model for Science Education"},{"paperId":"ca120bcaa4bc5e43f445bcfcc5e577d2112c5626","title":"TA-WHI: Text Analysis of Web-Based Health Information"},{"paperId":"1cfe781523b4b3469d822bc52a0721b1c70cee5f","title":"A rule-free workflow for the automated generation of databases from scientific literature"},{"paperId":"cde6b633703eda8c3fbd7f76a967e07b19b7623d","title":"A Multi-View Joint Learning Framework for Embedding Clinical Codes and Text Using Graph Neural Networks"},{"paperId":"5ab6fb406369148bd604fa5e0ecb841d45899225","title":"Entity and relation extraction from clinical case reports of COVID-19: a natural language processing approach"},{"paperId":"76bed13bac28090bd0498757cc0c02a0ddaaa28c","title":"Task formulation for Extracting Social Determinants of Health from Clinical Narratives"},{"paperId":"51c7ba41ab310f05f34d4e0b2bc777e554dd40e0","title":"A Survey on BERT and Its Applications"},{"paperId":"811bd3079580e93c5297c882d2c59ff39df308ac","title":"Knowledge-augmented Graph Neural Networks with Concept-aware Attention for Adverse Drug Event Detection"},{"paperId":"ad8991af165bb1ecd83251867aba392adfaa0033","title":"Cross-lingual Argument Mining in the Medical Domain"},{"paperId":"eb3f0b9a7ca8882d17328363f2eb182efda9a529","title":"Cross-lingual German Biomedical Information Extraction: from Zero-shot to Human-in-the-Loop"},{"paperId":"223a498fd673c0fc83b4db6883052f57dbbc4278","title":"Semi-Automated Construction of Food Composition Knowledge Base"},{"paperId":"45f293dd4a4e13747937153817e043a346ffe617","title":"Large-scale fine-grained semantic indexing of biomedical literature based on weakly-supervised deep learning"},{"paperId":"1226d0618689ac3a262721c4ca5cd4d964a4919f","title":"CARES: A Corpus for classification of Spanish Radiological reports"},{"paperId":"95907abcf78601b9336f527b21915e6e9f6b4f82","title":"SPEC5G: A Dataset for 5G Cellular Network Protocol Analysis"},{"paperId":"e3f839b01567ae73af822a3da5e160dac2fb4708","title":"Adapting a Language Model While Preserving its General Knowledge"},{"paperId":"336c242e64a056026090e276ba855f83c839164d","title":"JCSE: Contrastive Learning of Japanese Sentence Embeddings and Its Applications"},{"paperId":"547f09d090cb0a36eeb2791796bc887c28ad9bba","title":"Detection and Cross-domain Evaluation of Cyberbullying in Facebook Activity Contents for Turkish"},{"paperId":"393791e54a6fb3fbcd72775e9cdb7c3e221b1907","title":"The 2022 n2c2/UW Shared Task on Extracting Social Determinants of Health"},{"paperId":"924036c17537adc8574a1476bf13d9d17a6eebae","title":"ClimaBench: A Benchmark Dataset For Climate Change Text Understanding in English"},{"paperId":"7389b6ebbf36f4d869a02e305e2ef52ad2c92264","title":"Applications of transformer-based language models in bioinformatics: a survey"},{"paperId":"728a34d0cbd7fef95ce8c6016e4db803d565b860","title":"Neighborhood-Regularized Self-Training for Learning with Few Labels"},{"paperId":"27858514c71449e1368bc2ab235bd053db3dfc8a","title":"FullStop: Punctuation and Segmentation Prediction for Dutch with Transformers"},{"paperId":"4a07c92996411dcdc94d42a3b1bef7b5093f3158","title":"COVID QA Network: A Specific Case of Biomedical Question Answering"},{"paperId":"77f4b467f0c70520a192ec74b98fcf15bd27c9b7","title":"A BERT-Based Artificial Intelligence to Analyze Free-Text Clinical Notes for Binary Classification in Papillary Thyroid Carcinoma Recurrence"},{"paperId":"fae35a63ec53f8e862fca83225608be0308a33d9","title":"Extraction of clinical phenotypes for Alzheimer’s disease dementia from clinical notes using natural language processing"},{"paperId":"b62b2b24378165df6a3dc904dde2b600745310f2","title":"CiT: Curation in Training for Effective Vision-Language Data"},{"paperId":"fed4bbb0e81bbc8c4ea65d55656a1145dfc1abad","title":"A review of research on eligibility criteria for clinical trials"},{"paperId":"44d53aa6df04dc99c441972cfd14b11ba12b1e2c","title":"End-to-End Transformer-Based Models in Textual-Based NLP"},{"paperId":"cef8c1f509bdc88165a6696495754919b84ec22d","title":"Knowledge Adaptive Multi-Way Matching Network for Biomedical Named Entity Recognition via Machine Reading Comprehension"},{"paperId":"cf0b1e762d8d88086799849a15c5c06ba76e95d9","title":"A hybrid algorithm for clinical decision support in precision medicine based on machine learning"},{"paperId":"628f29add2f76eab14b9b7368e01b9a70d2b056a","title":"Adaptive Fine-tuning for Multiclass Classification over Software Requirement Data"},{"paperId":"7334e12961f8a15e2276217bbdb19709c427b337","title":"Extraction of knowledge graph of Covid-19 through mining of unstructured biomedical corpora"},{"paperId":"81d133d94211a42a1bf58e38cbabf7fd65d33e23","title":"Automatic Extraction of Medication Mentions from Tweets—Overview of the BioCreative VII Shared Task 3 Competition"},{"paperId":"d53233323d143ed357a7b4dd5e7323ee0186706f","title":"Stacking-BERT model for Chinese medical procedure entity normalization."},{"paperId":"77fae38e1790885ad8a4f913f1d7848e9d0efba2","title":"Cluster-based text mining for extracting drug candidates for the prevention of COVID-19 from the biomedical literature"},{"paperId":"4b3cfa21f1baa1c4ebfab5162f477d8b031709d7","title":"Negation-based transfer learning for improving biomedical Named Entity Recognition and Relation Extraction"},{"paperId":"7e4bca67428c3b9487df550c65b36146ed278894","title":"A Machine Learning Approach for the NLP-Based Analysis of Cyber Threats and Vulnerabilities of the Healthcare Ecosystem"},{"paperId":"6d90f477a970effb70e2d1863c968da400a95576","title":"Identifying and Analyzing Topic Clusters in a Nutri-, Food-, and Diet-Proteomic Corpus Using Machine Reading"},{"paperId":"ffa5d9a21a410b8a9416263f06d507be101d9628","title":"Discovering research articles containing evolutionary timetrees by machine learning"},{"paperId":"8d5fd95e1042ec0dad1a953345196c9e61496901","title":"A Controlled Attention for Nested Named Entity Recognition"},{"paperId":"7aa11ff454d33ba4e79ffdd75c8a807061ed4715","title":"How data science and AI-based technologies impact genomics"},{"paperId":"ad42d24f6f9cdbd2cc10da665c19e6b7dc033c80","title":"Chemical identification and indexing in full-text articles: an overview of the NLM-Chem track at BioCreative VII"},{"paperId":"f5c02e59b431227029f0c821d8f0fa72c75da700","title":"Using Semantic Text Similarity calculation for question matching in a rheumatoid arthritis question-answering system"},{"paperId":"b26182a81185552efaaec9e79579b333901410bd","title":"Navigating Alignment for Non-identical Client Class Sets: A Label Name-Anchored Federated Learning Framework"},{"paperId":"268fe1ee7dbe781533d618efa5f48291b50e34ee","title":"#ChronicPain: Automated Building of a Chronic Pain Cohort from Twitter Using Machine Learning"},{"paperId":"b97ed0d8f7e68cddb87a3016c6c454bc60487459","title":"Identifying Tweets with Personal Medication Intake Mentions using Attentive Character and Localized Context Representations"},{"paperId":"7593a16e7b966ba327856af52ff48fb5de92694f","title":"Building Large-Scale Registries from Unstructured Clinical Notes using a Low-Resource Natural Language Processing Pipeline"},{"paperId":"2cce05dcae43fe3fb5a251f500a455cafe0be0e8","title":"Natural Language Interfaces to Data"},{"paperId":"482b544dc4b26d41d728d0a08adedb6b2e7deef1","title":"ABEE: automated bio entity extraction from biomedical text documents"},{"paperId":"9b9faaf2c33bdd1418bdc8255711c4cf66f31fb8","title":"Explainable AI for Bioinformatics: Methods, Tools, and Applications"},{"paperId":"cfc946053eabf9c24443171da1c4cdd216bbaca4","title":"A Marker-based Neural Network System for Extracting Social Determinants of Health"},{"paperId":"136141b6eb604168d049df52f69ac2235a94cf64","title":"A Comprehensive Study of Gender Bias in Chemical Named Entity Recognition Models"},{"paperId":"7c6398d0b602a6b11691ddaf579e8a083ea2912b","title":"Generalizable Natural Language Processing Framework for Migraine Reporting from Social Media"},{"paperId":"4050bb719f353b7bbea902d3819115fbeb768325","title":"LiSA: an assisted literature search pipeline for detecting serious adverse drug events with deep learning"},{"paperId":"165b7bacf08ca0d5702268a19e99abce5e525eb6","title":"Can NLI Provide Proper Indirect Supervision for Low-resource Biomedical Relation Extraction?"},{"paperId":"d38a1c1a15d7bd48a70e91afbed13161d6829409","title":"Edge Weight Updating Neural Network for Named Entity Normalization"},{"paperId":"cf847918dc4e269699adc46535c225d7f8e54bef","title":"Exploring the effects of drug, disease, and protein dependencies on biomedical named entity recognition: A comparative analysis"},{"paperId":"1d18f579554549f4784306917674a4bb8322c167","title":"PLUE: Language Understanding Evaluation Benchmark for Privacy Policies in English"},{"paperId":"5f91dcf19aaf7b461689ccff059d2bfffc7d7a02","title":"Localising In-Domain Adaptation of Transformer-Based Biomedical Language Models"},{"paperId":"284d8112888d4511f953add862447cb6b46eb126","title":"Pre-trained Language Models for Keyphrase Generation: A Thorough Empirical Study"},{"paperId":"e1ee25d3b09a5eb43d0145ff5ac58c7c8ae965ed","title":"Detecting Contradictory COVID-19 Drug Efficacy Claims from Biomedical Literature"},{"paperId":"f0684f48f7409b94a2391641d9ecb4d15cdfc5ff","title":"ShockModes: A Multimodal Model for Prognosticating Intensive Care Outcomes from Physician Notes and Vitals"},{"paperId":"f8295290551f82b325c3a3598d7eb7593508ada7","title":"JCBIE: a joint continual learning neural network for biomedical information extraction"},{"paperId":"c6ff92e68ac2f263610f202b531906bd95be7879","title":"A comprehensive review on knowledge graphs for complex diseases"},{"paperId":"51b6c7d3209536fcfd31cb5e5d4c38587ff046e3","title":"Automated Diagnosis Code Assignment of Thai Free-text Clinical Notes"},{"paperId":"c5bae5fc68785de447c0131ac79400e39f2aff7e","title":"Mining Latent Disease Factors from Medical Literature using Causality"},{"paperId":"054dd061a42086ab2758141655bebd7559b144fa","title":"SciFoodNER: Food Named Entity Recognition for Scientific Text"},{"paperId":"9a7bce3c8161c45c9e3302a3e2a898e9124e3fbe","title":"Emotion Recognition on StackOverflow Posts Using BERT"},{"paperId":"f8ec289d7f4e12440c5b535a6fdd7bb8f0e42280","title":"Radiopaths: Deep Multimodal Analysis on Chest Radiographs"},{"paperId":"d0b74c24e95193896aec376910188e5c7bde1799","title":"Extracting Protein-Protein Interactions (PPIs) from Biomedical Literature using Attention-based Relational Context Information"},{"paperId":"9bc765306867be1a25a00a124abb32f48ccad437","title":"Symptoms Based Disease Prediction from Bengali Text Using Transformer Network Based Pretrained Model"},{"paperId":"0e2209e8fbd7b824600dbf45960c3c66dec03116","title":"Named Entity Recognition on COVID-19 Scientific Papers"},{"paperId":"939629c488543676aeee9ecb953f246e14a13945","title":"LegalRelectra: Mixed-domain Language Modeling for Long-range Legal Text Comprehension"},{"paperId":"ce0a398b240b2a3ae114bbaef805226759d107bd","title":"CafeteriaSA corpus: scientific abstracts annotated across different food semantic resources"},{"paperId":"c9fd961913713132e63960b88be0bc5a3e15f99f","title":"GeoBERT: Pre-Training Geospatial Representation Learning on Point-of-Interest"},{"paperId":"64dcb27e5c31c8567725c7f67000feaa487c32e0","title":"The Effects of In-domain Corpus Size on pre-training BERT"},{"paperId":"967dbdfecb348a7bce2dd58cbdd7ae245f796618","title":"Neural Rankers for Effective Screening Prioritisation in Medical Systematic Review Literature Search"},{"paperId":"040ec58865ab50b5e6d91a355ffc146ec5034e9f","title":"Artificial Intelligence for Health Message Generation: Theory, Method, and an Empirical Study Using Prompt Engineering"},{"paperId":"ad1669c3d6670b26e2cd5c50f463971cf7f92835","title":"Domain Adaptation of Transformer-Based Models Using Unlabeled Data for Relevance and Polarity Classification of German Customer Feedback"},{"paperId":"d6ebf7e57d6762ae04820fa4bce852741369b160","title":"Automated ICD Coding using Extreme Multi-label Long Text Transformer-based Models"},{"paperId":"45dfaebd44bbd8c67c046bc8b8cf013c38a897ae","title":"Associations Between Natural Language Processing–Enriched Social Determinants of Health and Suicide Death Among US Veterans"},{"paperId":"24d1d8235843b016a2cf06278606286f636dd8eb","title":"Pivot-based Unsupervised Domain Adaptation for Pre-trained Language Model"},{"paperId":"d716c03c81e708b6eb6aa6fe151fe1ef2988b125","title":"A Unified Knowledge Graph Augmentation Service for Boosting Domain-specific NLP Tasks"},{"paperId":"9d06ef8ecab568b371727df9a47dff3b2e26fc61","title":"Improving Precancerous Case Characterization via Transformer-based Ensemble Learning"},{"paperId":"87778991ffea60f1d4e80d726335338270ecc2f6","title":"AUC Maximization for Low-Resource Named Entity Recognition"},{"paperId":"5ef125153b17dede6b7eb5456067bee57f09eb5e","title":"MED-SE: Medical Entity Definition-based Sentence Embedding"},{"paperId":"c211b4d8045d8ee7a1759856456131ca5f909479","title":"Predicting medical specialty from text based on a domain-specific pre-trained BERT"},{"paperId":"94dadfb4d2e321e84073fb5359a35375db65d35e","title":"A Transfer Learning based Model for Knowledge Graph in Power Grid"},{"paperId":"35499f7b1aa579b1c19a0363d53a5d06184e642c","title":"Joint model of biomedical entity recognition and normalization labels based on self-attention"},{"paperId":"fd1e917dabadd533dba35bf73f91c808c1571404","title":"Enhanced neurologic concept recognition using a named entity recognition model based on transformers"},{"paperId":"5451f57c50fcdf05cb1ded2e55a6e92916e30448","title":"Detection of ADR on Covid Vaccine Safety Data"},{"paperId":"7557105c9aa6a26db4f8e73fabb25e8134013fb5","title":"Pivotal Role of Language Modeling in Recommender Systems: Enriching Task-specific and Task-agnostic Representation Learning"},{"paperId":"86c79317afa13ea4a74f656e3c48f012ee1fc326","title":"G-MAP: General Memory-Augmented Pre-trained Language Model for Domain Tasks"},{"paperId":"53d127af5c25320e14cb71a8c506bce51c2a0c60","title":"Truthful Meta-Explanations for Local Interpretability of Machine Learning Models"},{"paperId":"9ec324658f0499463ec1d67cd7f8a7ecfbc3fa76","title":"Natural language processing: using artificial intelligence to understand human language in orthopedics"},{"paperId":"6d3e935ece8bf076e69e61b132a5c135fc809853","title":"Information extraction using deep learning: study of patient populations identification in texts describing drug approvals. (Preprint)"},{"paperId":"930ef9e024e52ee125344e273bddabb0f597e558","title":"Automated Identification of Eviction Status from Electronic Health Record Notes"},{"paperId":"61a09494fd097e1875e0a20ee96498d851304f47","title":"Optimization of Biomedical Language Model with Optuna and a Sentencepiece Tokenization for NER"},{"paperId":"4f85cf06753035f58248384d3244bc2a89b32220","title":"CySecBERT: A Domain-Adapted Language Model for the Cybersecurity Domain"},{"paperId":"f6e6daacc9ff7d79cd92be9dd14d8088e10cc774","title":"BioNER-CFEM: Biomedical Named Entity Recognition Based on Character Feature Enhancement with Multimodal Method"},{"paperId":"f209c75506e9c39935118ce6d7259b6994f12e67","title":"BioELM: Integrating Biomedical Knowledge into Language Model with Entity-Linking"},{"paperId":"afa9b832cd699553854417e471aa291871fdf17f","title":"GCL-GO: A novel sequence-based hierarchy-aware method for protein function prediction"},{"paperId":"414e5e8199233764b69ea77ff53471a4d0ee85a7","title":"Multi-View Brain Network Analysis with Cross-View Missing Network Generation"},{"paperId":"26619860c4a83ebfdb3748e29ffbd90745a6a851","title":"DeepCausality: A general AI-powered causal inference framework for free text: A case study of LiverTox"},{"paperId":"5c9515ee1c01b5b3e8403dd6483eecd5ea0c2827","title":"Intent Recognition in Conversational Recommender Systems"},{"paperId":"e95e56263e919a2183d4ca62e3868f47f31bcac4","title":"Syntactic Type-aware Graph Attention Network for Drug-drug Interactions and their Adverse Effects Extraction"},{"paperId":"9e9c2af21d8e9b78db089190888a3cc95e10f266","title":"Extraction of Gene Regulatory Relation Using BioBERT"},{"paperId":"da4ca8bd29d1adfe82e50771c0d7b69ec4446a00","title":"Drug-Drug Interaction Extraction Using Drug Knowledge Graph"},{"paperId":"59dbe7d94d5f205b314b85339089939e5c5d4239","title":"Semi-Supervised Protein-Protein Interactions Extraction Method Based on Knowledge Distillation and Virtual Adversarial Training"},{"paperId":"daaeccc4567ef569a80b08c0b71abb0682422fc2","title":"Joint Extraction of Biomedical Entities and Relations based on Decomposition and Recombination Strategy"},{"paperId":"4086fc2bc4b7966698e31c0d5ddc19f31b4702c4","title":"REACTCLASS: Cross-Modal Supervision for Subword-Guided Reactant Entity Classification"},{"paperId":"19914cc2b2b3efd3ee460ae3c35ed23e7f4482a3","title":"DEAL: Construction of a Disease-aware Human Cell Knowledge Graph from Biomedicine Literature"},{"paperId":"a2964dfd9a84ceaf42046f274469bf9e9f47d1d3","title":"Efficient Gene Community Search to Discover Similar Aspects for Similarity Explanation"},{"paperId":"1ce47d91cfd5c071fe438bd41133ab4e58a7d37a","title":"Attention model-based and multi-organism driven gene recognition from text: application to a microbial biofilm organism set."},{"paperId":"84652a23a9fd4b5020f3d67cf9cd0495709ea415","title":"Unified Fine-Grained Biomedical Entity Recognition as a Combination of Boundary Detection and Sequence Generation"},{"paperId":"9526b7689b396a5b2f2a9fdc1a588477a2632b74","title":"S3 AAL: Support Set Selection based on Adversarial Active Learning for Medical Few-Shot Relation Extraction"},{"paperId":"6bfb462d225a1b2cee0bf76128402f3345847959","title":"Multimodal feature learning framework for disease biomarker discovery"},{"paperId":"83ab85a756f37fbf7a667b5ade9cc9f435e31a93","title":"Semantic Reasoning with NLI for Assertion Detection in Medical Text"},{"paperId":"b246d524606b2bc6fb8e9093f45c9614f293156e","title":"Discovering Social Determinants of Health from Case Reports using Natural Language Processing: Algorithmic Development and Validation"},{"paperId":"0136f2faaa902082d0e6ad15e5fd14f5842d2b14","title":"Relation-aware Language-Graph Transformer for Question Answering"},{"paperId":"934f03956a5092345ea941df915d79358ddd2581","title":"CLeBPI: Contrastive Learning for Bug Priority Inference"},{"paperId":"de4bf50f234074fda2c504cc79af273b23297fcc","title":"A Lightweight CNN and Class Weight Balancing on Chest X-ray Images for COVID-19 Detection"},{"paperId":"a0059d3baf01c6ad402d1b94232313965a059135","title":"Machine understanding surgical actions from intervention procedure textbooks"},{"paperId":"3f2c0ce0020285f278e467157b5475fe4b9c3fda","title":"Two-phase self-supervised pretraining for object re-identification"},{"paperId":"2532e493ebb926b38fe628aa681f97b7339c42d9","title":"Extract antibody and antigen names from biomedical literature"},{"paperId":"7bb12aa7eebceef1ff4053d98e5bc2a17d46dd6e","title":"CliMedBERT: A Pre-trained Language Model for Climate and Health-related Text"},{"paperId":"d9ea78cce9dafd95aea422341a05b4d1d50730df","title":"Using Twitter Data Analysis to Understand the Perceptions, Awareness, and Barriers to the Wide Use of Pre-Exposure Prophylaxis in the United States"},{"paperId":"7009fd9eb533df6882644a1c8e1019dc034b9cc5","title":"DCA-IoMT: Knowledge-Graph-Embedding-Enhanced Deep Collaborative Alert Recommendation Against COVID-19"},{"paperId":"a95cdf4670685a9e31bba505db9e20d3926f23ac","title":"Named entity recognition of Chinese electronic medical records based on a hybrid neural network and medical MC-BERT"},{"paperId":"c7b6c685b9694bec19ea96467fe4376ab62ae9ed","title":"CultureBERT: Fine-Tuning Transformer-Based Language Models for Corporate Culture"},{"paperId":"08ab9ba37d7fc63bb4a0969688028052ec4716c2","title":"Biomedical NER for the Enterprise with Distillated BERN2 and the Kazu Framework"},{"paperId":"7b15616d95d42c60245e2f23b68e249558ffdded","title":"Predicting drug characteristics using biomedical text embedding"},{"paperId":"5b4e106fe657cba6ab8122cb1b098f002b6568b3","title":"Automated Detection of Substance-Use Status and Related Information from Clinical Text"},{"paperId":"40fe011402ff452b0b134d8c5179ef52db986f33","title":"Clinical Application of Detecting COVID-19 Risks: A Natural Language Processing Approach"},{"paperId":"06adfb6a76b7c69b678d0b81c5f3087939652102","title":"Enhancements to Language Modeling Techniques for Adaptable Log Message Classification"},{"paperId":"acef23274e8216ad0af515e911a8579d0243a3c4","title":"The Use of Pretrained Model for Matching App Reviews and Bug Reports"},{"paperId":"befdd8ac38a8b52aa306c0f85566267a002b43d9","title":"Predicting Clinical Events via Graph Neural Networks"},{"paperId":"6a78ddf1c05f29f267e5cfd5dffe5d7f791192b9","title":"AAEBERT: Debiasing BERT-based Hate Speech Detection Models via Adversarial Learning"},{"paperId":"1ec63e1c726b2bfbfb2f3786a6b1dfca945ba1e2","title":"Joint Extraction of Entity Relations Based on Dependency Syntax and BERT in Medical Domain"},{"paperId":"4363427eba0051f1cfe69ad060a863ce5546fb29","title":"Chinese Clinical Named Entity Recognition from Electronic Medical Records based on Multi-semantic Features by using RoBERTa-wwm and CNN: Model Development and Validation (Preprint)"},{"paperId":"955e2a788f0a9ddb7c3f971f9e830159bcf1c8e6","title":"Research on Classification Method of Bank Statement in Low Resource and Cross-domain Scenarios"},{"paperId":"f6c5f392c1352f73dc0a5a7ccec09e345b646168","title":"Conversational System for Clinical Communication Training Supporting User-defined Tasks"},{"paperId":"c3d877b29594bc6f244e638f576be3dca5551f11","title":"A Comparative Study of Pretrained Language Models for Long Clinical Text"},{"paperId":"f3e438f5bdbe3b92c1c2bf0eb2938b9be18e2755","title":"AIONER: All-in-one scheme-based biomedical named entity recognition using deep learning"},{"paperId":"0885ecb3dae0fe12d7bb3af6fa5157e0a71ab549","title":"A Transformer-Based Model Trained on Large Scale Claims Data for Prediction of Severe COVID-19 Disease Progression"},{"paperId":"7821e7639ffaeea175422f35fae2eb1c095ed1a6","title":"Protein Language Models and Structure Prediction: Connection and Progression"},{"paperId":"eb2f1c096e4b27182bb7d987e946eceb5aa57c54","title":"The New Version of the ANDDigest Tool with Improved AI-Based Short Names Recognition"},{"paperId":"90e5769a13099c4c7e887c6f729a7775ce5cd569","title":"An Automatic SOAP Classification System Using Weakly Supervision And Transfer Learning"},{"paperId":"405e7d7f24b4f6755aa56930655448425ee60e12","title":"LitCovid ensemble learning for COVID-19 multi-label classification"},{"paperId":"b3011e3c34af29039ee6e62f8fd5c83307f8a358","title":"Towards semantic-driven boolean query formalization for biomedical systematic literature reviews"},{"paperId":"1f5d70da740a2c68cf066ad150312b2642ae72c1","title":"Biomedical Text NER Tagging Tool with Web Interface for Generating BERT-Based Fine-Tuning Dataset"},{"paperId":"0ef3a45e65f625b2cd46671d8118f75403471bc4","title":"Tracking biomedical articles along the translational continuum: a measure based on biomedical knowledge representation"},{"paperId":"768403710ec37fe612257384b19bf8f1c7bcce72","title":"Formative assessment strategies for students' conceptions - The potential of learning analytics"},{"paperId":"16ffa80289ca0e7d747ddae1ecdca58c64216180","title":"BioByGANS: biomedical named entity recognition by fusing contextual and syntactic features through graph attention network in node classification framework"},{"paperId":"b9289edf2f2a5a9ba9cc3bb17a8d51fee67f4640","title":"CBEAF-Adapting: Enhanced Continual Pretraining for Building Chinese Biomedical Language Model"},{"paperId":"5f98a78b1e17a594d4c9178d8610a053884a2203","title":"Evaluating the Knowledge Dependency of Questions"},{"paperId":"f304466f545f58137f317846530a2d2b65dc432e","title":"Leveraging Users' Social Network Embeddings for Fake News Detection on Twitter"},{"paperId":"ff859b26e94b0545365d6cc759dff632e788e2ae","title":"Context Variance Evaluation of Pretrained Language Models for Prompt-based Biomedical Knowledge Probing"},{"paperId":"f1af525155e4737fd8e43e45ed60a9fa0e6f03f6","title":"Notes on the data quality of bibliographic records from the MEDLINE database"},{"paperId":"dd07778ce114225e13307c05ee94eb97d445d884","title":"Explainable Personality Prediction Using Answers to Open-Ended Interview Questions"},{"paperId":"1eb72ef89e6b7722e114e195f68cd45c86e6f344","title":"The next-generation Open Targets Platform: reimagined, redesigned, rebuilt"},{"paperId":"63f02dfd1ecdd945e24d80b25e64f94c4f722bc1","title":"A Novel Automated Approach to Mutation-Cancer Relation Extraction by Incorporating Heterogeneous Knowledge"},{"paperId":"9455ace3e18bd8659c0ae8e661813ecc5f002394","title":"BERT-based Topic Modeling Approach for Malaria Research Publication"},{"paperId":"7ae089060ecb7ac2788e94ee7e04a3eb7009f6b4","title":"CDialog: A Multi-turn Covid-19 Conversation Dataset for Entity-Aware Dialog Generation"},{"paperId":"683f73dcd2b78ce0710e17b4e6316762bd169c93","title":"Lesion Guided Explainable Few Weak-Shot Medical Report Generation"},{"paperId":"491c65555afa2125b88191d7c81fadc4daf4c31b","title":"A correlation-based feature analysis of physical examination indicators can help predict the overall underlying health status using machine learning"},{"paperId":"d641bfcea49dc072fe471b433b0f6b0f414947b4","title":"An Automatic ICD Coding Network Using Partition-Based Label Attention"},{"paperId":"e787c56679064c19c97af82d8b4bb7b09e179868","title":"Relation Extraction from Texts Containing Pharmacologically Significant Information on base of Multilingual Language Models"},{"paperId":"c8957181e3905ac8eafacce99c7396571f09ce0c","title":"Few-Shot Learning for Clinical Natural Language Processing Using Siamese Neural Networks (Preprint)"},{"paperId":"54a316ecfb97352e55c5e85c06ccf7d013c4993b","title":"NLPeer: A Unified Resource for the Computational Study of Peer Review"},{"paperId":"f9ddbee4289bd536f7f2101c038b8d0d87fb2b83","title":"The STRING database in 2023: protein–protein association networks and functional enrichment analyses for any sequenced genome of interest"},{"paperId":"d7837d577fc6821d095dffb559cfdc0e3574166f","title":"Hardness-guided domain adaptation to recognise biomedical named entities under low-resource scenarios"},{"paperId":"b4b37f87e0357f2e4cec70af67b2f088f6efce70","title":"A Survey of Knowledge-Enhanced Pre-trained Language Models"},{"paperId":"4f9adb83bac0ee0458ee6f784c32e488efbec37c","title":"When BERT Started Traveling: TourBERT—A Natural Language Processing Model for the Travel Industry"},{"paperId":"cfddcdc764ccf38a3cb48becd560356535425a4a","title":"Message from General Chair"},{"paperId":"c9d90133c08a92d3d99cddee5d107933cf241367","title":"FormLM: Recommending Creation Ideas for Online Forms by Modelling Semantic and Structural Information"},{"paperId":"0c007e000eaf59e86788b45b70879612cde2e8ee","title":"Biomedical Multi-hop Question Answering Using Knowledge Graph Embeddings and Language Models"},{"paperId":"e3a3d3199f8516f818fd8afe51235432ec104974","title":"Combining Contrastive Learning and Knowledge Graph Embeddings to develop medical word embeddings for the Italian language"},{"paperId":"e5f5a058c26b87dc94b8b0c35e9395bac578969c","title":"Information extraction from electronic medical documents: state of the art and future research directions"},{"paperId":"ca77a3559a718c5d2b17dc109d585744f7b7fb43","title":"Fine-Tuning BERT for Question and Answering Using PubMed Abstract Dataset"},{"paperId":"f602784b5f46750cce1ca1e4c13b0f28a5e20d2b","title":"Assisted neuroscience knowledge extraction via machine learning applied to neural reconstruction metadata on NeuroMorpho.Org"},{"paperId":"cf8234789a528b855da6dd9c13e54539f1a6deff","title":"Next-Day Medical Activities Recommendation Model with Double Attention Mechanism Using Generative Adversarial Network"},{"paperId":"3fab990fb6b0efb67e3f6f39fe7eb65f95d4a111","title":"Community-in-the-loop: Creating Artificial Process Intelligence for Co-production of City Service"},{"paperId":"7618861e5ad468aa1fb9da7b7f23fac13b03efe9","title":"AD-BERT: Using Pre-trained contextualized embeddings to Predict the Progression from Mild Cognitive Impairment to Alzheimer's Disease"},{"paperId":"2d919381510fdc25dfebbfbf2cf3592257328e61","title":"MedBERT: A Pre-trained Language Model for Biomedical Named Entity Recognition"},{"paperId":"c4c33de2fbbe01f611ed79fb058d657405db95bd","title":"ViMRT: a text-mining tool and search engine for automated virus mutation recognition"},{"paperId":"bf5c307aa7c4bdb912408ffd831553f0ae86ffe7","title":"On the Domain Adaptation and Generalization of Pretrained Language Models: A Survey"},{"paperId":"5d239bb9d64ba63ff1390342706300f67ea7e8f2","title":"ConBERT: A Concatenation of Bidirectional Transformers for Standardization of Operative Reports from Electronic Medical Records"},{"paperId":"542e09e66dc7c9863e85e6b18765c1f737d028c4","title":"Forecasting User Interests Through Topic Tag Predictions in Online Health Communities"},{"paperId":"4ac21503eddebfee77351d2072b3884f24d43807","title":"Identification and Visualization of Key Topics in Scientific Publications with Transformer-Based Language Models and Document Clustering Methods"},{"paperId":"8fb7492dea45e01de7ea0feb59d8627312b2f939","title":"Improving Biomedical ReQA With Consistent NLI-Transfer and Post-Whitening"},{"paperId":"28980fd125402febec34913ed9540a70e55888c2","title":"BERT for Long Documents: A Case Study of Automated ICD Coding"},{"paperId":"2689c5246c33050df35f5b68dc15e68e69fb1099","title":"Federated Multilingual Models for Medical Transcript Analysis"},{"paperId":"9272a06ea006171f34d29d46319230438fd531a7","title":"Late Fusion with Triplet Margin Objective for Multimodal Ideology Prediction and Analysis"},{"paperId":"5bbb755529323897d33b9e4073385d2148f63bf1","title":"A Comparison of SVM against Pre-trained Language Models (PLMs) for Text Classification Tasks"},{"paperId":"1d150032f50d65770cd7fc4ddb8e76fba4cea257","title":"BERT-Deep CNN: State-of-the-Art for Sentiment Analysis of COVID-19 Tweets"},{"paperId":"7be18eba71211a16cd4d068cdee2575efab91271","title":"Biomedical named entity recognition with the combined feature attention and fully-shared multi-task learning"},{"paperId":"d5001b854785771ded1c9c8f52d5d4e8d0045bf4","title":"An Easy-to-use and Robust Approach for the Differentially Private De-Identification of Clinical Textual Documents"},{"paperId":"10f3cf6dc50f9d52a313481f103daf959d567479","title":"Cross-stitching Text and Knowledge Graph Encoders for Distantly Supervised Relation Extraction"},{"paperId":"9d7774f5188c270e2dd4aab512e616509d3a41a3","title":"Improved Fine-Tuning of In-Domain Transformer Model for Inferring COVID-19 Presence in Multi-Institutional Radiology Reports"},{"paperId":"b9779c84abf33a4a7d8a76b6f6344cde13c8be83","title":"TOE: A Grid-Tagging Discontinuous NER Model Enhanced by Embedding Tag/Word Relations and More Fine-Grained Tags"},{"paperId":"265afb13b7d235a0315a445ae901ca6db62d7fd3","title":"CCS Explorer: Relevance Prediction, Extractive Summarization, and Named Entity Recognition from Clinical Cohort Studies"},{"paperId":"ef63635d87d77f388451cdc88175917624b1c00e","title":"Named entity recognition of building construction defect information from text with linguistic noise"},{"paperId":"7e0fda71cc2e561385c0a2e29f5a81654a11157f","title":"VarMAE: Pre-training of Variational Masked Autoencoder for Domain-adaptive Language Understanding"},{"paperId":"743066ef8253f293c635b56aa856d2c474c261a6","title":"Global meta-analysis of evolution patterns for lake topics over centurial scale: A natural language understanding-based deep clustering approach with 130,000 studies"},{"paperId":"b6afdf1e125618db066a535cb978aec4d262e809","title":"An accessible, efficient, and accurate natural language processing method for extracting diagnostic data from pathology reports"},{"paperId":"91ee912db3ecc8cfcbafbc2fe4ca468b25ef41a4","title":"Transferring knowledge between topics in systematic reviews"},{"paperId":"6b37985840ff40bd75fa70854a01852ff245bbc4","title":"Training a Deep Contextualized Language Model for International Classification of Diseases, 10th Revision Classification via Federated Learning: Model Development and Validation Study"},{"paperId":"bc257b38b54b783ffb65b9b6c802d54351621c15","title":"Biomedical named entity normalization via interaction-based synonym marginalization"},{"paperId":"f4a8f86ce62995eccf4db80d37cd4e7952960452","title":"The Role of Natural Language Processing during the COVID-19 Pandemic: Health Applications, Opportunities, and Challenges"},{"paperId":"9a14a7882e66f683b40eb62d784b8c2418633600","title":"DCPC: Drug Candidates for the Prevention of COVID-19 Database"},{"paperId":"b7830b246c3f60a35b0bc2b2ab53d017a4e54abd","title":"MTBC-BioNER: Multi-task Learning Using BioBERT and CharCNN for Biomedical Named Entity Recognition"},{"paperId":"008cb42964fbdc5ccee79ef7b981b4ee105fb361","title":"Entity Level QA Pairs Dataset for Sentiment Analysis"},{"paperId":"803261d13ab134f0e3ce45f265e1d36d1ead32ef","title":"Improving Cause-of-Death Classification from Verbal Autopsy Reports"},{"paperId":"0882a2b2787b35dbcc6e341c953d964b77abd4df","title":"When FLUE Meets FLANG: Benchmarks and Large Pretrained Language Model for Financial Domain"},{"paperId":"8f6111c659df5487470b05e8d58a286694dd853b","title":"Italian, European, and international neuroinformatics efforts: An overview"},{"paperId":"8a2376750af3212cf22be81ddd7c650bb0eedf28","title":"IAnimal: a cross-species omics knowledgebase for animals"},{"paperId":"90b82c46173b5fe18cbd12371177e2281de6d647","title":"Leveraging knowledge graphs to update scientific word embeddings using latent semantic imputation"},{"paperId":"42da00c23a2c641728f090c06db5986da66ce511","title":"We are not ready yet: limitations of state-of-the-art disease named entity recognizers"},{"paperId":"2cea77f07e47dd10dd4b2aa21f786b6465a93859","title":"Towards Language-driven Scientific AI"},{"paperId":"fbb73c93a8cbd87bdbc52deb988b8831e45b6fc2","title":"Named Entity Recognition of Protein Complex Incorporating Syntactic Information"},{"paperId":"a138041f6fa2e4ea850afddbfde9945fe4ed11f4","title":"Biomedical Entity Linking Based on Global and Local Feature Fusion"},{"paperId":"316467770bc41a872cccfadd516cef8a79139398","title":"Civil Data Mining using Machine Learning"},{"paperId":"b32f967c0a5777a5045d4fd59f2cda9be68dc3bb","title":"PALT: Parameter-Lite Transfer of Language Models for Knowledge Graph Completion"},{"paperId":"73b222c2808a95878c8a3aa762b9ba1d4dd9ecd4","title":"Parameter-Efficient Legal Domain Adaptation"},{"paperId":"984dd8bd2d37ed5d8f56e72f941293977429badf","title":"Enhancing Label Consistency on Document-level Named Entity Recognition"},{"paperId":"810e7ff45d5cdb2318f88a2787a37b2fadd9bf82","title":"Legal-Tech Open Diaries: Lesson learned on how to develop and deploy light-weight models in the era of humongous Language Models"},{"paperId":"c6c8a77c47b5e54527390cba6d1c2a86dfd1d4c9","title":"On Cross-Domain Pre-Trained Language Models for Clinical Text Mining: How Do They Perform on Data-Constrained Fine-Tuning?"},{"paperId":"ff8f3dfd9e2f4a92310999722abefab202935521","title":"Do Language Models Understand Measurements?"},{"paperId":"1b2f6cacaa7305df32b8c70ef5c0ecd4db69a825","title":"PHEE: A Dataset for Pharmacovigilance Event Extraction from Text"},{"paperId":"033ce949ded20600b7a770fda78853eb48c5ce89","title":"BioLORD: Learning Ontological Representations from Definitions (for Biomedical Concepts and their Textual Descriptions)"},{"paperId":"8a5837b9245035972f97d6796f62790b6e7e9d71","title":"SpaBERT: A Pretrained Language Model from Geographic Data for Geo-Entity Representation"},{"paperId":"8dd9ba9af32b05152d944b24bf9e04fdeedbcb5f","title":"GLAF: Global-and-Local Attention Flow Model for Question Answering"},{"paperId":"a2a478b91f6e00b3fcda018384121097f435e5e0","title":"MixUp based Cross-Consistency Training for Named Entity Recognition"},{"paperId":"0d0900f2afa1db5c4133907aaaacb21b9a8d86b3","title":"Pre-trained Language Model-based Retrieval and Ranking for Web Search"},{"paperId":"504d1d9b6ecdeaadb79ad8ba80fa64ac34fca18e","title":"Evidence > Intuition: Transferability Estimation for Encoder Selection"},{"paperId":"774255e00ac21e1d6a9ac530a6789c026808fd96","title":"An automatic hypothesis generation for plausible linkage between xanthium and diabetes"},{"paperId":"c9b21d0a1244ad554ebb5e05d9a7c0ec38ccb94c","title":"SmarTxT: A Natural Language Processing Approach for Efficient Vehicle Defect Investigation"},{"paperId":"3a692230558c6ed35f13403aedd57f10803c37d7","title":"End-to-End Entity Detection with Proposer and Regressor"},{"paperId":"d9270a1d97319d0b01728e7a137dde2157f1e57d","title":"Type-supervised sequence labeling based on the heterogeneous star graph for named entity recognition"},{"paperId":"145fb51975ec4ca72708bf354713f568a7541358","title":"Towards Realistic Low-resource Relation Extraction: A Benchmark with Empirical Baseline Study"},{"paperId":"4b4fc9d41e40fdccc294c0074c545e4dafce743f","title":"MedGraph: A semantic biomedical information retrieval framework using knowledge graph embedding for PubMed"},{"paperId":"130204d04c6a427480a4df889b7aa4aca9ffa18d","title":"Miko Team: Deep Learning Approach for Legal Question Answering in ALQAC 2022"},{"paperId":"8db309acf6ca324bccae136c9a2d31fb5609578c","title":"Using Bottleneck Adapters to Identify Cancer in Clinical Notes under Low-Resource Constraints"},{"paperId":"21c71ab196f5139c2a26ff0c7ed5f30c3e099723","title":"Named Entity-based Question-Answering Pair Generator"},{"paperId":"6e391835815b88990da5d130a123c339c59b9457","title":"Extracting Drug-drug Interactions from Biomedical Texts using Knowledge Graph Embeddings and Multi-focal Loss"},{"paperId":"5cf75ca9549ba58f4d4c90ed8053a89b917f5fb9","title":"BidH: A Bidirectional Hierarchical Model for Nested Named Entity Recognition"},{"paperId":"ad3dfb2514cb0c899fcb9a14d229ff2a6018892f","title":"Deep Bidirectional Language-Knowledge Graph Pretraining"},{"paperId":"792c5e004430e88da5f3b013a1fda0091ec8b59c","title":"Drug Adverse Event Detection Using Text-Based Convolutional Neural Networks (TextCNN) Technique"},{"paperId":"0ae8428cde7369ba3628e571742648fb753b0cc2","title":"Comprehensive study of pre-trained language models: detecting humor in news headlines"},{"paperId":"7ed848832d03940932e3a641bfe1971d1968fae7","title":"Automated Classification of Free-Text Radiology Reports: Using Different Feature Extraction Methods to Identify Fractures of the Distal Fibula"},{"paperId":"8525f80d1ff2f3cd20c147b6665c6947d46a5080","title":"This Patient Looks Like That Patient: Prototypical Networks for Interpretable Diagnosis Prediction from Clinical Text"},{"paperId":"bb16531b2b4f3833a068f0745475dff971032e56","title":"Improving Healthcare Question Answering System by Identifying Suitable Answers"},{"paperId":"26c761a06fc1bf07345ccb2f5b1121926da8870b","title":"Handling missing values in healthcare data: A systematic review of deep learning-based imputation techniques"},{"paperId":"f9d01f8ab34bb707afdee382350f7ce23534c2d9","title":"AraLegal-BERT: A pretrained language model for Arabic Legal text"},{"paperId":"8bfff360bf6b46959a58aaa82d49142a13243312","title":"Improving Radiology Summarization with Radiograph and Anatomy Prompts"},{"paperId":"89037151d5645355b5eaee7e52bc5e5c5f705297","title":"Automatic Creation of Named Entity Recognition Datasets by Querying Phrase Representations"},{"paperId":"e02dce6ee032a13b1653f69b034a35676e7d4dc2","title":"Can language representation models think in bets?"},{"paperId":"1e2a5dca6f310241dd8a9b56873edf8ca211c781","title":"Enriching Biomedical Knowledge for Low-resource Language Through Translation"},{"paperId":"0eb074e0504d1062b36a7ced6efec8048e68ac1f","title":"Exploration of biomedical knowledge for recurrent glioblastoma using natural language processing deep learning models"},{"paperId":"7c71c6d3e03c098a9f7d4dcc56d9e0abfa05314a","title":"Natural language processing (NLP) aided qualitative method in health research"},{"paperId":"2406331c47019b8d3e0873d183c14d5ec994e2fa","title":"MiniALBERT: Model Distillation via Parameter-Efficient Recursive Transformers"},{"paperId":"50f268f24f257bbc27c7faf9037a5b5dc76a59ab","title":"RedHOT: A Corpus of Annotated Medical Questions, Experiences, and Claims on Social Media"},{"paperId":"38e995753f222ba7d198d6d54033058de84a77f1","title":"MedJEx: A Medical Jargon Extraction Model with Wiki’s Hyperlink Span and Contextualized Masked Language Model Score"},{"paperId":"0979695b5d74016e97ab8f306f632114e98bd6d9","title":"Task Compass: Scaling Multi-task Pre-training with Task Prefix"},{"paperId":"2185ee209832acfcb7b30a0e4fcec0aa8e5095dc","title":"Opinion analysis and aspect understanding during covid-19 pandemic using BERT-Bi-LSTM ensemble method"},{"paperId":"d7104dab8e5379f9566108857e3bd9dc832286df","title":"Developing a general-purpose clinical language inference model from a large corpus of clinical notes"},{"paperId":"c3d68633dc7047ad7e3e55c6ea9339d1eaf14b15","title":"The evaluation of RQE system to recognize question similarity of the regulatory documents in pharmaceutical industry: an evaluation study (Preprint)"},{"paperId":"54772ffae642a87b9a6122a6f1bae76b926a7230","title":"Enriching Biomedical Knowledge for Low-resource Language Through Large-scale Translation"},{"paperId":"a0d7be46514e5340fec6a5c9368a7bd755af0a55","title":"An expert‐in‐the‐loop method for domain‐specific document categorization based on small training data"},{"paperId":"8e5c2f0e7eb5b9977219e3c810941ee7ecc1ba04","title":"Adapting without forgetting: KnowBert-UMLS"},{"paperId":"8a8b774077bd0be706988e5d7c45133f20b26251","title":"Systematic Evaluation of Common Natural Language Processing Techniques to Codify Clinical Notes"},{"paperId":"6ae700c89a9a9a3da7e55dd51c4710b5ed8c8d4e","title":"Caption-Aware Medical VQA via Semantic Focusing and Progressive Cross-Modality Comprehension"},{"paperId":"391c313f2db33a7a6069b0d95d59f8b818f9ff6b","title":"Spread Love Not Hate: Undermining the Importance of Hateful Pre-training for Hate Speech Detection"},{"paperId":"940d32e2f5ac604393e8a9ef9195f53e00fc20ad","title":"Short Text Pre-training with Extended Token Classification for E-commerce Query Understanding"},{"paperId":"87fdfcc68e06489266fd01f0bc24a1f9bc0a235d","title":"KG-MTT-BERT: Knowledge Graph Enhanced BERT for Multi-Type Medical Text Classification"},{"paperId":"055fe58ea72df3fa296f2f0f3ad3abacafb84204","title":"GMA3D: Local-Global Attention Learning to Estimate Occluded Motions of Scene Flow"},{"paperId":"5d0acc2f780ca2012a1c9689cc8d1475799003b7","title":"Named Entity Recognition in Twitter: A Dataset and Analysis on Short-Term Temporal Shifts"},{"paperId":"dbab937db91b9ccdb39c2840257305b16b93ce8e","title":"In silico prediction methods of self-interacting proteins: an empirical and academic survey"},{"paperId":"3d78f898dbf363371bdf28aaa1ed86e2972502a2","title":"Token Classification for Disambiguating Medical Abbreviations"},{"paperId":"9da92d387cbc09c9b68e30a52ff472a83f4283d0","title":"Critical assessment of transformer-based AI models for German clinical notes"},{"paperId":"bd195095e921ab4d62cec12f37754f2e59883546","title":"Understanding Prior Bias and Choice Paralysis in Transformer-based Language Representation Models through Four Experimental Probes"},{"paperId":"ef66e86322b5ae1e2524ad48294002c71b94830d","title":"Assessing the Impact of Contextual Information in Hate Speech Detection"},{"paperId":"5bbf3bd92ef05e65580e5d673f7c9302287f99b0","title":"AI-based ICD coding and classification approaches using discharge summaries: A systematic literature review"},{"paperId":"9b349c140144c7a1d5e710ce3384f259863c8318","title":"How do others cope? Extracting coping strategies for adverse drug events from social media"},{"paperId":"ce5aa326cdae5958bf112859dd4ea34e684b9a72","title":"An automatic descriptors recognizer customized for materials science literature"},{"paperId":"5458e62da091f0911a0a209207863c0833853e7b","title":"RadioBERT: A deep learning-based system for medical report generation from chest X-ray images using contextual embeddings"},{"paperId":"45e77e0d4bfe98119c24b073146298ed22b167cf","title":"PubMed Author-assigned Keyword Extraction (PubMedAKE) Benchmark"},{"paperId":"826430735643c4334bbf0a0920c6093f51ac7eaa","title":"Semisupervised neural biomedical sense disambiguation approach for aspect-based sentiment analysis on social networks"},{"paperId":"a087c7f90597e826d94cd341f990db80e24a90eb","title":"BioKnowPrompt: Incorporating imprecise knowledge into prompt-tuning verbalizer with biomedical text for relation extraction"},{"paperId":"d6e2603227542bfe4a2bcc4551c3dbc0b2329f57","title":"Deep contextual multi-task feature fusion for enhanced concept, negation and speculation detection from clinical notes"},{"paperId":"9247c814900ed79690ba6832828470dfe60a639a","title":"Text Summarization towards Scientific Information Extraction"},{"paperId":"450946ff0120a3ce3b6842f104524796414115a8","title":"Medical Intention Recognition Based on MCBERT-TextCNN Model"},{"paperId":"3b34df9e3b9b41930f46adfd22331d62a830cdd7","title":"Self-Distillation for Further Pre-training of Transformers"},{"paperId":"5d8fd04c436367b18b35e28332ee8e452a477f3f","title":"Medical Image Understanding with Pretrained Vision Language Models: A Comprehensive Study"},{"paperId":"54a5e1698d9def60c1d205bd1e78a92954a704f1","title":"NLP-based classification of software tools for metagenomics sequencing data analysis into EDAM semantic annotation"},{"paperId":"9ab6727fade0fa3f7a121801069eae9cc1ab6f50","title":"DR.BENCH: Diagnostic Reasoning Benchmark for Clinical Natural Language Processing"},{"paperId":"88e3f5b9081bafffbde86c3fb4a2ae26ccd22ff2","title":"A Survey on Computational Methods for Investigation on ncRNA-Disease Association through the Mode of Action Perspective"},{"paperId":"b0a643b1ae1aad63defe25a713162dd5b0e69c89","title":"Full-text chemical identification with improved generalizability and tagging consistency"},{"paperId":"861e1b6989bba8c503b5cf209eaf8923f474703a","title":"YATO: Yet Another deep learning based Text analysis Open toolkit"},{"paperId":"e26197fb0fa409866b287f4bf63abe7997223b51","title":"A general-purpose material property data extraction pipeline from large polymer corpora using natural language processing"},{"paperId":"168db75f79cd2d39a7802451578662bb15572de4","title":"Improving Radiology Report Generation Systems by Removing Hallucinated References to Non-existent Priors"},{"paperId":"4c46df96d67bcca0fd98aaa9940f8d834f7213c4","title":"MediCoSpace: Visual Decision-Support for Doctor-Patient Consultations using Medical Concept Spaces from EHRs"},{"paperId":"44279244407a64431810f982be6d0c7da4429dd7","title":"BioGPT: Generative Pre-trained Transformer for Biomedical Text Generation and Mining"},{"paperId":"c0de2e91459f6e3519d8a4839bff19bc73a9c5e0","title":"Large-scale application of named entity recognition to biomedicine and epidemiology"},{"paperId":"25b6c9a11d9078d2cc45e02e284195320ce61f0f","title":"Variational Open-Domain Question Answering"},{"paperId":"3ff823d50875fee0dc2c6205709920ab022e2022","title":"Adaptation of domain-specific transformer models with text oversampling for sentiment analysis of social media posts on Covid-19 vaccines"},{"paperId":"3ed8168abced540feab67398712e102afdb39a80","title":"Training and intrinsic evaluation of lightweight word embeddings for the clinical domain in Spanish"},{"paperId":"0b21174036879d92c2770a156fa8aa6fd6749c79","title":"Generalizing through Forgetting - Domain Generalization for Symptom Event Extraction in Clinical Notes"},{"paperId":"a3e757fc4ef3fb929f9de43cd4558d3fa75a540c","title":"Automated MeSH Term Suggestion for Effective Query Formulation in Systematic Reviews Literature Search"},{"paperId":"173153d5567553f3477ff815dd53a713177cf672","title":"Mapping Climate Change Research via Open Repositories & AI: advantages and limitations for an evidence-based R&D policy-making"},{"paperId":"4fc4a4d66f29998e88750afbbf4839b9ac9ac98c","title":"Disease prediction based on multi-type data fusion from Chinese electronic health record."},{"paperId":"fabbbcf627e4efbf7eb56647c340df1e72a6da67","title":"What Went Wrong: A Survey of Wildfire UAS Mishaps through Named Entity Recognition"},{"paperId":"7d0abebf379383afbf9b7b3f8ab89561d1aa7596","title":"MaterialBERT for natural language processing of materials science texts"},{"paperId":"274dbb98c63cdd282eb86b0338bdc3c5dfd9b904","title":"Dataset Inference for Self-Supervised Models"},{"paperId":"e92392b73da56b226f1e71e495724c46f776a370","title":"De-Identification of French Unstructured Clinical Notes for Machine Learning Tasks"},{"paperId":"2cd7bd19c9c2868a88e4c80c0c4f3cf0dd63900a","title":"Examining Large Pre-Trained Language Models for Machine Translation: What You Don’t Know about It"},{"paperId":"daf55a55661a6d41f3bf0a543f4c277d4c01d83e","title":"Toward Improving Health Literacy in Patient Education Materials with Neural Machine Translation Models"},{"paperId":"ac5225708efd250d217424ba27885e90f186160d","title":"Prompt Combines Paraphrase: Teaching Pre-trained Models to Understand Rare Biomedical Words"},{"paperId":"6ab6e6f62323132e299fc6717ad0f5ca000414d5","title":"Natural language processing in clinical neuroscience and psychiatry: A review"},{"paperId":"f0e0806003ee509f1f4feeabf209a8e93dddf5d2","title":"A revised application of cognitive presence automatic classifiers for MOOCs: a new set of indicators revealed?"},{"paperId":"9947a338f83bf1207cb7a5c8cf0810311fa66201","title":"Pre-trained Language Models for the Legal Domain: A Case Study on Indian Law"},{"paperId":"ca2afde4470180dbcfaa27e7a5cd3c7679569771","title":"Large-scale Evaluation of Transformer-based Article Encoders on the Task of Citation Recommendation"},{"paperId":"1c7a4e8d9f4fcf19a5d1caa078c66ca39cb75dd2","title":"A Molecular Multimodal Foundation Model Associating Molecule Graphs with Natural Language"},{"paperId":"ca6b7c8b8389c495d53584d777d638e3307cfbca","title":"Challenges and opportunities in current vaccine technology and administration: A comprehensive survey examining oral vaccine potential in the United States"},{"paperId":"6825ca62bde5036fccc06567c3c06093150e14d3","title":"T-NER: An All-Round Python Library for Transformer-based Named Entity Recognition"},{"paperId":"45c9883d51cda85ed44502fa174bd42f6315d36b","title":"Background knowledge in ontology matching: A survey"},{"paperId":"3b97687f80cf2ed753789429ba6d2e53667a0dd7","title":"On the effectiveness of compact biomedical transformers"},{"paperId":"88dd119dba5ee747851ade8f5d517b381614d918","title":"Fengshenbang 1.0: Being the Foundation of Chinese Cognitive Intelligence"},{"paperId":"ab7706c845ce30e1f865a67259fac7e351474d01","title":"Transforming Drug-Drug Interaction Extraction from Biomedical Literature"},{"paperId":"bf6d47795c67cc47fc208736b87ccf49d36d0bca","title":"BertSRC: transformer-based semantic relation classification"},{"paperId":"d36c0b5f67bce9afccc8bdd8b68fafa8097d9dee","title":"OneEE: A One-Stage Framework for Fast Overlapping and Nested Event Extraction"},{"paperId":"228c4b30bef0a85e6cd3e633293323bb559c978a","title":"A BERT-based model for coupled biological strategies in biomimetic design"},{"paperId":"9370f880b6ca91754793cb7b076237043c0a73f3","title":"Query-focused Extractive Summarisation for Biomedical and COVID-19 Complex Question Answering (preprint)"},{"paperId":"78f4129e3c33beab4f7e4a17848828e897d4069f","title":"To Explore the Molecular Mechanism of Acupuncture Alleviating Inflammation and Treating Obesity Based on Text Mining"},{"paperId":"83d2d9f6a315ece46fde7cb2ad5a6fec2ec36d49","title":"Named Entity Recognition System for the Biomedical Domain"},{"paperId":"c05a9797e5ead09d0885efa9f1b464d319f80709","title":"Drug-Drug Interaction Extraction from Biomedical Text using Relation BioBERT with BLSTM"},{"paperId":"ead20e9d7b48b5e48465c8aa5d3a79223b02d8e0","title":"A multi-head adjacent attention-based pyramid layered model for nested named entity recognition"},{"paperId":"4c86ffcc12bf728706152ba8a36181374cccfd1e","title":"Attack Tactic Identification by Transfer Learning of Language Model"},{"paperId":"9d795925e30ab8a70c4cf66f314ea5435c9b426c","title":"Automated extraction of information of lung cancer staging from unstructured reports of PET-CT interpretation: natural language processing with deep-learning"},{"paperId":"0eb5b4292319a8be4b1508acd874d3fdc569b8c0","title":"Extracting drug-drug interactions from no-blinding texts using key semantic sentences and GHM loss"},{"paperId":"23702f6f1fa80b5dcc0fed7e94157e9d52d151bb","title":"A Cross‐Domain Ontology Semantic Representation Based on NCBI‐BlueBERT Embedding"},{"paperId":"1190a62c5b4376f08c7461816d7466d41cf5f622","title":"Heterogeneous deep graph convolutional network with citation relational BERT for COVID-19 inline citation recommendation"},{"paperId":"9cd4fc0517ea310cc85504f35857a74aedff65bb","title":"Leveraging weak supervision to perform named entity recognition in electronic health records progress notes to identify the ophthalmology exam"},{"paperId":"3061e490a1f9727175c7851f73de7c40d5f06216","title":"Gaze-assisted automatic captioning of fetal ultrasound videos using three-way multi-modal deep neural networks"},{"paperId":"cf4c627c0dc53d1524726b939887bf3b439d2b13","title":"Neural architectures for aggregating sequence labels from multiple annotators"},{"paperId":"0f6fbddef30c530b82de3d7d3f247c69ecec47d4","title":"MolRoPE-BERT: An enhanced molecular representation with Rotary Position Embedding for molecular property prediction."},{"paperId":"0f0e147cf0356ac1c81cc710e59e309f46c5ee61","title":"Natural Language Processing in Nephrology."},{"paperId":"3585118b0c75a45a3c7a00343e14336bd32d8e9f","title":"Language Model for Statistics Domain"},{"paperId":"e8004292b7cdf1d0988fe3242d2a720b725a90f4","title":"Building customized Named Entity Recognition models for specific process automation tasks"},{"paperId":"1b46e078b7303b0972599c514fa4b2a0fae7e7ff","title":"Few-Shot Learning for Clinical Natural Language Processing Using Siamese Neural Networks"},{"paperId":"6e59edb734194abef65a0404e425dbd823a4f07f","title":"Optimizing Bi-Encoder for Named Entity Recognition via Contrastive Learning"},{"paperId":"a076508f77cbfd5da45a73d34f31c5af8104970d","title":"A Deep Learning-Based Privacy-Preserving Model for Smart Healthcare in Internet of Medical Things Using Fog Computing"},{"paperId":"73d39a6ce4726b038f13b62307bc3cf4ef95776f","title":"No means ‘No’: a non-improper modeling approach, with embedded speculative context"},{"paperId":"969ae2aef666cafb1c4eed2707d6206caacfd88e","title":"Supporting Medical Relation Extraction via Causality-Pruned Semantic Dependency Forest"},{"paperId":"6a9602743366fab44e9fccddc5a4d02068c379ad","title":"Development and Validation of a Deep Learning Model for Predicting Treatment Response in Patients With Newly Diagnosed Epilepsy."},{"paperId":"0230c06c771bb6411614d685432ef076927e66f4","title":"Extracting Biomedical Factual Knowledge Using Pretrained Language Model and Electronic Health Record Context"},{"paperId":"9d2ad340318c2adda7ce68789678a012e4436986","title":"Application of Deep Learning in Generating Structured Radiology Reports: A Transformer-Based Technique"},{"paperId":"4cd4e271bf5cc365a016c312b7c8e66c5ed926e0","title":"POPDx: an automated framework for patient phenotyping across 392 246 individuals in the UK Biobank study"},{"paperId":"4ecbab28398acee9a5ddef3593f07338771152be","title":"Text mining of CHO bioprocess bibliome: Topic modeling and document classification"},{"paperId":"a28f9187b8c213182bdcbfaeaa4d4f04ae6366dd","title":"Review of Natural Language Processing in Pharmacology"},{"paperId":"ba14f67834ae93f0ab67d4c7acdf89afc84ab248","title":"#ChronicPain: Automatic establishment of a chronic pain cohort from social media using machine learning for studying opioid-alternative therapies"},{"paperId":"458fe4d368576278294fc80e55a60f2dbd1c4cb8","title":"Holistic Approach for Artificial Intelligence Implementation in Pharmaceutical Products Lifecycle: A Meta-Analysis"},{"paperId":"bb4f0b15b69d15b0c1616a32191c03f8294c3082","title":"Research on Medical Text Classification based on BioBERT-GRU-Attention"},{"paperId":"0175659d3da0c56753fac44ab646af76ce7220ff","title":"Enriching Pre-Trained Language Model with Multi-Task Learning and Context for Medical Concept Normalization"},{"paperId":"61ff8bd9e68401c8ed9617600b1f671bde0e6d05","title":"Extracting Medication Changes in Clinical Narratives using Pre-trained Language Models"},{"paperId":"a36e9583a0212c11020eefa1d9574512ad9964a4","title":"A pre-trained BERT for Korean medical natural language processing"},{"paperId":"381910542941f93fd59f1aac29384d6f711a35b7","title":"Entity Anchored ICD Coding"},{"paperId":"7c206d541552a57b4c0cfe5b5113d3cc8e31df01","title":"IMSE: interaction information attention and molecular structure based drug drug interaction extraction"},{"paperId":"d132e80f80ce0a65733f57464354b989a8209929","title":"Chemical named entity recognition in the texts of scientific publications using the naïve Bayes classifier approach"},{"paperId":"8ef7b433bb78a5a47595c0c756054d01459e2248","title":"Improving medical experts’ efficiency of misinformation detection: an exploratory study"},{"paperId":"c75b1fb2348c0f54259319b3595ececaf1d98430","title":"Medical terminology-based computing system: a lightweight post-processing solution for out-of-vocabulary multi-word terms"},{"paperId":"be6277ac11b91d7970f8b4f064bfe2b02270ec45","title":"A machine learning framework for discovery and enrichment of metagenomics metadata from open access publications"},{"paperId":"7504aeee4c344c4cf9c6fc071dcc4b4b34d124cc","title":"Non-Contrastive Self-Supervised Learning for Utterance-Level Information Extraction From Speech"},{"paperId":"f3d7789c627d3e62d92c225a272e408f287c6317","title":"Non-Contrastive Self-Supervised Learning of Utterance-Level Speech Representations"},{"paperId":"43a7b2a3a0259bdbf9ace5954f23c697d69d5b86","title":"An Embarrassingly Easy but Strong Baseline for Nested Named Entity Recognition"},{"paperId":"8d4c9f61fb0c31b7239a6bbe908b58bcf6f62915","title":"A Multimodal Transformer: Fusing Clinical Notes with Structured EHR Data for Interpretable In-Hospital Mortality Prediction"},{"paperId":"53510eb719df9e79b2e5bfad8b7af84d2abc202d","title":"Biomedical Named Entity Recognition Using Transformers with biLSTM + CRF and Graph Convolutional Neural Networks"},{"paperId":"3fb69a33340c1968f2ca1b3cf5d30fe1d370e532","title":"Applications of natural language processing in ophthalmology: present and future"},{"paperId":"f1653d36f223becc395cb726c55886bdaa27ece7","title":"Semantic taxonomy enrichment to improve business text classification for dynamic environments"},{"paperId":"4b454d73088451200192b34ecaacd80006806eb5","title":"An Italian lexicon-based sentiment analysis approach for medical applications"},{"paperId":"10a69d0b1ebdf04fa079a4245609f21a245806dd","title":"ArcheGEO"},{"paperId":"032496954f7d221b5d2b0ded8f0c9b628fc49ade","title":"Using natural language processing on free-text clinical notes to identify patients with long-term COVID effects"},{"paperId":"69c791a3ec196f0b046758581c472507482469cd","title":"Introducing Improved Transformer to Land Cover Classification Using Multispectral LiDAR Point Clouds"},{"paperId":"c2aae385224b907ae141fe1d7d52f3a6097e970b","title":"ArcheGEO: towards improving relevance of gene expression omnibus search results"},{"paperId":"f7b0bc71bc810440152f412371796bf7a52ee340","title":"Multi-label classification of symptom terms from free-text bilingual adverse drug reaction reports using natural language processing"},{"paperId":"12d04565528d51c3d841aea1fe16b59dc7455ecd","title":"Using language models and ontology topology to perform semantic mapping of traits between biomedical datasets"},{"paperId":"2bb16e81db12bd36242531b97387ac9a8f58f593","title":"DeepProphet2 - A Deep Learning Gene Recommendation Engine"},{"paperId":"570cae4358f8e25d0adb75ab7d5c2a319b9931c2","title":"Joint Learning-based Causal Relation Extraction from Biomedical Literature"},{"paperId":"07c72a83bbc51c30316751d2997c1080360102de","title":"Identification of bacteriophage genome sequences with representation learning"},{"paperId":"67b684626ba5ff1d54003dad7e0871be8275ed67","title":"Beyond word embeddings: A survey"},{"paperId":"e962e3acf17529002ad1aa95c38499a2667f13ae","title":"YTLR: Extracting yeast transcription factor-gene associations from the literature using automated literature readers"},{"paperId":"378831c67fe5f79d91fe3421ac37c044704a80ae","title":"Retraining a BERT Model for Transfer Learning in Requirements Engineering: A Preliminary Study"},{"paperId":"19f39313208d15c318a219664de78dd3cee52f2b","title":"AsthmaKGxE: An asthma-environment interaction knowledge graph leveraging public databases and scientific literature"},{"paperId":"59e95b0a0a13ef5ac321b7e7de2c86e6dafaddf2","title":"Accurately Identifying Cerebroarterial Stenosis from Angiography Reports Using Natural Language Processing Approaches"},{"paperId":"041e3960ba823d8370cddf4fda6704de185259b4","title":"Development and Validation of Machine Models Using Natural Language Processing to Classify Substances Involved in Overdose Deaths"},{"paperId":"0e1033d886d32ea37ed82ef08bb84a294c5da828","title":"On modeling and utilizing chemical compound information with deep learning technologies: A task-oriented approach"},{"paperId":"889a23cfc57dbef77d921fdc6a5240201ac7edcb","title":"DrNote: An open medical annotation service"},{"paperId":"e3820a16e3c242a0fb30c389da873edf9136ae91","title":"NeuroCORD: A Language Model to Facilitate COVID-19-Associated Neurological Disorder Studies"},{"paperId":"c98b20c695f921451b8953fd74e748dc803febc4","title":"Natural Language Processing in Pathology: Current Trends and Future Insights."},{"paperId":"8919a186c717e7298798ac838daa49ea1654a79f","title":"How do Patent Claims Reflect Scientific Knowledge : Insights from Human Genomics"},{"paperId":"0fae17f1cf72963694ff3f06065f7ebd12b6565d","title":"Domain Model Extraction from User-authored Scenarios and Word Embeddings"},{"paperId":"2b2cf91d12f0928a8d1c37e3f95a2c20d194e3f8","title":"Year 2021: COVID-19, Information Extraction and BERTization among the Hottest Topics in Medical Natural Language Processing"},{"paperId":"34f91cc1696071a7dab8e141b2c8a6cf62e88a19","title":"A multi-layer soft lattice based model for Chinese clinical named entity recognition"},{"paperId":"69767adf5b48e7c1f570b34af31d0f3c50e91ff1","title":"Automatic text classification of actionable radiology reports of tinnitus patients using bidirectional encoder representations from transformer (BERT) and in-domain pre-training (IDPT)"},{"paperId":"6b610232f4ae3a7cc764599b8bfb71c45629b0df","title":"Using Multi-modal Data for Improving Generalizability and Explainability of Disease Classification in Radiology"},{"paperId":"6e9636f1195770a187e40213cd028885c2f913c6","title":"Prediction of biomarker-disease associations based on graph attention network and text representation"},{"paperId":"079dac66f13edc1afa80e64dffe7910e6907d266","title":"Knowledge-Driven Mechanistic Enrichment of the Preeclampsia Ignorome"},{"paperId":"483733446c603be75f24985716c452c472b3b4ef","title":"Calibrating a Transformer-Based Model’s Confidence on Community-Engaged Research Studies: Decision Support Evaluation Study"},{"paperId":"b6fce90925c977ab1f98f4db9487596c9595f97b","title":"Learning structures of the French clinical language: development and validation of word embedding models using 21 million clinical reports from electronic health records"},{"paperId":"3c49a23f72f6337ee68d57911e211fcbbdca6091","title":"Fine-Tuning BERT for Automatic ADME Semantic Labeling in FDA Drug Labeling to Enhance Product-Specific Guidance Assessment"},{"paperId":"65b492af374605ebf62484e06a62bc14a128e868","title":"ChanFAD: A Functional Annotation Database for Ion Channels"},{"paperId":"1c9664b78ed9ea461f8e68ea2637383c4acb4e4d","title":"Utilizing Deep Learning for Detecting Adverse Drug Events in Structured and Unstructured Regulatory Drug Data Sets"},{"paperId":"b18619ca04606f532dd5b08bbce1682174a5ad59","title":"Assessing mortality prediction through different representation models based on concepts extracted from clinical notes"},{"paperId":"cb60fd88468ee02465ea81d3af73e5c9de79a7ae","title":"Matching Biomedical Ontologies via a Hybrid Graph Attention Network"},{"paperId":"ca3d845ee518365ca73a3f7a7143c2a1875591a7","title":"Multi-Level Fine-Tuning, Data Augmentation, and Few-Shot Learning for Specialized Cyber Threat Intelligence"},{"paperId":"06ea568379211ffa07d9605f66f26f6f736ea5e0","title":"PanGu-Coder: Program Synthesis with Function-Level Language Modeling"},{"paperId":"b311dbc8e8869c040a3866bb1844870b1b009a37","title":"Automatic question answering for multiple stakeholders, the epidemic question answering dataset"},{"paperId":"c962afecd74910dc607f64eecfe33cc474126da1","title":"Enhancing Cross-lingual Biomedical Concept Normalization Using Deep Neural Network Pretrained Language Models"},{"paperId":"2ac3bacbbee520b701707ebcf7b9ca7a3f233129","title":"Medical visual question answering via corresponding feature fusion combined with semantic attention."},{"paperId":"e85f6948e7906ec974134ec443f26064d9fe3d95","title":"Transforming unstructured digital clinical notes for improved health literacy"},{"paperId":"e5fd2317c0ac51ecf947d0f40ab715a78a0fe052","title":"Multilingual Transformer Encoders: a Word-Level Task-Agnostic Evaluation"},{"paperId":"e6b4f085a1e9f1c0c6ea37d89f6bb683526d60d5","title":"3Rs: Data Augmentation Techniques Using Document Contexts For Low-Resource Chinese Named Entity Recognition"},{"paperId":"c0a85e549131133bc6fc512f7001d8cb897f8e9c","title":"Fine-tuning Strategies for Classifying Community-Engaged Research Studies Using Transformer-Based Models: Algorithm Development and Improvement Study"},{"paperId":"df0554828bb4056b488e2a7164c024cd9dc96974","title":"An Annotated PubMed Corpus to Support Supervised Relation Extraction between Suicide-Related Entities and Drugs (Preprint)"},{"paperId":"124804b9ab1fcae6ca114902d828216de6c83a06","title":"Bi-PointFlowNet: Bidirectional Learning for Point Cloud Based Scene Flow Estimation"},{"paperId":"87cb212eeb40171b32a4411a6919e22c36822140","title":"Design of a Novel Information System for Semi-automated Management of Cybersecurity in Industrial Control Systems"},{"paperId":"9153fa1e824c556118b29f3d85658dda20c7c083","title":"PLM-ICD: Automatic ICD Coding with Pretrained Language Models"},{"paperId":"e9b06dfbad49a1c573af09a8a6e430a008f973cc","title":"Analyzing Transfer Learning of Vision Transformers for Interpreting Chest Radiography"},{"paperId":"22f4412f9fa0bcd2fabb72f2a7ef81ccc82f566b","title":"Developing an NLP-based Recommender System for the Ethical, Legal, and Social Implications of Synthetic Biology"},{"paperId":"889030bdb7e96d0abfb7c00d5c084d8b46c5671e","title":"Study of Question Answering on Legal Software Document using BERT based models"},{"paperId":"3f3a62d754add66350cb71319927de4714a252e6","title":"Win-Win Cooperation: Bundling Sequence and Span Models for Named Entity Recognition"},{"paperId":"d0b3c35f736391d68a193a4c3dbf1724697dcef7","title":"Natural language processing for clusterization of genes according to their functions"},{"paperId":"6cf8a4d05e66266233380f989edaf647eba7e1a5","title":"BioTABQA: Instruction Learning for Biomedical Table Question Answering"},{"paperId":"38c1a356684d4f7f5c579898fad257b7f102ea99","title":"Knowledge Graph and Deep Learning-based Text-to-GraphQL Model for Intelligent Medical Consultation Chatbot"},{"paperId":"31727aa453b1b83f0639e7c6bead508b40abb5d6","title":"Interpreting Patient Descriptions using Distantly Supervised Similar Case Retrieval"},{"paperId":"36aae858e5c57aaeb46ee484e86b0b5a738f3caf","title":"Do We Need a Specific Corpus and Multiple High-Performance GPUs for Training the BERT Model? An Experiment on COVID-19 Dataset"},{"paperId":"7b19b0b4e5e6278ea9475cb97c52ebcc209f7f50","title":"State-of-the-Art Evidence Retriever for Precision Medicine: Algorithm Development and Validation"},{"paperId":"d49e46606348b131d8352d40005337ab7d9c6872","title":"Learning Feature Recovery Transformer for Occluded Person Re-Identification"},{"paperId":"3ac52f3e6f185ef61bd35b74ae8a5c2bde4d1997","title":"Shifting machine learning for healthcare from development to deployment and from models to data"},{"paperId":"14efaa989b8db89ad907e6a15253d482cbba77d9","title":"Linguistically inspired roadmap for building biologically reliable protein language models"},{"paperId":"85eaaa985b184e7e0fd4f3eb2138e1cf20107434","title":"Enhancing Automated Software Traceability by Transfer Learning from Open-World Data"},{"paperId":"6a154672fa2efe952137208e0242d27da3136692","title":"A Biomedical Pipeline to Detect Clinical and Non-Clinical Named Entities"},{"paperId":"3e0f229c677cec20a4e5c31d99b5dd628a97b60b","title":"A comparative study on deep learning models for text classification of unstructured medical notes with various levels of class imbalance"},{"paperId":"e520b7cdbfd2e9548ddd0ada2fc686816e33da24","title":"Classifying the lifestyle status for Alzheimer’s disease from clinical notes using deep learning with weak supervision"},{"paperId":"f4fcafa3b415ff023f3ab45fbb986bd33dac4f18","title":"AgriBERT: Knowledge-Infused Agricultural Language Models for Matching Food and Nutrition"},{"paperId":"ee44e1ffe88d64fbf3a9451e47d5b23b50e71772","title":"Gated tree-structured RecurNN for Detecting Biomedical Event Trigger"},{"paperId":"d614607eef5ea32811023da07b3731e4396348f3","title":"Improving Low-Resource Speech Recognition with Pretrained Speech Models: Continued Pretraining vs. Semi-Supervised Training"},{"paperId":"795e78f020b4922f1648d488ecc03b9a0ea726ac","title":"Multi-attention deep neural network fusing character and word embedding for clinical and biomedical concept extraction"},{"paperId":"c8cb3098b753f0472ede12af9b582e091ecfea5c","title":"NILINKER: Attention-based approach to NIL Entity Linking"},{"paperId":"6015a06afa659fa496a2a5f1301ec38d4b8da868","title":"Visual and buying sequence features-based product image recommendation using optimization based deep residual network."},{"paperId":"c796b23df835d9b7a1d01094c67dac2be034e7e9","title":"A Syntax-enhanced model based on category keywords for biomedical relation extraction"},{"paperId":"3f3487f7918ccccc5c9b5a9f6fb449a3b7f4deac","title":"BERT-Promoter: An improved sequence-based predictor of DNA promoter using BERT pre-trained model and SHAP feature selection"},{"paperId":"582b051cdf46525803299e63a1c7b09dad409c3c","title":"Call for papers: Semantics-enabled biomedical literature analytics"},{"paperId":"f1d8dbe4e217d2221c5276f1e0c616ba5f82d1d3","title":"A review on Natural Language Processing Models for COVID-19 research"},{"paperId":"ae4acc595d460358c0ee44afc4483f2d9c5cb766","title":"Pre-trained language models with domain knowledge for biomedical extractive summarization"},{"paperId":"bb15f7c8c01635a42c27ab1f2f842f9b6b2b43d4","title":"Multiview Incomplete Knowledge Graph Integration with application to cross-institutional EHR data harmonization"},{"paperId":"14d38a8f1fc61b243eb8cbe6f415074000c1b15c","title":"Natural Language Processing in Diagnostic Texts from Nephropathology"},{"paperId":"7677aa43ea685ff396a9194b98a42c3f299d69f6","title":"Enhancing Entity Representations with Prompt Learning for Biomedical Entity Linking"},{"paperId":"3d7584711440c9fe879675977d37be565b090ad6","title":"A Comprehensive and Holistic Health Database"},{"paperId":"572f6be261fcc457a45bc2ece174347fbb4a94b1","title":"A Drug Repositioning Approach Using Drug and Disease Features"},{"paperId":"e680e6e5ce5c886cc84b1ef7d0757154d9937212","title":"Deep learning to extract Breast Cancer diagnosis concepts"},{"paperId":"fc0a33256909b0d9c76288d1978d52453580c1d4","title":"MinpakuBERT: A Language Model for Understanding Cultural Properties in Museum"},{"paperId":"c47bc7d845f707933f240c85e88a6f3dac19842b","title":"Analyzing the Structure of U.S. Patents Using Patent Families"},{"paperId":"8d834506eced6f7e8aaadbbf5bf111309fd9c2b2","title":"Similarity Measures Based on Multi-knowledge Integration"},{"paperId":"216cb67823c5dc727871cb846a438e91f5a8cb5b","title":"MULTICLASS CLASSIFICATION OF SCIENTIFIC TEXTS WRITTEN IN TURKISH BY APPLYING DEEP LEARNING TECHNIQUE"},{"paperId":"ee9f79f0f652011f7df2548b8fca62cb5c48ba66","title":"Trial2Vec: Zero-Shot Clinical Trial Document Similarity Search using Self-Supervision"},{"paperId":"096748b7210afca47587f4845e67680da858ba6a","title":"Note: Using Causality to Mine Sjögren’s Syndrome related Factors from Medical Literature"},{"paperId":"0af095ab3f45a665699162a3656b1603bc359e68","title":"GERNERMED++: Transfer Learning in German Medical NLP"},{"paperId":"a32be54b31a91036eaaadf6e39d62108508710fa","title":"Multi-probe attention neural network for COVID-19 semantic indexing"},{"paperId":"c76779e705b005cb87241f490fb0957baafb7005","title":"Enhancing medical-imaging artificial intelligence through holistic use of time-tested key imaging and clinical parameters: Future insights"},{"paperId":"e015e07c76ff85f624b78112fd58937221f5ea0c","title":"Clinical-BERT: Vision-Language Pre-training for Radiograph Diagnosis and Reports Generation"},{"paperId":"acb5ba438e005f407fd24aa6bb4bf5e3c67c8ea1","title":"Contextual embedding and model weighting by fusing domain knowledge on biomedical question answering"},{"paperId":"0787cee29a1a3c592f7c20ec5bc67ceffaf452bb","title":"Analyzing Twitter Conversations on Side Effects of Covid-19 Vaccine"},{"paperId":"3f0ff7887c6d922c4310dd8c92de4f07fa2612ae","title":"TabText: a Systematic Approach to Aggregate Knowledge Across Tabular Data Structures"},{"paperId":"e9480d62e216f77d5556b7eda769daa4c92d004d","title":"VQAMix: Conditional Triplet Mixup for Medical Visual Question Answering"},{"paperId":"b6856133bfebe68e26b06ff469a6c65278e3d1aa","title":"Fine-tuned Sentiment Analysis of COVID-19 Vaccine–Related Social Media Data: Comparative Study"},{"paperId":"b52e681c39d31e6abf58ad1f5d9709e2dce12e1f","title":"Automatic data extraction to support meta-analysis statistical analysis: a case study on breast cancer"},{"paperId":"905e1376e26cfcb3f9c8abe3f7c970caee952938","title":"Network approaches for modeling the effect of drugs and diseases"},{"paperId":"eab6381a34c128abb5246348898d4c6df45c8fa6","title":"Theoretical Perspectives on Terminology"},{"paperId":"55976ee561064f301fcfce26356a6da9e062494c","title":"An Accuracy-Maximization Approach for Claims Classifiers in Document Content Analytics for Cybersecurity"},{"paperId":"48078f2561917a8c64fee24e86185eba796a7ce9","title":"RadBERT: Adapting Transformer-based Language Models to Radiology."},{"paperId":"94cc16a0d283fb7bc98cf4e6b152687587b63558","title":"Task Transfer and Domain Adaptation for Zero-Shot Question Answering"},{"paperId":"be7aa2416c28d25bdbcd7666b02131a55ca869c0","title":"Knowledge Graph Construction and Its Application in Automatic Radiology Report Generation from Radiologist's Dictation"},{"paperId":"62f488d3178ce448c9916caf131b83d22ff5d5ff","title":"Comparison of text preprocessing methods"},{"paperId":"b4354536b8259c147301cdc4468aa8fa67432437","title":"How can natural language processing help model informed drug development?: a review"},{"paperId":"f37efd3d94f1bffc28951e073b27f4be06505de0","title":"PreQR: Pre-training Representation for SQL Understanding"},{"paperId":"27bf606c781a3db61099427a393ae3bab950b629","title":"Graph-in-Graph Network for Automatic Gene Ontology Description Generation"},{"paperId":"04a90a79bdfe2f9d54811141c104a4c12f74c07a","title":"SsciBERT: a pre-trained language model for social science texts"},{"paperId":"ff998672ac14ced3e53fb658c237f0b610a85229","title":"Discovering Content through Text Mining for a Synthetic Biology Knowledge System."},{"paperId":"306519f049fe470a2380326f7d52f813b0723be2","title":"Iterative Annotation of Biomedical NER Corpora with Deep Neural Networks and Knowledge Bases"},{"paperId":"1eeb28e450772ee0ab0adcd60056480faf99d150","title":"Training Subset Selection for Weak Supervision"},{"paperId":"24d942c45f8fbf9a7d75c55c2ac8f48477cd1d5b","title":"Adding an Attention Layer Improves the Performance of a Neural Network Architecture for Synonymy Prediction in the UMLS Metathesaurus"},{"paperId":"fb5855e68b029f1391bea0cc4d8b1ca1e7335bb2","title":"BERT Models for Arabic Text Classification: A Systematic Review"},{"paperId":"f695e38bdef6dbd2bdcf80faa6359c0e7918d729","title":"Taxonomy of machine learning paradigms: A data‐centric perspective"},{"paperId":"421b8fc8c0058cefede7f4bf5421494aee235b21","title":"Combining Attention-based Models with the MeSH Ontology for Semantic Textual Similarity in Clinical Notes"},{"paperId":"b07bfd5cac5d2f35f2a97d29ed1270183ed125a6","title":"Geoscience language models and their intrinsic evaluation"},{"paperId":"fd5921e058932d406a39cc83c59958d277cefd06","title":"Multilabel classification of medical concepts for patient clinical profile identification"},{"paperId":"5a0cb34032209fed9c18e538c25f5d6515584248","title":"Accelerated variant curation from scientific literature using biomedical text mining"},{"paperId":"6b866e2516ad95b5db30ba2eb2e31d0589f70eb2","title":"Discovering novel drug-supplement interactions using SuppKG generated from the biomedical literature"},{"paperId":"8971ac20865868c18bcba441ed0a37a33d6b02a4","title":"A Decade's Experience in Pediatric Chromosomal Microarray Reveals Distinct Characteristics Across Ordering Specialties."},{"paperId":"6f3e7b7b6fffd08ee7cb63aaaf5777b232f4bd02","title":"Historical profile will tell? A deep learning-based multi-level embedding framework for adverse drug event detection and extraction"},{"paperId":"0f055c1fbcb0942c1d6d75fdf1452110db4cf210","title":"Automatic International Classification of Diseases Coding System: Deep Contextualized Language Model With Rule-Based Approaches"},{"paperId":"8c9a9a1bbba2a3e3bab34bce533b3b2acfda32b0","title":"Medical visual question answering based on question-type reasoning and semantic space constraint"},{"paperId":"fa73acfcaa2e7eb08623f129420c716048baf4bf","title":"Graph-Augmented Cyclic Learning Framework for Similarity Estimation of Medical Clinical Notes"},{"paperId":"102c3436913d659a5c34518a916f2078a1fad7db","title":"Developing Pretrained Language Models for Turkish Biomedical Domain"},{"paperId":"848852ef5953643b6524e1ee1b73905ab74fa4de","title":"Improving Sentence Classification in Abstracts of Randomized Controlled Trial using Prompt Learning"},{"paperId":"e0aceea5c0d00e602eb0179f6bd6a3a32bb8100a","title":"FinBERT-MRC: financial named entity recognition using BERT under the machine reading comprehension paradigm"},{"paperId":"3e03e8cf0fd49ebfd7947fcf3e9cd4dd058e4676","title":"SeSG: a search string generator for Secondary Studies with hybrid search strategies using text mining"},{"paperId":"e3c429d5a2703b0ac2608c0a2f7bf5b6bdf5ae94","title":"Fast medical concept normalization for biomedical literature based on stack and index optimized self-attention"},{"paperId":"00cfcae8a594b23ef33b458e34b9b82452d0e2cc","title":"Sparse Conditional Hidden Markov Model for Weakly Supervised Named Entity Recognition"},{"paperId":"7d8b4c9bd3938868b815e79ea8d9af946615ee96","title":"Multi-model Comparison for Classification of Medical Records using the BioBERT Models"},{"paperId":"f6f51bedd048139dd70b98c80fe47f0077b9bffb","title":"Clinical Dialogue Transcription Error Correction using Seq2Seq Models"},{"paperId":"6fa3ce7831ab75ee784428ae8e3a0b7372651e48","title":"Plant phenotype relationship corpus for biomedical relationships between plants and phenotypes"},{"paperId":"f7b96c7075ffb4551a1d79a57d055c4ab4a2b2b2","title":"Sparse*BERT: Sparse Models Generalize To New tasks and Domains"},{"paperId":"686d9ee744fa013cc21cdd86acd864c936e9e456","title":"Large language models are few-shot clinical information extractors"},{"paperId":"e35c16ef1b097efe321f3d57768d72007f911af8","title":"Natural Language Processing for Information Extraction of Gastric Diseases and Its Application in Large-Scale Clinical Research"},{"paperId":"24ecf000f9a2ea678eb29a6a720f36ad91a305c5","title":"K-12BERT: BERT for K-12 education"},{"paperId":"9ebd20a621ea9f458d4919c8e247932ddc7664fd","title":"The Diminishing Returns of Masked Language Models to Science"},{"paperId":"f420c9cf9aac2d3f0e0b358af89c29eafb42849a","title":"Development of a phenotype ontology for autism spectrum disorder by natural language processing on electronic health records"},{"paperId":"e257f213cbe37f78375ad4f25566840b5cfa04b7","title":"Artificial intelligence for topic modelling in Hindu philosophy: Mapping themes between the Upanishads and the Bhagavad Gita"},{"paperId":"4962ce1242ff905b71e199964eafea3d2ad9688d","title":"A Domain-adaptive Pre-training Approach for Language Bias Detection in News"},{"paperId":"2b3b2d8849aacfa66724f92bdf3ef7f567d94a51","title":"Discovering trends and hotspots of biosafety and biosecurity research via machine learning"},{"paperId":"99be6dc17a7fd399f4af80c4c1cd7ee5247591a1","title":"Pre-training Data Quality and Quantity for a Low-Resource Language: New Corpus and BERT Models for Maltese"},{"paperId":"74db6506ee770b25ed1f940aa9d9e2a8058d0590","title":"Identification and Impact Analysis of Family History of Psychiatric Disorder in Mood Disorder Patients With Pretrained Language Model"},{"paperId":"b3bc37a15aa74c523d656ad89b1896651f5eef72","title":"Continual Pre-Training Mitigates Forgetting in Language and Vision"},{"paperId":"088cd170f3067de026fe2665782f639e64c6f8ad","title":"A reproducible experimental survey on biomedical sentence similarity: A string-based method sets the state of the art"},{"paperId":"6f0a840579e80067bf774468654d45fe7abf9630","title":"Past and future uses of text mining in ecology and evolution"},{"paperId":"ce14126ffb0999cec9137b5565f4069eacaa4127","title":"A Scalable Workflow to Build Machine Learning Classifiers with Clinician-in-the-Loop to Identify Patients in Specific Diseases"},{"paperId":"a1d5476c1bface0b20e6e943c3f36868186375be","title":"Generic and Trend-aware Curriculum Learning for Relation Extraction"},{"paperId":"b7d2db37e8af746b9324ff4c647123e92b59f5ba","title":"Biomedical Argument Mining Based on Sequential Multi-Task Learning"},{"paperId":"80c732ae7adf7eb222e51fbd3ed2cb3b5a8037c5","title":"Automatically Identifying Twitter Users for Interventions to Support Dementia Family Caregivers: Annotated Data Set and Benchmark Classification Models"},{"paperId":"55db77c96e57e692f5bbee3083e00720b438fa71","title":"PTM4Tag"},{"paperId":"77cb1ca1485f88f4061570dc999524da339863af","title":"Hero-Gang Neural Model For Named Entity Recognition"},{"paperId":"e86eb6ee7daa5e6aa46f1538c6fc503ef9cb688d","title":"The Case of Aspect in Sentiment Analysis: Seeking Attention or Co-Dependency?"},{"paperId":"8e8110cc601eb35f9f2e33528981a3fbd7528ffb","title":"PathologyBERT - Pre-trained Vs. A New Transformer Language Model for Pathology Domain"},{"paperId":"468bfd9971b9712d2538e7825723925dd2f2dbbd","title":"neoMS: Attention-based Prediction of MHC-I Epitope Presentation"},{"paperId":"6ee0a4bc4241b71610337e552f1dbf632791a4da","title":"Building Better Machine Learning Models for Rhetorical Analyses: The Use of Rhetorical Feature Sets for Training Artificial Neural Network Models"},{"paperId":"a1a1c3f60f8b8adca96edbb1fa9b2b77bc1b17b5","title":"ScAN: Suicide Attempt and Ideation Events Dataset"},{"paperId":"3dc771a2f49cf8bfa30a0076ed99b8b6aca9eec9","title":"Automated medical chart review for breast cancer outcomes research: a novel natural language processing extraction system"},{"paperId":"1b4d785f4f57f1dae5445c7192dd3663bb0c48c3","title":"Multi-domain adaptation for named entity recognition with multi-aspect relevance learning"},{"paperId":"c51950111fc29ac83be64a5e220d93b202dc07ef","title":"Construction of an Assisted Model Based on Natural Language Processing for Automatic Early Diagnosis of Autoimmune Encephalitis"},{"paperId":"fa7e728c4c612025b9fb7601af65c4a8f5a2b33e","title":"Clinical Prompt Learning with Frozen Language Models"},{"paperId":"c794a26766cceb4e0c373af0d01d9d129b316cf2","title":"SuMe: A Dataset Towards Summarizing Biomedical Mechanisms"},{"paperId":"6eee5bee8b5b058547f2af164fa54179ece046ba","title":"pubmedKB: an interactive web server for exploring biomedical entity relations in the biomedical literature"},{"paperId":"e75fac9090ac82b75c841ffa8fd468dcb7e7eb4b","title":"Biomedical Relation Extraction With Knowledge Graph-Based Recommendations"},{"paperId":"5cc58bcfb9bf39d4114eab88fca36eb0ce36afd9","title":"Building a knowledge graph to enable precision medicine"},{"paperId":"89d9b122bcd7ddf4bca0ac8c92f94ab4a730c19f","title":"Natural Language Processing of Radiology Reports to Detect Complications of Ischemic Stroke"},{"paperId":"5476911896491f32ca123e7030e9bae1116d8de2","title":"BatteryBERT: A Pretrained Language Model for Battery Database Enhancement"},{"paperId":"a3b7f8bc5bfec9542f946faa03428b0d1e82efa0","title":"AKI-BERT: a Pre-trained Clinical Language Model for Early Prediction of Acute Kidney Injury"},{"paperId":"d061d5e1f8135a1819b1e260cb1efb97757cfc74","title":"Seed-Guided Topic Discovery with Out-of-Vocabulary Seeds"},{"paperId":"9f37278355ae2820a08f4145e476d3499bfef693","title":"A Dataset for N-ary Relation Extraction of Drug Combinations"},{"paperId":"29b7ab3e305b9a57ae81d7f15596cd91a2e99139","title":"Mixed-effects transformers for hierarchical adaptation"},{"paperId":"9439aae01306f8c508b7af8626ea16a3f23024f7","title":"A Library Perspective on Nearly-Unsupervised Information Extraction Workflows in Digital Libraries"},{"paperId":"182e296d349662376d91f2b2ee1f05c5d577ea6e","title":"POLITICS: Pretraining with Same-story Article Comparison for Ideology Prediction and Stance Detection"},{"paperId":"7f369202dfbfc57ae89efccbc54b232cabefe66d","title":"A Novel Patient Similarity Network (PSN) Framework Based on Multi-Model Deep Learning for Precision Medicine"},{"paperId":"92bfb252dccfe950c08c00561d7d19e8cb9561ba","title":"Crude Oil-related Events Extraction and Processing: A Transfer Learning Approach"},{"paperId":"b2431ccff95909db6d40a266c4461bfc6d2d7ec1","title":"Artificial intelligence in COVID-19 evidence syntheses was underutilized, but impactful: a methodological study"},{"paperId":"7bb115c518eab1e3a421b7fa3823159455fadbb6","title":"Evaluating Biomedical Word Embeddings for Vocabulary Alignment at Scale in the UMLS Metathesaurus Using Siamese Networks"},{"paperId":"ccf6a056efa73d83c7b120e2e4e3515e9fbd2543","title":"Discovering Thematically Coherent Biomedical Documents Using Contextualized Bidirectional Encoder Representations from Transformers-Based Clustering"},{"paperId":"49bc72be5ee7085be32c753b8480a2f8d39b5c58","title":"A Deep Learning Approach to Estimate the Incidence of Infectious Disease Cases for Routinely Collected Ambulatory Records: The Example of Varicella-Zoster"},{"paperId":"ee18ec08a0cdfd23e8cb85bb9c2421b88453e314","title":"Artificial Intelligence Based on Machine Learning in Pharmacovigilance: A Scoping Review"},{"paperId":"86dad974d555e8239d1ad3ca62978706e94d14ac","title":"Comparison of state-of-the-art machine and deep learning algorithms to classify proximal humeral fractures using radiology text."},{"paperId":"2c6ac205e11ecb522d990e4a257f907f738c2539","title":"Leveraging fusion of sequence tagging models for toxic spans detection"},{"paperId":"696b682e5bcfaddae4810392293acf591f930477","title":"Utilization of sentiment analysis to assess and compare negative finding reporting in veterinary and human literature."},{"paperId":"0120de89fbd45a2e3e2adf0f64d812afa2f5f130","title":"Scoping review and classification of deep learning in medical genetics."},{"paperId":"7fef58043d0a80af816447a0e440d59627af2ea6","title":"Evaluation of clinical named entity recognition methods for Serbian electronic health records"},{"paperId":"f2b41971a705a69b56279df5e928e4847c693c50","title":"A text mining framework for screening catalysts and critical process parameters from scientific literature - a study on Hydrogen production from alcohol"},{"paperId":"555a689b3771102c94fe0a9f40130bce443ca5d8","title":"Medical Coding with Biomedical Transformer Ensembles and Zero/Few-shot Learning"},{"paperId":"42a093d34ed6c233107dbf613681d3d7ec55ad5a","title":"Machine Learning Approach to Facilitate Knowledge Synthesis at the Intersection of Liver Cancer, Epidemiology, and Health Disparities Research"},{"paperId":"f7e8f369ae12bdcc76eaab4e42839fd36406abc0","title":"Semantics Driven Embedding Learning for Effective Entity Alignment"},{"paperId":"45dfe98c74c3f65a4a0349ece03c6f12914a6217","title":"Named Entity Recognition for Pet Disease Q&A System"},{"paperId":"e77078d5ddcaf5c76fe1c291c2b7cbc7518afc23","title":"Improving Biomedical Question Answering by Data Augmentation and Model Weighting"},{"paperId":"87ce562e937dacc3f8a6efaab93f4f11fa0fda6a","title":"BPI-MVQA: a bi-branch model for medical visual question answering"},{"paperId":"c378bbcc7d339809642ff7c2b202ea034c60cc11","title":"Knowledge integration and decision support for accelerated discovery of antibiotic resistance genes"},{"paperId":"1fafaccebc4a74898a74c606f846318c4c2c7536","title":"On the Effect of Pretraining Corpora on In-context Learning by a Large-scale Language Model"},{"paperId":"db2bd466953f3ea49280988e1659b6ac3f639e45","title":"Cross-modal Memory Networks for Radiology Report Generation"},{"paperId":"61843c4915c1cf633069b04e4628545cfffb5660","title":"BiTimeBERT: Extending Pre-Trained Language Representations with Bi-Temporal Information"},{"paperId":"0d1aacf97cb67c6b4af9f7297f6affbdf559caa8","title":"UBERT: A Novel Language Model for Synonymy Prediction at Scale in the UMLS Metathesaurus"},{"paperId":"513d947c064694ecb781d6b1cbe51dacee7bb379","title":"Machine learning approaches for electronic health records phenotyping: A methodical review"},{"paperId":"c3637ae891321c0b97e90d9c526a55e603a77eb4","title":"HerbKG: Constructing a Herbal-Molecular Medicine Knowledge Graph Using a Two-Stage Framework Based on Deep Transfer Learning"},{"paperId":"0b2a24ab36d9b30c2dc2472421ffa02a5a25b2a4","title":"SkillSpan: Hard and Soft Skill Extraction from English Job Postings"},{"paperId":"7e68adf6c97a6279a08831f563b5be46df1fe874","title":"Propose-and-Refine: A Two-Stage Set Prediction Network for Nested Named Entity Recognition"},{"paperId":"623fa1d6f3389fc081c43dacf2c16b43150b2989","title":"German Medical NER Model and Dataset Creation using Machine Translation and Word Alignment: Feasibility Study (Preprint)"},{"paperId":"3f200791d33165673740a3224a0afc5daf36f387","title":"Attention mechanism in neural networks: where it comes and where it goes"},{"paperId":"b7967741deb2410e58f01cadf924381a6b0d601c","title":"PhenoBERT: A Combined Deep Learning Method for Automated Recognition of Human Phenotype Ontology"},{"paperId":"3842b66b82bd9b8e6a220c84ab15c026b404d767","title":"CoVERT: A Corpus of Fact-checked Biomedical COVID-19 Tweets"},{"paperId":"ee68b72ecef019adef17ae0cd375ab5d9659062d","title":"Using Machine Learning to Fuse Verbal Autopsy Narratives and Binary Features in the Analysis of Deaths from Hyperglycaemia"},{"paperId":"dc08b6ce4a6d922d550ae5d452590524dce82f41","title":"Detecting Regulation Violations for an Indian Regulatory Body through Multi Label Classification"},{"paperId":"ee7a2e19af5f0497d305f31005dc14b549b7883c","title":"Unsupervised Representation Learning of Player Behavioral Data with Confidence Guided Masking"},{"paperId":"5516fca857d319c689879eb5d9c00264d192f112","title":"Novel Artificial Intelligence Applications in Cardiology: Current Landscape, Limitations, and the Road to Real-World Applications"},{"paperId":"4fd46afb416a8a3bd24c664bd4e21c4c90fdd200","title":"Hierarchical Label-wise Attention Transformer Model for Explainable ICD Coding"},{"paperId":"6ae63be462816bdd0da32894985d16f1e8cbdd5c","title":"KALA: Knowledge-Augmented Language Model Adaptation"},{"paperId":"fba2b199350acea210cd7dc9dc1599f814e131fe","title":"An Attention-Based Model for Predicting Contextual Informativeness and Curriculum Learning Applications"},{"paperId":"d23fd0226c4355fc954aacfc7036d88b5d8a384e","title":"Recovering Patient Journeys: A Corpus of Biomedical Entities and Relations on Twitter (BEAR)"},{"paperId":"3b60fa113f9d448eb17d855173b24f30d105aed8","title":"Making the Most of Text Semantics to Improve Biomedical Vision-Language Processing"},{"paperId":"5b1fe75845271debc984c5e347b9d8fb486e28d3","title":"Leveraging Part-of-Speech Tagging Features and a Novel Regularization Strategy for Chinese Medical Named Entity Recognition"},{"paperId":"9727fa4acdc5312ff86745875ef3db8578f153ac","title":"Decorate the Examples: A Simple Method of Prompt Design for Biomedical Relation Extraction"},{"paperId":"18504ce364a5e7dc7c90a855e22b1635a62e8e72","title":"ICDBigBird: A Contextual Embedding Model for ICD Code Classification"}],"references":[{"paperId":"31fe8afa6531400e3b76982a3984c7e2605074f8","title":"Document-level attention-based BiLSTM-CRF incorporating disease dictionary for disease named entity recognition"},{"paperId":"045d4883c85036948b4a4956001205996094be64","title":"The cell line ontology-based representation, integration and analysis of cell lines used in China"},{"paperId":"2a567ebd78939d0861d788f0fedff8d40ae62bf2","title":"Publicly Available Clinical BERT Embeddings"},{"paperId":"d08be34cb90718b331aa6574dc80b0370ca5895f","title":"A Silver Standard Corpus of Human Phenotype-Gene Relations"},{"paperId":"e65715d20baac11d0a53fd7107de18cb7f67e775","title":"MedMentions: A Large Biomedical Corpus Annotated with UMLS Concepts"},{"paperId":"8b13ef2ac652221715259b85f792c49ae14c7dd7","title":"Clinical Concept Extraction with Contextual Word Embedding"},{"paperId":"f8b901c330e7f946ef93453b24682f294b8764a1","title":"In-domain Context-aware Token Embeddings Improve Biomedical Named Entity Recognition"},{"paperId":"1b6fb32ea70614b134517d591133e05bebdca9a6","title":"CollaboNet: collaboration of deep neural networks for biomedical named entity recognition"},{"paperId":"cc09d65183d3b986d1477df8584c96ecfee2b184","title":"Automatic extraction of gene-disease associations from literature using joint ensemble learning"},{"paperId":"2966e82ec5f89f23ec7636acc00c9ee74d491968","title":"Chemical–gene relation extraction using recursive neural network"},{"paperId":"fa4e9eb4020ef7152adbd62906d605d63236ef37","title":"Transfer learning for biomedical named entity recognition with neural networks"},{"paperId":"dbcceca38e09de4a69ff2920e2fe3efc90511fa5","title":"An attention‐based BiLSTM‐CRF approach to document‐level chemical named entity recognition"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","title":"Deep Contextualized Word Representations"},{"paperId":"418cd8e078ed9b91b08e7915ac6753ed811053e4","title":"Cross-type Biomedical Named Entity Recognition with Deep Multi-Task Learning"},{"paperId":"d5400c4cc068eda5fd4c6f7c7f4bfcea41a5a1f8","title":"A Pilot Study of Biomedical Text Comprehension using an Attention-Based Deep Neural Reader: Design and Experimental Analysis"},{"paperId":"91c035165ee5f251c4a0c0b2af67d2891b404316","title":"NSML: A Machine Learning Platform That Enables You to Focus on Your Models"},{"paperId":"9223c95f0e600aee2dcf476094a5102adc386e0f","title":"Effective Use of Bidirectional Language Modeling for Transfer Learning in Biomedical Named Entity Recognition"},{"paperId":"c54e8c7a4f9c2ebd8787aecafa4cfdb35bfd49e0","title":"Effective Use of Bidirectional Language Modeling for Medical Named Entity Recognition"},{"paperId":"bc8fa64625d9189f5801837e7b133e7fe3c581f7","title":"Learned in Translation: Contextualized Word Vectors"},{"paperId":"db8562bb8dc69a7628c49c92ac8e08b5da91130e","title":"A transition‐based joint model for disease named entity recognition and normalization"},{"paperId":"91d4498849fe1966a629cddb187d3cd62eedb2ca","title":"Deep learning with word embeddings improves biomedical named entity recognition"},{"paperId":"4c16a6fd7b4aad8c1331e4753b30701fdf6d12f4","title":"Neural Domain Adaptation for Biomedical Question Answering"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"f27c51137c40940e2facc0ec932cf560967e1f5a","title":"ChimerDB 3.0: an enhanced database for fusion genes from cancer transcriptome and literature data mining"},{"paperId":"dbde7dfa6cae81df8ac19ef500c42db96c3d1edd","title":"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"},{"paperId":"05dd7254b632376973f3a1b4d39485da17814df5","title":"SQuAD: 100,000+ Questions for Machine Comprehension of Text"},{"paperId":"61322ec6cfc54fe9723d4637239b8fb9938dc501","title":"BioCreative V CDR task corpus: a resource for chemical disease relation extraction"},{"paperId":"a4cec122a08216fe8a3bc19b22e78fbaea096256","title":"Deep Learning"},{"paperId":"c4dd9a19d822c965ce8cde55ab23b8a0b628278a","title":"An overview of the BIOASQ large-scale biomedical semantic indexing and question answering competition"},{"paperId":"932117813af889104cafb457ebbc52a38d8b64eb","title":"The CHEMDNER corpus of chemicals and drugs and its annotation principles"},{"paperId":"f37e1b62a767a307c046404ca96bc140b3e68cb5","title":"GloVe: Global Vectors for Word Representation"},{"paperId":"cfb4edb7541fafcf593b466320c63ae32d27f57e","title":"Extraction of relations between genes and diseases from text and large-scale data analysis: implications for translational research"},{"paperId":"696753d59185436ec95ecf3021c413f353be4874","title":"NCBI disease corpus: A resource for disease name recognition and concept normalization"},{"paperId":"87f40e6f3022adbc1f1905e3e506abad05a9964f","title":"Distributed Representations of Words and Phrases and their Compositionality"},{"paperId":"c83d05b15797ade0f8dffb9a311a859682d43a27","title":"The SPECIES and ORGANISMS Resources for Fast and Accurate Identification of Taxonomic Names in Text"},{"paperId":"506c7e333efa0b31823c1b1914b1180c346773ee","title":"The EU-ADR corpus: Annotated drugs, diseases, targets, and their relationships"},{"paperId":"5e095981ebf4d389e9356bd56e59e0ade1b42e88","title":"2010 i2b2/VA challenge on concepts, assertions, and relations in clinical text"},{"paperId":"e1039346942874c59d89028bb934b72524827b8c","title":"LINNAEUS: A species name identification system for biomedical literature"},{"paperId":"f5a0c6593ba95d23c025608ce9280848da8b929f","title":"Overview of BioCreative II gene mention recognition"},{"paperId":"3bd4d2de49d8a092abb295b845dba14874f8787d","title":"Introduction to the Bio-entity Recognition Task at JNLPBA"},{"paperId":"6c2b28f9354f667cd5bd07afc0471d8334430da7","title":"A Neural Probabilistic Language Model"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"afc317b098cd6744611049ff16f351032ab14f83","title":"A BERT-based Universal Model for Both Within- and Cross-sentence Clinical Temporal Relation Extraction"},{"paperId":"eed781f498b563df5a9e8a241c67d63dd1d92ad5","title":"Overview of the BioCreative VI chemical-protein interaction Track"},{"paperId":"e2f28568031e1902d4f8ee818261f0f2c20de6dd","title":"Distributional Semantics Resources for Biomedical Text Processing"}],"id":"1e43c7084bdcb6b3102afaf301cce10faead2702","summary":"This article introduces BioBERT (Bidirectional Encoder Representations from Transformers for Biomedical Text Mining), which is a domain-specific language representation model pre-trained on large-scale biomedical corpora that largely outperforms BERT and previous state-of-the-art models in a variety of biomedical text mining tasks when pre- trained on biomedical Corpora."},{"url":"https://www.semanticscholar.org/paper/4ef3d9e492479e28fa57d107e52acc6a0c803de2","title":"Does CLIP Benefit Visual Question Answering in the Medical Domain as Much as it Does in the General Domain?","venue":"arXiv.org","year":2021,"referenceCount":30,"citationCount":32,"influentialCitationCount":7,"publicationDate":"27/12/2021","authors":"Sedigheh Eslami,Gerard de Melo,C. Meinel","citations":[{"paperId":"bf40c9e7832e1b2887cbf5798455f91705ea11ba","title":"Masked Vision and Language Pre-training with Unimodal and Multimodal Contrastive Losses for Medical Visual Question Answering"},{"paperId":"f89b8e79a1b4b9a2febd9b8ab3f7933c89e1c3e0","title":"A ChatGPT Aided Explainable Framework for Zero-Shot Medical Image Diagnosis"},{"paperId":"14b9561062faeaee8bc62c1a618f04b105f7b291","title":"Dental CLAIRES: Contrastive LAnguage Image REtrieval Search for Dental Research"},{"paperId":"c9bf47b1f54484a0de53cfc03551ccccf6734f62","title":"Quilt-1M: One Million Image-Text Pairs for Histopathology"},{"paperId":"c50ebdb0be883bb4e21270ba990220745c6a9cf9","title":"RemoteCLIP: A Vision Language Foundation Model for Remote Sensing"},{"paperId":"31a7d8c4a5ab6bab522494b57270249105c8748e","title":"BiomedGPT: A Unified and Generalist Biomedical Generative Pre-trained Transformer for Vision, Language, and Multimodal Tasks"},{"paperId":"cb5f53cd7d43ac6f48abf4dde6fc58e53a083b0e","title":"Accelerating the integration of ChatGPT and other large‐scale AI models into biomedical research and healthcare"},{"paperId":"c5bcc78ae708b29edb03481e12213eca53c28963","title":"A multi-modal model based on transformers for medical visual question answering"},{"paperId":"2587cb7b1c02fde76e3c23c13f1bd40d6a199c57","title":"Q2ATransformer: Improving Medical VQA via an Answer Querying Decoder"},{"paperId":"61b877c3ef91286fe3fca51c899bf6c857ea4add","title":"Leveraging medical Twitter to build a visual–language foundation model for pathology AI"},{"paperId":"f7ea746cd2cc25628a7a553ac27d228198be42cb","title":"Pre-trained multilevel fuse network based on vision-conditioned reasoning and bilinear attentions for medical image visual question answering"},{"paperId":"ce865a1d2ad7ac6850bfc72edcea9e6cf3930976","title":"Increasing Textual Context Size Boosts Medical Image-Text Matching"},{"paperId":"a5bc3c0bce8d105a6b95f999fed4ea59c342cb1d","title":"Multi-Modal Bias: Introducing a Framework for Stereotypical Bias Assessment beyond Gender and Race in Vision–Language Models"},{"paperId":"d183cc170400e43535c5e2c37121c37ee0ba23dc","title":"Open-Ended Medical Visual Question Answering Through Prefix Tuning of Language Models"},{"paperId":"d8da72e7857cc1a0d3505e6c8a746eac815901b2","title":"Large-Scale Domain-Specific Pretraining for Biomedical Vision-Language Processing"},{"paperId":"4d4a96708fc67403176bb2b891b564af7a20c148","title":"RAMM: Retrieval-augmented Biomedical Visual Question Answering with Multi-modal Pre-training"},{"paperId":"c011a59b155965caf2ab5cdf9e61e6544142a981","title":"X-TRA: Improving Chest X-ray Tasks with Cross-Modal Retrieval Augmentation"},{"paperId":"57e811638f72d42f33181a4585fad8aee45ab2fd","title":"CLIP-Driven Universal Model for Organ Segmentation and Tumor Detection"},{"paperId":"2ea26b243171e37ef20af269942ffde414f9f8cc","title":"UnICLAM: Contrastive Representation Learning with Adversarial Masking for Unified and Interpretable Medical Vision Question Answering"},{"paperId":"56d8d9fff399f798da97a69e891de4eeb4568d4f","title":"MHKD-MVQA: Multimodal Hierarchical Knowledge Distillation for Medical Visual Question Answering"},{"paperId":"170667a96f04adf3b3b83526f75fe8d1063e0f7a","title":"Self-supervised vision-language pretraining for Medical visual question answering"},{"paperId":"4be48792dfdb5876023fb0523cd25d4b89083ef4","title":"Remote sensing visual question answering with a self-attention multi-modal encoder"},{"paperId":"51d2032637f97fd0a10d01d80ced906d0b6e6444","title":"Transfer Learning for Medical Image Classification on Multiple Datasets using PubMedCLIP"},{"paperId":"6ae700c89a9a9a3da7e55dd51c4710b5ed8c8d4e","title":"Caption-Aware Medical VQA via Semantic Focusing and Progressive Cross-Modality Comprehension"},{"paperId":"5d8fd04c436367b18b35e28332ee8e452a477f3f","title":"Medical Image Understanding with Pretrained Vision Language Models: A Comprehensive Study"},{"paperId":"28ff0816f19a5e3e37eac5569de41872fd262f0a","title":"Align, Reason and Learn: Enhancing Medical Vision-and-Language Pre-training with Knowledge"},{"paperId":"24aa57dae649b6683d8f5bc8deaf2ff549cdacc4","title":"Transformers in Medical Imaging: A Survey"},{"paperId":"6dd9f99cecd38504b667d320eb2a6267a9fee35d","title":"Contrastive Learning of Medical Visual Representations from Paired Images and Text"},{"paperId":"b047b3b7d76b79958e23b0fcab985be22b1ce42d","title":"Alternating Cross-attention Vision-Language Model for Efficient Learning with Medical Image and Report without Curation"},{"paperId":"79478a2ac67b9fdbeadcde13faa2d84eb239e080","title":"Vision-Language Pretraining Enables Radiographs and Reports to be Learned without Curation"},{"paperId":"2441230bd2f3cca924d597b3044ad63aaff269ec","title":"Self-supervised Co-learning of Uncurated Images and Reports Enables Oversight AI in Radiology"},{"paperId":"49b43e98c4ffc4ee0383f15940c97e9540a64c9f","title":"Language over Labels: Contrastive Language Supervision Exceeds Purely Label-Supervised Classification Performance on Chest X-Rays"}],"references":[{"paperId":"8f167ec1149921fac63b1ea855443de109bb013a","title":"How Much Can CLIP Benefit Vision-and-Language Tasks?"},{"paperId":"e4f99837e02e7fbcccec1bf15cececacaaabbe32","title":"MuVAM: A Multi-View Attention-based Model for Medical Visual Question Answering"},{"paperId":"ac74a160e0ca53d3ffb15f79f0b9d3911df2fc28","title":"Glance-and-Gaze Vision Transformer"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"93b6b79b4ef6c345f31722ce7c829385c6dce0d6","title":"Slake: A Semantically-Labeled Knowledge-Enhanced Dataset For Medical Visual Question Answering"},{"paperId":"a6ca91afe845ef5294c40c2029e0c1cba19ba40b","title":"Unifying Vision-and-Language Tasks via Text Generation"},{"paperId":"268d347e8a55b5eb82fb5e7d2f800e33c75ab18a","title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"},{"paperId":"b8d5b853f2212cbb48a43f1edec9b96d76d388ec","title":"Medical Visual Question Answering via Conditional Reasoning"},{"paperId":"6dd9f99cecd38504b667d320eb2a6267a9fee35d","title":"Contrastive Learning of Medical Visual Representations from Paired Images and Text"},{"paperId":"1e5fb8003f866f9beeb1003f0be9a129d480ae75","title":"Problems and Opportunities in Training Deep Learning Software Systems: An Analysis of Variance"},{"paperId":"6adb61121ca4560915ade532910acde56440b88f","title":"A Question-Centric Model for Visual Question Answering in Medical Imaging"},{"paperId":"3c8a456509e6c0805354bd40a35e3f2dbf8069b1","title":"PyTorch: An Imperative Style, High-Performance Deep Learning Library"},{"paperId":"33301b25a297b701bdc287e985c006375cb7bb21","title":"Overcoming Data Limitation in Medical Visual Question Answering"},{"paperId":"2527626c11a84f15709e943fbfa2356e19930e3b","title":"VL-BERT: Pre-training of Generic Visual-Linguistic Representations"},{"paperId":"18f9a6045ba01cb079c4fa49a630d71bbd27cd92","title":"A dataset of clinically generated visual questions and answers about radiology images"},{"paperId":"a564fabf130ff6e2742cfba90c7a4018937d764d","title":"Radiology Objects in COntext (ROCO): A Multimodal Image Dataset"},{"paperId":"a5d10341717c0519cf63151b496a6d2ed67aa05f","title":"Bilinear Attention Networks"},{"paperId":"b14a60a1c3e6bb45baddd754a1cfe83ffc1bbb81","title":"Tips and Tricks for Visual Question Answering: Learnings from the 2017 Challenge"},{"paperId":"c889d6f98e6d79b89c3a6adf8a921f88fa6ba518","title":"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks"},{"paperId":"e7eef2ac4136ec93bd306d2c9c353a13729a4553","title":"Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization"},{"paperId":"2c03df8b48bf3fa39054345bafabfeff15bfd11d","title":"Deep Residual Learning for Image Recognition"},{"paperId":"2c1890864c1c2b750f48316dc8b650ba4772adc5","title":"Stacked Attention Networks for Image Question Answering"},{"paperId":"97ad70a9fa3f99adf18030e5e38ebe3d90daa2db","title":"VQA: Visual Question Answering"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"f37e1b62a767a307c046404ca96bc140b3e68cb5","title":"GloVe: Global Vectors for Word Representation"},{"paperId":"1c6d990c80e60aa0b0059415444cdf94b3574f0f","title":"Stacked Convolutional Auto-Encoders for Hierarchical Feature Extraction"},{"paperId":"357fc385caf2e5b9898c9140fa3ac9955e6bb3c6","title":"TeamS at VQA-Med 2021: BBN-Orchestra for Long-tailed Medical Visual Question Answering"},{"paperId":"519799a9e2a72b808d81c6bcba5de263503de053","title":"Contrastive Pre-training and Representation Distillation for Medical Visual Question Answering Based on Radiology Images"},{"paperId":"13e7212d5af59137ad770f42712d762247ebd3ed","title":"SYSU-HCP at VQA-Med 2021: A Data-centric Model with Efficient Training Methodology for Medical Visual Question Answering"},{"paperId":"39dbb2e49fb33351044a9b8c152a173b31f4c405","title":"Overview of the VQA-Med Task at ImageCLEF 2021: Visual Question Answering and Generation in the Medical Domain"}],"id":"4ef3d9e492479e28fa57d107e52acc6a0c803de2","summary":"A fine-tuned version of CLIP for the medical domain based on PubMed articles is presented, which leads to noticeable improvements for MedVQA and fundamental performance differences of VQA in general versus medical domains are witnessed."},{"url":"https://www.semanticscholar.org/paper/967907503b24423b9b74621051811fcf684e3957","title":"Generalized Decoding for Pixel, Image, and Language","venue":"arXiv.org","year":2022,"referenceCount":99,"citationCount":22,"influentialCitationCount":0,"publicationDate":"21/12/2022","authors":"Xueyan Zou,Zi-Yi Dou,Jianwei Yang,Zhe Gan,Linjie Li,Chunyuan Li,Xiyang Dai,Harkirat Singh Behl,Jianfeng Wang,Lu Yuan,Nanyun Peng,Lijuan Wang,Yong Jae Lee,Jianfeng Gao","citations":[{"paperId":"d6be6ae7f4d60d83097912e953a300ab3fd742f4","title":"Unified Open-Vocabulary Dense Visual Prediction"},{"paperId":"4ec198f910d4cac4ef9b6e36da33b4b29b8b2412","title":"Multimodal Diffusion Segmentation Model for Object Segmentation from Manipulation Instructions"},{"paperId":"2eb617e1242e8f0856f2e7b36073461cb741570d","title":"Semantic-SAM: Segment and Recognize Anything at Any Granularity"},{"paperId":"0fea183c4f49a015a8ee7d89ef2e6885b7023c10","title":"SVIT: Scaling up Visual Instruction Tuning"},{"paperId":"5f0336e58fbc5ae7b8f0d072c99aa8e8f330f4c2","title":"Towards Open Vocabulary Learning: A Survey"},{"paperId":"a517ab05e8d1bdbd0181ffbefb00b9b6243303c3","title":"What a MESS: Multi-Domain Evaluation of Zero-Shot Semantic Segmentation"},{"paperId":"a762954f102a9ece458f02eb955607bf6e456540","title":"DesCo: Learning Object Recognition with Rich Language Descriptions"},{"paperId":"863bde644a979f96914b28c24b01856b4fb479c9","title":"Segment Any Point Cloud Sequences by Distilling Vision Foundation Models"},{"paperId":"75b8cf0945e02761526e8d091c11ebc1c3849895","title":"Towards Adaptable and Interactive Image Captioning with Data Augmentation and Episodic Memory"},{"paperId":"86cbd30d1096b0c7e4ac6b03d97a8df12fd21457","title":"PathAsst: Redefining Pathology through Generative Foundation AI Assistant for Pathology"},{"paperId":"0f4c4aa25ce52b60341bb835fe7d8f5ab8215e52","title":"3D Open-vocabulary Segmentation with Foundation Models"},{"paperId":"727ad8860d6534258b56b11a991562b913d414a2","title":"Instance-Level Semantic Maps for Vision Language Navigation"},{"paperId":"e18dd8d2dd71b56950172ca9988b3873a5c8023f","title":"Advancing Referring Expression Segmentation Beyond Single Image"},{"paperId":"d6d3604f369bb0415cbe814e43ca3131323b03e2","title":"Otter: A Multi-Modal Model with In-Context Instruction Tuning"},{"paperId":"d203076c28587895aa344d088b2788dbab5e82a1","title":"Transformer-Based Visual Segmentation: A Survey"},{"paperId":"de8121dc3d2c69bdab172f37e31168ddf2e6e62f","title":"Segment Everything Everywhere All at Once"},{"paperId":"0931888d5cd7c26427cc116af2ac33863552da27","title":"SAM.MD: Zero-shot medical image segmentation capabilities of the Segment Anything Model"},{"paperId":"f8e37aa69b3c9b743055e648851c530b18dc54d2","title":"Neural Implicit Vision-Language Feature Fields"},{"paperId":"c7a9c7302a72301ed79a7c0696d5af2e03ad3ac4","title":"MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action"},{"paperId":"33827bb0bb8188817083be024614f82bec002c42","title":"A Simple Framework for Open-Vocabulary Segmentation and Detection"},{"paperId":"994a1ce6677b496bd3c0c63aceafc6556005e994","title":"GLIGEN: Open-Set Grounded Text-to-Image Generation"},{"paperId":"81e4040489fd18393a28d35acb4176de9af1be51","title":"Grounding Pretrained Features in 3D Representations"}],"references":[{"paperId":"b287a2765e5bceb732de39dafdf70594dc9cd664","title":"Vision-Language Pre-training: Basics, Recent Advances, and Future Trends"},{"paperId":"667bb464a348b2bc85e7a8e8159b948498850ec7","title":"DetCLIP: Dictionary-Enriched Visual-Concept Paralleled Pre-training for Open-world Detection"},{"paperId":"02251886950770e82b3d68564d60cdfe15e73199","title":"Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks"},{"paperId":"5deef7fc161cbdc884aff15b9810f8a432c1489a","title":"k-means Mask Transformer"},{"paperId":"8b5eab31e1c5689312fff3181a75bfbf5c13e51c","title":"Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks"},{"paperId":"06761cb27e14aa55a6c3d98b949898aa26416698","title":"A Unified Sequence Interface for Vision Tasks"},{"paperId":"4c559d29e19f1226353f52ffe9f8068db1cef943","title":"Coarse-to-Fine Vision-Language Pre-training with Fusion in the Backbone"},{"paperId":"49b5ffebdbcbd683010a2558a19eaa9b21cd8c34","title":"GLIPv2: Unifying Localization and Vision-Language Understanding"},{"paperId":"add94abe9502e1853e870d63abaddfcbe36a0e8e","title":"Towards General Purpose Vision Systems: An End-to-End Task-Agnostic Vision-Language Architecture"},{"paperId":"60ee030773ba1b68eb222a265b052ca028353362","title":"GIT: A Generative Image-to-text Transformer for Vision and Language"},{"paperId":"055cd2faeebc7a9df43923d554a61ae924a4af6b","title":"UViM: A Unified Modeling Approach for Vision with Learned Guiding Codes"},{"paperId":"5598c4ece8ffc69a7eb584d16f6de6629044e76a","title":"Vision Transformer Adapter for Dense Predictions"},{"paperId":"9dae204dad41633188022002a04c8aa67c79a4e1","title":"Simple Open-Vocabulary Object Detection with Vision Transformers"},{"paperId":"a26a7a74f1e5fd562be95c3611a0680759fbdf84","title":"CoCa: Contrastive Captioners are Image-Text Foundation Models"},{"paperId":"7d3b912398c6132d506bebb070211f6b9c339482","title":"ELEVATER: A Benchmark and Toolkit for Evaluating Language-Augmented Visual Models"},{"paperId":"dc25df2fa756ec90cfe8f7cac0cba34b570b7d0b","title":"X-DETR: A Versatile Architecture for Instance-wise Vision-Language Tasks"},{"paperId":"4b8d8d36a18fd61a6eda3322d8dd3baad2819600","title":"Unified Contrastive Learning in Image-Text-Label Space"},{"paperId":"15480f3e0a909938f874bed983399cbac064e653","title":"DaViT: Dual Attention Vision Transformers"},{"paperId":"fa717a2e31f0cef4e26921f3b147a98644d2e64c","title":"Focal Modulation Networks"},{"paperId":"0b5f27a5766c5d1394a6282ad94fec21d620bd6b","title":"GroupViT: Semantic Segmentation Emerges from Text Supervision"},{"paperId":"1bfa62ddfa3f6691e0e40c06f8ead594b6449cfa","title":"Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework"},{"paperId":"cc9826c222ac1e81b4b374dd9e0df130f298b1e8","title":"Language-driven Semantic Segmentation"},{"paperId":"c10075b3746a9f3dd5811970e93c8ca3ad39b39d","title":"High-Resolution Image Synthesis with Latent Diffusion Models"},{"paperId":"e77c484af99fc1eb3d3c36699ac81822e98cb74d","title":"Image Segmentation Using Text and Image Prompts"},{"paperId":"837173ef1f260adc0d50b76675915776e1cc8ade","title":"RegionCLIP: Region-based Language-Image Pretraining"},{"paperId":"2fd6f77540c1cc8e70b96208ccf9971b4251fc02","title":"FLAVA: A Foundational Language And Vision Alignment Model"},{"paperId":"5341b412383c43f4a693ad63ec4489e3ec7688c8","title":"Grounded Language-Image Pre-training"},{"paperId":"bbb583dccbec7407f0d01502b03deb67323724fe","title":"LAVT: Language-Aware Vision Transformer for Referring Image Segmentation"},{"paperId":"658a017302d29e4acf4ca789cb5d9f27983717ff","title":"Masked-attention Mask Transformer for Universal Image Segmentation"},{"paperId":"6d1ef4436904de111c8b1975bbf25d3fe2f165f7","title":"DenseCLIP: Language-Guided Dense Prediction with Context-Aware Prompting"},{"paperId":"74f4439c6a0ec7baa17d1829c9dbc7d2010404fd","title":"Open-Vocabulary Instance Segmentation via Robust Cross-Modal Pseudo-Labeling"},{"paperId":"22312f763328cf540791de8c2449ea1e7436f476","title":"UniTAB: Unifying Text and Box Outputs for Grounded Vision-Language Modeling"},{"paperId":"21ec90872abd986c12afe39bebe807732ffa70c9","title":"Florence: A New Foundation Model for Computer Vision"},{"paperId":"c05cd00ae61f3c1c39be2603a2f96fdfe0c59dd8","title":"UFO: A UniFied TransfOrmer for Vision-Language Representation Learning"},{"paperId":"94ff111c4d81bd03f159321728ceec8b4711c89d","title":"An Empirical Study of Training End-to-End Vision-and-Language Transformers"},{"paperId":"ca30f4371367f07a17ba42d9dab76cac1d9fd943","title":"Panoptic SegFormer: Delving Deeper into Panoptic Segmentation with Transformers"},{"paperId":"5e00596fa946670d894b1bdaeff5a98e3867ef13","title":"SimVLM: Simple Visual Language Model Pretraining with Weak Supervision"},{"paperId":"b82c5f9efdb2ae56baa084ca41aeddd8a665c1d1","title":"Align before Fuse: Vision and Language Representation Learning with Momentum Distillation"},{"paperId":"3d4b1c580c4df032549a84ee1a5114a09863ce18","title":"SOLQ: Segmenting Objects by Learning Queries"},{"paperId":"63c74d15940af1af9b386b5762e4445e54c73719","title":"VinVL: Revisiting Visual Representations in Vision-Language Models"},{"paperId":"e3d7778a47c6cab4ea1ef3ee9d19ec1510c15c60","title":"SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers"},{"paperId":"cf9b8da26d9b92e75ba49616ed2a1033f59fce14","title":"Open-vocabulary Object Detection via Vision and Language Knowledge Distillation"},{"paperId":"7ba9c013988eaff5cd186d73704af329d027872d","title":"MDETR - Modulated Detection for End-to-End Multi-Modal Understanding"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"a87bb9e70d1127b6a9c0e721e48c0b58c997c70e","title":"UniT: Multimodal Multitask Learning with a Unified Transformer"},{"paperId":"141a5033d9994242b18bb3b217e79582f1ee9306","title":"Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision"},{"paperId":"a6ca91afe845ef5294c40c2029e0c1cba19ba40b","title":"Unifying Vision-and-Language Tasks via Text Generation"},{"paperId":"0839722fb5369c0abaff8515bfc08299efc790a1","title":"ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision"},{"paperId":"81002fbb777f860f9aac2bbc24467a62345af279","title":"Decoupling the Role of Data, Attention, and Losses in Multimodal Transformers"},{"paperId":"787119e3c3f819244c82b7d97779473773e60696","title":"MaX-DeepLab: End-to-End Panoptic Segmentation with Mask Transformers"},{"paperId":"268d347e8a55b5eb82fb5e7d2f800e33c75ab18a","title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"},{"paperId":"39ca8f8ff28cc640e3b41a6bd7814ab85c586504","title":"Deformable DETR: Deformable Transformers for End-to-End Object Detection"},{"paperId":"4cec2f39cbf10d670140d7146dd221049fdc2afc","title":"A survey on instance segmentation: state of the art"},{"paperId":"1a09b8d7946bb61deffe21cdd453b2c53cdd1634","title":"MSeg: A Composite Dataset for Multi-Domain Semantic Segmentation"},{"paperId":"962dc29fdc3fbdc5930a10aba114050b82fe5a3e","title":"End-to-End Object Detection with Transformers"},{"paperId":"862f2e2e5ba7b8b83f42226170c634ecb02834e4","title":"Conditional Convolutions for Instance Segmentation"},{"paperId":"d8a305b9366608d54452ac30459ee57b4f5cf1c9","title":"UNITER: UNiversal Image-TExt Representation Learning"},{"paperId":"2527626c11a84f15709e943fbfa2356e19930e3b","title":"VL-BERT: Pre-training of Generic Visual-Linguistic Representations"},{"paperId":"79c93274429d6355959f1e4374c2147bb81ea649","title":"LXMERT: Learning Cross-Modality Encoder Representations from Transformers"},{"paperId":"5aec474c31a2f4b74703c6f786c0a8ff85c450da","title":"VisualBERT: A Simple and Performant Baseline for Vision and Language"},{"paperId":"65a9c7b0800c86a196bc14e7621ff895cc6ab287","title":"ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks"},{"paperId":"69455376f5ad52cac5b72d5e8c6cf03fb466b55c","title":"Cross-Modal Self-Attention Network for Referring Image Segmentation"},{"paperId":"653d92ca61d77a906eabb880f40cac12f6f1dc12","title":"YOLACT: Real-Time Instance Segmentation"},{"paperId":"cc628fee1e83bfba1d581bfa128c9cb6c28ef8ad","title":"YouTube-VOS: A Large-Scale Video Object Segmentation Benchmark"},{"paperId":"810eafc9e854ea9b1d7a9e9f755f8102310d5db6","title":"Dynamic Multimodal Instance Segmentation guided by natural language queries"},{"paperId":"b4df354db88a70183a64dbc9e56cf14e7669a6c0","title":"Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning"},{"paperId":"4f0b8f730273e9f11b2bfad2415485414b96299f","title":"BDD100K: A Diverse Driving Video Database with Scalable Annotation Tooling"},{"paperId":"dce916351ef589afa7a63452648dd8acba931e92","title":"Panoptic Segmentation"},{"paperId":"d07284a6811f1b2745d91bdb06b040b57f226882","title":"Decoupled Weight Decay Regularization"},{"paperId":"a82c1d1ccaa3a3d1d6ee6677de0eed2e93ddb6e8","title":"Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering"},{"paperId":"2cc6ba3dfd6bf1f6257b2e4651f4cae355284286","title":"Obj2Text: Generating Visually Descriptive Language from Object Layouts"},{"paperId":"2a5667702b0f1ff77dde8fb3e2e10d4e05e8de9d","title":"Scene Parsing through ADE20K Dataset"},{"paperId":"ee4a012a4b12d11d7ab8c0e79c61e807927a163c","title":"Rethinking Atrous Convolution for Semantic Image Segmentation"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"3d69edb02e935b782b90175cb691f6ab5f4bd64f","title":"Recurrent Multimodal Interaction for Referring Image Segmentation"},{"paperId":"1a0912bb76777469295bb2c059faee907e7f3258","title":"Mask R-CNN"},{"paperId":"86eef3a1dff2bd2808847358cdb7f5ba2b7e0214","title":"Modeling Context Between Objects for Referring Expression Understanding"},{"paperId":"29efbe391950ae438c63d86ad5c82b2942efb0b4","title":"Modeling Context in Referring Expressions"},{"paperId":"b133e361e2f8af22b823d25060b2e7c47f690985","title":"Segmentation from Natural Language Expressions"},{"paperId":"afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d","title":"Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations"},{"paperId":"e65142010431ffc089b272a1174214e00693e503","title":"Generation and Comprehension of Unambiguous Object Descriptions"},{"paperId":"11c9c31dff70de92ada9160c78ff8bb46b2912d6","title":"Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models"},{"paperId":"97ad70a9fa3f99adf18030e5e38ebe3d90daa2db","title":"VQA: Visual Question Answering"},{"paperId":"696ca58d93f6404fea0fc75c62d1d7b378f47628","title":"Microsoft COCO Captions: Data Collection and Evaluation Server"},{"paperId":"317aee7fc081f2b137a85c4f20129007fd8e717e","title":"Fully convolutional networks for semantic segmentation"},{"paperId":"3419ccd5c94d301ee08d716d037f0c3c6a62e78e","title":"The Role of Context for Object Detection and Semantic Segmentation in the Wild"},{"paperId":"71b7178df5d2b112d07e45038cb5637208659ff7","title":"Microsoft COCO: Common Objects in Context"},{"paperId":"8e080b98efbe65c02a116439205ca2344b9f7cd4","title":"Im2Text: Describing Images Using 1 Million Captioned Photographs"},{"paperId":"d2c733e34d48784a37d717fe43d9e93277a8c53e","title":"ImageNet: A large-scale hierarchical image database"},{"paperId":"42254a2361f8a1380f3593fbc56ece21da806103","title":"Open-Vocabulary Panoptic Segmentation with MaskCLIP"},{"paperId":"90b95c6a63df4cbcc52aa8eb77e5f8bc71b90deb","title":"Open-Vocabulary Image Segmentation"},{"paperId":"c8b25fab5608c3e033d34b4483ec47e68ba109b7","title":"Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"},{"paperId":"acae6171717fcf3795dd8fc187384d70ea1b9076","title":"A REVIEW ON IMAGE SEGMENTATION TECHNIQUES"},{"paperId":"5ffd74d2873b7cba2cbc5fd295cc7fbdedca22a2","title":"The Cityscapes Dataset"},{"paperId":"0ec48ac86456cea3d6d6172ca81ef68e98b21a61","title":"The PASCAL Visual Object Classes Challenge"},{"paperId":"26e1f436f653dcae6d6c4a3e104629ff4da6ffe2","title":"Survey on Image Segmentation"},{"paperId":null,"title":"SegInW results with tuning on X-Decoder for different image shots and backbone architectures. (0.26M parameters tuned in the setting"},{"paperId":null,"title":"Dataset Scene Annotation Format # Images # Classes Sem Inst Pano ADE-150 common"},{"paperId":null,"title":"Oscar: Object-semantics aligned pretraining for vision-language"}],"id":"967907503b24423b9b74621051811fcf684e3957","summary":"X-Decoder is the first work that provides a unified way to support all types of image segmentation and a variety of vision-language (VL) tasks and achieves state-of-the-art results on open-vocabulary segmentsation and referring segmentation on eight datasets."},{"url":"https://www.semanticscholar.org/paper/02251886950770e82b3d68564d60cdfe15e73199","title":"Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks","venue":"arXiv.org","year":2022,"referenceCount":74,"citationCount":257,"influentialCitationCount":25,"publicationDate":"22/08/2022","authors":"Wenhui Wang,Hangbo Bao,Li Dong,Johan Bjorck,Zhiliang Peng,Qiang Liu,Kriti Aggarwal,O. Mohammed,Saksham Singhal,S. Som,Furu Wei","citations":[{"paperId":"83c48aa341850af478247e3b34ba1ee1db9f1236","title":"Meta-Transformer: A Unified Framework for Multimodal Learning"},{"paperId":"444155b2abacb9934030477495a7acef19cc909d","title":"MAMO: Fine-Grained Vision-Language Representations Learning with Masked Multimodal Modeling"},{"paperId":"1b4012f38daa8f09299e16771973c91ce9464ee2","title":"DVPT: Dynamic Visual Prompt Tuning of Large Pre-trained Models for Medical Image Analysis"},{"paperId":"fee492fe200fd8dbb0f9d762dfbbe188fd200599","title":"Mining of Single-Class by Active Learning for Semantic Segmentation"},{"paperId":"177bf0086a714ff305e45b720cda82e992c9fc7c","title":"PiTL: Cross-modal Retrieval with Weakly-supervised Vision-language Pre-training via Prompting"},{"paperId":"06f7066e76e5acf7b03d4fda2c0d12d71a209185","title":"Fine-grained Text-Video Retrieval with Frozen Image Encoders"},{"paperId":"d58b89b64a0565e77d9b9734d871c58e4a7af6d8","title":"mBLIP: Efficient Bootstrapping of Multilingual Vision-LLMs"},{"paperId":"94053805cd59f2e9a47fe3f080c7e7afefb337cc","title":"Generative Pretraining in Multimodality"},{"paperId":"9048483d7c9f44588818f4e0fc7876c65192cedc","title":"All in One: Exploring Unified Vision-Language Tracking with Multi-Modal Alignment"},{"paperId":"420f03d44a3bee8c1c51216e1045d0ff92e8fefc","title":"Vision Language Transformers: A Survey"},{"paperId":"2f8e2feb7161135c778b0f128e9e6557a51fc7bf","title":"Distilling Large Vision-Language Model with Out-of-Distribution Generalizability"},{"paperId":"c5202ab27294d5c1eb4d2f0ca7e82afef91888f0","title":"VideoGLUE: Video General Understanding Evaluation of Foundation Models"},{"paperId":"7619a98ef077c8f75e0bfb98953457649209e07e","title":"Review of Large Vision Models and Visual Prompt Engineering"},{"paperId":"17f45b2863a1ecd9dd58ba721cdb806dfcb57816","title":"Challenges of Zero-Shot Recognition with Vision-Language Models: Granularity and Correctness"},{"paperId":"d212fa27f5868f0fd106e1a7bba908fd47da0816","title":"MotionGPT: Human Motion as a Foreign Language"},{"paperId":"dedbac319177d04ce63fe00d2fec24bdaab90d6d","title":"SugarCrepe: Fixing Hackable Benchmarks for Vision-Language Compositionality"},{"paperId":"8c88c693d5dc2802d3b76b85740e1f04fdaaf801","title":"Large Sequence Models for Sequential Decision-Making: A Survey"},{"paperId":"e9e6e08a9a39aa8d60b41da22cfb5670e64adeaf","title":"RS5M: A Large Scale Vision-Language Dataset for Remote Sensing Vision-Language Foundation Model"},{"paperId":"40a342a5e48be59a674aa8a6d980133a1fc138b3","title":"LabelBench: A Comprehensive Framework for Benchmarking Label-Efficient Learning"},{"paperId":"8efc20988021ce3b4b05dd44b13e27260ee9b99b","title":"Investigating Prompting Techniques for Zero- and Few-Shot Visual Question Answering"},{"paperId":"c2b7d6a57229ac2bdef69b9e7bc3c280fb95eaf1","title":"Parameter-efficient is not sufficient: Exploring Parameter, Memory, and Time Efficient Adapter Tuning for Dense Predictions"},{"paperId":"1474d4248e1cbcc91183456cdf1e7272e8a931de","title":"COSA: Concatenated Sample Pretrained Vision-Language Foundation Model"},{"paperId":"6d7766ad375128ffd43b8d718b9b4cb440a877df","title":"Transferring Knowledge for Food Image Segmentation using Transformers and Convolutions"},{"paperId":"3da05d18593683d50db57cebfc0c20069f8ea11e","title":"LOVM: Language-Only Vision Model Selection"},{"paperId":"0983883619a0ca597d055d0e58da2f514052913d","title":"Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration"},{"paperId":"dc8768de2287c90e99852635dc04b6e922459300","title":"ZeroForge: Feedforward Text-to-Shape Without 3D Supervision"},{"paperId":"a42fc49a300136d60aaebb668369010ee7746150","title":"Visual Language Pretrained Multiple Instance Zero-Shot Transfer for Histopathology Images"},{"paperId":"c581d2ad3b092a2cc152d0c6f55fd6320f78eb3a","title":"A Survey of Vision-Language Pre-training from the Lens of Multimodal Machine Translation"},{"paperId":"e03295bd92bf0f85d95784a75851613ee2bd47c5","title":"Global and Local Semantic Completion Learning for Vision-Language Pre-training"},{"paperId":"e18fce17da0bcd8449ff6d70735281732ce13c81","title":"UniBoost: Unsupervised Unimodal Pre-training for Boosting Zero-shot Vision-Language Tasks"},{"paperId":"ad4a998b7b2af6e607ea917d5e6974b0a097cc54","title":"Efficient Anomaly Detection with Budget Annotation Using Semi-Supervised Residual Transformer"},{"paperId":"99368d6fc86e2eb181d9d36165cfed578bfe938d","title":"Q: How to Specialize Large Vision-Language Models to Data-Scarce VQA Tasks? A: Self-Train on Unlabeled Images!"},{"paperId":"f1892409fdbb396b00bb180891bb1c130fe3c7f4","title":"Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding"},{"paperId":"8213492345c67d2b0e692b6bb5c814d4f1aef8d2","title":"Benchmarking Robustness of Adaptation Methods on Pre-trained Vision-Language Models"},{"paperId":"f308832cba50169cb0e53da31ec2bd04ecaa5551","title":"UniDiff: Advancing Vision-Language Models with Generative and Discriminative Learning"},{"paperId":"2c600b2829a608ce366614dc4911ef672f2e4255","title":"On Masked Pre-training and the Marginal Likelihood"},{"paperId":"0dd88453703f0008019a9a55a364064f0e8aa5d0","title":"PV2TEA: Patching Visual Modality to Textual-Established Information Extraction"},{"paperId":"b42ceaedfd9830c5c3a67a0c312afc737443e561","title":"ManagerTower: Aggregating the Insights of Uni-Modal Experts for Vision-Language Representation Learning"},{"paperId":"fd928577d67dd01048d13f284a6256164bbcf2f0","title":"Learning without Forgetting for Vision-Language Models"},{"paperId":"3099d6f4965b4d73aa1e2b2880522ec89ed2dc0a","title":"PaLI-X: On Scaling up a Multilingual Vision and Language Model"},{"paperId":"5c183d241fe54a6d67e21eea48fbd5ea6b31ec1c","title":"VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset"},{"paperId":"76bc2f184738134ec41f9d2b847ec27ac221e3db","title":"Emergent Modularity in Pre-trained Transformers"},{"paperId":"267f6d0ff0d6e278d8bf31dcc91430ba50c5ba78","title":"Benchmarking Diverse-Modal Entity Linking with Generative Models"},{"paperId":"cf5c2563f0ffa6c6057787cd6e710312d751fa85","title":"Toward Understanding Generative Data Augmentation"},{"paperId":"c0647907bb37de70950fb5467476ac35ef43a8c9","title":"LANISTR: Multimodal Learning from Structured and Unstructured Data"},{"paperId":"d989dbf8cefa286aaf62cb9f67a85db216eca849","title":"Detect Any Shadow: Segment Anything for Video Shadow Detection"},{"paperId":"c2c7ad3112c4b575e5d8163a0e574f9eb743cb52","title":"Zero-shot Visual Question Answering with Language Model Feedback"},{"paperId":"06091944b864d6dc473cab63321a95fb9c4067cc","title":"ChatCAD+: Towards a Universal and Reliable Interactive CAD using LLMs"},{"paperId":"42e726e2ea5bbb946001947d1a5b31ccc6b7aef9","title":"VioLA: Unified Codec Language Models for Speech Recognition, Synthesis, and Translation"},{"paperId":"13b5b69355555e0c8b702261c5de3b4172ba653c","title":"The Art of SOCRATIC QUESTIONING: Zero-shot Multimodal Reasoning with Recursive Thinking and Self-Questioning"},{"paperId":"f45a3474bd38d65c1b2cc3342a64dacbf07f445a","title":"Cream: Visually-Situated Natural Language Understanding with Contrastive Reading Model and Frozen Large Language Models"},{"paperId":"d57caa06a42baa90eb741a9afb10fe4fff8be82a","title":"SmartTrim: Adaptive Tokens and Parameters Pruning for Efficient Vision-Language Models"},{"paperId":"f99d3dd98c5ed66ecd167884b35940647a8c8991","title":"Multi-granularity Text Representation and Transformer-Based Fusion Method for Visual Question Answering"},{"paperId":"6209b614db0065de89331196a1ae8aa59404f0db","title":"Know Your Self-supervised Learning: A Survey on Image-based Generative and Discriminative Training"},{"paperId":"37dfbd88667cdfa42cc547b56210bda5860381f0","title":"Perception Test: A Diagnostic Benchmark for Multimodal Video Models"},{"paperId":"4e8e21acf81363ef29aeef6a64cc8c5ddf512880","title":"Training Transitive and Commutative Multimodal Transformers with LoReTTa"},{"paperId":"065dcc6074ffc9e314799d97c1757e5d23e7e2b1","title":"S-CLIP: Semi-supervised Vision-Language Pre-training using Few Specialist Captions"},{"paperId":"6fe2a4f7594abadafc9feb05e96ab7e9e634ce8f","title":"Album Storytelling with Iterative Story-aware Captioning and Large Language Models"},{"paperId":"23c2ba16591edae0d5f461715c46f3a894946248","title":"VLAB: Enhancing Video Language Pre-training by Feature Adapting and Blending"},{"paperId":"bc2e8b613335259598ea5c49aea270469e9a35ed","title":"A PhD Student's Perspective on Research in NLP in the Era of Very Large Language Models"},{"paperId":"cb861bd77070e4441d66ffee0801f7048a9eacbe","title":"i-Code V2: An Autoregressive Generation Framework over Vision, Language, and Speech Data"},{"paperId":"6118eb18023429fa8bad64b7a1d95533127a62d7","title":"Prompt ChatGPT In MNER: Improved multimodal named entity recognition method based on auxiliary refining knowledge from ChatGPT"},{"paperId":"8ae2e81495d426419e6fd96940b651002c046b61","title":"Enhancing Vision-Language Pre-Training with Jointly Learned Questioner and Dense Captioner"},{"paperId":"385d3b5ba6faa8ad360df0c79da04985c7968e00","title":"Visual Question Answering: A Survey on Techniques and Common Trends in Recent Literature"},{"paperId":"bfa9df38bd8597d8cdf0c3497fe5e5a5e31f646f","title":"ONE-PEACE: Exploring One General Representation Model Toward Unlimited Modalities"},{"paperId":"d886fc1b43b1c14b1c82ce8e4eab7c48e2c6d7af","title":"Paxion: Patching Action Knowledge in Video-Language Foundation Models"},{"paperId":"42a30dc5470f54ec249f25d3c31e05d7c376c8e3","title":"VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks"},{"paperId":"f1749d29b5e443c711f3feb2f4f9bf2ee7f163c6","title":"Sequence-to-Sequence Pre-training with Unified Modality Masking for Visual Document Understanding"},{"paperId":"77f38934bc18217227b94a71ecc5afb52fcfaff7","title":"CLIP-VG: Self-paced Curriculum Adapting of CLIP for Visual Grounding"},{"paperId":"3f14234d55d50662d698c838a73fb0847094adb6","title":"Mode Approximation Makes Good Multimodal Prompts"},{"paperId":"38be7643bcad936739550a1802220eb53ca9b1df","title":"Simple Token-Level Confidence Improves Caption Correctness"},{"paperId":"8badb0587fef2ffc078b0cec549eb8ec96ed3ad4","title":"Self-Chained Image-Language Model for Video Localization and Question Answering"},{"paperId":"0340c850e033abbf71c7214e403c8fe2be5ef91f","title":"Visual Tuning"},{"paperId":"8d3cc62fbe79b280a9084a43b295a4c77f7092ad","title":"Alternating Gradient Descent and Mixture-of-Experts for Integrated Multimodal Perception"},{"paperId":"b07fa63a6d2f39900f0f2cae8f58cd5507010aad","title":"Multi-Prompt with Depth Partitioned Cross-Modal Learning"},{"paperId":"379b4d871cebf431208154adaaff7ea946b2bb38","title":"Structure-CLIP: Enhance Multi-modal Language Representations with Structure Knowledge"},{"paperId":"e1ff32753e20e48b4b01e40b5e820254396e6e70","title":"LMEye: An Interactive Perception Network for Large Language Models"},{"paperId":"9a22b33b529484c912d1ea9f8698369d4546a1c1","title":"Transfer Visual Prompt Generator across LLMs"},{"paperId":"c3068e2a9f4cd374c7ff3be1b8f877b3d653e880","title":"Multimodal Neural Databases"},{"paperId":"5b678b4c4737c7d2e8ba98e211fed4834dd43732","title":"ArK: Augmented Reality with Knowledge Interactive Emergent Ability"},{"paperId":"131c6f328c11706de2c43cd16e0b7c5d5e610b6a","title":"Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond"},{"paperId":"268fa90820d0a1db5c35e056d291984ede3f0aed","title":"A Strong and Reproducible Object Detector with Only Public Datasets"},{"paperId":"6bfafb32b423c3f0456a10984814f89046def489","title":"A Cookbook of Self-Supervised Learning"},{"paperId":"2bc22b50f2a517fa57ae48135f42b18ba3da1cb4","title":"CoT-MoTE: Exploring ConTextual Masked Auto-Encoder Pre-training with Mixture-of-Textual-Experts for Passage Retrieval"},{"paperId":"5b69c50a3699c8c46c3016b0dcf176b8e0bfe3aa","title":"Enhancing Textbooks with Visuals from the Web for Improved Learning"},{"paperId":"22a7d8293f1b487372938977a76706462aed6045","title":"Shuffled Transformer for Privacy-Preserving Split Learning"},{"paperId":"b7d73f22d861f526541575a3b17449bd3c58ca74","title":"MVP-SEG: Multi-View Prompt Learning for Open-Vocabulary Semantic Segmentation"},{"paperId":"a43a3fadc9190e61b34f59a913f1716e443519e4","title":"On the Opportunities and Challenges of Foundation Models for Geospatial Artificial Intelligence"},{"paperId":"e8acb3e6ae754b18eb5e1d8466b11d6e1d81d1ae","title":"Modeling Dense Multimodal Interactions Between Biological Pathways and Histology for Survival Prediction"},{"paperId":"2a8d0f85252f486d6585621268ead2c941f0bd8a","title":"Improving Image Recognition by Retrieving from Web-Scale Image-Text Data"},{"paperId":"45c2f1672ef1de301868453acaf23f6df9a34b8a","title":"A Billion-scale Foundation Model for Remote Sensing Images"},{"paperId":"51615bee01c966ba055920e10778d5331d35bccd","title":"MoMo: A shared encoder Model for text, image and multi-Modal representations"},{"paperId":"0312e70901f352a6d95f23573788f9f7b737c983","title":"Training Large Language Models Efficiently with Sparsity and Dataflow"},{"paperId":"aae77fd74c99b2f6c10366267ce993b2d94141d5","title":"On Robustness in Multimodal Learning"},{"paperId":"0ebc861f5478561f12941e6b48aad30574e996d8","title":"Video ChatCaptioner: Towards Enriched Spatiotemporal Descriptions"},{"paperId":"87df8e436b91823b69ce660e1913bf723bc1cec5","title":"Token Boosting for Robust Self-Supervised Visual Transformer Pre-training"},{"paperId":"13581a46d32822e44cbeb1acdba4a59cef2b2ec1","title":"On Efficient Training of Large-Scale Deep Learning Models: A Literature Review"},{"paperId":"41721e0e0b7ce945d9d317bb83df2d9cb74b3114","title":"Attention: Marginal Probability is All You Need?"},{"paperId":"db1c83ef73d2f7731b0dd255835f2f26db749e17","title":"Not All Features Matter: Enhancing Few-shot CLIP with Adaptive Prior Refinement"},{"paperId":"599080fc0e2af249d2dc2e4ecb90a900c380926c","title":"A Novel Scenarios Engineering Methodology for Foundation Models in Metaverse"},{"paperId":"d8b0425e83bdf0335092f21147884236956872db","title":"Self-Supervised Multimodal Learning: A Survey"},{"paperId":"c84e2801512069acbc63f1a7f73273281939428c","title":"A Study of Autoregressive Decoders for Multi-Tasking in Computer Vision"},{"paperId":"5643d0f64b326ce07797ab78da1ec013096dc0cd","title":"MaMMUT: A Simple Architecture for Joint Learning for MultiModal Tasks"},{"paperId":"a757999ed260d7bc45484dc6b4456bf33fe6f679","title":"LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention"},{"paperId":"d7adb2e6c8e381ca6c7a9a74ec5c54061573f9e1","title":"Revisiting Multimodal Representation in Contrastive Learning: From Patch and Token Embeddings to Finite Discrete Tokens"},{"paperId":"f9badd638eb683f2ba39fd089fbabb87c9a63787","title":"IFSeg: Image-free Semantic Segmentation via Vision-Language Model"},{"paperId":"d064075c47e358f604034d06df4b985356757c71","title":"Equivariant Similarity for Vision-Language Foundation Models"},{"paperId":"830d4beeaf56db871db842e3445c16b571f5a904","title":"Accelerating Vision-Language Pretraining with Free Language Modeling"},{"paperId":"8a4dd69533378b4e1e1d6429de4f2c6eab18e101","title":"CoBIT: A Contrastive Bi-directional Image-Text Generation Model"},{"paperId":"67317a73151316933c3943ff68b5f7cfcbc7e4c7","title":"MAGVLT: Masked Generative Vision-and-Language Transformer"},{"paperId":"3049c992adbd56e29c4d957ee0c4e9d05fe3c6d1","title":"EVA-02: A Visual Representation for Neon Genesis"},{"paperId":"f760912a793b53b0164670956e27bad9edd8bff1","title":"MXM-CLR: A Unified Framework for Contrastive Learning of Multifold Cross-Modal Representations"},{"paperId":"466a1f233ccb6b110a906a599627fe4c2caa1832","title":"eP-ALM: Efficient Perceptual Augmentation of Language Models"},{"paperId":"4396e30f28eb49bb07c63cf62ca90415ebbe43d4","title":"IRGen: Generative Modeling for Image Retrieval"},{"paperId":"6c5cfc5a8debff3dab55764b6b6b534d7c47ce72","title":"Enhancing the Role of Context in Region-Word Alignment for Object Detection"},{"paperId":"bb8075a3ac5375566bab20a244da74a2d10b1352","title":"Dual-path Adaptation from Image to Video Transformers"},{"paperId":"45d29ab954a9cc3888883217b686e7ab80296716","title":"Reinforce Data, Multiply Impact: Improved Model Accuracy and Robustness with Dataset Reinforcement"},{"paperId":"9d12916dd46df7a6446cbec0bc4d054f7dafcdab","title":"Scaling Vision-Language Models with Sparse Mixture of Experts"},{"paperId":"530bee65ee844ed794d98b1120e4cf2738558316","title":"ViM: Vision Middleware for Unified Downstream Transferring"},{"paperId":"f5df5a3cc5435b3cfd517bded331e05fed961d78","title":"Align and Attend: Multimodal Summarization with Dual Contrastive Losses"},{"paperId":"3d45e69557f0c6a54ec698304c2e27ec29bc1c2b","title":"PMC-CLIP: Contrastive Language-Image Pre-training using Biomedical Documents"},{"paperId":"6827e87642874d9bf69f0f1548d79a164aaa5e1e","title":"One Transformer Fits All Distributions in Multi-Modal Diffusion at Scale"},{"paperId":"69cfdc8df16ae63b7acba4ac6f727f78b86893c3","title":"ChatGPT Asks, BLIP-2 Answers: Automatic Questioning Towards Enriched Visual Descriptions"},{"paperId":"374dc9612e3507d1d3517492589c177a73be8e21","title":"Understanding and Constructing Latent Modality Structures in Multi-modal Representation Learning"},{"paperId":"9cbf01298c09d7f9729be962a86ea368bb812050","title":"Tag2Text: Guiding Vision-Language Model via Image Tagging"},{"paperId":"980668424b68c02271f39b3f71dc416eb0439cc5","title":"HumanBench: Towards General Human-centric Perception with Projector Assisted Pretraining"},{"paperId":"7ad627f426691601aa4ffea2092cb51c2f37ed8a","title":"Exploiting the Textual Potential from Vision-Language Pre-training for Text-based Person Search"},{"paperId":"b5b136ac59f1a75bdb1273f398526062ca223359","title":"New Audio Representations Image Gan Generation from BriVL"},{"paperId":"36291f1e302d95fa5558442d61a2d64e538ea206","title":"Bootstrap The Original Latent: Learning a Private Model from a Black-box Model"},{"paperId":"a935ba7ce7fd44fe372c6860504fbc164f012f03","title":"HiCLIP: Contrastive Language-Image Pretraining with Hierarchy-aware Attention"},{"paperId":"0a0acae0a9466f24d17d447a575f0efa5b90ee0e","title":"FAME-ViL: Multi-Tasking Vision-Language Model for Heterogeneous Fashion Tasks"},{"paperId":"e64dc184ec4ef8c6f3e0046e25e20b5dbe043ff4","title":"Visual Atoms: Pre-training Vision Transformers with Sinusoidal Waves"},{"paperId":"a339df8024808b006f8f91db613961dc1c346afe","title":"DejaVu: Conditional Regenerative Learning to Enhance Dense Prediction"},{"paperId":"4d4a96708fc67403176bb2b891b564af7a20c148","title":"RAMM: Retrieval-augmented Biomedical Visual Question Answering with Multi-modal Pre-training"},{"paperId":"fbfef4723d8c8467d7bd523e1d0b703cce0e0f9c","title":"Language Is Not All You Need: Aligning Perception with Language Models"},{"paperId":"facc16fa1226124cca9cc8f91411c3d6303e53d0","title":"Few-shot Multimodal Multitask Multilingual Learning"},{"paperId":"da9579539385daedd33a0de0f814e2977ad0d1f5","title":"Towards Unifying Medical Vision-and-Language Pre-training via Soft Prompts"},{"paperId":"97fa699cd5403f6a1fed6f79e02af4ae37f15c4d","title":"UniAdapter: Unified Parameter-Efficient Transfer Learning for Cross-modal Modeling"},{"paperId":"ea8134912282c4f8743800051050889ff7f38ff8","title":"Less is More: Selective Layer Finetuning with SubTuning"},{"paperId":"0e57006711cc83095eeee02c12e371a5d991a2e7","title":"Analyzing Multimodal Objectives Through the Lens of Generative Diffusion Guidance"},{"paperId":"42c7c81f6b388334d65392a76ca37661660c8087","title":"Re-ViLM: Retrieval-Augmented Visual Language Model for Zero and Few-Shot Image Captioning"},{"paperId":"f2b528e716f09f9e4bd6a45847c95814c47722c9","title":"SimCon Loss with Multiple Views for Text Supervised Semantic Segmentation"},{"paperId":"fac388b8c24044dea06cc8c7b03dd1d99c8439a0","title":"AIM: Adapting Image Models for Efficient Video Action Recognition"},{"paperId":"64caaab51d8339f1b99874d3bddb79debbe661ca","title":"mPLUG-2: A Modularized Multi-modal Foundation Model Across Text, Image and Video"},{"paperId":"6d7534a41fc933f4f6a99e039f585dc57a370a29","title":"ADAPT: Action-aware Driving Caption Transformer"},{"paperId":"463910f5a3abaa8d41dbbeeedd49d5746c1ab6b8","title":"UPop: Unified and Progressive Pruning for Compressing Vision-Language Transformers"},{"paperId":"81620597ffafb4368cf0fe4fab7b7cd4506e09cd","title":"Advancing Radiograph Representation Learning with Masked Record Modeling"},{"paperId":"3f5b31c4f7350dc88002c121aecbdc82f86eb5bb","title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models"},{"paperId":"af34181ae916e01c72513f915f984a1d2c7febab","title":"Augmented Behavioral Annotation Tools, with Application to Multimodal Datasets and Models: A Systematic Review"},{"paperId":"874deb5f06f35e52ae13a921b23611eec4abd1da","title":"ClimaX: A foundation model for weather and climate"},{"paperId":"fcaafd1064c6aa32d6517002ff7c5ebd878c90ed","title":"Masked Autoencoding Does Not Help Natural Language Supervision at Scale"},{"paperId":"b759f3fcf2459013c710bc0b000c46c8e70f9bf8","title":"PRUDEX-Compass: Towards Systematic Evaluation of Reinforcement Learning in Financial Markets"},{"paperId":"9b5a11d9bb3790dbbb02725231b290f67579469a","title":"A Survey of Self-Supervised Learning from Multiple Perspectives: Algorithms, Theory, Applications and Future Trends"},{"paperId":"701a9882884a473faa92324ea6c1ff6c9dacc3ce","title":"Toward Building General Foundation Models for Language, Vision, and Vision-Language Understanding Tasks"},{"paperId":"d16ac1cc0036ffda0d44383304df8bd4f8e38c95","title":"Vision Transformers Are Good Mask Auto-Labelers"},{"paperId":"e0b63fd4dd74239a7eb1b75e0108ca55bcad782d","title":"All in Tokens: Unifying Output Space of Visual Tasks via Soft Token"},{"paperId":"aa5645b4896acb72aa4893d174af765d962aa708","title":"Policy Pre-training for Autonomous Driving via Self-supervised Geometric Modeling"},{"paperId":"6f6c19b44c1e82e24d2b34683669e93277539021","title":"Disjoint Masking with Joint Distillation for Efficient Masked Image Modeling"},{"paperId":"7096894fe8b91ff05f9969cff559097b15069245","title":"On Realization of Intelligent Decision-Making in the Real World: A Foundation Decision Model Perspective"},{"paperId":"ab972a92dd5ac31f8b8b026a64707bfeb3149397","title":"Do DALL-E and Flamingo Understand Each Other?"},{"paperId":"007323e9a19faa7be415eb2122dd331b11a54989","title":"Reversible Column Networks"},{"paperId":"0c0300f53c01ae609c97395c98de4c9d85d92876","title":"MultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning"},{"paperId":"967907503b24423b9b74621051811fcf684e3957","title":"Generalized Decoding for Pixel, Image, and Language"},{"paperId":"9575afb5702bc33d7df14c48feeee5901ea00369","title":"A Length-Extrapolatable Transformer"},{"paperId":"8165c92e8794cc197b5f9909487b79d1bcf2c0b2","title":"Position-guided Text Prompt for Vision-Language Pre-training"},{"paperId":"fbed623ca22abaa493081f7d97be51b1c317d437","title":"Transferring General Multimodal Pretrained Models to Text Recognition"},{"paperId":"89a1dbbfd4c96d90b769f5d3427bd970b082898e","title":"BEATs: Audio Pre-Training with Acoustic Tokenizers"},{"paperId":"22471140ae31b15dd55241e4be0c8bb851961ddc","title":"Autoencoders as Cross-Modal Teachers: Can Pretrained 2D Image Transformers Help 3D Representation Learning?"},{"paperId":"30279ffe74bc5eccbb37bf7082056e7065727bc4","title":"On Human Visual Contrast Sensitivity and Machine Vision Robustness: A Comparative Study"},{"paperId":"1b31dbf44e68b698120552366df03e6e35a1e428","title":"Objaverse: A Universe of Annotated 3D Objects"},{"paperId":"ec8f75e22ffbb5ad7e2f9cfc20a7780eed45715b","title":"CLIPPO: Image-and-Language Understanding from Pixels Only"},{"paperId":"497a1accfd0be6cad1be4f2b6fa88078dae7414a","title":"Quant 4.0: Engineering Quantitative Investment with Automated, Explainable and Knowledge-driven Artificial Intelligence"},{"paperId":"fe34137e5cc07235eae65ce53a54cd226b9f8b23","title":"MAGVIT: Masked Generative Video Transformer"},{"paperId":"3e8251f259dc529b3aa2366fc68c1516b202cfb9","title":"REVEAL: Retrieval-Augmented Visual-Language Pre-Training with Multi-Source Multimodal Knowledge Memory"},{"paperId":"8ca316a10a2749e4c6bf3d0284e8cce2f56a4543","title":"Open Vocabulary Semantic Segmentation with Patch Aligned Contrastive Learning"},{"paperId":"f5c853861fcde704a7100e24e963c5262e625229","title":"Video-Text Modeling with Zero-Shot Transfer from Contrastive Captioners"},{"paperId":"c8dbf43fc20160814b9506de32be86ada91fa725","title":"VindLU: A Recipe for Effective Video-and-Language Pretraining"},{"paperId":"d232d97761490828f20e9b77d2c91a135a7270ee","title":"OFASys: A Multi-Modal Multi-Task Learning System for Building Generalist Models"},{"paperId":"933b37b21e9d61139660088adb032ff3fdf56d86","title":"Learning Video Representations from Large Language Models"},{"paperId":"508d9b43832790b4d35f4ae1fa76e9712859d6aa","title":"Vision and Structured-Language Pretraining for Cross-Modal Food Retrieval"},{"paperId":"458e3d2be80c401fb47e562d9d57012bd63da1c3","title":"Talking Head Generation with Probabilistic Audio-to-Visual Diffusion Priors"},{"paperId":"325d8e9501af05e594bd668b6cd6d43ed42c8b4d","title":"InternVideo: General Video Foundation Models via Generative and Discriminative Learning"},{"paperId":"17066da1e298a997c123f551bf0515daccc2b7b5","title":"Unifying Vision, Text, and Layout for Universal Document Processing"},{"paperId":"36306b2de6e9b2b11d53b029e754b03977de6072","title":"Compound Tokens: Channel Fusion for Vision-Language Representation Learning"},{"paperId":"1db3db7e00fd53290f0c0d07f22937ed5fedfabf","title":"Masked Contrastive Pre-Training for Efficient Video-Text Retrieval"},{"paperId":"040fdeabc5e934f13eb7b05c1907568b7d7efe81","title":"Knowledge Helps Pretrained Model: An Ensemble Model of Knowledge Model and CLIP for Zero-shot Image Recognition"},{"paperId":"3dc7cb6c14cec8b02a150bfb8ce95e8e3e8a01f2","title":"Synaptic Dynamics Realize First-order Adaptive Learning and Weight Symmetry"},{"paperId":"91694fc5f0bae350157f4fc565d0207ae12f7eb9","title":"FusionBrain: Research Project in Multimodal and Multitask Learning"},{"paperId":"4e6a2d863aeaafed82a8411f01be6e5a9f801b44","title":"SLAN: Self-Locator Aided Network for Cross-Modal Understanding"},{"paperId":"f64111aa1a5695e9209bca131469b1dc184d91d0","title":"Seeing What You Miss: Vision-Language Pre-training with Semantic Completion Learning"},{"paperId":"1c199e3d50349153c0b6200006b02fbe66c27acd","title":"X2-VLM: All-In-One Pre-trained Model For Vision-Language Tasks"},{"paperId":"804313d73567df395aa2cb4badabb735435f6bd1","title":"DETRs with Collaborative Hybrid Assignments Training"},{"paperId":"00b3421e147da7a0c1b60c15c878532cfc93ece6","title":"VATLM: Visual-Audio-Text Pre-Training with Unified Masked Prediction for Speech Representation Learning"},{"paperId":"fd8c1b8741163d8737652fbcd3507bcd7d6225c7","title":"Multitask Vision-Language Prompt Tuning"},{"paperId":"33ef78737ba57ecc1ff98c22369a8e17ed90eb98","title":"Synthesizing Coherent Story with Auto-Regressive Latent Diffusion Models"},{"paperId":"ee96ec926f06ff2f3ce3d131cffcbfe63af39f0c","title":"Towards All-in-one Pre-training via Maximizing Multi-modal Mutual Information"},{"paperId":"30a3731f09e7a391e79a28fa736fa6bdd8331866","title":"Uni-Perceiver v2: A Generalist Model for Large-Scale Vision and Vision-Language Tasks"},{"paperId":"96282f6456b10da5acfb8268632ef86645f4b339","title":"Cross-Modal Adapter for Text-Video Retrieval"},{"paperId":"b2286da2b293b50644e5dc8ddd75eb8651e8b257","title":"UniFormerV2: Spatiotemporal Learning by Arming Image ViTs with Video UniFormer"},{"paperId":"78281482c1fdad8e167bab39cc9955c73d58ae8f","title":"EVA: Exploring the Limits of Masked Visual Representation Learning at Scale"},{"paperId":"db3b99c407ff8a06bdc96151dfae1328fadfb858","title":"Grafting Pre-trained Models for Multimodal Headline Generation"},{"paperId":"26c80bd65baa90f5b18157de4951f4eb0b62ab69","title":"InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions"},{"paperId":"6a993404e07687b7edb7fb9a05092213a9419859","title":"OneFormer: One Transformer to Rule Universal Image Segmentation"},{"paperId":"b839b60ffcdedf3f0dfa43da3eefe843307679f3","title":"Towards Reasoning-Aware Explainable VQA"},{"paperId":"c90a33f1f0049d524e9b5b3174d35611fd9a8096","title":"Pretraining in Deep Reinforcement Learning: A Survey"},{"paperId":"58a18a0937fc199e87fbd455af3a53b157462217","title":"Group DETR v2: Strong Object Detector with Encoder-Decoder Pretraining"},{"paperId":"3526c4f136b70b0b8c050a5e2e2926103da1c871","title":"Semantic Segmentation Algorithms for Ground AGV and UAV Medical Transport Scenes"},{"paperId":"3c2b12824b0027edb49b68300cbeab02cfc49ca8","title":"Chinese CLIP: Contrastive Vision-Language Pretraining in Chinese"},{"paperId":"82a867d6d699231bf4de20dcac8efb293d11e7df","title":"State-of-the-art Models for Object Detection in Various Fields of Application"},{"paperId":"c3d38dcba5b954d7b9919eb3f6f90afd095f1801","title":"Behavioral Intention Prediction in Driving Scenes: A Survey"},{"paperId":"9a6d83c836ce6389b526b941d971eee775aa573e","title":"ERNIE-ViLG 2.0: Improving Text-to-Image Diffusion Model with Knowledge-Enhanced Mixture-of-Denoising-Experts"},{"paperId":"30477855d76058a9b542cabea3058aad1a837d51","title":"A Case for Business Process-Specific Foundation Models"},{"paperId":"aeaf6966d460f28db97609e9baa45276395d05d5","title":"Less is More: Learning Simplicity in Datacenter Scheduling"},{"paperId":"eba51c023f3ae9eeca783893b973db60e7a99a6c","title":"A Unified View of Masked Image Modeling"},{"paperId":"07099fe26ee8850c9ccba6fe2ee139d67289b67c","title":"Foundation Transformers"},{"paperId":"6540916e3ebaeaccefeaa303ba94c50bd581ff2a","title":"Like a bilingual baby: The advantage of visually grounding a bilingual language model"},{"paperId":"a7336f6afd4eb817e66ca3eeb9c0a89ffacd53ed","title":"VoLTA: Vision-Language Transformer with Weakly-Supervised Local-Feature Alignment"},{"paperId":"29c2d3d77b6d6f24f4356d5ba20c1a6ab4229c76","title":"Open-Vocabulary Semantic Segmentation with Mask-adapted CLIP"},{"paperId":"25425e299101b13ec2872417a14f961f4f8aa18e","title":"VIMA: General Robot Manipulation with Multimodal Prompts"},{"paperId":"10667c1ae4b49808772b5a377c5b52196701267f","title":"When and why vision-language models behave like bags-of-words, and what to do about it?"},{"paperId":"836ca61c0226fd5f763335ad4c13acc784251343","title":"Towards a Unified View on Visual Parameter-Efficient Transfer Learning"},{"paperId":"13d78bde4dc7059ab941871048ffa91d556584c8","title":"Where Should I Spend My FLOPS? Efficiency Evaluations of Visual Pre-training Methods"},{"paperId":"9b9fb973e5d3b413baa90648d9eb0743bd889747","title":"Spotlight: Mobile UI Understanding using Vision-Language Models with a Focus"},{"paperId":"28630034bb29760df01ab033b743e30b37f336ae","title":"PaLI: A Jointly-Scaled Multilingual Language-Image Model"},{"paperId":"2b6e343fb16848c5d94f8723512dcfb266ed3162","title":"Correlation Information Bottleneck: Towards Adapting Pretrained Multimodal Models for Robust Visual Question Answering"},{"paperId":"0afc96eca8b94d19a4c98fdd78e5fd9c68f6859a","title":"Statistical Foundation Behind Machine Learning and Its Impact on Computer Vision"},{"paperId":"29ac542838974c75a3ac40e5855ec7d346ea87ee","title":"Design of the topology for contrastive visual-textual alignment"},{"paperId":"599be9043ef3571f65758cf36e184c9dc1781baf","title":"BEiT v2: Masked Image Modeling with Vector-Quantized Visual Tokenizers"},{"paperId":"620369d6ed3ed68c3e4374d6ddf282e0b036d2f8","title":"Masked Vision and Language Modeling for Multi-modal Representation Learning"},{"paperId":"8b5eab31e1c5689312fff3181a75bfbf5c13e51c","title":"Unified-IO: A Unified Model for Vision, Language, and Multi-Modal Tasks"},{"paperId":"4c2668b3ae22fa592716480ec56012775b139f52","title":"Bridge-Tower: Building Bridges Between Encoders in Vision-Language Representation Learning"},{"paperId":"622428f5122ad12a40229e1768ecb929fd747ee7","title":"Multimodal Learning with Transformers: A Survey"},{"paperId":"53ae1072fd04080e4fc2c9205ebcbc2683d7264c","title":"Sparse Mixture-of-Experts are Domain Generalizable Learners"},{"paperId":"5598c4ece8ffc69a7eb584d16f6de6629044e76a","title":"Vision Transformer Adapter for Dense Predictions"},{"paperId":"c26bb68806a992bf4fc85b5639e1657a445c4781","title":"On the Representation Collapse of Sparse Mixture of Experts"},{"paperId":"fa717a2e31f0cef4e26921f3b147a98644d2e64c","title":"Focal Modulation Networks"},{"paperId":"3c9ba25baca64151af4e9d50c7947de28eb2a599","title":"Survey of Hallucination in Natural Language Generation"},{"paperId":"2bd6afb11d81fb97bf6e2114043c35bd12c96ce9","title":"P A LI: A J OINTLY -S CALED M ULTILINGUAL L ANGUAGE -I MAGE M ODEL"},{"paperId":"f920895447be7eef1e53e415d7f0b7d69a9e8551","title":"CLIP-VG: Self-paced Curriculum Adapting of CLIP via Exploiting Pseudo-Language Labels for Visual Grounding"},{"paperId":"397156af1b4fc1f1c3bbbe5c2dbc698ef0b9b6ec","title":"Policy Pre-training for End-to-end Autonomous Driving via Self-supervised Geometric Modeling"},{"paperId":"14c840a7faa15a7e42e5664b5e896878d91dd8ae","title":"Self Supervision Does Not Help Natural Language Supervision at Scale"},{"paperId":"240906480b02f4903d74d8b30477f45683599b95","title":"SubTuning: Efficient Finetuning for Multi-Task Learning"},{"paperId":"e39e5601bde42fcc3ef770dc03e4607f42963bbe","title":"Bootstrap The Original Latent: Freeze-and-thaw Adapter for Back-Propagated Black-Box Adaptation"},{"paperId":"14cee97a36e2baca80317d94665bbf3508736fcf","title":"Object Detection and X-Ray Security Imaging: A Survey"},{"paperId":"22851d98d76886308bc7cf817efe3fc1bc4f041a","title":"Self-Supervised Pretraining via Multimodality Images With Transformer for Change Detection"},{"paperId":"8e6394886157578aae839f72bac933414eaa8b1b","title":"ArK: Augmented Reality with Knowledge Emergent Infrastructure"},{"paperId":"7bf7172cb53c1c009bfed7c4b6e6f6898519168a","title":"MODALITY-AWARE ADAPTATION OF CONTRASTIVE LANGUAGE-IMAGE MODELS"},{"paperId":"ef1f21278ac55c4ef37c8a6a11cfebbfa6539d69","title":"Unifying Cross-Lingual and Cross-Modal Modeling Towards Weakly Supervised Multilingual Vision-Language Pre-training"},{"paperId":"290400d20170480edb36399cafe0f2fad510c635","title":"S POTLIGHT : M OBILE UI U NDERSTANDING USING V ISION -L ANGUAGE M ODELS WITH A F OCUS"},{"paperId":"8d6da4ea99898b208d93e7cba4d0ab0b8e160002","title":"W HEN AND WHY V ISION -L ANGUAGE M ODELS BE HAVE LIKE B AGS - OF -W ORDS , AND WHAT TO DO ABOUT IT ?"},{"paperId":"0d0269f8533a33c3c310fd0a59815aa16a0c47ff","title":"Perception Test : A Diagnostic Benchmark for Multimodal Models"},{"paperId":"363270facc8a3ff229a2688f29f16800d45092ff","title":"A Unified View of Masked Image Modeling"},{"paperId":"2096246df0649fbbd9adc7895d2a50beeb46b6b7","title":"Structured Vision-Language Pretraining for Computational Cooking"},{"paperId":"660ddea322b193c7428f2a3149ebe3d24ff9d88d","title":"Image-and-Language Understanding from Pixels Only"},{"paperId":"abb205e2d1a20acc3088b93d92de1ee754e37e1e","title":"C ONNECTING REPRESENTATION AND GENERATION VIA MASKED VISION - LANGUAGE TRANSFORMER"},{"paperId":null,"title":"COMPRESSING VISION-LANGUAGE TRANSFORMERS"}],"references":[{"paperId":"599be9043ef3571f65758cf36e184c9dc1781baf","title":"BEiT v2: Masked Image Modeling with Vector-Quantized Visual Tokenizers"},{"paperId":"3e448df5aa191f7a3945d0fd609c8bc5966a2333","title":"HorNet: Efficient High-Order Spatial Interactions with Recursive Gated Convolutions"},{"paperId":"a8fd9c1625011741f74401ff9bdc1c584e25c86d","title":"Language Models are General-Purpose Interfaces"},{"paperId":"49b5ffebdbcbd683010a2558a19eaa9b21cd8c34","title":"GLIPv2: Unifying Localization and Vision-Language Understanding"},{"paperId":"eb940c4169b83d2a204aec4b8547d1b1a8d0491c","title":"Mask DINO: Towards A Unified Transformer-based Framework for Object Detection and Segmentation"},{"paperId":"0aa3cd5ab502b3dd7f23cf4781cd44305a642bea","title":"VL-BEiT: Generative Vision-Language Pretraining"},{"paperId":"5c4f8de98525eebd762773093d149ba459cef290","title":"Contrastive Learning Rivals Masked Image Modeling in Fine-tuning via Feature Distillation"},{"paperId":"5598c4ece8ffc69a7eb584d16f6de6629044e76a","title":"Vision Transformer Adapter for Dense Predictions"},{"paperId":"a26a7a74f1e5fd562be95c3611a0680759fbdf84","title":"CoCa: Contrastive Captioners are Image-Text Foundation Models"},{"paperId":"26218bdcc3945c7edae7aa2adbfba4cd820a2df3","title":"Flamingo: a Visual Language Model for Few-Shot Learning"},{"paperId":"2ad12a7be5eaf339a98c4defd8669e11fe726acc","title":"MaxViT: Multi-Axis Vision Transformer"},{"paperId":"a09cbcaac305884f043810afc4fa4053099b5970","title":"Exploring Plain Vision Transformer Backbones for Object Detection"},{"paperId":"54020e5fe48ebb250f27d744e20a63cac2988a84","title":"Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time"},{"paperId":"850aafb5c565e3ba4e374a9367bc464c1b8d8676","title":"DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection"},{"paperId":"1bfa62ddfa3f6691e0e40c06f8ead594b6449cfa","title":"Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework"},{"paperId":"a3b42a83669998f65df60d7c065a70d07ca95e99","title":"BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation"},{"paperId":"2fd6f77540c1cc8e70b96208ccf9971b4251fc02","title":"FLAVA: A Foundational Language And Vision Alignment Model"},{"paperId":"658a017302d29e4acf4ca789cb5d9f27983717ff","title":"Masked-attention Mask Transformer for Universal Image Segmentation"},{"paperId":"9137efc758f80dd22bb56f82cca5c94f78a5db3e","title":"MViTv2: Improved Multiscale Vision Transformers for Classification and Detection"},{"paperId":"21ec90872abd986c12afe39bebe807732ffa70c9","title":"Florence: A New Foundation Model for Computer Vision"},{"paperId":"be0fbb810583930c071d0b9b2c5187fe260783f5","title":"Swin Transformer V2: Scaling Up Capacity and Resolution"},{"paperId":"f675c62abfa788ea0be85d3124eba15a14d5e9d6","title":"FILIP: Fine-grained Interactive Language-Image Pre-Training"},{"paperId":"cf7c2e0e4fb2af689aaf4b7a7cddf7b1f4d5e3f0","title":"VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts"},{"paperId":"633d88b3b9c2dd578d9cfa90aa0abecd6ac86789","title":"s2s-ft: Fine-Tuning Pretrained Transformer Encoders for Sequence-to-Sequence Learning"},{"paperId":"5e00596fa946670d894b1bdaeff5a98e3867ef13","title":"SimVLM: Simple Visual Language Model Pretraining with Weak Supervision"},{"paperId":"b82c5f9efdb2ae56baa084ca41aeddd8a665c1d1","title":"Align before Fuse: Vision and Language Representation Learning with Momentum Distillation"},{"paperId":"fd51da088c5fe89eba0e363edad746bb3c2407d1","title":"End-to-End Semi-Supervised Object Detection with Soft Teacher"},{"paperId":"722ad6ac92286507437b31486f47987d6ece05c9","title":"BEiT: BERT Pre-Training of Image Transformers"},{"paperId":"9f4b69762ffb1ba42b573fd4ced996f3153e21c0","title":"CoAtNet: Marrying Convolution and Attention for All Data Sizes"},{"paperId":"2a805d0e1b067444a554c5169d189fa1f649f411","title":"Scaling Vision Transformers"},{"paperId":"1ee1160b8c7c70ded02e786c184a6da651e88bed","title":"Dynamic Head: Unifying Object Detection Heads with Attentions"},{"paperId":"63c74d15940af1af9b386b5762e4445e54c73719","title":"VinVL: Revisiting Visual Representations in Vision-Language Models"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"394be105b87e9bfe72c20efe6338de10604e1a11","title":"Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts"},{"paperId":"141a5033d9994242b18bb3b217e79582f1ee9306","title":"Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision"},{"paperId":"0839722fb5369c0abaff8515bfc08299efc790a1","title":"ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision"},{"paperId":"268d347e8a55b5eb82fb5e7d2f800e33c75ab18a","title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"},{"paperId":"2f5f81bc516a6d085d39479378af1fc27104f91e","title":"Large-Scale Adversarial Training for Vision-and-Language Representation Learning"},{"paperId":"818e5cbc337e4e1b98e65a2d7c2d6d2a0318cd57","title":"Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks"},{"paperId":"f64e1d6bc13aae99aab5449fc9ae742a9ba7761e","title":"UniLMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training"},{"paperId":"c5ff974a69fd0c760b4855b819e61e89f31cfffe","title":"Objects365: A Large-Scale, High-Quality Dataset for Object Detection"},{"paperId":"d8a305b9366608d54452ac30459ee57b4f5cf1c9","title":"UNITER: UNiversal Image-TExt Representation Learning"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"bc626a52664e948a0ffb2b95d0e1e6377a01171a","title":"Cascade R-CNN: High Quality Object Detection and Instance Segmentation"},{"paperId":"f4327b978dec52f16b089c222c43543f8ecf4717","title":"arXiv"},{"paperId":"1c71771c701aadfd72c5866170a9f5d71464bb88","title":"Unified Language Model Pre-training for Natural Language Understanding and Generation"},{"paperId":"cf336d272a30d6ad6141db67faa64deb8791cd61","title":"A Corpus for Reasoning about Natural Language Grounded in Photographs"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"b4df354db88a70183a64dbc9e56cf14e7669a6c0","title":"Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning"},{"paperId":"d7b6753a2d4a2b286c396854063bde3a91b75535","title":"A Simple Method for Commonsense Reasoning"},{"paperId":"155b7782dbd713982a4133df3aee7adfd0b6b304","title":"Unsupervised Feature Learning via Non-parametric Instance Discrimination"},{"paperId":"d07284a6811f1b2745d91bdb06b040b57f226882","title":"Decoupled Weight Decay Regularization"},{"paperId":"a82c1d1ccaa3a3d1d6ee6677de0eed2e93ddb6e8","title":"Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"53c0aa8d33d240197caff824a6225fb223c1181c","title":"Soft-NMS — Improving Object Detection with One Line of Code"},{"paperId":"7e232313a59d735ef7c8a9f4cc7bc980a29deb5e","title":"Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering"},{"paperId":"88512be44744615f4baa8e14f600f036db4c2433","title":"Semantic Understanding of Scenes Through the ADE20K Dataset"},{"paperId":"51db1f3c8dfc7d4077da39c96bb90a6358128111","title":"Deep Networks with Stochastic Depth"},{"paperId":"afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d","title":"Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations"},{"paperId":"0e6824e137847be0599bb0032e37042ed2ef5045","title":"Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books"},{"paperId":"11c9c31dff70de92ada9160c78ff8bb46b2912d6","title":"Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models"},{"paperId":"55e022fb7581bb9e1fce678d21fb25ffbb3fbb88","title":"Deep Visual-Semantic Alignments for Generating Image Descriptions"},{"paperId":"e74f9b7f8eec6ba4704c206b93bc8079af3da4bd","title":"ImageNet Large Scale Visual Recognition Challenge"},{"paperId":"71b7178df5d2b112d07e45038cb5637208659ff7","title":"Microsoft COCO: Common Objects in Context"},{"paperId":"8e080b98efbe65c02a116439205ca2344b9f7cd4","title":"Im2Text: Describing Images Using 1 Million Captioned Photographs"},{"paperId":"bc6dff14a130c57a91d5a21339c23471faf1d46f","title":"Et al"},{"paperId":"b5d998e26c6ca1eb24f0008125baddac273c80f7","title":"Tasks"},{"paperId":null,"title":"In 2021 IEEE/CVF International Conference on Computer Vision"},{"paperId":null,"title":"In 7th International Conference on Learning Representations"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","title":"Improving Language Understanding by Generative Pre-Training"},{"paperId":null,"title":"and Ilya Sutskever"},{"paperId":null,"title":"In 2018 IEEE Conference on Computer Vision and Pattern Recognition"},{"paperId":null,"title":"editors"}],"id":"02251886950770e82b3d68564d60cdfe15e73199","summary":"This work introduces a general-purpose multimodal foundation model BEiT-3, which achieves state-of-the-art transfer performance on both vision and vision-language tasks and introduces Multiway Transformers for general- Purpose modeling, where the modular architecture enables both deep fusion and modality-specific encoding."},{"url":"https://www.semanticscholar.org/paper/336ce63b472a65f053f854d45851d6f0e896f05e","title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models","venue":"arXiv.org","year":2023,"referenceCount":43,"citationCount":286,"influentialCitationCount":10,"publicationDate":"30/01/2023","authors":"Junnan Li,Dongxu Li,S. Savarese,Steven Hoi","citations":[{"paperId":"d1496fda35c92e187e4d05c0c33da90c36dfc62b","title":"Identifying Interpretable Subspaces in Image Representations"},{"paperId":"2386728cb0d8de41cc617529ee79fca7a8d5dfb7","title":"Large language models shape and are shaped by society: A survey of arXiv publication patterns"},{"paperId":"9481025cbdfce21953b75fa395483cb368a2f270","title":"Improving Multimodal Datasets with Image Captioning"},{"paperId":"2e52178416b93a0a7dc1cb7d21d04c7e72c2ce0f","title":"ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring Instruction Tuning"},{"paperId":"8ab6174791a0299e779804300142237d1669c743","title":"Multimodal LLMs for health grounded in individual-specific data"},{"paperId":"962ccf1fc49c83817fb031e5b24b81b19cdfb89d","title":"BuboGPT: Enabling Visual Grounding in Multi-Modal LLMs"},{"paperId":"4d3b07859ee1fec686d82ba1975028e01fb5dd83","title":"BUS:Efficient and Effective Vision-language Pre-training with Bottom-Up Patch Summarization"},{"paperId":"63cf8add3a25c546190f5b76c31e63dd3d218c40","title":"Planting a SEED of Vision in Large Language Model"},{"paperId":"c342ed4b2f0b4ce665a7fc9f33e6adc9c90ef75c","title":"SINC: Self-Supervised In-Context Learning for Vision-Language Tasks"},{"paperId":"06f7066e76e5acf7b03d4fda2c0d12d71a209185","title":"Fine-grained Text-Video Retrieval with Frozen Image Encoders"},{"paperId":"d58b89b64a0565e77d9b9734d871c58e4a7af6d8","title":"mBLIP: Efficient Bootstrapping of Multilingual Vision-LLMs"},{"paperId":"369b449415d50387fba048bbd4d26ee890df84b5","title":"InternVid: A Large-scale Video-Text Dataset for Multimodal Understanding and Generation"},{"paperId":"ba63203d7f91d4135d2a392e841ce532c006e31c","title":"VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models"},{"paperId":"6f8ec2fd581c50f06ffbb9b4b44d0e0a591a433a","title":"MMBench: Is Your Multi-modal Model an All-around Player?"},{"paperId":"f5945b1421ee8e0cd59e674b73f4ad82a71f66c7","title":"T2I-CompBench: A Comprehensive Benchmark for Open-world Compositional Text-to-image Generation"},{"paperId":"94053805cd59f2e9a47fe3f080c7e7afefb337cc","title":"Generative Pretraining in Multimodality"},{"paperId":"650d7cf842b536dc6a3f46c189b9d8e3c8cc07a1","title":"Divide, Evaluate, and Refine: Evaluating and Improving Text-to-Image Alignment with Iterative VQA Feedback"},{"paperId":"85d9151aa2efd0cbe822e403138cfe49f9536703","title":"SITTA: A Semantic Image-Text Alignment for Image Captioning"},{"paperId":"0fea183c4f49a015a8ee7d89ef2e6885b7023c10","title":"SVIT: Scaling up Visual Instruction Tuning"},{"paperId":"34f1e90c4e73e05105b0b75fd72d8e7a7345fb74","title":"SAS Video-QA: Self-Adaptive Sampling for Efficient Video Question-Answering"},{"paperId":"ef4b604fca0c62dcd0d5caf7ca24ad74e285632d","title":"MultiQG-TI: Towards Question Generation from Multi-modal Sources"},{"paperId":"420f03d44a3bee8c1c51216e1045d0ff92e8fefc","title":"Vision Language Transformers: A Survey"},{"paperId":"2f8e2feb7161135c778b0f128e9e6557a51fc7bf","title":"Distilling Large Vision-Language Model with Out-of-Distribution Generalizability"},{"paperId":"53937df61d496572e90ee34c670ddd00337e558d","title":"What Matters in Training a GPT4-Style Language Model with Multimodal Inputs?"},{"paperId":"df710c46594c04fb59ef9a93d3b4e1cb387a1b2b","title":"Embodied Task Planning with Large Language Models"},{"paperId":"16160a4bdd0f239e47f120547e6ecee44636d5e8","title":"JourneyDB: A Benchmark for Generative Image Understanding"},{"paperId":"8d1f2e1beaf6905641740c6fee995f0b3f3e0938","title":"ProbVLM: Probabilistic Adapter for Frozen Vison-Language Models"},{"paperId":"72160b3c0f73c968fcb903db71817d1bed695f4d","title":"Look, Remember and Reason: Visual Reasoning with Grounded Rationales"},{"paperId":"cce44bda2346e4036a68b10b4d1950bf895fb81f","title":"CLIPAG: Towards Generator-Free Text-to-Image Generation"},{"paperId":"a9d5d97733ccb15002ff3cfb95b0a7d8ba5236e3","title":"LLaVAR: Enhanced Visual Instruction Tuning for Text-Rich Image Understanding"},{"paperId":"efc694164312006c543ef745611348ef64e68dda","title":"Towards Language Models That Can See: Computer Vision Through the LENS of Natural Language"},{"paperId":"a8f2c10466d356542b63dd0659dd113f8f0a3b34","title":"Federated Generative Learning with Foundation Models"},{"paperId":"2aa22c271390653cab888e5f8b22fadcdf68c7a9","title":"Palm: Predicting Actions through Language Models @ Ego4D Long-Term Action Anticipation Challenge 2023"},{"paperId":"697ccf8dc77a600286f14bc9cc4cd34d6451e670","title":"REFLECT: Summarizing Robot Experiences for Failure Explanation and Correction"},{"paperId":"def6c12724dec95ec1276a77fd1cf7e200883bdb","title":"Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic"},{"paperId":"b6856376d350edd656f9872b6619b540bd209116","title":"Explainable Multimodal Emotion Reasoning"},{"paperId":"cde934546bbdb19094d8a53cc047d002c827f884","title":"Large Multimodal Models: Notes on CVPR 2023 Tutorial"},{"paperId":"84dc889beff9d51fe429cff8c92735e7410ee3c2","title":"Aligning Large Multi-Modal Model with Robust Instruction Tuning"},{"paperId":"dedbac319177d04ce63fe00d2fec24bdaab90d6d","title":"SugarCrepe: Fixing Hackable Benchmarks for Vision-Language Compositionality"},{"paperId":"3b6179c293df29e31d31cea46476f104ab6950f2","title":"Kosmos-2: Grounding Multimodal Large Language Models to the World"},{"paperId":"8724579d3f126e753a0451d98ff57b165f722e72","title":"Are aligned neural networks adversarially aligned?"},{"paperId":"d212fa27f5868f0fd106e1a7bba908fd47da0816","title":"MotionGPT: Human Motion as a Foreign Language"},{"paperId":"f8401e317f7f76b8772ace70df11327f079dc090","title":"Chain-of-Thought Prompt Distillation for Multimodal Named Entity and Multimodal Relation Extraction"},{"paperId":"ebedc4d7a2356090904baba4104ef0832bc236df","title":"A Survey on Multimodal Large Language Models"},{"paperId":"697e0add95e880bd42e00bef838181e105f91981","title":"MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models"},{"paperId":"f3a8d7ae2ea571ca879368167ab51b71cfa423bf","title":"TaCA: Upgrading Your Visual Foundation Model with Task-agnostic Compatible Adapter"},{"paperId":"738852940591ecf864abf402878ecf66e2945267","title":"Visual Adversarial Examples Jailbreak Large Language Models"},{"paperId":"5147e51ecb72424bb8dc57beaa2ade460134c3f9","title":"Generative Multimodal Entity Linking"},{"paperId":"e9e6e08a9a39aa8d60b41da22cfb5670e64adeaf","title":"RS5M: A Large Scale Vision-Language Dataset for Remote Sensing Vision-Language Foundation Model"},{"paperId":"e5d27e79d10a056cdeb86ca25853da8797413afb","title":"Improving Image Captioning Descriptiveness by Ranking and LLM-based Fusion"},{"paperId":"8ca84be6788622ed6700d2799bb6460b15707fca","title":"Dense Video Object Captioning from Disjoint Supervision"},{"paperId":"c50ebdb0be883bb4e21270ba990220745c6a9cf9","title":"RemoteCLIP: A Vision Language Foundation Model for Remote Sensing"},{"paperId":"7839d037bb0e41f8a9898f177d2710cfe23633fc","title":"Path to Medical AGI: Unify Domain-specific Medical LLMs with the Lowest Cost"},{"paperId":"bc2333c9a667af90ee7ce52b911d2e04aed01526","title":"MotionGPT: Finetuned LLMs are General-Purpose Motion Generators"},{"paperId":"454850fcb311faf1de3f4028a312cfeb781857b4","title":"LLMVA-GEBC: Large Language Model with Video Adapter for Generic Event Boundary Captioning"},{"paperId":"8efc20988021ce3b4b05dd44b13e27260ee9b99b","title":"Investigating Prompting Techniques for Zero- and Few-Shot Visual Question Answering"},{"paperId":"a8d02ff6d075c3cc48f0b97801cc52765c8f9ac9","title":"LVLM-eHub: A Comprehensive Evaluation Benchmark for Large Vision-Language Models"},{"paperId":"3c979b0d1ee60280837e955508da12342be54b1c","title":"Training Multimedia Event Extraction With Generated Images and Captions"},{"paperId":"1474d4248e1cbcc91183456cdf1e7272e8a931de","title":"COSA: Concatenated Sample Pretrained Vision-Language Foundation Model"},{"paperId":"bb7dcc08a1865942e9dda0c1d183d67e63303d77","title":"Contrasting Intra-Modal and Ranking Cross-Modal Hard Negatives to Enhance Visio-Linguistic Fine-grained Understanding"},{"paperId":"0983883619a0ca597d055d0e58da2f514052913d","title":"Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration"},{"paperId":"b937b5ad3c1ebe6007e744fa7864ec095e0070ab","title":"Tell Me Where to Go: A Composable Framework for Context-Aware Embodied Robot Navigation"},{"paperId":"966852963a88a28786b798c91b6662d6e501e590","title":"AssistGPT: A General Multi-modal Assistant that can Plan, Execute, Inspect, and Learn"},{"paperId":"051549d8ef56937b2f4d113afdcf8c7586d3770b","title":"Towards AGI in Computer Vision: Lessons Learned from GPT and Large Language Models"},{"paperId":"d4d4ca3f4e41c8121b07fb5d3f1f9514fc6b4c6e","title":"Babel-ImageNet: Massively Multilingual Evaluation of Vision-and-Language Representations"},{"paperId":"84dc5de1b9e7fce69f0546c9a247229baafff501","title":"Retrieve Anyone: A General-purpose Person Re-identification Task with Instructions"},{"paperId":"0b4fd518368afd639912a3425004d5ccc348c4d4","title":"Paste, Inpaint and Harmonize via Denoising: Subject-Driven Image Editing with Pre-Trained Diffusion Model"},{"paperId":"224a6feac6525c56b81c27ecef500aa41603c93d","title":"I See Dead People: Gray-Box Adversarial Attack on Image-To-Text Models"},{"paperId":"29cd4e8504df8c762b0b6eef8299584118feeb88","title":"Image Captioners Are Scalable Vision Learners Too"},{"paperId":"e8cc6a50bc9c6a1e1ed7fdde0e7ccbb4efd4b505","title":"AVIS: Autonomous Visual Information Seeking with Large Language Models"},{"paperId":"7b2e18dbaf71c8f79e1af4c63c68fa4eb465501b","title":"MemoriEase: An Interactive Lifelog Retrieval System for LSC’23"},{"paperId":"6b138a2288a0d72ec3031f14dc103343fa6d3912","title":"Fill-Up: Balancing Long-Tailed Data with Generative Models"},{"paperId":"74538984e72a26254697d4e7eeeb169000cf762a","title":"Valley: Video Assistant with Large Language model Enhanced abilitY"},{"paperId":"4279a38a098d1d359881b73c6a88a112fe93443a","title":"Scalable 3D Captioning with Pretrained Models"},{"paperId":"c581d2ad3b092a2cc152d0c6f55fd6320f78eb3a","title":"A Survey of Vision-Language Pre-training from the Lens of Multimodal Machine Translation"},{"paperId":"79150cb420d15830c8d36f0e91eea1b02e177f0f","title":"Sticker820K: Empowering Interactive Retrieval with Stickers"},{"paperId":"76b7ddc6005a7c8af8486f193c2c995b6a9ac715","title":"Controlling Text-to-Image Diffusion by Orthogonal Finetuning"},{"paperId":"fd755dc7b5b206c17fd953db04e1c888d45b6e4e","title":"LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark"},{"paperId":"2933acb28b7369c7ea5b8728f6d8cb55e1beef98","title":"Customizing General-Purpose Foundation Models for Medical Report Generation"},{"paperId":"fed150a219f9c31bdb4920e615c7c9264c634736","title":"On the Challenges and Perspectives of Foundation Models for Medical Image Analysis"},{"paperId":"a07ced99bc0a02fa9c975c554fb39d4db41bcd41","title":"DocumentCLIP: Linking Figures and Main Body Text in Reflowed Documents"},{"paperId":"a31fc06977e246f86aa1e155759b617343611297","title":"M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models"},{"paperId":"bf7025a2e5dbb3c09deae02a1aa98a256ca559e2","title":"Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models"},{"paperId":"d47524cd5c3c4b57af2e5a29f6f91c420310f236","title":"MIMIC-IT: Multi-Modal In-Context Instruction Tuning"},{"paperId":"da061a6e0016d6b625a8e86d64a797ca8ddb92a5","title":"Modular Visual Question Answering via Code Generation"},{"paperId":"d818f40ea693a335e02f32dab520351d271c58bf","title":"Artificial General Intelligence for Medical Imaging"},{"paperId":"d59c3daedc192ed40a5fe0fe83a594acbc04ed1d","title":"AircraftVerse: A Large-Scale Multimodal Dataset of Aerial Vehicle Designs"},{"paperId":"6a2a756c60dbc99f666ae6e32b0dd1a58e1e2de8","title":"M3IT: A Large-Scale Dataset towards Multi-Modal Multilingual Instruction Tuning"},{"paperId":"a01a9c4a114fbf201540268f928ccf77bc3f9357","title":"Fine-Grained Visual Prompting"},{"paperId":"d7a4b09a0e2c2d7b118144cf09895c640896da7b","title":"Youku-mPLUG: A 10 Million Large-scale Chinese Video-Language Dataset for Pre-training and Benchmarks"},{"paperId":"99368d6fc86e2eb181d9d36165cfed578bfe938d","title":"Q: How to Specialize Large Vision-Language Models to Data-Scarce VQA Tasks? A: Self-Train on Unlabeled Images!"},{"paperId":"705e0f1887d76c956e3a1750f0176f2b8fe121ff","title":"Zero-Shot 3D Shape Correspondence"},{"paperId":"f1892409fdbb396b00bb180891bb1c130fe3c7f4","title":"Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding"},{"paperId":"3aa59581448441b7a05478c6922179ffba086afd","title":"Weakly-Supervised Conditional Embedding for Referred Visual Search"},{"paperId":"ae7ae7b7e123b8d5ec86cc1c53548943e88f386f","title":"Learning Probabilistic Symmetrization for Architecture Agnostic Equivariance"},{"paperId":"16877baf3874038233279e07e330f891455fd880","title":"ChatGPT as a mapping assistant: A novel method to enrich maps with generative AI and content derived from street-level photographs"},{"paperId":"1edbde8f2cf41e74ab2c5fdb4e8aafd6599899d6","title":"MoviePuzzle: Visual Narrative Reasoning through Multimodal Order Learning"},{"paperId":"8213492345c67d2b0e692b6bb5c814d4f1aef8d2","title":"Benchmarking Robustness of Adaptation Methods on Pre-trained Vision-Language Models"},{"paperId":"5c7cd897030eace63862f734d5600b977c7f1d0c","title":"VisualGPTScore: Visio-Linguistic Reasoning with Multimodal Generative Pre-Training Scores"},{"paperId":"77424562deba33e94ea5ca3c662ccfdc2b95fb5c","title":"Vocabulary-free Image Classification"},{"paperId":"31a68755ca6899e6c360ec8568704ae74f223a25","title":"GPT4Image: Can Large Pre-trained Models Help Vision Models on Perception Tasks?"},{"paperId":"3bdfc51afcfb62426b73814067f7ac90cfe42888","title":"Evaluating the Capabilities of Multi-modal Reasoning Models with Synthetic Task Data"},{"paperId":"f7c6553d47bbe5682377e975058592d5d495ad39","title":"“Let’s not Quote out of Context”: Unified Vision-Language Pretraining for Context Assisted Image Captioning"},{"paperId":"5aba372929ddd6184378a018f18fb9beb374f45f","title":"The Hidden Language of Diffusion Models"},{"paperId":"0867f7029b3726740fb41ca8171833bf6f82e483","title":"Exploring Open-Vocabulary Semantic Segmentation without Human Labels"},{"paperId":"2b7c9fd2a94deaee3e7e56dc57bab0bd39d3683c","title":"AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration"},{"paperId":"eafce53443e9e6800c3850807dff74a5bb8c7c2b","title":"Using Visual Cropping to Enhance Fine-Detail Question Answering of BLIP-Family Models"},{"paperId":"7e3bbd7be60bb50a8093152795f269a69a4a0fd9","title":"Chatting Makes Perfect - Chat-based Image Retrieval"},{"paperId":"5fb7afae5fcacae1d40f109a348b43e00aa5d486","title":"Dense and Aligned Captions (DAC) Promote Compositional Reasoning in VL Models"},{"paperId":"d92a9b40ff621b2cc46a1c0266919b643f0f2d28","title":"Boosting Text-to-Image Diffusion Models with Fine-Grained Semantic Rewards"},{"paperId":"d7f1a876b7df0e10627bccb0c5b63faf2a1005e4","title":"PanoGen: Text-Conditioned Panoramic Environment Generation for Vision-and-Language Navigation"},{"paperId":"80be1426825288ff876acb8cc0babcc6629fa644","title":"AlphaBlock: Embodied Finetuning for Vision-Language Reasoning in Robot Manipulation"},{"paperId":"b458fc5261595f44b36325e5eaea1f874d65138f","title":"GPT4Tools: Teaching Large Language Model to Use Tools via Self-instruction"},{"paperId":"fd928577d67dd01048d13f284a6256164bbcf2f0","title":"Learning without Forgetting for Vision-Language Models"},{"paperId":"3798f68b213d2b586199be107fce86335fe01cdc","title":"LANCE: Stress-testing Visual Models by Generating Language-guided Counterfactual Images"},{"paperId":"af804beaa068a8835465c5f027c40b3c6212ffdc","title":"Real-World Image Variation by Aligning Diffusion Inversion Chain"},{"paperId":"ca3a71df6d2ec2abe3ec8a0049ac6fd93b307d96","title":"Generate then Select: Open-ended Visual Question Answering Guided by World Knowledge"},{"paperId":"a08e36ca94f702a6916010dabb09d8e8235ba6b7","title":"InstructEdit: Improving Automatic Masks for Diffusion-based Image Editing With User Instructions"},{"paperId":"50c1414fe41d0cb9db6f0933c9319aa124beac5d","title":"Contextual Object Detection with Multimodal Large Language Models"},{"paperId":"5fbe4c92791fbecb179c1ab79bba9a59b2e155ba","title":"GlyphControl: Glyph Conditional Control for Visual Text Generation"},{"paperId":"5c183d241fe54a6d67e21eea48fbd5ea6b31ec1c","title":"VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset"},{"paperId":"49cc2cf58c66e9d20e32c319b910ece15dc2fab0","title":"FuseCap: Leveraging Large Language Models to Fuse Visual Data into Enriched Image Captions"},{"paperId":"119a3ed0898499fce0ce6af6958d566d82390ba5","title":"GIMLET: A Unified Graph-Text Model for Instruction-Based Molecule Zero-Shot Learning"},{"paperId":"fdeb00439db0c8092503bc7254a63e073e28a1b3","title":"CrossGET: Cross-Guided Ensemble of Tokens for Accelerating Vision-Language Transformers"},{"paperId":"5712960ca1d637ba6e57de43fad3daac04bff4e2","title":"Towards Consistent Video Editing with Text-to-Image Diffusion Models"},{"paperId":"b13242323021bc1483a0d76e23428e324d409315","title":"NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models"},{"paperId":"f8f8267a2acd7598de6c15327f3953241901a62d","title":"On Evaluating Adversarial Robustness of Large Vision-Language Models"},{"paperId":"0fb10f3eea52a1d542b25c818f1308e20c24cbfb","title":"A Reminder of its Brittleness: Language Reward Shaping May Hinder Learning for Instruction Following Agents"},{"paperId":"d0410aae3f6648f5ee4176dc3308e98645e1d4fa","title":"S4M: Generating Radiology Reports by A Single Model for Multiple Body Parts"},{"paperId":"85ed22fe8a7c44900d850fe6bcda51758297a37b","title":"Generating Images with Multimodal Language Models"},{"paperId":"7657e124622858a970815a96991727884088d648","title":"Custom-Edit: Text-Guided Image Editing with Customized Diffusion Models"},{"paperId":"c6ac708b65b24c20f80831d518c1795ce8133ad5","title":"ChatBridge: Bridging Modalities with Large Language Model as a Language Catalyst"},{"paperId":"06091944b864d6dc473cab63321a95fb9c4067cc","title":"ChatCAD+: Towards a Universal and Reliable Interactive CAD using LLMs"},{"paperId":"5eceaeac5d45d49e1d5698947ed8292ff3fccd81","title":"Candidate Set Re-ranking for Composed Image Retrieval with Dual Multi-modal Encoder"},{"paperId":"d3f79210b54e168c76b8c311488f42d7d1048b81","title":"PandaGPT: One Model To Instruction-Follow Them All"},{"paperId":"13b5b69355555e0c8b702261c5de3b4172ba653c","title":"The Art of SOCRATIC QUESTIONING: Zero-shot Multimodal Reasoning with Recursive Thinking and Self-Questioning"},{"paperId":"c5c5b9c6660bb2089b781d851cb3fd0ba271d742","title":"Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models"},{"paperId":"f45a3474bd38d65c1b2cc3342a64dacbf07f445a","title":"Cream: Visually-Situated Natural Language Understanding with Contrastive Reading Model and Frozen Large Language Models"},{"paperId":"00fcc983728346a5f3f8f005f1365be54456728e","title":"EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought"},{"paperId":"81dc25d636c022496dc930cb3c5ba2175592bc6e","title":"Interpretable by Design Visual Question Answering"},{"paperId":"6238fd213197ccf0d79191662828e38118a06d79","title":"ECHo: Event Causality Inference via Human-centric Reasoning"},{"paperId":"8f2f6d90de822888ec4e283788bc09917018e9d6","title":"ImageNetVC: Zero-Shot Visual Commonsense Evaluation on 1000 ImageNet Categories"},{"paperId":"7cf64070fd3d7e53d80f260c10e6bd7018d580e1","title":"IdealGPT: Iteratively Decomposing Vision and Language Reasoning via Large Language Models"},{"paperId":"9837349417e36ef5be06da0fd6c74042148bdaa2","title":"Visual Programming for Text-to-Image Generation and Evaluation"},{"paperId":"37db2cf2ff8f641a30462a6304a9a6e6f0fcd27b","title":"Exploring the Grounding Issues in Image Caption"},{"paperId":"dc0c132b273456b288a785414db2fa72edf87b1a","title":"BLIP-Diffusion: Pre-trained Subject Representation for Controllable Text-to-Image Generation and Editing"},{"paperId":"86cbd30d1096b0c7e4ac6b03d97a8df12fd21457","title":"PathAsst: Redefining Pathology through Generative Foundation AI Assistant for Pathology"},{"paperId":"d57caa06a42baa90eb741a9afb10fe4fff8be82a","title":"SmartTrim: Adaptive Tokens and Parameters Pruning for Efficient Vision-Language Models"},{"paperId":"0744783bbefc12b2b1383bed137e8a80061274b7","title":"Exploring Diverse In-Context Configurations for Image Captioning"},{"paperId":"261937a8fe7228d6c8a3f47bd087118a2be46ca5","title":"Weakly-Supervised Learning of Visual Relations in Multimodal Pretraining"},{"paperId":"b82c1b0512d25307e3c81bb8d9df1607267a7a52","title":"MemeCap: A Dataset for Captioning and Interpreting Memes"},{"paperId":"e9a37d881abf7d94cb2c586f1cb26978343750ba","title":"ReSee: Responding through Seeing Fine-grained Visual Knowledge in Open-domain Dialogue"},{"paperId":"cea7521edef8e4a367ce4d0c278a1057c5204b88","title":"LLM-grounded Diffusion: Enhancing Prompt Understanding of Text-to-Image Diffusion Models with Large Language Models"},{"paperId":"0a61802b71aa044cf1fe0e81befec148e0d5001b","title":"VisorGPT: Learning Visual Prior via Generative Pre-Training"},{"paperId":"2ad8183c72a90511383a32ccaeea313eb85f4085","title":"DetGPT: Detect What You Need via Reasoning"},{"paperId":"065dcc6074ffc9e314799d97c1757e5d23e7e2b1","title":"S-CLIP: Semi-supervised Vision-Language Pre-training using Few Specialist Captions"},{"paperId":"43a55dbd95c9d5cd82de8db276f41adeec4a937d","title":"Interactive Data Synthesis for Systematic Vision Adaptation via LLMs-AIGCs Collaboration"},{"paperId":"0cc49320f77e384a9acde7fa9c1b7c776a4f04a4","title":"If at First You Don't Succeed, Try, Try Again: Faithful Diffusion-based Text-to-Image Generation by Selection"},{"paperId":"ca055cfb9d4d47124cc035c346f38577825fcacf","title":"Enhance Reasoning Ability of Visual-Language Models via Large Language Models"},{"paperId":"684c8dccfe7afc1b05057ebd5df0c90379443796","title":"AudioToken: Adaptation of Text-Conditioned Diffusion Models for Audio-to-Image Generation"},{"paperId":"f9bfc6d9ba1665b73af3323d46c7642b852759ef","title":"VideoLLM: Modeling Video Sequence with Large Language Models"},{"paperId":"251445d8b22b1c25ccad96f284c085dca49b57f3","title":"UVOSAM: A Mask-free Paradigm for Unsupervised Video Object Segmentation via Segment Anything Model"},{"paperId":"3c2fcef50b952097a31cfe1b9e1b1b89d5599744","title":"ViT-TTS: Visual Text-to-Speech with Scalable Diffusion Transformer"},{"paperId":"6118eb18023429fa8bad64b7a1d95533127a62d7","title":"Prompt ChatGPT In MNER: Improved multimodal named entity recognition method based on auxiliary refining knowledge from ChatGPT"},{"paperId":"6a5525c316b9be7909c433a79e090ed731425083","title":"What Makes for Good Visual Tokenizers for Large Language Models?"},{"paperId":"9f411fda2ad5b141a3115f707bcf5ee865b3fb94","title":"Any-to-Any Generation via Composable Diffusion"},{"paperId":"8ae2e81495d426419e6fd96940b651002c046b61","title":"Enhancing Vision-Language Pre-Training with Jointly Learned Questioner and Dense Captioner"},{"paperId":"33f9ddca2469bf4831dcab085e1620792b1a6a80","title":"LLM Itself Can Read and Generate CXR Images"},{"paperId":"a90f2b1fa484a9ee9a6efa13c1734f7510eaf044","title":"VideoFactory: Swap Attention in Spatiotemporal Diffusions for Text-to-Video Generation"},{"paperId":"972501b057e2b84d6ce6506f70bcac697bab7872","title":"LLMScore: Unveiling the Power of Large Language Models in Text-to-Image Synthesis Evaluation"},{"paperId":"d886fc1b43b1c14b1c82ce8e4eab7c48e2c6d7af","title":"Paxion: Patching Action Knowledge in Video-Language Foundation Models"},{"paperId":"86ea4aa29241149c3999301f0285d8cbb8542b11","title":"Vision-Language Pre-training with Object Contrastive Learning for 3D Scene Understanding"},{"paperId":"daf34122a0c38531aeeb55069ba98e564c263d53","title":"MedBLIP: Bootstrapping Language-Image Pre-training from 3D Medical Images and Texts"},{"paperId":"bfa9df38bd8597d8cdf0c3497fe5e5a5e31f646f","title":"ONE-PEACE: Exploring One General Representation Model Toward Unlimited Modalities"},{"paperId":"1bdd5fc17cc580efe998304692639c57c857cc84","title":"Going Denser with Open-Vocabulary Part Segmentation"},{"paperId":"42a30dc5470f54ec249f25d3c31e05d7c376c8e3","title":"VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks"},{"paperId":"c3eee48481b3b8f4be18026e389fadf9a53ad192","title":"Content-based Unrestricted Adversarial Attack"},{"paperId":"8ce6ad6d8a73757309d3b9f525cf15cb68e32397","title":"Efficient Prompting via Dynamic In-Context Learning"},{"paperId":"2da2f8284c03a0c0df0e8b13e90bf215dd5ac786","title":"Listen, Think, and Understand"},{"paperId":"757940cef62b06f6abdb427d5a7fe61d512a2e3f","title":"UniControl: A Unified Diffusion Model for Controllable Visual Generation In the Wild"},{"paperId":"a979975d1a0aea0e01423f092249cc3de575b6cd","title":"X-IQE: eXplainable Image Quality Evaluation for Text-to-Image Generation with Visual Large Language Models"},{"paperId":"413bc628ef54effa9d59f93aa7ffd86a38fd1143","title":"What You See is What You Read? Improving Text-Image Alignment Evaluation"},{"paperId":"f4793adffd6f67ffcb93ccfc5672ab301b8a2b96","title":"PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering"},{"paperId":"3f0c4d50050e8d74993b020897abaee8d1e8054d","title":"Evaluating Object Hallucination in Large Vision-Language Models"},{"paperId":"1a3d6119d9513ad27fa4fc3262e517ec6a6d2261","title":"FastComposer: Tuning-Free Multi-Subject Image Generation with Localized Attention"},{"paperId":"9b3ff4c05be2ee57021099ab07fadfb77440be45","title":"IMAD: IMage-Augmented multi-modal Dialogue"},{"paperId":"3ef08e54c8f97804a89383b497347c8a7eefff7d","title":"A Video Is Worth 4096 Tokens: Verbalize Story Videos To Understand Them In Zero Shot"},{"paperId":"5f51eda9f7abddca027941d50fb0b6bf6f508eff","title":"Make-A-Protagonist: Generic Video Editing with An Ensemble of Experts"},{"paperId":"adf4655e6bb531ec1a03d8ec9e8c5c63ae771fb6","title":"Edit As You Wish: Video Description Editing with Multi-grained Commands"},{"paperId":"83a734dee0809a46bc7189c12cd9956927d14836","title":"Bridging the Domain Gap: Self-Supervised 3D Scene Understanding with Foundation Models"},{"paperId":"ebe9fecee36516832b980e98dd156201fbfbd639","title":"Semantic Composition in Visually Grounded Language Models"},{"paperId":"4b203ee52e27cbf27d210dd671951150729a8259","title":"ULIP-2: Towards Scalable Multimodal Pre-training for 3D Understanding"},{"paperId":"848e690a62c327e1210532d58a6b914097cac763","title":"On the Hidden Mystery of OCR in Large Multimodal Models"},{"paperId":"65051f6836a4a618586c01deff43b46ab5e3f887","title":"Measuring Progress in Fine-grained Vision-and-Language Understanding"},{"paperId":"abac9af9174e8be2f605453695d98e3686768a27","title":"ArtGPT-4: Artistic Vision-Language Understanding with Adapter-enhanced MiniGPT-4"},{"paperId":"8bd6a2a89503be083176f2cc26fabedb79238cbd","title":"InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning"},{"paperId":"8badb0587fef2ffc078b0cec549eb8ec96ed3ad4","title":"Self-Chained Image-Language Model for Video Localization and Question Answering"},{"paperId":"240bc60c98c9b860c27c6f962992618a6775cab1","title":"Musketeer (All for One, and One for All): A Generalist Vision-Language Model with Task Explanation Prompts"},{"paperId":"38be7643bcad936739550a1802220eb53ca9b1df","title":"Simple Token-Level Confidence Improves Caption Correctness"},{"paperId":"1509f216ef556a68aa5639d07b61882945651e60","title":"ChinaOpen: A Dataset for Open-world Multimodal Learning"},{"paperId":"d48cb91b9e555194f7494c4d4bb9815021d3ee45","title":"VideoChat: Chat-Centric Video Understanding"},{"paperId":"b07fa63a6d2f39900f0f2cae8f58cd5507010aad","title":"Multi-Prompt with Depth Partitioned Cross-Modal Learning"},{"paperId":"54a8b153ed04a872da878d695239bdc413dc782c","title":"InternGPT: Solving Vision-Centric Tasks by Interacting with Chatbots Beyond Language"},{"paperId":"80c44fab16852ea9599411da14de7079c4514172","title":"Vision-Language Models in Remote Sensing: Current Progress and Future Trends"},{"paperId":"43e6e8d6663d83f1b74cf5a2be7b040b0928f867","title":"X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages"},{"paperId":"e1ff32753e20e48b4b01e40b5e820254396e6e70","title":"LMEye: An Interactive Perception Network for Large Language Models"},{"paperId":"a6d201a6e73064c2fb3fd9d7aa43e168b8e0a1b0","title":"Towards Applying Powerful Large AI Models in Classroom Teaching: Opportunities, Challenges and Prospects"},{"paperId":"d6d3604f369bb0415cbe814e43ca3131323b03e2","title":"Otter: A Multi-Modal Model with In-Context Instruction Tuning"},{"paperId":"6f8b9192b1f215254ee7625d752710182c05d2f9","title":"Caption Anything: Interactive Image Description with Diverse Multimodal Controls"},{"paperId":"042459a38c1efb4118030309f0bcd7d5ed77d83f","title":"Image Captioners Sometimes Tell More Than Images They See"},{"paperId":"b7e6408841054dcf6efcd9cd77de2561841210b4","title":"Personalize Segment Anything Model with One Shot"},{"paperId":"9a22b33b529484c912d1ea9f8698369d4546a1c1","title":"Transfer Visual Prompt Generator across LLMs"},{"paperId":"b1c26d02a44407de29ee11205d62a9ae72d51057","title":"Segment Anything is A Good Pseudo-label Generator for Weakly Supervised Semantic Segmentation"},{"paperId":"c3068e2a9f4cd374c7ff3be1b8f877b3d653e880","title":"Multimodal Neural Databases"},{"paperId":"570079bbdd8758dfe865097e05719313c9c1301a","title":"LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model"},{"paperId":"7e32aac43e9f1df49e116add03327ee6f365dbf3","title":"mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality"},{"paperId":"c56a51728678e5b2e3ff95e51caf21d267439c36","title":"ChatVideo: A Tracklet-centric Multimodal and Versatile Video Understanding System"},{"paperId":"59dfa986cc7468561d2c19cdb43f816406ea30d8","title":"TR0N: Translator Networks for 0-Shot Plug-and-Play Conditional Generation"},{"paperId":"598b3961f767c1ad40cbb393afd936de4e30d578","title":"SATIN: A Multi-Task Metadataset for Classifying Satellite Imagery using Vision-Language Models"},{"paperId":"8fce3142bc144bdc08bf0cab1db908c7ad3f8454","title":"Contrastive Language, Action, and State Pre-training for Robot Learning"},{"paperId":"27d0d2923a42bd2bced1b100844e232ff87368e3","title":"SkinGPT-4: An Interactive Dermatology Diagnostic System with Visual Large Language Model"},{"paperId":"f44ad7ad67ddd5fe74598fe491ca75c5221380df","title":"MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models"},{"paperId":"f603b9dc81dfea56d437e967b724636d4d72d000","title":"LLM as A Robotic Brain: Unifying Egocentric Memory and Control"},{"paperId":"1a8eb2cae1833df3bf12fe3b41b03d60b4a4a98d","title":"Visual Instruction Tuning"},{"paperId":"b7d73f22d861f526541575a3b17449bd3c58ca74","title":"MVP-SEG: Multi-View Prompt Learning for Open-Vocabulary Semantic Segmentation"},{"paperId":"f12ebcc9a0a7296cc6c85b243a003f7205c68b3d","title":"What does CLIP know about a red circle? Visual prompt engineering for VLMs"},{"paperId":"a43a3fadc9190e61b34f59a913f1716e443519e4","title":"On the Opportunities and Challenges of Foundation Models for Geospatial Artificial Intelligence"},{"paperId":"4d94dcc6c9c261c8edcd0f3c5a1318a98a45b79d","title":"HRS-Bench: Holistic, Reliable and Scalable Benchmark for Text-to-Image Models"},{"paperId":"0ebc861f5478561f12941e6b48aad30574e996d8","title":"Video ChatCaptioner: Towards Enriched Spatiotemporal Descriptions"},{"paperId":"db1c83ef73d2f7731b0dd255835f2f26db749e17","title":"Not All Features Matter: Enhancing Few-shot CLIP with Adaptive Prior Refinement"},{"paperId":"10d2316bcb0ad7818a0a2e22477d9a6f7f2dd9b7","title":"GlyphDraw: Seamlessly Rendering Text with Intricate Spatial Structures in Text-to-Image Generation"},{"paperId":"c84e2801512069acbc63f1a7f73273281939428c","title":"A Study of Autoregressive Decoders for Multi-Tasking in Computer Vision"},{"paperId":"a757999ed260d7bc45484dc6b4456bf33fe6f679","title":"LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention"},{"paperId":"a08b7123a7158f1a7fbbc18e8b5aaebd47980ecf","title":"EVA-CLIP: Improved Training Techniques for CLIP at Scale"},{"paperId":"d064075c47e358f604034d06df4b985356757c71","title":"Equivariant Similarity for Vision-Language Foundation Models"},{"paperId":"d84616f108ccbd958735fef7622e58d148b32139","title":"Make-It-3D: High-Fidelity 3D Creation from A Single Image with Diffusion Prior"},{"paperId":"0d3817ae7fecc204c7c79a039dc47ae88890d5f3","title":"ChatGPT for Shaping the Future of Dentistry: The Potential of Multi-Modal Large Language Model"},{"paperId":"994e08ac813028601907516aee9c4699234a6b4d","title":"Large AI Models in Health Informatics: Applications, Challenges, and the Future"},{"paperId":"285dae5c2f2ef55c70971094a1ddd45afe720eee","title":"Fundamentals of Generative Large Language Models and Perspectives in Cyber-Defense"},{"paperId":"049a62ac86f59f2a912cd59f1cb179b82c4ae6b9","title":"TIFA: Accurate and Interpretable Text-to-Image Faithfulness Evaluation with Question Answering"},{"paperId":"7733cf84e5447339dd57ca96133e14e36c29e0e7","title":"Contrastive Alignment of Vision to Language Through Parameter-Efficient Transfer Learning"},{"paperId":"3049c992adbd56e29c4d957ee0c4e9d05fe3c6d1","title":"EVA-02: A Visual Representation for Neon Genesis"},{"paperId":"052a5e2bcc999810ee6f1eedcf758c528e4f125f","title":"Retrieving Multimodal Information for Augmented Generation: A Survey"},{"paperId":"c7a9c7302a72301ed79a7c0696d5af2e03ad3ac4","title":"MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action"},{"paperId":"4396e30f28eb49bb07c63cf62ca90415ebbe43d4","title":"IRGen: Generative Modeling for Image Retrieval"},{"paperId":"3c39a600adb254f7520f513ed9c3412c9c62f17f","title":"MRIS: A Multi-modal Retrieval Approach for Image Synthesis on Diverse Modalities"},{"paperId":"6e754273d54a91371efbc928cd6b156364d517da","title":"ViperGPT: Visual Inference via Python Execution for Reasoning"},{"paperId":"e5a7be5b9e6c368a1839455bfbb51bc07ed161f1","title":"ODIN: On-demand Data Formulation to Mitigate Dataset Lock-in"},{"paperId":"9d12916dd46df7a6446cbec0bc4d054f7dafcdab","title":"Scaling Vision-Language Models with Sparse Mixture of Experts"},{"paperId":"cf41ae462687f81ce95b27113c6a4f9c2751de42","title":"Vision-Language Models as Success Detectors"},{"paperId":"32709ac303247af89262a41510900f96bb349016","title":"Breaking Common Sense: WHOOPS! A Vision-and-Language Benchmark of Synthetic and Compositional Images"},{"paperId":"69cfdc8df16ae63b7acba4ac6f727f78b86893c3","title":"ChatGPT Asks, BLIP-2 Answers: Automatic Questioning Towards Enriched Visual Descriptions"},{"paperId":"af997821231898a5f8d0fd78dad4eec526acabe5","title":"Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models"},{"paperId":"6a4ef6c4799dc871a4253c0536126d397ca3ec1e","title":"Interpretable Visual Question Answering Referring to Outside Knowledge"},{"paperId":"a7b3a868a80dbe97689135c99b1a6b6e10dcdfe5","title":"A Comprehensive Survey of AI-Generated Content (AIGC): A History of Generative AI from GAN to ChatGPT"},{"paperId":"467b839cb8a2475477ca004df94b797d967ad057","title":"Human-Art: A Versatile Human-Centric Dataset Bridging Natural and Artificial Scenes"},{"paperId":"fbfef4723d8c8467d7bd523e1d0b703cce0e0f9c","title":"Language Is Not All You Need: Aligning Perception with Language Models"},{"paperId":"c8f98f28f1f28a9f5db7c4b4d9a6b7853a100214","title":"Can Pre-trained Vision and Language Models Answer Visual Information-Seeking Questions?"},{"paperId":"a3ff4df653b6970898c04e6b768e58b99786d073","title":"Learning gain differences between ChatGPT and human tutor generated algebra hints"},{"paperId":"fccada3fc530ea98d612126399f13ecb0844fc21","title":"Contrast with Reconstruct: Contrastive 3D Representation Learning Guided by Generative Pretraining"},{"paperId":"a082b61a7d9d6c890861661be919fd9190893b38","title":"Bias-to-Text: Debiasing Unknown Visual Biases through Language Interpretation"},{"paperId":"3062bb79d12ff55c29c8731211a84e8cf344e235","title":"Vision Learners Meet Web Image-Text Pairs"},{"paperId":"30c0cdc414f68211d5d0514df027cec22e005174","title":"A Survey for In-context Learning"},{"paperId":"1367dcff4ccb927a5e95c452041288b3f0dd0eff","title":"Tune-A-Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation"},{"paperId":"d00ca5c49415d3a45bfcf3fabaf0a60a1c52a6ff","title":"PromptCap: Prompt-Guided Task-Aware Image Captioning"},{"paperId":"2d69c71d6000fc89015ed7201cec17406268b268","title":"Deep Learning Approaches on Image Captioning: A Review"},{"paperId":"4d5de60287a636825ce966a4095837602b621716","title":"Description Enhancement of Generated Images via Automatic Visual Question Generation and Answering"},{"paperId":"c979783eb02eb60e55c181e37e79b4c4810d4b89","title":"LAVIS: A One-stop Library for Language-Vision Intelligence"},{"paperId":"00c1ff63468305ea3fa430c2b3aef156d580c4ff","title":"P ROMPT C AP : Prompt-Guided Image Captioning for VQA with GPT-3"},{"paperId":"d1120d67b700e4dfe8b39eb1e48fbdea4e1a0c43","title":"HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face"},{"paperId":"1c7471996a4f2e08a5ef592a6ffcde65a034a1e4","title":"GlyphDraw: Learning to Draw Chinese Characters in Image Synthesis Models Coherently"},{"paperId":"7562e25b666cba841b1dd5cf6e700978922beb04","title":"SkinGPT: A Dermatology Diagnostic System with Vision Large Language Model"},{"paperId":"cb1f3829fe02153593c93e8cc9eb5ca98952a064","title":"Novel Data Augmentation for resource constrained Image captioning"},{"paperId":"5360808a865d43e7bae9329498b01cbcbab889e0","title":"Summarizing Charts and Graphs with Context"},{"paperId":"77efb9b9561a2c51b62babbfb036ab316644225f","title":"PERSONALIZING TEXT-TO-IMAGE GENERATION WITH VISUAL PROMPTS USING BLIP-2"},{"paperId":"67188a50e1d8a601896f1217451b99f646af4ac8","title":"TOWARDS A UNIFIED AGENT WITH FOUNDATION MODELS"},{"paperId":"2a464ef66662b93789d25f395a82a967a8eb5b6a","title":"Combining Tradition with Modernness: Exploring Event Representations in Vision-and-Language Models for Visual Goal-Step Inference"},{"paperId":"fa0bd60ba33584d488df517aa6a248db15514e5c","title":"The D-WISE Tool Suite: Multi-Modal Machine-Learning-Powered Tools Supporting and Enhancing Digital Discourse Analysis"},{"paperId":"d8a59a00468e5a6ca57482e50f7d3d7542adf048","title":"Rutgers Multimedia Image Processing Lab at SemEval-2023 Task-1: Text-Augmentation-based Approach for Visual Word Sense Disambiguation"},{"paperId":"69630953cd28eed4ebf7c441766b34e562b7ad03","title":"NeoDescriber: An image-to-text model for automatic style description of neoclassical architecture"}],"references":[{"paperId":"1a310b5d357a16c8d909cc5a5106ca1ae3e47ed1","title":"From Images to Textual Prompts: Zero-shot VQA with Frozen Large Language Models"},{"paperId":"78281482c1fdad8e167bab39cc9955c73d58ae8f","title":"EVA: Exploring the Limits of Masked Visual Representation Learning at Scale"},{"paperId":"5484d228bfc50efbac6e86677bc2ec2ee4ede1a6","title":"Scaling Instruction-Finetuned Language Models"},{"paperId":"26fd105d0b5a458979c012cddb3ba2de943388c4","title":"Plug-and-Play VQA: Zero-shot VQA by Conjoining Large Pretrained Models with Zero Training"},{"paperId":"1f86bf1e334200ec0481349255559fbfe7a33caa","title":"MAPL: Parameter-Efficient Adaptation of Unimodal Pre-Trained Models for Vision-Language Few-Shot Prompting"},{"paperId":"28630034bb29760df01ab033b743e30b37f336ae","title":"PaLI: A Jointly-Scaled Multilingual Language-Image Model"},{"paperId":"02251886950770e82b3d68564d60cdfe15e73199","title":"Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks"},{"paperId":"a26a7a74f1e5fd562be95c3611a0680759fbdf84","title":"CoCa: Contrastive Captioners are Image-Text Foundation Models"},{"paperId":"26218bdcc3945c7edae7aa2adbfba4cd820a2df3","title":"Flamingo: a Visual Language Model for Few-Shot Learning"},{"paperId":"8342b592fe238f3d230e4959b06fd10153c45db1","title":"Training Compute-Optimal Large Language Models"},{"paperId":"7f71875f8214dffa4f3276da123c4990a6d437cc","title":"Enabling Multimodal Generation on CLIP via Vision-Language Knowledge Distillation"},{"paperId":"a3b42a83669998f65df60d7c065a70d07ca95e99","title":"BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation"},{"paperId":"197d5867a45a2988f4dd159063cdfbfe90164962","title":"LiT: Zero-Shot Transfer with Locked-image text Tuning"},{"paperId":"f675c62abfa788ea0be85d3124eba15a14d5e9d6","title":"FILIP: Fine-grained Interactive Language-Image Pre-Training"},{"paperId":"cf7c2e0e4fb2af689aaf4b7a7cddf7b1f4d5e3f0","title":"VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts"},{"paperId":"b668ce936cff0b0ca8b635cd5f25a62eaf4eb3df","title":"LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs"},{"paperId":"32d59ab951be74be351f9777da2cbc71bb68c3c1","title":"A Good Prompt Is Worth Millions of Parameters: Low-resource Prompt-based Learning for Vision-Language Models"},{"paperId":"5e00596fa946670d894b1bdaeff5a98e3867ef13","title":"SimVLM: Simple Visual Language Model Pretraining with Weak Supervision"},{"paperId":"b82c5f9efdb2ae56baa084ca41aeddd8a665c1d1","title":"Align before Fuse: Vision and Language Representation Learning with Momentum Distillation"},{"paperId":"01b5412f3d17e90e09226d7c40ad4d4468a1414d","title":"Multimodal Few-Shot Learning with Frozen Language Models"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"616e0ed02ca024a8c1d4b86167f7486ea92a13d9","title":"VisualGPT: Data-efficient Adaptation of Pretrained Language Models for Image Captioning"},{"paperId":"394be105b87e9bfe72c20efe6338de10604e1a11","title":"Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts"},{"paperId":"141a5033d9994242b18bb3b217e79582f1ee9306","title":"Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision"},{"paperId":"a6ca91afe845ef5294c40c2029e0c1cba19ba40b","title":"Unifying Vision-and-Language Tasks via Text Generation"},{"paperId":"be0014c1fbc3e664686610d2c85f75038a4f6e4f","title":"VinVL: Making Visual Representations Matter in Vision-Language Models"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"818e5cbc337e4e1b98e65a2d7c2d6d2a0318cd57","title":"Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks"},{"paperId":"d8a305b9366608d54452ac30459ee57b4f5cf1c9","title":"UNITER: UNiversal Image-TExt Representation Learning"},{"paperId":"79c93274429d6355959f1e4374c2147bb81ea649","title":"LXMERT: Learning Cross-Modality Encoder Representations from Transformers"},{"paperId":"28ad018c39d1578bea84e7cedf94459e3dbe1e70","title":"OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge"},{"paperId":"1c71771c701aadfd72c5866170a9f5d71464bb88","title":"Unified Language Model Pre-training for Natural Language Understanding and Generation"},{"paperId":"1ab7f7c1d328589f25c79515b9a5d824d7ffbbd1","title":"GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering"},{"paperId":"b4df354db88a70183a64dbc9e56cf14e7669a6c0","title":"Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning"},{"paperId":"d07284a6811f1b2745d91bdb06b040b57f226882","title":"Decoupled Weight Decay Regularization"},{"paperId":"7e232313a59d735ef7c8a9f4cc7bc980a29deb5e","title":"Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering"},{"paperId":"afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d","title":"Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations"},{"paperId":"11c9c31dff70de92ada9160c78ff8bb46b2912d6","title":"Flickr30k Entities: Collecting Region-to-Phrase Correspondences for Richer Image-to-Sentence Models"},{"paperId":"71b7178df5d2b112d07e45038cb5637208659ff7","title":"Microsoft COCO: Common Objects in Context"},{"paperId":"8e080b98efbe65c02a116439205ca2344b9f7cd4","title":"Im2Text: Describing Images Using 1 Million Captioned Photographs"},{"paperId":null,"title":"OFA: unifying architectures, tasks, and modalities through a simple sequence-tosequence learning framework"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"8b55402ffee2734bfc7d5d7595500916e1ef04e8","title":"nocaps: novel object captioning at scale"}],"id":"336ce63b472a65f053f854d45851d6f0e896f05e","summary":"BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods, and is demonstrated's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions."},{"url":"https://www.semanticscholar.org/paper/6edcb09a09c8df43cb62119133df9bb2eb75e5cf","title":"From Images to Textual Prompts: Zero-shot VQA with Frozen Large Language Models","venue":"arXiv.org","year":2022,"referenceCount":70,"citationCount":11,"influentialCitationCount":0,"publicationDate":"21/12/2022","authors":"Jiaxian Guo,Junnan Li,Dongxu Li,A. M. H. Tiong,Boyang Li,Dacheng Tao,Steven Hoi","citations":[{"paperId":"85d9151aa2efd0cbe822e403138cfe49f9536703","title":"SITTA: A Semantic Image-Text Alignment for Image Captioning"},{"paperId":"66cbe0bb018c04b2197777d7332898e6670de06a","title":"ClipSitu: Effectively Leveraging CLIP for Conditional Predictions in Situation Recognition"},{"paperId":"efc694164312006c543ef745611348ef64e68dda","title":"Towards Language Models That Can See: Computer Vision Through the LENS of Natural Language"},{"paperId":"c2c7ad3112c4b575e5d8163a0e574f9eb743cb52","title":"Zero-shot Visual Question Answering with Language Model Feedback"},{"paperId":"f646d3056ca02daa99820917b3ba48a43a0022e2","title":"SUR-adapter: Enhancing Text-to-Image Pre-trained Diffusion Models with Large Language Models"},{"paperId":"e1ff32753e20e48b4b01e40b5e820254396e6e70","title":"LMEye: An Interactive Perception Network for Large Language Models"},{"paperId":"5dea6facab090a070be1444920230689e7189599","title":"SurgicalGPT: End-to-End Language-Vision GPT for Visual Question Answering in Surgery"},{"paperId":"09840a5c151f858ed0eaf1db2a4d3741516f693b","title":"ICL-D3IE: In-Context Learning with Diverse Demonstrations Updating for Document Information Extraction"},{"paperId":"0f19e94f30b99d6c4b349900057cdae9262034f9","title":"The Contribution of Knowledge in Visiolinguistic Learning: A Survey on Tasks and Challenges"},{"paperId":"3f5b31c4f7350dc88002c121aecbdc82f86eb5bb","title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models"},{"paperId":"2859de53b8309a1389715f54b50bc84bab9893d3","title":"Towards Explainable Automatic Knowledge Graph Construction with Human-in-the-Loop"}],"references":[{"paperId":"bb15f3727f827a3cb88b5d3ca48415c09b40a88f","title":"What Language Model to Train if You Have One Million GPU Hours?"},{"paperId":"26fd105d0b5a458979c012cddb3ba2de943388c4","title":"Plug-and-Play VQA: Zero-shot VQA by Conjoining Large Pretrained Models with Zero Training"},{"paperId":"dac3a172b504f4e33c029655e9befb3386e5f63a","title":"Emergent Abilities of Large Language Models"},{"paperId":"47a67e76ed84260ff19f7a948d764005d1edf1c9","title":"A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge"},{"paperId":"57c64f233a0db4d17e0e750c12516364ca009fb2","title":"REVIVE: Regional Visual Representation Matters in Knowledge-Based Visual Question Answering"},{"paperId":"e7ad08848d5d7c5c47673ffe0da06af443643bda","title":"Large Language Models are Zero-Shot Reasoners"},{"paperId":"5437e8adab596d7294124c0e798708e050e25321","title":"Least-to-Most Prompting Enables Complex Reasoning in Large Language Models"},{"paperId":"7f5170b8ec68629164a98f8dfa1d2cbef5bbe5f5","title":"All You May Need for VQA are Image Captions"},{"paperId":"13a0d8bb38f739990c8cd65a44061c6534f17221","title":"OPT: Open Pre-trained Transformer Language Models"},{"paperId":"26218bdcc3945c7edae7aa2adbfba4cd820a2df3","title":"Flamingo: a Visual Language Model for Few-Shot Learning"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"04ef9a02775ee98caca79c7c4d92e9e46eee9ae5","title":"Pseudo-Q: Generating Pseudo Language Queries for Visual Grounding"},{"paperId":"7f71875f8214dffa4f3276da123c4990a6d437cc","title":"Enabling Multimodal Generation on CLIP via Vision-Language Knowledge Distillation"},{"paperId":"79956ac2a4164c298387546fc10139c3d5192842","title":"Webly Supervised Concept Expansion for General Purpose Vision Models"},{"paperId":"a3b42a83669998f65df60d7c065a70d07ca95e99","title":"BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation"},{"paperId":"1b6e810ce0afd0dd093f789d2b2742d047e316d5","title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"ab8ee8d06ed581c876da3a2e7a5fdb1cbec21a45","title":"KAT: A Knowledge Augmented Transformer for Vision-and-Language"},{"paperId":"2fd6f77540c1cc8e70b96208ccf9971b4251fc02","title":"FLAVA: A Foundational Language And Vision Alignment Model"},{"paperId":"21ec90872abd986c12afe39bebe807732ffa70c9","title":"Florence: A New Foundation Model for Computer Vision"},{"paperId":"a7aa150b55d64d339b1c154d6d88455fc3cbc44f","title":"ClipCap: CLIP Prefix for Image Captioning"},{"paperId":"118962f61df9ab6c8310d5a3eb0ab61f22802360","title":"Language bias in Visual Question Answering: A Survey and Taxonomy"},{"paperId":"32d59ab951be74be351f9777da2cbc71bb68c3c1","title":"A Good Prompt Is Worth Millions of Parameters: Low-resource Prompt-based Learning for Vision-Language Models"},{"paperId":"467e5a2164cf78c5be70c91129e1c6e843685fb3","title":"Discovering the Unknown Knowns: Turning Implicit Knowledge in the Dataset into Explicit Training Examples for Visual Question Answering"},{"paperId":"2672777d25562c9df6fc13b653181db62d39bece","title":"An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA"},{"paperId":"4e92fec0a61972ae076707d0630d1333affccdfc","title":"Weakly-Supervised Visual-Retriever-Reader for Knowledge-based Question Answering"},{"paperId":"f5a76442659066434b1bdf480cf11f4f549411ab","title":"QACE: Asking Questions to Evaluate an Image Caption"},{"paperId":"5e00596fa946670d894b1bdaeff5a98e3867ef13","title":"SimVLM: Simple Visual Language Model Pretraining with Weak Supervision"},{"paperId":"b82c5f9efdb2ae56baa084ca41aeddd8a665c1d1","title":"Align before Fuse: Vision and Language Representation Learning with Momentum Distillation"},{"paperId":"01b5412f3d17e90e09226d7c40ad4d4468a1414d","title":"Multimodal Few-Shot Learning with Frozen Language Models"},{"paperId":"63c74d15940af1af9b386b5762e4445e54c73719","title":"VinVL: Revisiting Visual Representations in Vision-Language Models"},{"paperId":"57ed901be5d1b4d853d4f8998dadc1b60e2151f9","title":"On Attention Redundancy: A Comprehensive Study"},{"paperId":"8dce342a435034fa0521b24b61393397df95c095","title":"Multi-Modal Answer Validation for Knowledge-Based VQA"},{"paperId":"7e5008713c404445dd8786753526f1a45b93de12","title":"GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow"},{"paperId":"a6ca91afe845ef5294c40c2029e0c1cba19ba40b","title":"Unifying Vision-and-Language Tasks via Text Generation"},{"paperId":"1a9015e511ec3da873f6114eeb542905a92d7d62","title":"KRISP: Integrating Implicit and Symbolic Knowledge for Open-Domain Knowledge-Based VQA"},{"paperId":"1a575075ba357723009a9a8905d5dccf9115ae6c","title":"WeaQA: Weak Supervision via Captions for Visual Question Answering"},{"paperId":"9958887e8dd5f84595818c50fb734b566996541a","title":"ConceptBert: Concept-Aware Representation for Visual Question Answering"},{"paperId":"0030605bfa0a11e7474a8c5ff5b00f3ccdb22b22","title":"Boosting Visual Question Answering with Context-aware Knowledge Aggregation"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"ad5970584754cc7a1d91c95ab84a1e210258183a","title":"UnifiedQA: Crossing Format Boundaries With a Single QA System"},{"paperId":"64a548ca02c8d647358cac809d9c059d34dc4f3a","title":"Radial Graph Convolutional Network for Visual Question Generation"},{"paperId":"818e5cbc337e4e1b98e65a2d7c2d6d2a0318cd57","title":"Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks"},{"paperId":"6548a60a6bcdf6c402d9de1c05ba7afe4f49fee9","title":"12-in-1: Multi-Task Vision and Language Representation Learning"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"79c93274429d6355959f1e4374c2147bb81ea649","title":"LXMERT: Learning Cross-Modality Encoder Representations from Transformers"},{"paperId":"65a9c7b0800c86a196bc14e7621ff895cc6ab287","title":"ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks"},{"paperId":"8faa293b037fb1dc32041b09faa93f7d1aa098e4","title":"Generating Question Relevant Captions to Aid Visual Question Answering"},{"paperId":"28ad018c39d1578bea84e7cedf94459e3dbe1e70","title":"OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge"},{"paperId":"1536e8958697c5364f68b2e2448905dbbeb3a0ca","title":"Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering"},{"paperId":"36c3972569a6949ecca90bfa6f8e99883e092845","title":"Pythia v0.1: the Winning Entry to the VQA Challenge 2018"},{"paperId":"4d1c856275744c0284312a3a50efb6ca9dc4cd4c","title":"Know What You Don’t Know: Unanswerable Questions for SQuAD"},{"paperId":"99ad0533f84c110da2d0713d5798e6e14080b159","title":"Looking Beyond the Surface: A Challenge Set for Reading Comprehension over Multiple Sentences"},{"paperId":"29de7c0fb3c09eaf55b20619bceaeafe72fd87a6","title":"Hierarchical Neural Story Generation"},{"paperId":"90873a97aa9a43775e5aeea01b03aea54b28bfbd","title":"Don't Just Assume; Look and Answer: Overcoming Priors for Visual Question Answering"},{"paperId":"7e4b638e028498e900747b600f46cd723f1f231e","title":"Data Augmentation for Visual Question Answering"},{"paperId":"a82c1d1ccaa3a3d1d6ee6677de0eed2e93ddb6e8","title":"Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering"},{"paperId":"915b5b12f9bdebc321e970ecd713458c3479d70e","title":"An Analysis of Visual Question Answering Algorithms"},{"paperId":"26aa6fe2028b5eefbaa40ab54ef725bbbe7d9810","title":"ConceptNet 5.5: An Open Multilingual Graph of General Knowledge"},{"paperId":"7e232313a59d735ef7c8a9f4cc7bc980a29deb5e","title":"Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering"},{"paperId":"e7eef2ac4136ec93bd306d2c9c353a13729a4553","title":"Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization"},{"paperId":"2c1890864c1c2b750f48316dc8b650ba4772adc5","title":"Stacked Attention Networks for Image Question Answering"},{"paperId":"97ad70a9fa3f99adf18030e5e38ebe3d90daa2db","title":"VQA: Visual Question Answering"},{"paperId":"e6a6b66eeb506dc326e3c3f7f49a1f260469c281","title":"VC-GPT: Visual Conditioned GPT for End-to-End Generative Vision-and-Language Pre-training"},{"paperId":"4593c88fb33023bec84f8f443d16262810b9047a","title":"CrossVQA: Scalably Generating Benchmarks for Systematically Testing VQA Generalization"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"421cb75cc91e8e5683d41ee6a918121aedf6d24d","title":"Social IQA: Commonsense Reasoning about Social Interactions"},{"paperId":"c21a4d70d83e0f6eb2a9e1c41d034842dd561e47","title":"CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge"},{"paperId":null,"title":"Success case analysis for VQAv2. Green color indicates answer cues and correct prediction"},{"paperId":null,"title":"Lavis: A library for languagevision intelligence"},{"paperId":null,"title":"GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https:// github.com/kingoflolz/mesh-transformerjax, May 2021"}],"id":"6edcb09a09c8df43cb62119133df9bb2eb75e5cf","summary":"Img2Prompt is a plug-and-play module that provides the prompts that can bridge the aforementioned modality and task disconnections, so that LLMs can perform zero-shot VQA tasks without end-to-end training."},{"url":"https://www.semanticscholar.org/paper/6e754273d54a91371efbc928cd6b156364d517da","title":"ViperGPT: Visual Inference via Python Execution for Reasoning","venue":"arXiv.org","year":2023,"referenceCount":68,"citationCount":42,"influentialCitationCount":1,"publicationDate":"14/03/2023","authors":"D'idac Sur'is,Sachit Menon,Carl Vondrick","citations":[{"paperId":"ba63203d7f91d4135d2a392e841ce532c006e31c","title":"VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models"},{"paperId":"ec592e12f45e20819afe203164bbbd0de8990510","title":"AmadeusGPT: a natural language interface for interactive animal behavioral analysis"},{"paperId":"3d02e5503caa2f444cbd61778c7cdc00a5b2e98d","title":"GPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest"},{"paperId":"72160b3c0f73c968fcb903db71817d1bed695f4d","title":"Look, Remember and Reason: Visual Reasoning with Grounded Rationales"},{"paperId":"efc694164312006c543ef745611348ef64e68dda","title":"Towards Language Models That Can See: Computer Vision Through the LENS of Natural Language"},{"paperId":"8efc20988021ce3b4b05dd44b13e27260ee9b99b","title":"Investigating Prompting Techniques for Zero- and Few-Shot Visual Question Answering"},{"paperId":"b937b5ad3c1ebe6007e744fa7864ec095e0070ab","title":"Tell Me Where to Go: A Composable Framework for Context-Aware Embodied Robot Navigation"},{"paperId":"966852963a88a28786b798c91b6662d6e501e590","title":"AssistGPT: A General Multi-modal Assistant that can Plan, Execute, Inspect, and Learn"},{"paperId":"051549d8ef56937b2f4d113afdcf8c7586d3770b","title":"Towards AGI in Computer Vision: Lessons Learned from GPT and Large Language Models"},{"paperId":"ba29fa70020825747fd8d0c449e1ef0a48448f57","title":"Toward Grounded Social Reasoning"},{"paperId":"e8cc6a50bc9c6a1e1ed7fdde0e7ccbb4efd4b505","title":"AVIS: Autonomous Visual Information Seeking with Large Language Models"},{"paperId":"473eb062612a17c965eaa62136322f0dec6b1f8e","title":"Data-Copilot: Bridging Billions of Data and Humans with Autonomous Workflow"},{"paperId":"caae5e44957dea66fc309a55925e132de0fdb456","title":"Looking Around Corners: Generative Methods in Terrain Extension"},{"paperId":"ed9943d73eb42116fe33564b5065c78b5ca0b16e","title":"RestGPT: Connecting Large Language Models with Real-World Applications via RESTful APIs"},{"paperId":"fd755dc7b5b206c17fd953db04e1c888d45b6e4e","title":"LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark"},{"paperId":"da061a6e0016d6b625a8e86d64a797ca8ddb92a5","title":"Modular Visual Question Answering via Code Generation"},{"paperId":"50f44ef10335d59cec145b15effae20ff22c1fdb","title":"ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory"},{"paperId":"9218a7ac19f79ff5c22c1c2bbef01ba442280028","title":"Data Augmentation Approaches for Source Code Models: A Survey"},{"paperId":"af705d648b5b16daa3dcc593bc593f2574d76c07","title":"Grammar Prompting for Domain-Specific Language Generation with Large Language Models"},{"paperId":"b595b55ed27935d306b0a5e0b06a3b0a771275b1","title":"SheetCopilot: Bringing Software Productivity to the Next Level through Large Language Models"},{"paperId":"1a28e9c62eeb76a1a77dc152197027c15310927b","title":"ANPL: Compiling Natural Programs with Interactive Decomposition"},{"paperId":"7cf64070fd3d7e53d80f260c10e6bd7018d580e1","title":"IdealGPT: Iteratively Decomposing Vision and Language Reasoning via Large Language Models"},{"paperId":"13b5b69355555e0c8b702261c5de3b4172ba653c","title":"The Art of SOCRATIC QUESTIONING: Zero-shot Multimodal Reasoning with Recursive Thinking and Self-Questioning"},{"paperId":"9837349417e36ef5be06da0fd6c74042148bdaa2","title":"Visual Programming for Text-to-Image Generation and Evaluation"},{"paperId":"405bc18b9d2f783f22f50d5feb02c51b4b34655f","title":"LayoutGPT: Compositional Visual Planning and Generation with Large Language Models"},{"paperId":"6238fd213197ccf0d79191662828e38118a06d79","title":"ECHo: Event Causality Inference via Human-centric Reasoning"},{"paperId":"ba704774f194938b04b1e2be40b1d111a4ca08e1","title":"CREATOR: Disentangling Abstract and Concrete Reasonings of Large Language Models through Tool Creation"},{"paperId":"d886fc1b43b1c14b1c82ce8e4eab7c48e2c6d7af","title":"Paxion: Patching Action Knowledge in Video-Language Foundation Models"},{"paperId":"2195676f111ad492c50f4d4c96abb2bd3d72f7fc","title":"Instruct2Act: Mapping Multi-modality Instructions to Robotic Actions with Large Language Model"},{"paperId":"f71ce6b09756ea26df283640b4c1c1c125411d2e","title":"Towards Generalist Robots: A Promising Paradigm via Generative Simulation"},{"paperId":"d6d3604f369bb0415cbe814e43ca3131323b03e2","title":"Otter: A Multi-Modal Model with In-Context Instruction Tuning"},{"paperId":"570079bbdd8758dfe865097e05719313c9c1301a","title":"LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model"},{"paperId":"f44ad7ad67ddd5fe74598fe491ca75c5221380df","title":"MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models"},{"paperId":"170c97c7215f42edfb20c2248f954879e91ef86e","title":"Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models"},{"paperId":"0ebc861f5478561f12941e6b48aad30574e996d8","title":"Video ChatCaptioner: Towards Enriched Spatiotemporal Descriptions"},{"paperId":"2d3905c1a92c28c056dff1225d89e4ca72ac4d8e","title":"Man vs the machine: The Struggle for Effective Text Anonymisation in the Age of Large Language Models"},{"paperId":"c7a9c7302a72301ed79a7c0696d5af2e03ad3ac4","title":"MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action"},{"paperId":"887138f4c365b9d1325de41a522d27bec34e0d7e","title":"Reasoning with Language Model Prompting: A Survey"},{"paperId":"75c08892179fc478f87d7020b5daff9fca4f3389","title":"Beyond One-Model-Fits-All: A Survey of Domain Specialization for Large Language Models"},{"paperId":"14c255e1b399ab9f98a5a9c36c0b454e513369b0","title":"Domain Specialization as the Key to Make Large Language Models Disruptive: A Comprehensive Survey"},{"paperId":"a3711dbf296b5ddd97ba93826660cd3995611625","title":"Towards A Foundation Model for Generalist Robots: Diverse Skill Learning at Scale via Automated Task and Scene Generation"},{"paperId":"d1120d67b700e4dfe8b39eb1e48fbdea4e1a0c43","title":"HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face"}],"references":[{"paperId":"fbfef4723d8c8467d7bd523e1d0b703cce0e0f9c","title":"Language Is Not All You Need: Aligning Perception with Language Models"},{"paperId":"53d128ea815bcc0526856eb5a9c42cc977cb36a7","title":"Toolformer: Language Models Can Teach Themselves to Use Tools"},{"paperId":"3f5b31c4f7350dc88002c121aecbdc82f86eb5bb","title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models"},{"paperId":"b909c1905063fe247a7c9359842e8437448f929d","title":"HiTeA: Hierarchical Temporal-Aware Video-Language Pre-training"},{"paperId":"4eb5198062f78ecf844ff48bcaefe4c1c0f395cc","title":"Doubly Right Object Recognition: A Why Prompt for Visual Rationales"},{"paperId":"3e8251f259dc529b3aa2366fc68c1516b202cfb9","title":"REVEAL: Retrieval-Augmented Visual-Language Pre-Training with Multi-Source Multimodal Knowledge Memory"},{"paperId":"e1c2a926df37107358ac51e460361e2a249c8b26","title":"Open-vocabulary Attribute Detection"},{"paperId":"598d9b235f5ab148fc757240d9bc39a47b8eaf72","title":"Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks"},{"paperId":"6c1e1cc1e0e1f8fd026fe517607b2d4535565fa7","title":"PAL: Program-aided Language Models"},{"paperId":"af1c871282ec122869d03f5420ef5d9143358a91","title":"Visual Programming: Compositional visual reasoning without training"},{"paperId":"d00ca5c49415d3a45bfcf3fabaf0a60a1c52a6ff","title":"PromptCap: Prompt-Guided Task-Aware Image Captioning"},{"paperId":"26fd105d0b5a458979c012cddb3ba2de943388c4","title":"Plug-and-Play VQA: Zero-shot VQA by Conjoining Large Pretrained Models with Zero Training"},{"paperId":"11f86abe3d6b1de0678390fed442fdcb62667768","title":"COFAR: Commonsense and Factual Reasoning in Image Search"},{"paperId":"a42b091adaf29b06a092b67192ac07cb93312f2a","title":"Visual Classification via Description from Large Language Models"},{"paperId":"39e40821b7207125e54e6ed7112e55cd38c6f0c3","title":"Language Models of Code are Few-Shot Commonsense Learners"},{"paperId":"6f85ec89d9c07a8db4545e64888ced820370a21b","title":"Retrieval Augmented Visual Question Answering with Outside Knowledge"},{"paperId":"f58ca7ba4a08b7082e86b7a5989b4b0fda2107ab","title":"Binding Language Models in Symbolic Languages"},{"paperId":"009e40cdc9d98b9e5f6279d38b46936ceffcc124","title":"Video Graph Transformer for Video Question Answering"},{"paperId":"57c64f233a0db4d17e0e750c12516364ca009fb2","title":"REVIVE: Regional Visual Representation Matters in Knowledge-Based Visual Question Answering"},{"paperId":"809822d59203a462bc9f2e0f0e9a8314d6d469d4","title":"Revisiting the “Video” in Video-Language Understanding"},{"paperId":"c1ace33daf974d3d16752c7a8565f32a63b09c49","title":"Language Models with Image Descriptors are Strong Few-Shot Video-Language Learners"},{"paperId":"9dae204dad41633188022002a04c8aa67c79a4e1","title":"Simple Open-Vocabulary Object Detection with Vision Transformers"},{"paperId":"26218bdcc3945c7edae7aa2adbfba4cd820a2df3","title":"Flamingo: a Visual Language Model for Few-Shot Learning"},{"paperId":"408efdd599b2b27ecb95a4d799869c9ff568fb31","title":"ReCLIP: A Strong Zero-Shot Baseline for Referring Expression Comprehension"},{"paperId":"ada81a4de88a6ce474df2e2446ad11fea480616e","title":"Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language"},{"paperId":"1bfa62ddfa3f6691e0e40c06f8ead594b6449cfa","title":"Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework"},{"paperId":"1b6e810ce0afd0dd093f789d2b2742d047e316d5","title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"bba57c53ab9b600f71d888601ed0aa03812c8199","title":"MuMuQA: Multimedia Multi-Hop News Question Answering via Cross-Media Knowledge Extraction and Grounding"},{"paperId":"ab8ee8d06ed581c876da3a2e7a5fdb1cbec21a45","title":"KAT: A Knowledge Augmented Transformer for Vision-and-Language"},{"paperId":"5341b412383c43f4a693ad63ec4489e3ec7688c8","title":"Grounded Language-Image Pre-training"},{"paperId":"ec8afc75ec219f2a5f9ed9d7c9dde0720f69b5a2","title":"Multi-Grained Vision Language Pre-Training: Aligning Texts with Visual Concepts"},{"paperId":"09f2b1f1bd313cf9183c138fca8f17bb228b4435","title":"Coarse-to-Fine Reasoning for Visual Question Answering"},{"paperId":"2672777d25562c9df6fc13b653181db62d39bece","title":"An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"f46f77630b35a43e8c247916da5d809d6e5b4210","title":"Interpretable visual reasoning: A survey"},{"paperId":"6be64445935dcdf4053a6e78b623b80a314d9bbc","title":"Separating Skills and Concepts for Novel Visual Question Answering"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"0416fda32c39fc9531e87bab6a8a1a552bf9ada0","title":"Obtaining Faithful Interpretations from Compositional Neural Networks"},{"paperId":"3c8a456509e6c0805354bd40a35e3f2dbf8069b1","title":"PyTorch: An Imperative Style, High-Performance Deep Learning Library"},{"paperId":"007ca8ca7a68451c32da034c72a06238434843c1","title":"Learning to Learn Words from Visual Scenes"},{"paperId":"79c93274429d6355959f1e4374c2147bb81ea649","title":"LXMERT: Learning Cross-Modality Encoder Representations from Transformers"},{"paperId":"136c05cb8dd359fb8e0dc7947172a9ecb74ccbec","title":"Learning by Abstraction: The Neural State Machine"},{"paperId":"7bd83b055702bc178aa26def5b6df463f8eab7b9","title":"Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-Shot Cross-Dataset Transfer"},{"paperId":"28ad018c39d1578bea84e7cedf94459e3dbe1e70","title":"OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge"},{"paperId":"2dc698077cb178286c737484dcf67c5ab19314d0","title":"Language-Conditioned Graph Networks for Relational Reasoning"},{"paperId":"1ab7f7c1d328589f25c79515b9a5d824d7ffbbd1","title":"GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering"},{"paperId":"9d15ebe3f5aaf32a9f835f88703241461324c35b","title":"Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding"},{"paperId":"6c7494a47cc5421a7b636c244e13586dc2dab007","title":"Systematic Generalization: What Is Required and Can It Be Learned?"},{"paperId":"97b93509f6c3c33dd3665d05b1878e36d58a1efb","title":"Interpretable Visual Question Answering by Reasoning on Dependency Trees"},{"paperId":"b1b852d4bf934863397e7b965a5dd0124ad8670c","title":"Interpretable Visual Question Answering by Visual Grounding From Attention Supervision Mining"},{"paperId":"f27b833c4a0dcb809215b185e8e2601aef6e7fb8","title":"Visual Reasoning by Progressive Module Networks"},{"paperId":"8d1c8dd0559642a3bdd5c7234d2ce4611e911e23","title":"Visual Grounding via Accumulated Attention"},{"paperId":"289fb3709475f5c87df8d97f129af54029d27fee","title":"Compositional Attention Networks for Machine Reasoning"},{"paperId":"ef153ece43ee50f8208f6197f0eaf3d324e4475b","title":"Multimodal Explanations: Justifying Decisions and Pointing to the Evidence"},{"paperId":"8165d6217a2f623f7d9e613c791e94102921cd3b","title":"Thinking Fast and Slow"},{"paperId":"0fff5c49c05c27c22ac7685130197146491f0b36","title":"The Consciousness Prior"},{"paperId":"2e17cf6a339fd071ad222062f868e882ef4120a4","title":"Inferring and Executing Programs for Visual Reasoning"},{"paperId":"a396a6febdacb84340d139096455e67049ac1e22","title":"Learning to Reason: End-to-End Module Networks for Visual Question Answering"},{"paperId":"e7eef2ac4136ec93bd306d2c9c353a13729a4553","title":"Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization"},{"paperId":"21c99706bb26e9012bfb4d8d48009a3d45af59b2","title":"Neural Module Networks"},{"paperId":"4d8f2d14af5991d4f0d050d22216825cac3157bd","title":"Show, Attend and Tell: Neural Image Caption Generation with Visual Attention"},{"paperId":"83cfb832eace6b378d868bba1d32825531438cbd","title":"Visual Programming"},{"paperId":"f9bdd27c48c57426179b1b09ffc517e94cbfca56","title":"Information Streams Sharing a Finite Buffer"},{"paperId":"450b8dff662a5d41388d04d994e5117020777ce5","title":"Code4Struct: Code Generation for Few-Shot Structured Prediction from Natural Language"},{"paperId":null,"title":"Transform-retrievegenerate: Natural language-centric outside-knowledge visual question answering"},{"paperId":null,"title":"arXiv:2201.11903 [cs"},{"paperId":null,"title":"Felipe Petroski Such"}],"id":"6e754273d54a91371efbc928cd6b156364d517da","summary":"ViperGPT is introduced, a framework that leverages code-generation models to compose vision-and-language models into subroutines to produce a result for any query and achieves state-of-the-art results across various complex visual tasks."},{"url":"https://www.semanticscholar.org/paper/d8da72e7857cc1a0d3505e6c8a746eac815901b2","title":"Large-Scale Domain-Specific Pretraining for Biomedical Vision-Language Processing","venue":"arXiv.org","year":2023,"referenceCount":76,"citationCount":8,"influentialCitationCount":0,"publicationDate":"02/03/2023","authors":"Shenmin Zhang,Yanbo Xu,Naoto Usuyama,J. Bagga,Robert Tinn,Sam Preston,Rajesh N. Rao,Mu-Hsin Wei,Naveen Valluri,Cliff Wong,M. Lungren,Tristan Naumann,Hoifung Poon","citations":[{"paperId":"feb7a09e0027777c299bcacdd5552f02f48bde71","title":"To pretrain or not to pretrain? A case study of domain-specific pretraining for semantic segmentation in histopathology"},{"paperId":"0f8d12775a4685575f1489796b5dee9e11fbdfb5","title":"OphGLM: Training an Ophthalmology Large Language-and-Vision Assistant based on Instructions and Dialogue"},{"paperId":"c9bf47b1f54484a0de53cfc03551ccccf6734f62","title":"Quilt-1M: One Million Image-Text Pairs for Histopathology"},{"paperId":"c50ebdb0be883bb4e21270ba990220745c6a9cf9","title":"RemoteCLIP: A Vision Language Foundation Model for Remote Sensing"},{"paperId":"a42fc49a300136d60aaebb668369010ee7746150","title":"Visual Language Pretrained Multiple Instance Zero-Shot Transfer for Histopathology Images"},{"paperId":"fed150a219f9c31bdb4920e615c7c9264c634736","title":"On the Challenges and Perspectives of Foundation Models for Medical Image Analysis"},{"paperId":"31a7d8c4a5ab6bab522494b57270249105c8748e","title":"BiomedGPT: A Unified and Generalist Biomedical Generative Pre-trained Transformer for Vision, Language, and Multimodal Tasks"},{"paperId":"065dcc6074ffc9e314799d97c1757e5d23e7e2b1","title":"S-CLIP: Semi-supervised Vision-Language Pre-training using Few Specialist Captions"}],"references":[{"paperId":"16de2006e2960ba410772c6b6d460b83c0a5cc4b","title":"Reproducible scaling laws for contrastive language-image learning"},{"paperId":"cfca7eedc6ede9d363d1662280a74d78dcdc9d4a","title":"Scaling Language-Image Pre-training via Masking"},{"paperId":"282bed72d56115077bb6cc0004c991a99a2c216a","title":"Self-Supervised Pretraining Enables High-Performance Chest X-Ray Interpretation Across Clinical Distributions"},{"paperId":"cdd9c1d23f9e89d5113f3e31821bb174c6a6afed","title":"MedCLIP: Contrastive Learning from Unpaired Medical Images and Text"},{"paperId":"e5c8960eb2ec034ffbd353ef39fd1cb541d3c7c9","title":"LAION-5B: An open large-scale dataset for training next generation image-text models"},{"paperId":"44279244407a64431810f982be6d0c7da4429dd7","title":"BioGPT: Generative Pre-trained Transformer for Biomedical Text Generation and Mining"},{"paperId":"3b60fa113f9d448eb17d855173b24f30d105aed8","title":"Making the Most of Text Semantics to Improve Biomedical Vision-Language Processing"},{"paperId":"7d3b912398c6132d506bebb070211f6b9c339482","title":"ELEVATER: A Benchmark and Toolkit for Evaluating Language-Augmented Visual Models"},{"paperId":"57e6a77d58b7f06ff8578e8b34f1b5072098f082","title":"Beyond Medical Imaging - A Review of Multimodal Deep Learning in Radiology"},{"paperId":"4ef3d9e492479e28fa57d107e52acc6a0c803de2","title":"Does CLIP Benefit Visual Question Answering in the Medical Domain as Much as it Does in the General Domain?"},{"paperId":"c10075b3746a9f3dd5811970e93c8ca3ad39b39d","title":"High-Resolution Image Synthesis with Latent Diffusion Models"},{"paperId":"d4d74b7902cc81b186ad80ba98e28ac38d1662d0","title":"Contrastive Vision-Language Pre-training with Limited Resources"},{"paperId":"2fca2821ac2beb60fa0e26866e8f063261713951","title":"Joint Learning of Localized Representations from Medical Images and Reports"},{"paperId":"94ff111c4d81bd03f159321728ceec8b4711c89d","title":"An Empirical Study of Training End-to-End Vision-and-Language Transformers"},{"paperId":"0b500aa5fcc175f07aecf26c0e8ddc4f0c6a931d","title":"GLoRIA: A Multimodal Global-Local Representation Learning Framework for Label-efficient Medical Image Recognition"},{"paperId":"c49d8a576ee4c1778eafd75f00565f75864054e4","title":"Self-supervised Image-text Pre-training With Mixed Data In Chest X-rays"},{"paperId":"25a493caee870b9950a9d972bdfae6c478d0816d","title":"Multimodal Representation Learning via Maximization of Local Mutual Information"},{"paperId":"2972bd6cb49883a5c75e26f8f7266dc91e1af25a","title":"Multiple Instance Captioning: Learning Representations from Histopathology Textbooks and Articles"},{"paperId":"98e565fa06f6c7bf7c46833b5106b26dc45130c4","title":"WIT: Wikipedia-based Image Text Dataset for Multimodal Multilingual Machine Learning"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"2cd605106b88c85d7d8b865b1ef0f8c8293debf1","title":"Zero-Shot Text-to-Image Generation"},{"paperId":"07559ad8ccaf1110232a4be78f825691e8416d8c","title":"MedAug: Contrastive learning leveraging patient metadata improves representations for chest X-ray interpretation"},{"paperId":"394be105b87e9bfe72c20efe6338de10604e1a11","title":"Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts"},{"paperId":"141a5033d9994242b18bb3b217e79582f1ee9306","title":"Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision"},{"paperId":"268d347e8a55b5eb82fb5e7d2f800e33c75ab18a","title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"},{"paperId":"fea4a2819f9146e5d1ad2fad3d89436523d530a4","title":"Fusion of medical imaging and electronic health records using deep learning: a systematic review and implementation guidelines"},{"paperId":"b8d5b853f2212cbb48a43f1edec9b96d76d388ec","title":"Medical Visual Question Answering via Conditional Reasoning"},{"paperId":"6dd9f99cecd38504b667d320eb2a6267a9fee35d","title":"Contrastive Learning of Medical Visual Representations from Paired Images and Text"},{"paperId":"b0181353f32b1ad3ac6bc59838c69b0e5c64137a","title":"Learning Visual Representations with Caption Annotations"},{"paperId":"54523ff961a1ac57a86696ef9a53b3a630b482c0","title":"Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing"},{"paperId":"3e86f5a0e2a97894de1cf1f1587799ac79bad0f2","title":"VirTex: Learning Visual Representations from Textual Annotations"},{"paperId":"6c5a72aaa3c29d52d46ef904f15719478b6cdfc2","title":"Lung and Colon Cancer Histopathological Image Dataset (LC25000)"},{"paperId":"3c8a456509e6c0805354bd40a35e3f2dbf8069b1","title":"PyTorch: An Imperative Style, High-Performance Deep Learning Library"},{"paperId":"d1f407b16fb8d99487baee37ed0805676c58e7ac","title":"MIMIC-CXR, a de-identified publicly available database of chest radiographs with free-text reports"},{"paperId":"d8a305b9366608d54452ac30459ee57b4f5cf1c9","title":"UNITER: UNiversal Image-TExt Representation Learning"},{"paperId":"26500af6bb97bf2dab1cc14dfb3b8b08fef67940","title":"Visual Semantic Reasoning for Image-Text Matching"},{"paperId":"79c93274429d6355959f1e4374c2147bb81ea649","title":"LXMERT: Learning Cross-Modality Encoder Representations from Transformers"},{"paperId":"65a9c7b0800c86a196bc14e7621ff895cc6ab287","title":"ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks"},{"paperId":"844286473f9afba35b46b960c706e5ecea190b0d","title":"Aligning Visual Regions and Textual Concepts for Semantic-Grounded Image Representations"},{"paperId":"b3c2c9f53ab130f3eb76eaaab3afa481c5a405eb","title":"ClinicalBERT: Modeling Clinical Notes and Predicting Hospital Readmission"},{"paperId":"1e43c7084bdcb6b3102afaf301cce10faead2702","title":"BioBERT: a pre-trained biomedical language representation model for biomedical text mining"},{"paperId":"89a816719613e220a64ab2590c938c23bbfe187e","title":"CheXpert: A Large Chest Radiograph Dataset with Uncertainty Labels and Expert Comparison"},{"paperId":"95b19e31af5385800855f245744aabfb0b0ee74e","title":"Augmenting the National Institutes of Health Chest Radiograph Dataset with Expert Annotations of Possible Pneumonia."},{"paperId":"18f9a6045ba01cb079c4fa49a630d71bbd27cd92","title":"A dataset of clinically generated visual questions and answers about radiology images"},{"paperId":"a564fabf130ff6e2742cfba90c7a4018937d764d","title":"Radiology Objects in COntext (ROCO): A Multimodal Image Dataset"},{"paperId":"b5246fa284f86b544a7c31f050b3bd0defd053fd","title":"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing"},{"paperId":"b227f3e4c0dc96e5ac5426b85485a70f2175a205","title":"Representation Learning with Contrastive Predictive Coding"},{"paperId":"b4df354db88a70183a64dbc9e56cf14e7669a6c0","title":"Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning"},{"paperId":"35ebe95db7ab148e25904604d3b06a9412f6b4a4","title":"Illustrative Language Understanding: Large-Scale Visual Grounding with Image Search"},{"paperId":"2a96afaf3261a87f0daa51699b4b3cf169e092c4","title":"Rotation Equivariant CNNs for Digital Pathology"},{"paperId":"0e8b061e08eb1ac8968e44edc0e54da658afad0e","title":"Spatial Organization and Molecular Correlation of Tumor-Infiltrating Lymphocytes Using Deep Learning on Pathology Images."},{"paperId":"d07284a6811f1b2745d91bdb06b040b57f226882","title":"Decoupled Weight Decay Regularization"},{"paperId":"f7ab6c52be9351ac3f6cf8fe6ad5efba1c1595e8","title":"VSE++: Improving Visual-Semantic Embeddings with Hard Negatives"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"8938988d82c6eb18e47deb5e69220652446c60bd","title":"Learning Visual N-Grams from Web Data"},{"paperId":"f651593fa6c83d717fc961482696a53b6fca5ab5","title":"Dual Attention Networks for Multimodal Reasoning and Matching"},{"paperId":"3a29aa4eff48624752c07059a44d3288a678c8ab","title":"Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)"},{"paperId":"2c03df8b48bf3fa39054345bafabfeff15bfd11d","title":"Deep Residual Learning for Image Recognition"},{"paperId":"8c30968d96e0c601adaa74db8907fa6ad73bae31","title":"Learning Visual Features from Large Weakly Supervised Data"},{"paperId":"1518039b5001f1836565215eb047526b3ac7f462","title":"Neural Machine Translation of Rare Words with Subword Units"},{"paperId":"2f9916a5a7e19e720a3db98171484e0911b4b116","title":"Overview of the ImageCLEF 2015 Medical Classification Task"},{"paperId":"97ad70a9fa3f99adf18030e5e38ebe3d90daa2db","title":"VQA: Visual Question Answering"},{"paperId":"7f1b111f0bb703b0bd97aba505728a9b0d9b2a54","title":"Deep Fragment Embeddings for Bidirectional Image Sentence Mapping"},{"paperId":"71b7178df5d2b112d07e45038cb5637208659ff7","title":"Microsoft COCO: Common Objects in Context"},{"paperId":"0ca7d208ff8d81377e0eaa9723820aeae7a7322d","title":"Grounded Compositional Semantics for Finding and Describing Images with Sentences"},{"paperId":"4aa4069693bee00d1b0759ca3df35e59284e9845","title":"DeViSE: A Deep Visual-Semantic Embedding Model"},{"paperId":"251c39c1b9372f3055cd53d0001fc122bfb2e418","title":"The Cancer Imaging Archive (TCIA): Maintaining and Operating a Public Information Repository"},{"paperId":"d2c733e34d48784a37d717fe43d9e93277a8c53e","title":"ImageNet: A large-scale hierarchical image database"},{"paperId":"f7609414ae3a117cd2828c7dae19ad34ff7d72e6","title":"PubMed Central: The GenBank of the published literature."},{"paperId":"c8b25fab5608c3e033d34b4483ec47e68ba109b7","title":"Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":"c213af6582c0d518a6e8e14217611c733eeb1ef1","title":"Catastrophic Interference in Connectionist Networks: The Sequential Learning Problem"},{"paperId":null,"title":"Tumor-infiltrating lymphocytes maps from tcga h&e whole slide pathology"},{"paperId":null,"title":"2017) peak learning rate 5.0e-4 weight decay"},{"paperId":null,"title":"Pytorch distributed: Experiences on accelerating data parallel training"},{"paperId":null,"title":"Slake: A semanticallylabeled knowledge-enhanced dataset for medical visual question answering"}],"id":"d8da72e7857cc1a0d3505e6c8a746eac815901b2","summary":"This paper conducted by far the largest study on biomedical VLP, using 15 million figure-caption pairs extracted from biomedical research articles in PubMed Central, and established new state of the art in a wide range of standard datasets, substantially outperformed prior VLP approaches."},{"url":"https://www.semanticscholar.org/paper/af997821231898a5f8d0fd78dad4eec526acabe5","title":"Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models","venue":"arXiv.org","year":2023,"referenceCount":57,"citationCount":120,"influentialCitationCount":6,"publicationDate":"08/03/2023","authors":"Chenfei Wu,Sheng-Kai Yin,Weizhen Qi,Xiaodong Wang,Zecheng Tang,Nan Duan","citations":[{"paperId":"1b4012f38daa8f09299e16771973c91ce9464ee2","title":"DVPT: Dynamic Visual Prompt Tuning of Large Pre-trained Models for Medical Image Analysis"},{"paperId":"2e52178416b93a0a7dc1cb7d21d04c7e72c2ce0f","title":"ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring Instruction Tuning"},{"paperId":"962ccf1fc49c83817fb031e5b24b81b19cdfb89d","title":"BuboGPT: Enabling Visual Grounding in Multi-Modal LLMs"},{"paperId":"a68dc9208aae7578e8ee384caa8ccbcf34e539e8","title":"Mini-Giants:\"Small\"Language Models and Open Source Win-Win"},{"paperId":"65d7663b60d95f98e6281ecc4da9c7a975119b91","title":"GeoGPT: Understanding and Processing Geospatial Tasks through An Autonomous GPT"},{"paperId":"0fea183c4f49a015a8ee7d89ef2e6885b7023c10","title":"SVIT: Scaling up Visual Instruction Tuning"},{"paperId":"3d02e5503caa2f444cbd61778c7cdc00a5b2e98d","title":"GPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest"},{"paperId":"53937df61d496572e90ee34c670ddd00337e558d","title":"What Matters in Training a GPT4-Style Language Model with Multimodal Inputs?"},{"paperId":"ea566f87f6253bb2d32cf7b61cd3e2535a0c3f42","title":"mPLUG-DocOwl: Modularized Multimodal Large Language Model for Document Understanding"},{"paperId":"4d2ca6f007f19a2923f059d96b0987565f00475a","title":"The Segment Anything Model (SAM) for Remote Sensing Applications: From Zero to One Shot"},{"paperId":"145be18b3e2e1baf3fed09e919da68da6e14a839","title":"Stone Needle: A General Multimodal Large-scale Model Framework towards Healthcare"},{"paperId":"91dc1f98417d7d2ca225df22b41f0643b8e54347","title":"Paradigm Shift in Sustainability Disclosure Analysis: Empowering Stakeholders with CHATREPORT, a Language Model-Based Tool"},{"paperId":"ebedc4d7a2356090904baba4104ef0832bc236df","title":"A Survey on Multimodal Large Language Models"},{"paperId":"697e0add95e880bd42e00bef838181e105f91981","title":"MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models"},{"paperId":"9dbb39eccbcd31b8f6b4ff0a2c96f61a7c34e54b","title":"RM-PRT: Realistic Robotic Manipulation Simulator and Benchmark with Progressive Reasoning Tasks"},{"paperId":"966852963a88a28786b798c91b6662d6e501e590","title":"AssistGPT: A General Multi-modal Assistant that can Plan, Execute, Inspect, and Learn"},{"paperId":"051549d8ef56937b2f4d113afdcf8c7586d3770b","title":"Towards AGI in Computer Vision: Lessons Learned from GPT and Large Language Models"},{"paperId":"7ca30ad71a113ab12a0089824d8bf9d0b4e623ae","title":"Synapse: Leveraging Few-Shot Exemplars for Human-Level Computer Control"},{"paperId":"e8cc6a50bc9c6a1e1ed7fdde0e7ccbb4efd4b505","title":"AVIS: Autonomous Visual Information Seeking with Large Language Models"},{"paperId":"473eb062612a17c965eaa62136322f0dec6b1f8e","title":"Data-Copilot: Bridging Billions of Data and Humans with Autonomous Workflow"},{"paperId":"79150cb420d15830c8d36f0e91eea1b02e177f0f","title":"Sticker820K: Empowering Interactive Retrieval with Stickers"},{"paperId":"74538984e72a26254697d4e7eeeb169000cf762a","title":"Valley: Video Assistant with Large Language model Enhanced abilitY"},{"paperId":"ed9943d73eb42116fe33564b5065c78b5ca0b16e","title":"RestGPT: Connecting Large Language Models with Real-World Applications via RESTful APIs"},{"paperId":"fd755dc7b5b206c17fd953db04e1c888d45b6e4e","title":"LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark"},{"paperId":"2933acb28b7369c7ea5b8728f6d8cb55e1beef98","title":"Customizing General-Purpose Foundation Models for Medical Report Generation"},{"paperId":"fed150a219f9c31bdb4920e615c7c9264c634736","title":"On the Challenges and Perspectives of Foundation Models for Medical Image Analysis"},{"paperId":"d9ecd192be910effeddf5df263ffc42cc659c7f3","title":"Decision Stacks: Flexible Reinforcement Learning via Modular Generative Models"},{"paperId":"d47524cd5c3c4b57af2e5a29f6f91c420310f236","title":"MIMIC-IT: Multi-Modal In-Context Instruction Tuning"},{"paperId":"2d338cdd12091814dec11155d3f6f848d7bab4d8","title":"Prompt Space Optimizing Few-shot Reasoning Success with Large Language Models"},{"paperId":"5bbc3b014f7c2dd151dc6b3cfb183889c44e772d","title":"Natural Language Commanding via Program Synthesis"},{"paperId":"f1892409fdbb396b00bb180891bb1c130fe3c7f4","title":"Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding"},{"paperId":"705e0f1887d76c956e3a1750f0176f2b8fe121ff","title":"Zero-Shot 3D Shape Correspondence"},{"paperId":"486a8c8655b81c7f87ff257141466ec1186d4aea","title":"Prompt Sapper: LLM-Empowered Software Engineering Infrastructure for AI-Native Services"},{"paperId":"615962d8969c8e0ffe43319689dce6c50cbf1f29","title":"Responsible Task Automation: Empowering Large Language Models as Responsible Task Automators"},{"paperId":"c85c90ef9e9a71efe031c3f7d6e34561f91168fe","title":"Deliberate then Generate: Enhanced Prompting Framework for Text Generation"},{"paperId":"7e3bbd7be60bb50a8093152795f269a69a4a0fd9","title":"Chatting Makes Perfect - Chat-based Image Retrieval"},{"paperId":"b458fc5261595f44b36325e5eaea1f874d65138f","title":"GPT4Tools: Teaching Large Language Model to Use Tools via Self-instruction"},{"paperId":"af705d648b5b16daa3dcc593bc593f2574d76c07","title":"Grammar Prompting for Domain-Specific Language Generation with Large Language Models"},{"paperId":"50c1414fe41d0cb9db6f0933c9319aa124beac5d","title":"Contextual Object Detection with Multimodal Large Language Models"},{"paperId":"1a28e9c62eeb76a1a77dc152197027c15310927b","title":"ANPL: Compiling Natural Programs with Interactive Decomposition"},{"paperId":"702e3b669912cbe9df70c7fb731842bb9801f25f","title":"Diffusion Model is an Effective Planner and Data Synthesizer for Multi-Task Reinforcement Learning"},{"paperId":"7e72eb196b7c90b3a5d6385af536fe8e5934fb82","title":"ConvGenVisMo: Evaluation of Conversational Generative Vision Models"},{"paperId":"f8f8267a2acd7598de6c15327f3953241901a62d","title":"On Evaluating Adversarial Robustness of Large Vision-Language Models"},{"paperId":"b13242323021bc1483a0d76e23428e324d409315","title":"NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models"},{"paperId":"51b169701290cd129e0781fc9f3a9918604c89b5","title":"Improving accuracy of GPT-3/4 results on biomedical data using a retrieval-augmented language model"},{"paperId":"c6ac708b65b24c20f80831d518c1795ce8133ad5","title":"ChatBridge: Bridging Modalities with Large Language Model as a Language Catalyst"},{"paperId":"06091944b864d6dc473cab63321a95fb9c4067cc","title":"ChatCAD+: Towards a Universal and Reliable Interactive CAD using LLMs"},{"paperId":"ef8c21e1f574495f0c80b8c1037dbdb886f0808d","title":"Towards Language-guided Interactive 3D Generation: LLMs as Layout Interpreter with Generative Feedback"},{"paperId":"f0888b9c0ef63e68c7758e6aec2370961c0eede9","title":"On the Tool Manipulation Capability of Open-source Large Language Models"},{"paperId":"13b5b69355555e0c8b702261c5de3b4172ba653c","title":"The Art of SOCRATIC QUESTIONING: Zero-shot Multimodal Reasoning with Recursive Thinking and Self-Questioning"},{"paperId":"c5c5b9c6660bb2089b781d851cb3fd0ba271d742","title":"Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models"},{"paperId":"00fcc983728346a5f3f8f005f1365be54456728e","title":"EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought"},{"paperId":"7c4f6fd4c7eadcc7189a6797db215895340f93c7","title":"ChatFace: Chat-Guided Real Face Editing via Diffusion Latent Space Manipulation"},{"paperId":"9837349417e36ef5be06da0fd6c74042148bdaa2","title":"Visual Programming for Text-to-Image Generation and Evaluation"},{"paperId":"abab79d1135684d039cfbebd0097e48ef4c1940c","title":"Vision + Language Applications: A Survey"},{"paperId":"86cbd30d1096b0c7e4ac6b03d97a8df12fd21457","title":"PathAsst: Redefining Pathology through Generative Foundation AI Assistant for Pathology"},{"paperId":"405bc18b9d2f783f22f50d5feb02c51b4b34655f","title":"LayoutGPT: Compositional Visual Planning and Generation with Large Language Models"},{"paperId":"7cf64070fd3d7e53d80f260c10e6bd7018d580e1","title":"IdealGPT: Iteratively Decomposing Vision and Language Reasoning via Large Language Models"},{"paperId":"6238fd213197ccf0d79191662828e38118a06d79","title":"ECHo: Event Causality Inference via Human-centric Reasoning"},{"paperId":"bd0488e0c1bb3ab375f28b3157058101350d3766","title":"ChipGPT: How far are we from natural language hardware design"},{"paperId":"ba704774f194938b04b1e2be40b1d111a4ca08e1","title":"CREATOR: Disentangling Abstract and Concrete Reasonings of Large Language Models through Tool Creation"},{"paperId":"90027ca7802645671a69b00b65e1fa94e6b63544","title":"ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models"},{"paperId":"13a5140fc0b269c408ecfc666cb297410bc753c5","title":"Matcher: Segment Anything with One Shot Using All-Purpose Feature Matching"},{"paperId":"43a55dbd95c9d5cd82de8db276f41adeec4a937d","title":"Interactive Data Synthesis for Systematic Vision Adaptation via LLMs-AIGCs Collaboration"},{"paperId":"6fe2a4f7594abadafc9feb05e96ab7e9e634ce8f","title":"Album Storytelling with Iterative Story-aware Captioning and Large Language Models"},{"paperId":"ca055cfb9d4d47124cc035c346f38577825fcacf","title":"Enhance Reasoning Ability of Visual-Language Models via Large Language Models"},{"paperId":"205d2ed0906440f07a0275d7d6a63bced60951fc","title":"InstructVid2Vid: Controllable Video Editing with Natural Language Instructions"},{"paperId":"49bb6f42879031e2374235715c942006af5c88d9","title":"Empower Large Language Model to Perform Better on Industrial Domain-Specific Question Answering"},{"paperId":"fbd4a876cee20eaf98f344aca597a55338f663f5","title":"Examining the Inter-Consistency of Large Language Models: An In-depth Analysis via Debate"},{"paperId":"2195676f111ad492c50f4d4c96abb2bd3d72f7fc","title":"Instruct2Act: Mapping Multi-modality Instructions to Robotic Actions with Large Language Model"},{"paperId":"42a30dc5470f54ec249f25d3c31e05d7c376c8e3","title":"VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks"},{"paperId":"1bdd5fc17cc580efe998304692639c57c857cc84","title":"Going Denser with Open-Vocabulary Part Segmentation"},{"paperId":"0e402bc4bdd774a28a2e2ba31d01687d68b94d0b","title":"Chain-of-Symbol Prompting Elicits Planning in Large Langauge Models"},{"paperId":"cb5f53cd7d43ac6f48abf4dde6fc58e53a083b0e","title":"Accelerating the integration of ChatGPT and other large‐scale AI models into biomedical research and healthcare"},{"paperId":"7787efaf502421eac9b6b0fd946a82e1ecf4c8c9","title":"Generating coherent comic with rich story using ChatGPT and Stable Diffusion"},{"paperId":"8dbb29f93292d8b1b861c322d232fe087b2ef7b1","title":"Small Models are Valuable Plug-ins for Large Language Models"},{"paperId":"0340c850e033abbf71c7214e403c8fe2be5ef91f","title":"Visual Tuning"},{"paperId":"d48cb91b9e555194f7494c4d4bb9815021d3ee45","title":"VideoChat: Chat-Centric Video Understanding"},{"paperId":"7a6dc7071891cb3d658c93418801942a4c6ed373","title":"Autonomous GIS: the next-generation AI-powered GIS"},{"paperId":"54a8b153ed04a872da878d695239bdc413dc782c","title":"InternGPT: Solving Vision-Centric Tasks by Interacting with Chatbots Beyond Language"},{"paperId":"e0dc8e113dbdd2896fb6420ac93e0b976c47f2a2","title":"Augmented Large Language Models with Parametric Knowledge Guiding"},{"paperId":"43e6e8d6663d83f1b74cf5a2be7b040b0928f867","title":"X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages"},{"paperId":"e1ff32753e20e48b4b01e40b5e820254396e6e70","title":"LMEye: An Interactive Perception Network for Large Language Models"},{"paperId":"d6d3604f369bb0415cbe814e43ca3131323b03e2","title":"Otter: A Multi-Modal Model with In-Context Instruction Tuning"},{"paperId":"6f8b9192b1f215254ee7625d752710182c05d2f9","title":"Caption Anything: Interactive Image Description with Diverse Multimodal Controls"},{"paperId":"c77d908ba29567445a9a4ad1bd4461d441cce174","title":"AutoML-GPT: Automatic Machine Learning with GPT"},{"paperId":"d473847dff63e3f5d238251cb23597f8205f72f2","title":"Generating Virtual On-body Accelerometer Data from Virtual Textual Descriptions for Human Activity Recognition"},{"paperId":"37ba1833e844f5178f91f50d82bfff616551e6ad","title":"The Role of Summarization in Generative Agents: A Preliminary Perspective"},{"paperId":"570079bbdd8758dfe865097e05719313c9c1301a","title":"LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model"},{"paperId":"c56a51728678e5b2e3ff95e51caf21d267439c36","title":"ChatVideo: A Tracklet-centric Multimodal and Versatile Video Understanding System"},{"paperId":"7e32aac43e9f1df49e116add03327ee6f365dbf3","title":"mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality"},{"paperId":"8bc617c9139648d7a92991d70c671230bac7b2e2","title":"AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head"},{"paperId":"f6654702479eddb01d6f1bd72d6d56102ebefbfc","title":"The Potential of Visual ChatGPT For Remote Sensing"},{"paperId":"4c8ef2db0c77aba453783f5211ebafc6695d3835","title":"ChatABL: Abductive Learning via Natural Language Interaction with ChatGPT"},{"paperId":"e36e5df3dde73c4e3606cdd4498cfc304a29bf5c","title":"Learning to Program with Natural Language"},{"paperId":"f44ad7ad67ddd5fe74598fe491ca75c5221380df","title":"MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models"},{"paperId":"170c97c7215f42edfb20c2248f954879e91ef86e","title":"Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models"},{"paperId":"352420ee61a8da783ca7750170793613b18b8d9c","title":"Tool Learning with Foundation Models"},{"paperId":"be2b0396de9431bae931642516a1d3e4906329f5","title":"Low-code LLM: Visual Programming over LLMs"},{"paperId":"1a8eb2cae1833df3bf12fe3b41b03d60b4a4a98d","title":"Visual Instruction Tuning"},{"paperId":"ba2f935d2578fbf77ec1aa79e26e3db396771e38","title":"Self-collaboration Code Generation via ChatGPT"},{"paperId":"de8121dc3d2c69bdab172f37e31168ddf2e6e62f","title":"Segment Everything Everywhere All at Once"},{"paperId":"6316cbb4f1e7dba5806a3310ec7f89f3571bc3db","title":"Boosting Cross-task Transferability of Adversarial Patches with Visual Relations"},{"paperId":"b3289a17ed0864f7b7737e50d2effc750c71ad87","title":"OpenAGI: When LLM Meets Domain Experts"},{"paperId":"0ebc861f5478561f12941e6b48aad30574e996d8","title":"Video ChatCaptioner: Towards Enriched Spatiotemporal Descriptions"},{"paperId":"26ccfbd8bfad44aeed695c12579ff7126adbfae9","title":"Summary of ChatGPT/GPT-4 Research and Perspective Towards the Future of Large Language Models"},{"paperId":"1d29334cfbe9a1a943082058876f0c22d44c62fd","title":"A Survey of Large Language Models"},{"paperId":"ac7771c332da42b29a913b116bd6ef622cbf89cf","title":"TaskMatrix.AI: Completing Tasks by Connecting Foundation Models with Millions of APIs"},{"paperId":"74e8ae03a385e72f5ae377667ba9858fb3e0bfa0","title":"Unleashing the Power of Edge-Cloud Generative AI in Mobile Networks: A Survey of AIGC Services"},{"paperId":"994e08ac813028601907516aee9c4699234a6b4d","title":"Large AI Models in Health Informatics: Applications, Challenges, and the Future"},{"paperId":"c7a9c7302a72301ed79a7c0696d5af2e03ad3ac4","title":"MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action"},{"paperId":"9fe9af7cf3d54b707a7be3c53ce94b77dcc3bae5","title":"A Short Survey of Viewing Large Language Models in Legal Aspect"},{"paperId":"53df959bcf6499c45e316086a96a624389a39a52","title":"Accountable Textual-Visual Chat Learns to Reject Human Instructions in Image Re-creation"},{"paperId":"887138f4c365b9d1325de41a522d27bec34e0d7e","title":"Reasoning with Language Model Prompting: A Survey"},{"paperId":"09ca5072a76796c65e5936b6fb4968afead61944","title":"Semantics-Empowered Communication: A Tutorial-cum-Survey"},{"paperId":"419eb47fea3931c4098232f44ccbc216275d3f56","title":"Decoding Visual Neural Representations by Multimodal Learning of Brain-Visual-Linguistic Features"},{"paperId":"6139b6bc065b24562cb7f4f08227a42f5766138f","title":"Diffusion Models: A Comprehensive Survey of Methods and Applications"},{"paperId":"d1120d67b700e4dfe8b39eb1e48fbdea4e1a0c43","title":"HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face"},{"paperId":"eb291a2e237774b162d9c51c21c4868795589e94","title":"Diving into the Inter-Consistency of Large Language Models: An Insightful Analysis through Debate"}],"references":[{"paperId":"e55695dfe6cde42ee195aa6672fe720ec92ee8c3","title":"Adding Conditional Control to Text-to-Image Diffusion Models"},{"paperId":"780a7f5e8ba9b4b451e3dfee1bcfb0f68aba5050","title":"Multimodal Chain-of-Thought Reasoning in Language Models"},{"paperId":"3f5b31c4f7350dc88002c121aecbdc82f86eb5bb","title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models"},{"paperId":"a2d2bbe4c542173662a444b33b76c66992697830","title":"InstructPix2Pix: Learning to Follow Image Editing Instructions"},{"paperId":"964bd39b546f0f6625ff3b9ef1083f797807ef2e","title":"BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"},{"paperId":"90350aa626bed47b02d0c162462e5b0ca82be6b2","title":"Automatic Chain of Thought Prompting in Large Language Models"},{"paperId":"d3135733aa39dec20ce72aa138589dda27c8406d","title":"Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering"},{"paperId":"e7ad08848d5d7c5c47673ffe0da06af443643bda","title":"Large Language Models are Zero-Shot Reasoners"},{"paperId":"5437e8adab596d7294124c0e798708e050e25321","title":"Least-to-Most Prompting Enables Complex Reasoning in Large Language Models"},{"paperId":"26218bdcc3945c7edae7aa2adbfba4cd820a2df3","title":"Flamingo: a Visual Language Model for Few-Shot Learning"},{"paperId":"ada81a4de88a6ce474df2e2446ad11fea480616e","title":"Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language"},{"paperId":"28e89e57092634fefd25bd764c432c5645bbfe3e","title":"STaR: Bootstrapping Reasoning With Reasoning"},{"paperId":"5f19ae1135a9500940978104ec15a5b8751bc7d2","title":"Self-Consistency Improves Chain of Thought Reasoning in Language Models"},{"paperId":"d766bffc357127e0dc86dd69561d5aeb520d6f4c","title":"Training language models to follow instructions with human feedback"},{"paperId":"1b6e810ce0afd0dd093f789d2b2742d047e316d5","title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"a3b42a83669998f65df60d7c065a70d07ca95e99","title":"BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation"},{"paperId":"f4b11a696aa5a03fed1bfc47e65fdb7eb0e529c1","title":"UniFormer: Unifying Convolution and Self-attention for Visual Recognition"},{"paperId":"400d619cbabeb669115bb7281a889ab869829ef5","title":"MERLOT RESERVE: Neural Script Knowledge through Vision and Language and Sound"},{"paperId":"c10075b3746a9f3dd5811970e93c8ca3ad39b39d","title":"High-Resolution Image Synthesis with Latent Diffusion Models"},{"paperId":"bdea16e93fc70f316002e5f6aac8ce17388c6ee9","title":"MAGMA - Multimodal Augmentation of Generative Models through Adapter-based Finetuning"},{"paperId":"ba9d736006b897d06f75586ad46e28e00a5e566e","title":"VIOLET : End-to-End Video-Language Transformers with Masked Visual-token Modeling"},{"paperId":"197d5867a45a2988f4dd159063cdfbfe90164962","title":"LiT: Zero-Shot Transfer with Locked-image text Tuning"},{"paperId":"cf7c2e0e4fb2af689aaf4b7a7cddf7b1f4d5e3f0","title":"VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts"},{"paperId":"767923635f2fd4467d848dba9655866e4f9b55c8","title":"Supervision Exists Everywhere: A Data Efficient Contrastive Language-Image Pre-training Paradigm"},{"paperId":"2672777d25562c9df6fc13b653181db62d39bece","title":"An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA"},{"paperId":"260ad39a1dac4b451019e2bf17925f4df8e3b69a","title":"Per-Pixel Classification is Not All You Need for Semantic Segmentation"},{"paperId":"01b5412f3d17e90e09226d7c40ad4d4468a1414d","title":"Multimodal Few-Shot Learning with Frozen Language Models"},{"paperId":"2a805d0e1b067444a554c5169d189fa1f649f411","title":"Scaling Vision Transformers"},{"paperId":"0cceb0393b87d3ff65a1f0beea696ce40e889597","title":"Towards Light-Weight and Real-Time Line Segment Detection"},{"paperId":"8e33914d6051dd031a5e096962b9398fc1d16067","title":"Vision Transformers for Dense Prediction"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"616e0ed02ca024a8c1d4b86167f7486ea92a13d9","title":"VisualGPT: Data-efficient Adaptation of Pretrained Language Models for Image Captioning"},{"paperId":"be0014c1fbc3e664686610d2c85f75038a4f6e4f","title":"VinVL: Making Visual Representations Matter in Vision-Language Models"},{"paperId":"053b1d7b97eb2c91fc3921d589c160b0923c70b1","title":"Learning to summarize from human feedback"},{"paperId":"4a657ceb97829a4ab7502f6d01235d1f0140eb0f","title":"Text as Neural Operator:Image Manipulation by Text Instruction"},{"paperId":"2f5f81bc516a6d085d39479378af1fc27104f91e","title":"Large-Scale Adversarial Training for Vision-and-Language Representation Learning"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"818e5cbc337e4e1b98e65a2d7c2d6d2a0318cd57","title":"Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks"},{"paperId":"953667588e089ae99f049e8574d013bb70aa8517","title":"ManiGAN: Text-Guided Image Manipulation"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2","title":"Transformers: State-of-the-Art Natural Language Processing"},{"paperId":"d8a305b9366608d54452ac30459ee57b4f5cf1c9","title":"UNITER: UNiversal Image-TExt Representation Learning"},{"paperId":"7bd83b055702bc178aa26def5b6df463f8eab7b9","title":"Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-Shot Cross-Dataset Transfer"},{"paperId":"29ddc1f43f28af7c846515e32cc167bc66886d0c","title":"Parameter-Efficient Transfer Learning for NLP"},{"paperId":"6dfc2ff03534a4325d06c6f88c3144831996629b","title":"From Recognition to Cognition: Visual Commonsense Reasoning"},{"paperId":"19c12e12946eb3bb9aa7fdeb511eef79fc53b6b3","title":"Canny edge detection based on Open CV"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"9e8db1519245426f3a78752a3d8360484f4626b1","title":"Realtime Multi-person 2D Pose Estimation Using Part Affinity Fields"},{"paperId":"778ce81457383bd5e3fdb11b145ded202ebb4970","title":"Semantic Compositional Networks for Visual Captioning"},{"paperId":"8acbe90d5b852dadea7810345451a99608ee54c7","title":"Image-to-Image Translation with Conditional Adversarial Networks"},{"paperId":"9a522bdd86531839ff292d096e0c4050b787de02","title":"Commonsense reasoning and commonsense knowledge in artificial intelligence"},{"paperId":"97ad70a9fa3f99adf18030e5e38ebe3d90daa2db","title":"VQA: Visual Question Answering"},{"paperId":"8da55e685a7bef9c897788ab519a8710c695c419","title":"Holistically-Nested Edge Detection"},{"paperId":"d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0","title":"Show and tell: A neural image caption generator"},{"paperId":"71b7178df5d2b112d07e45038cb5637208659ff7","title":"Microsoft COCO: Common Objects in Context"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"}],"id":"af997821231898a5f8d0fd78dad4eec526acabe5","summary":"A system to enable the user to interact with ChatGPT by sending and receiving not only languages but also images and providing complex visual questions or visual editing instructions that require the collaboration of multiple AI models with multi-steps, and opens the door to investigating the visual roles ofChatGPT with the help of Visual Foundation Models."},{"url":"https://www.semanticscholar.org/paper/f44ad7ad67ddd5fe74598fe491ca75c5221380df","title":"MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models","venue":"","year":2023,"referenceCount":36,"citationCount":98,"influentialCitationCount":1,"publicationDate":"20/04/2023","authors":"Deyao Zhu,Jun Chen,Xiaoqian Shen,Xiang Li,Mohamed Elhoseiny","citations":[{"paperId":"2e52178416b93a0a7dc1cb7d21d04c7e72c2ce0f","title":"ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring Instruction Tuning"},{"paperId":"962ccf1fc49c83817fb031e5b24b81b19cdfb89d","title":"BuboGPT: Enabling Visual Grounding in Multi-Modal LLMs"},{"paperId":"63cf8add3a25c546190f5b76c31e63dd3d218c40","title":"Planting a SEED of Vision in Large Language Model"},{"paperId":"d58b89b64a0565e77d9b9734d871c58e4a7af6d8","title":"mBLIP: Efficient Bootstrapping of Multilingual Vision-LLMs"},{"paperId":"369b449415d50387fba048bbd4d26ee890df84b5","title":"InternVid: A Large-scale Video-Text Dataset for Multimodal Understanding and Generation"},{"paperId":"f5945b1421ee8e0cd59e674b73f4ad82a71f66c7","title":"T2I-CompBench: A Comprehensive Benchmark for Open-world Compositional Text-to-image Generation"},{"paperId":"94053805cd59f2e9a47fe3f080c7e7afefb337cc","title":"Generative Pretraining in Multimodality"},{"paperId":"85d9151aa2efd0cbe822e403138cfe49f9536703","title":"SITTA: A Semantic Image-Text Alignment for Image Captioning"},{"paperId":"0fea183c4f49a015a8ee7d89ef2e6885b7023c10","title":"SVIT: Scaling up Visual Instruction Tuning"},{"paperId":"0d7102a69aff01b75b25e4af7359120260d9a522","title":"On decoder-only architecture for speech-to-text and large language model integration"},{"paperId":"c43fde4e5edcc52d2612d3f96374190e8a5376e6","title":"AutoDecoding Latent 3D Diffusion Models"},{"paperId":"3d02e5503caa2f444cbd61778c7cdc00a5b2e98d","title":"GPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest"},{"paperId":"53937df61d496572e90ee34c670ddd00337e558d","title":"What Matters in Training a GPT4-Style Language Model with Multimodal Inputs?"},{"paperId":"1e3ef48abeef882e12f9553a1baf8944f3782c88","title":"Several Categories of Large Language Models (LLMs): A Short Survey"},{"paperId":"ea566f87f6253bb2d32cf7b61cd3e2535a0c3f42","title":"mPLUG-DocOwl: Modularized Multimodal Large Language Model for Document Understanding"},{"paperId":"df710c46594c04fb59ef9a93d3b4e1cb387a1b2b","title":"Embodied Task Planning with Large Language Models"},{"paperId":"da08e9f21ef361e0e1242f8849a18a4ea1a3d27e","title":"SCITUNE: Aligning Large Language Models with Scientific Multimodal Instructions"},{"paperId":"c713d4eb8dc7f1c80cee98eb5ae6f171ee33b18b","title":"RefSAM: Efficiently Adapting Segmenting Anything Model for Referring Video Object Segmentation"},{"paperId":"16160a4bdd0f239e47f120547e6ecee44636d5e8","title":"JourneyDB: A Benchmark for Generative Image Understanding"},{"paperId":"44bdd340aa7d54c3afb1831ffe6b6a8035b41200","title":"UMASS_BioNLP at MEDIQA-Chat 2023: Can LLMs generate high-quality synthetic note-oriented doctor-patient conversations?"},{"paperId":"a9d5d97733ccb15002ff3cfb95b0a7d8ba5236e3","title":"LLaVAR: Enhanced Visual Instruction Tuning for Text-Rich Image Understanding"},{"paperId":"efc694164312006c543ef745611348ef64e68dda","title":"Towards Language Models That Can See: Computer Vision Through the LENS of Natural Language"},{"paperId":"c94f0acf00530dbf9f275dad8515e23dc30666d3","title":"Prompting Large Language Models for Zero-Shot Domain Adaptation in Speech Recognition"},{"paperId":"def6c12724dec95ec1276a77fd1cf7e200883bdb","title":"Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic"},{"paperId":"8724579d3f126e753a0451d98ff57b165f722e72","title":"Are aligned neural networks adversarially aligned?"},{"paperId":"84dc889beff9d51fe429cff8c92735e7410ee3c2","title":"Aligning Large Multi-Modal Model with Robust Instruction Tuning"},{"paperId":"cde934546bbdb19094d8a53cc047d002c827f884","title":"Large Multimodal Models: Notes on CVPR 2023 Tutorial"},{"paperId":"697e0add95e880bd42e00bef838181e105f91981","title":"MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models"},{"paperId":"ebedc4d7a2356090904baba4104ef0832bc236df","title":"A Survey on Multimodal Large Language Models"},{"paperId":"738852940591ecf864abf402878ecf66e2945267","title":"Visual Adversarial Examples Jailbreak Large Language Models"},{"paperId":"9dbb39eccbcd31b8f6b4ff0a2c96f61a7c34e54b","title":"RM-PRT: Realistic Robotic Manipulation Simulator and Benchmark with Progressive Reasoning Tasks"},{"paperId":"7839d037bb0e41f8a9898f177d2710cfe23633fc","title":"Path to Medical AGI: Unify Domain-specific Medical LLMs with the Lowest Cost"},{"paperId":"bc2333c9a667af90ee7ce52b911d2e04aed01526","title":"MotionGPT: Finetuned LLMs are General-Purpose Motion Generators"},{"paperId":"859baf28d0c2530307f3242ae7662a4dee89acd1","title":"Sample-Efficient Learning of Novel Visual Concepts"},{"paperId":"a8d02ff6d075c3cc48f0b97801cc52765c8f9ac9","title":"LVLM-eHub: A Comprehensive Evaluation Benchmark for Large Vision-Language Models"},{"paperId":"0983883619a0ca597d055d0e58da2f514052913d","title":"Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration"},{"paperId":"966852963a88a28786b798c91b6662d6e501e590","title":"AssistGPT: A General Multi-modal Assistant that can Plan, Execute, Inspect, and Learn"},{"paperId":"051549d8ef56937b2f4d113afdcf8c7586d3770b","title":"Towards AGI in Computer Vision: Lessons Learned from GPT and Large Language Models"},{"paperId":"9e8b7b0d4c628c12b6a65ab56ac5f33a35eff2e6","title":"Unifying Large Language Models and Knowledge Graphs: A Roadmap"},{"paperId":"66d7d8dc54ea3dff10a11df2f29dc2104df86a57","title":"XrayGPT: Chest Radiographs Summarization using Medical Vision-Language Models"},{"paperId":"74538984e72a26254697d4e7eeeb169000cf762a","title":"Valley: Video Assistant with Large Language model Enhanced abilitY"},{"paperId":"79150cb420d15830c8d36f0e91eea1b02e177f0f","title":"Sticker820K: Empowering Interactive Retrieval with Stickers"},{"paperId":"fd755dc7b5b206c17fd953db04e1c888d45b6e4e","title":"LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark"},{"paperId":"d198b0b155313afe350e91a77c3d73cffa39d2a9","title":"Leveraging Large Language Models for Scalable Vector Graphics-Driven Image Understanding"},{"paperId":"bf7025a2e5dbb3c09deae02a1aa98a256ca559e2","title":"Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models"},{"paperId":"d47524cd5c3c4b57af2e5a29f6f91c420310f236","title":"MIMIC-IT: Multi-Modal In-Context Instruction Tuning"},{"paperId":"d818f40ea693a335e02f32dab520351d271c58bf","title":"Artificial General Intelligence for Medical Imaging"},{"paperId":"6a2a756c60dbc99f666ae6e32b0dd1a58e1e2de8","title":"M3IT: A Large-Scale Dataset towards Multi-Modal Multilingual Instruction Tuning"},{"paperId":"d7a4b09a0e2c2d7b118144cf09895c640896da7b","title":"Youku-mPLUG: A 10 Million Large-scale Chinese Video-Language Dataset for Pre-training and Benchmarks"},{"paperId":"f1892409fdbb396b00bb180891bb1c130fe3c7f4","title":"Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding"},{"paperId":"0244aeb7c6927e2fb0c2e668687e160a00737dbe","title":"Orca: Progressive Learning from Complex Explanation Traces of GPT-4"},{"paperId":"705e0f1887d76c956e3a1750f0176f2b8fe121ff","title":"Zero-Shot 3D Shape Correspondence"},{"paperId":"31a68755ca6899e6c360ec8568704ae74f223a25","title":"GPT4Image: Can Large Pre-trained Models Help Vision Models on Perception Tasks?"},{"paperId":"0867f7029b3726740fb41ca8171833bf6f82e483","title":"Exploring Open-Vocabulary Semantic Segmentation without Human Labels"},{"paperId":"5fb7afae5fcacae1d40f109a348b43e00aa5d486","title":"Dense and Aligned Captions (DAC) Promote Compositional Reasoning in VL Models"},{"paperId":"b458fc5261595f44b36325e5eaea1f874d65138f","title":"GPT4Tools: Teaching Large Language Model to Use Tools via Self-instruction"},{"paperId":"f6c23f3fe897b3836e29e91e7842c12f711c68fe","title":"Pre-trained transformer for adversarial purification"},{"paperId":"f8f8267a2acd7598de6c15327f3953241901a62d","title":"On Evaluating Adversarial Robustness of Large Vision-Language Models"},{"paperId":"d3f79210b54e168c76b8c311488f42d7d1048b81","title":"PandaGPT: One Model To Instruction-Follow Them All"},{"paperId":"5d44f16a36ba7ae6b3d9d7c98bbc1b877e598f35","title":"The False Promise of Imitating Proprietary LLMs"},{"paperId":"c5c5b9c6660bb2089b781d851cb3fd0ba271d742","title":"Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models"},{"paperId":"86cbd30d1096b0c7e4ac6b03d97a8df12fd21457","title":"PathAsst: Redefining Pathology through Generative Foundation AI Assistant for Pathology"},{"paperId":"f45a3474bd38d65c1b2cc3342a64dacbf07f445a","title":"Cream: Visually-Situated Natural Language Understanding with Contrastive Reading Model and Frozen Large Language Models"},{"paperId":"7cf64070fd3d7e53d80f260c10e6bd7018d580e1","title":"IdealGPT: Iteratively Decomposing Vision and Language Reasoning via Large Language Models"},{"paperId":"00fcc983728346a5f3f8f005f1365be54456728e","title":"EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought"},{"paperId":"b82c1b0512d25307e3c81bb8d9df1607267a7a52","title":"MemeCap: A Dataset for Captioning and Interpreting Memes"},{"paperId":"7bf902fb94a577d15293ac4f90d8967163850fb1","title":"Let's Think Frame by Frame: Evaluating Video Chain of Thought with Video Infilling and Prediction"},{"paperId":"e9a37d881abf7d94cb2c586f1cb26978343750ba","title":"ReSee: Responding through Seeing Fine-grained Visual Knowledge in Open-domain Dialogue"},{"paperId":"2ad8183c72a90511383a32ccaeea313eb85f4085","title":"DetGPT: Detect What You Need via Reasoning"},{"paperId":"13a5140fc0b269c408ecfc666cb297410bc753c5","title":"Matcher: Segment Anything with One Shot Using All-Purpose Feature Matching"},{"paperId":"6a5525c316b9be7909c433a79e090ed731425083","title":"What Makes for Good Visual Tokenizers for Large Language Models?"},{"paperId":"33f9ddca2469bf4831dcab085e1620792b1a6a80","title":"LLM Itself Can Read and Generate CXR Images"},{"paperId":"bfa9df38bd8597d8cdf0c3497fe5e5a5e31f646f","title":"ONE-PEACE: Exploring One General Representation Model Toward Unlimited Modalities"},{"paperId":"42a30dc5470f54ec249f25d3c31e05d7c376c8e3","title":"VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks"},{"paperId":"a979975d1a0aea0e01423f092249cc3de575b6cd","title":"X-IQE: eXplainable Image Quality Evaluation for Text-to-Image Generation with Visual Large Language Models"},{"paperId":"3f0c4d50050e8d74993b020897abaee8d1e8054d","title":"Evaluating Object Hallucination in Large Vision-Language Models"},{"paperId":"f4793adffd6f67ffcb93ccfc5672ab301b8a2b96","title":"PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering"},{"paperId":"f71ce6b09756ea26df283640b4c1c1c125411d2e","title":"Towards Generalist Robots: A Promising Paradigm via Generative Simulation"},{"paperId":"848e690a62c327e1210532d58a6b914097cac763","title":"On the Hidden Mystery of OCR in Large Multimodal Models"},{"paperId":"abac9af9174e8be2f605453695d98e3686768a27","title":"ArtGPT-4: Artistic Vision-Language Understanding with Adapter-enhanced MiniGPT-4"},{"paperId":"8bd6a2a89503be083176f2cc26fabedb79238cbd","title":"InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning"},{"paperId":"d48cb91b9e555194f7494c4d4bb9815021d3ee45","title":"VideoChat: Chat-Centric Video Understanding"},{"paperId":"54a8b153ed04a872da878d695239bdc413dc782c","title":"InternGPT: Solving Vision-Centric Tasks by Interacting with Chatbots Beyond Language"},{"paperId":"80c44fab16852ea9599411da14de7079c4514172","title":"Vision-Language Models in Remote Sensing: Current Progress and Future Trends"},{"paperId":"81e7e82245c2f230eeb8aaaa1a2b2604c143754a","title":"MultiModal-GPT: A Vision and Language Model for Dialogue with Humans"},{"paperId":"43e6e8d6663d83f1b74cf5a2be7b040b0928f867","title":"X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages"},{"paperId":"e1ff32753e20e48b4b01e40b5e820254396e6e70","title":"LMEye: An Interactive Perception Network for Large Language Models"},{"paperId":"d6d3604f369bb0415cbe814e43ca3131323b03e2","title":"Otter: A Multi-Modal Model with In-Context Instruction Tuning"},{"paperId":"9a22b33b529484c912d1ea9f8698369d4546a1c1","title":"Transfer Visual Prompt Generator across LLMs"},{"paperId":"570079bbdd8758dfe865097e05719313c9c1301a","title":"LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model"},{"paperId":"7e32aac43e9f1df49e116add03327ee6f365dbf3","title":"mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality"},{"paperId":"131f499e4d3503da93022d07fcf804a18483bea9","title":"WizardLM: Empowering Large Language Models to Follow Complex Instructions"},{"paperId":"27d0d2923a42bd2bced1b100844e232ff87368e3","title":"SkinGPT-4: An Interactive Dermatology Diagnostic System with Visual Large Language Model"},{"paperId":"30c0cdc414f68211d5d0514df027cec22e005174","title":"A Survey for In-context Learning"},{"paperId":"7562e25b666cba841b1dd5cf6e700978922beb04","title":"SkinGPT: A Dermatology Diagnostic System with Vision Large Language Model"},{"paperId":"a3711dbf296b5ddd97ba93826660cd3995611625","title":"Towards A Foundation Model for Generalist Robots: Diverse Skill Learning at Scale via Automated Task and Scene Generation"}],"references":[{"paperId":"0ebc861f5478561f12941e6b48aad30574e996d8","title":"Video ChatCaptioner: Towards Enriched Spatiotemporal Descriptions"},{"paperId":"c7a9c7302a72301ed79a7c0696d5af2e03ad3ac4","title":"MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action"},{"paperId":"6e754273d54a91371efbc928cd6b156364d517da","title":"ViperGPT: Visual Inference via Python Execution for Reasoning"},{"paperId":"32709ac303247af89262a41510900f96bb349016","title":"Breaking Common Sense: WHOOPS! A Vision-and-Language Benchmark of Synthetic and Compositional Images"},{"paperId":"69cfdc8df16ae63b7acba4ac6f727f78b86893c3","title":"ChatGPT Asks, BLIP-2 Answers: Automatic Questioning Towards Enriched Visual Descriptions"},{"paperId":"af997821231898a5f8d0fd78dad4eec526acabe5","title":"Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models"},{"paperId":"38fe8f324d2162e63a967a9ac6648974fc4c66f3","title":"PaLM-E: An Embodied Multimodal Language Model"},{"paperId":"57e849d0de13ed5f91d086936296721d4ff75a75","title":"LLaMA: Open and Efficient Foundation Language Models"},{"paperId":"fbfef4723d8c8467d7bd523e1d0b703cce0e0f9c","title":"Language Is Not All You Need: Aligning Perception with Language Models"},{"paperId":"3f5b31c4f7350dc88002c121aecbdc82f86eb5bb","title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models"},{"paperId":"78281482c1fdad8e167bab39cc9955c73d58ae8f","title":"EVA: Exploring the Limits of Masked Visual Representation Learning at Scale"},{"paperId":"964bd39b546f0f6625ff3b9ef1083f797807ef2e","title":"BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"},{"paperId":"5484d228bfc50efbac6e86677bc2ec2ee4ede1a6","title":"Scaling Instruction-Finetuned Language Models"},{"paperId":"26fd105d0b5a458979c012cddb3ba2de943388c4","title":"Plug-and-Play VQA: Zero-shot VQA by Conjoining Large Pretrained Models with Zero Training"},{"paperId":"a970c8fadef8497576660b288c52c0ec8eebdc12","title":"Zero-Shot Video Question Answering via Frozen Bidirectional Language Models"},{"paperId":"dac3a172b504f4e33c029655e9befb3386e5f63a","title":"Emergent Abilities of Large Language Models"},{"paperId":"13a0d8bb38f739990c8cd65a44061c6534f17221","title":"OPT: Open Pre-trained Transformer Language Models"},{"paperId":"26218bdcc3945c7edae7aa2adbfba4cd820a2df3","title":"Flamingo: a Visual Language Model for Few-Shot Learning"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"8342b592fe238f3d230e4959b06fd10153c45db1","title":"Training Compute-Optimal Large Language Models"},{"paperId":"d766bffc357127e0dc86dd69561d5aeb520d6f4c","title":"Training language models to follow instructions with human feedback"},{"paperId":"a3b42a83669998f65df60d7c065a70d07ca95e99","title":"BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation"},{"paperId":"7cbc2a7843411a1768ab762930707af0a3c33a19","title":"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model"},{"paperId":"b668ce936cff0b0ca8b635cd5f25a62eaf4eb3df","title":"LAION-400M: Open Dataset of CLIP-Filtered 400 Million Image-Text Pairs"},{"paperId":"01b5412f3d17e90e09226d7c40ad4d4468a1414d","title":"Multimodal Few-Shot Learning with Frozen Language Models"},{"paperId":"616e0ed02ca024a8c1d4b86167f7486ea92a13d9","title":"VisualGPT: Data-efficient Adaptation of Pretrained Language Models for Image Captioning"},{"paperId":"394be105b87e9bfe72c20efe6338de10604e1a11","title":"Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"43f2ad297941db230c089ba353efc3f281ab678c","title":"5分で分かる!? 有名論文ナナメ読み：Jacob Devlin et al. : BERT : Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"b4df354db88a70183a64dbc9e56cf14e7669a6c0","title":"Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning"},{"paperId":"8e080b98efbe65c02a116439205ca2344b9f7cd4","title":"Im2Text: Describing Images Using 1 Million Captioned Photographs"},{"paperId":null,"title":"Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023"},{"paperId":null,"title":"Stanford alpaca: An instruction-following llama model"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"}],"id":"f44ad7ad67ddd5fe74598fe491ca75c5221380df","summary":"MiniGPT-4 is presented, which aligns a frozen visual encoder with a frozen LLM, Vicuna, using just one projection layer and possesses many capabilities similar to those exhibited by G PT-4 like detailed image description generation and website creation from hand-written drafts."},{"url":"https://www.semanticscholar.org/paper/994e08ac813028601907516aee9c4699234a6b4d","title":"Large AI Models in Health Informatics: Applications, Challenges, and the Future","venue":"arXiv.org","year":2023,"referenceCount":272,"citationCount":7,"influentialCitationCount":0,"publicationDate":"21/03/2023","authors":"Jianing Qiu,Lin Li,Jiankai Sun,Jiachuan Peng,Peilun Shi,Rui Zhang,Yinzhao Dong,Kyle Lam,F. P. Lo,Bo Xiao,Wu Yuan,Dong Xu,Benny P. L. Lo","citations":[{"paperId":"121a7077db9314d63cc9ef5718e196ff0d75ab92","title":"BioSignal Copilot: Leveraging the power of LLMs in drafting reports for biomedical signals"},{"paperId":"e1de1ef317976159b9fb6e4128fbfbe8846a720c","title":"EHRSHOT: An EHR Benchmark for Few-Shot Evaluation of Foundation Models"},{"paperId":"362d4e00506f9bb39d42185a0b128f8602e139a8","title":"Utilizing ChatGPT to Enhance Clinical Trial Enrollment"},{"paperId":"06091944b864d6dc473cab63321a95fb9c4067cc","title":"ChatCAD+: Towards a Universal and Reliable Interactive CAD using LLMs"},{"paperId":"d0ee000f30420953f10dfcfd608a7f9ad40f1635","title":"Humans are Still Better than ChatGPT: Case of the IEEEXtreme Competition"},{"paperId":"80785017029cab501fcdb90b98985cd2b36e1fb8","title":"Evaluation of GPT-3.5 and GPT-4 for supporting real-world information needs in healthcare delivery"},{"paperId":"78b03df885cbb57361e8efc5ce5ad5eea211dda6","title":"Generalist Vision Foundation Models for Medical Imaging: A Case Study of Segment Anything Model on Zero-Shot Medical Segmentation"}],"references":[{"paperId":"77877e028e5dace94f1f042c2a4e5e71e840c6a5","title":"A Step Towards Conditional Autonomy - Robotic Appendectomy"},{"paperId":"8ca62fdf4c276ea3052dc96dcfd8ee96ca425a48","title":"GPT-4 Technical Report"},{"paperId":"6470b561d3426714847fd9201c8ea4ab8585fb96","title":"GeneTuring tests GPT models in genomics"},{"paperId":"af997821231898a5f8d0fd78dad4eec526acabe5","title":"Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models"},{"paperId":"703a72ad206272d2022c9b1d7eb775e275f4b39c","title":"Empowering Beginners in Bioinformatics with ChatGPT"},{"paperId":"38fe8f324d2162e63a967a9ac6648974fc4c66f3","title":"PaLM-E: An Embodied Multimodal Language Model"},{"paperId":"e648e802b0ed38dd9a2a941a026642730a4d30de","title":"Google USM: Scaling Automatic Speech Recognition Beyond 100 Languages"},{"paperId":"fbfef4723d8c8467d7bd523e1d0b703cce0e0f9c","title":"Language Is Not All You Need: Aligning Perception with Language Models"},{"paperId":"57e849d0de13ed5f91d086936296721d4ff75a75","title":"LLaMA: Open and Efficient Foundation Language Models"},{"paperId":"a4701fadfd92683f2df4245d3ea873f1df61a71a","title":"MimicPlay: Long-Horizon Imitation Learning by Watching Human Play"},{"paperId":"5c66d3d97746b7438d5b374e0322e79ab9e5ac5e","title":"On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective"},{"paperId":"efa06fe7c6a4abbe465dbea4f7130f45720ac6f0","title":"Tuning computer vision models with task rewards"},{"paperId":"5ef821267fa68d3231ed8135ff8ec09f25bb1398","title":"ChatCAD: Interactive Computer-Aided Diagnosis on Medical Image using Large Language Models"},{"paperId":"61e721334296ebfbbf6443b5ed9eb8c83b708c95","title":"Scaling Vision Transformers to 22 Billion Parameters"},{"paperId":"60f78afe2040f33988c71d585c3f42f06814d0de","title":"ChatGPT: the future of discharge summaries?"},{"paperId":"3f5b31c4f7350dc88002c121aecbdc82f86eb5bb","title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models"},{"paperId":"9a23e7bd9177ce404e0415bbb8b245c6458d0102","title":"Medical artificial intelligence is as much social as it is technological"},{"paperId":"da15ff9f152c14618e4d4987bc98b45fd0e2d0ab","title":"DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature"},{"paperId":"428854d9e75f94f0e61f37c6887c77800437d516","title":"MusicLM: Generating Music From Text"},{"paperId":"68be99a37c23486fcdd016fdf7833bb9092146ae","title":"Data Augmentation Alone Can Improve Adversarial Training"},{"paperId":"35cdc00a4e2bc1f6c253a34a5e3a6f697050d1e9","title":"Evaluating the Performance of ChatGPT in Ophthalmology: An Analysis of its Successes and Shortcomings"},{"paperId":"874deb5f06f35e52ae13a921b23611eec4abd1da","title":"ClimaX: A foundation model for weather and climate"},{"paperId":"23cae400cfd1a7c455c721256b838e98a307d5e6","title":"ChatGPT Makes Medicine Easy to Swallow: An Exploratory Case Study on Simplified Radiology Reports"},{"paperId":"6052486bc9144dc1730c12bf35323af3792a1fd0","title":"Large Language Models Encode Clinical Knowledge"},{"paperId":"cf1f26e7cbed3958b3c2870656568c299fece6e3","title":"Performance of ChatGPT on USMLE: Potential for AI-assisted medical education using large language models"},{"paperId":"7d5f921d473d5a861fde132b727efa0dce4b1e06","title":"RT-1: Robotics Transformer for Real-World Control at Scale"},{"paperId":"9b5744d9274fe69fb95ede5035acce49fc5ab27a","title":"ManyFold: an efficient and flexible library for training and validating protein folding models"},{"paperId":"750676b67abef11d102f0a5e7e221bbb56fca2c8","title":"UNETR++: Delving into Efficient and Accurate 3D Medical Image Segmentation"},{"paperId":"a02fbaf22237a1aedacb1320b6007cd70c1fe6ec","title":"Robust Speech Recognition via Large-Scale Weak Supervision"},{"paperId":"332dc8b2ca9d49fad607c7282f3360bb2a9aacf3","title":"A large language model for electronic health records"},{"paperId":"50d2dd9a96fe83dd5e6c044c341dde84384319a8","title":"OpenFold: Retraining AlphaFold2 yields new insights into its learning mechanisms and capacity for generalization"},{"paperId":"0b82188924a8cd249469a845f8107604f668436c","title":"Improved the Protein Complex Prediction with Protein Language Models"},{"paperId":"ee96ec926f06ff2f3ce3d131cffcbfe63af39f0c","title":"Towards All-in-one Pre-training via Maximizing Multi-modal Mutual Information"},{"paperId":"c6d24ca67b3592adc23503f7af0635f9f94c4116","title":"A Review on the Use of Mobile Service Robots in Elderly Care"},{"paperId":"26c80bd65baa90f5b18157de4951f4eb0b62ab69","title":"InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions"},{"paperId":"964bd39b546f0f6625ff3b9ef1083f797807ef2e","title":"BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"},{"paperId":"5484d228bfc50efbac6e86677bc2ec2ee4ede1a6","title":"Scaling Instruction-Finetuned Language Models"},{"paperId":"ad95ce28151b7b77e88b425cb936c84e043e94d6","title":"Modular robotic platform for precision neurosurgery with a bio-inspired needle: System overview and first in-vivo deployment"},{"paperId":"cdd9c1d23f9e89d5113f3e31821bb174c6a6afed","title":"MedCLIP: Contrastive Learning from Unpaired Medical Images and Text"},{"paperId":"e5c8960eb2ec034ffbd353ef39fd1cb541d3c7c9","title":"LAION-5B: An open large-scale dataset for training next generation image-text models"},{"paperId":"f5225015bcdad0a1daf7d205c67fc297fb9ec978","title":"Adapting Pretrained Vision-Language Foundational Models to Medical Imaging Domains"},{"paperId":"25425e299101b13ec2872417a14f961f4f8aa18e","title":"VIMA: General Robot Manipulation with Multimodal Prompts"},{"paperId":"7698498dcb14db063154f4c955fc041114d1960d","title":"Single-sequence protein structure prediction using a language model and deep learning"},{"paperId":"ebb85974e06c4879b451fdfcb4f472a09471935b","title":"AudioGen: Textually Guided Audio Generation"},{"paperId":"74eae12620bd1c1393e268bddcb6f129a5025166","title":"Improving alignment of dialogue agents via targeted human judgements"},{"paperId":"168db75f79cd2d39a7802451578662bb15572de4","title":"Improving Radiology Report Generation Systems by Removing Hallucinated References to Non-existent Priors"},{"paperId":"44279244407a64431810f982be6d0c7da4429dd7","title":"BioGPT: Generative Pre-trained Transformer for Biomedical Text Generation and Mining"},{"paperId":"76120de60a9e59c23a372457a056da3c220c64b6","title":"Expert-level detection of pathologies from unannotated chest X-ray images via self-supervised learning"},{"paperId":"81adb80e390f25d4a2d764b1063eeaa2c334d441","title":"Multi-modal Masked Autoencoders for Medical Vision-and-Language Pre-training"},{"paperId":"28630034bb29760df01ab033b743e30b37f336ae","title":"PaLI: A Jointly-Scaled Multilingual Language-Image Model"},{"paperId":"60c8d0619481eaafdd1189af610d0e636271fed5","title":"Perceiver-Actor: A Multi-Task Transformer for Robotic Manipulation"},{"paperId":"fdcb65bda2f2ff57763c640628540848f16ad9bb","title":"Accurate prediction of protein structures and interactions using a three-track neural network"},{"paperId":"efa1647594b236361610a20d507127f0586a379b","title":"Diffusion Models in Vision: A Survey"},{"paperId":"ace4d199aa72ab0808af0f30a61fc16727c95dec","title":"AudioLM: A Language Modeling Approach to Audio Generation"},{"paperId":"79198b4009b375ae3746b7331a57f2aef54a456c","title":"Uni-Fold: An Open-Source Platform for Developing Protein Folding Models beyond AlphaFold"},{"paperId":"790f73b4019edb6a90bdc7e7063bb541786b17f3","title":"Clustering Egocentric Images in Passive Dietary Monitoring with Self-Supervised Learning"},{"paperId":"8ad0578c56a7ebe26a308592a1c4e4813dd66ccb","title":"BRAX, Brazilian labeled chest x-ray dataset"},{"paperId":"bc103c1f62e93ba31f012a1503a99cb481310b7b","title":"Interpretable bilinear attention network with domain adaptation improves drug–target prediction"},{"paperId":"867d80c8779e1d301a5fc6e267e263f7e4c4c5c7","title":"High-resolution de novo structure prediction from primary sequence"},{"paperId":"c036f75da24ba64a583e0b6d41c5b792347bffa6","title":"Diffsound: Discrete Diffusion Model for Text-to-Sound Generation"},{"paperId":"d697b440dd0e65a05fe027e4c0ea85f62dcba033","title":"Can large language models reason about medical questions?"},{"paperId":"bd1cd034470c1d3983e93302acab720f6af281d1","title":"HelixFold: An Efficient Implementation of AlphaFold2 using PaddlePaddle"},{"paperId":"84912ef04ee98e325d8335bc64d73ca3f3aa5328","title":"E2Efold-3D: End-to-End Deep Learning Method for accurate de novo RNA 3D Structure Prediction"},{"paperId":"1243e13254bb4ea1f71b4be8a3e4e54ffd02d2fe","title":"Scaling Autoregressive Models for Content-Rich Text-to-Image Generation"},{"paperId":"dac3a172b504f4e33c029655e9befb3386e5f63a","title":"Emergent Abilities of Large Language Models"},{"paperId":"686d9ee744fa013cc21cdd86acd864c936e9e456","title":"Large language models are few-shot clinical information extractors"},{"paperId":"9695824d7a01fad57ba9c01d7d76a519d78d65e7","title":"Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding"},{"paperId":"5922f437512158970c417f4413bface021df5f78","title":"A Generalist Agent"},{"paperId":"13a0d8bb38f739990c8cd65a44061c6534f17221","title":"OPT: Open Pre-trained Transformer Language Models"},{"paperId":"c57293882b2561e1ba03017902df9fc2f289dea2","title":"Hierarchical Text-Conditional Image Generation with CLIP Latents"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"cb5e3f085caefd1f3d5e08637ab55d39e61234fc","title":"Do As I Can, Not As I Say: Grounding Language in Robotic Affordances"},{"paperId":"00df82d233825b6d9a1fe5ae482e27f981a91024","title":"PanGu Drug Model: learn a molecule like a human"},{"paperId":"9068d48f34b70e96cb4f9fd35ad87ce31e98de96","title":"Emotion Recognizing by a Robotic Solution Initiative (EMOTIVE Project)"},{"paperId":"07264347e959913a6ea37953d9c0e30ed4efb3ba","title":"Interpretable RNA Foundation Model from Unannotated Data for Highly Accurate RNA Structure and Function Predictions"},{"paperId":"a83cdcc0135c58fddf89fc72f1b92b7a9d1e170f","title":"LinkBERT: Pretraining Language Models with Document Links"},{"paperId":"8342b592fe238f3d230e4959b06fd10153c45db1","title":"Training Compute-Optimal Large Language Models"},{"paperId":"c9bdc9ad2c3cf3230ba9aac7b5783ab411f0d204","title":"R3M: A Universal Visual Representation for Robot Manipulation"},{"paperId":"9dac4cecd2099e8f73970fb589f884a4e460a0fe","title":"LocATe: End-to-end Localization of Actions in 3D with Transformers"},{"paperId":"d766bffc357127e0dc86dd69561d5aeb520d6f4c","title":"Training language models to follow instructions with human feedback"},{"paperId":"315ae320de0d6b31c94d70999b23028b688c272d","title":"FastFold: Reducing AlphaFold Training Time from 11 Days to 67 Hours"},{"paperId":"a3b42a83669998f65df60d7c065a70d07ca95e99","title":"BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation"},{"paperId":"1b6e810ce0afd0dd093f789d2b2742d047e316d5","title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"7cbc2a7843411a1768ab762930707af0a3c33a19","title":"Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model"},{"paperId":"229ca1d46d63ccac4b82e5cca5c4dd676aaa870d","title":"Autonomous robotic laparoscopic surgery for intestinal anastomosis"},{"paperId":"dcb21a5a0bc8b6d1bcfff10659e192a95ea20773","title":"DrugOOD: Out-of-Distribution (OOD) Dataset Curator and Benchmark for AI-aided Drug Discovery - A Focus on Affinity Prediction Problems with Noise Annotations"},{"paperId":"b3848d32f7294ec708627897833c4097eb4d8778","title":"LaMDA: Language Models for Dialog Applications"},{"paperId":"b7dc007054cf17dea3b22a2d1e71ba4cc8606648","title":"Revisiting Weakly Supervised Pre-Training of Visual Perception Models"},{"paperId":"177e957f5cd93229c9794ea652c646d2557b4a69","title":"A ConvNet for the 2020s"},{"paperId":"3141e5cf27e69e5e8bd8b5c6cc97c23380e8a9c1","title":"Can AlphaFold2 predict the impact of missense mutations on structure?"},{"paperId":"c10075b3746a9f3dd5811970e93c8ca3ad39b39d","title":"High-Resolution Image Synthesis with Latent Diffusion Models"},{"paperId":"7002ae048e4b8c9133a55428441e8066070995cb","title":"GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models"},{"paperId":"2f3efe44083af91cef562c1a3451eee2f8601d22","title":"WebGPT: Browser-assisted question-answering with human feedback"},{"paperId":"3c6eefec0dfd60c33a1611275fa301a265d8b3a1","title":"Deciphering antibody affinity maturation with language models and weakly supervised learning"},{"paperId":"2fd6f77540c1cc8e70b96208ccf9971b4251fc02","title":"FLAVA: A Foundational Language And Vision Alignment Model"},{"paperId":"002c256d30d6be4b23d365a8de8ae0e67e4c9641","title":"Improving language models by retrieving from trillions of tokens"},{"paperId":"68f141724814839d556a989646194be88641b143","title":"Scaling Language Models: Methods, Analysis & Insights from Training Gopher"},{"paperId":"5341b412383c43f4a693ad63ec4489e3ec7688c8","title":"Grounded Language-Image Pre-training"},{"paperId":"7494f7fd1cd30ca2a2753b69271ed75b3967cb70","title":"scBERT as a Large-scale Pretrained Deep Language Model for Cell Type Annotation of Single-cell RNA-seq Data"},{"paperId":"dfe72973324814adba51fac772c9a52ca53024cd","title":"Transformer-Based Generative Model Accelerating the Development of Novel BRAF Inhibitors"},{"paperId":"076a8e778f2e9efb3c2fd45fed534ae9e6035f1b","title":"Self-Supervised Pre-Training of Swin Transformers for 3D Medical Image Analysis"},{"paperId":"21ec90872abd986c12afe39bebe807732ffa70c9","title":"Florence: A New Foundation Model for Computer Vision"},{"paperId":"da4261a957eaa96bf626e9641ef68ebed1d5333f","title":"RedCaps: web-curated image-text data created by the people, for the people"},{"paperId":"be0fbb810583930c071d0b9b2c5187fe260783f5","title":"Swin Transformer V2: Scaling Up Capacity and Resolution"},{"paperId":"19dbb57ad106137553bff4282149ac2800b5c176","title":"XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale"},{"paperId":"6351ebb4a3287f5f3e1273464b3b91e5df5a16d7","title":"Masked Autoencoders Are Scalable Vision Learners"},{"paperId":"4a02061f8623f68502991d8bdf7728ae50669091","title":"A decade retrospective of medical robotics research from 2010 to 2020"},{"paperId":"f675c62abfa788ea0be85d3124eba15a14d5e9d6","title":"FILIP: Fine-grained Interactive Language-Image Pre-Training"},{"paperId":"e3a8ea14fd7c7355ac4af43fcac99961dcfe2bfa","title":"Egocentric Human Trajectory Forecasting With a Wearable Camera and Multi-Modal Fusion"},{"paperId":"a66606a0bb11a1d5a7a14ec09df8a3481121ad6c","title":"MedMNIST v2 - A large-scale lightweight benchmark for 2D and 3D biomedical image classification"},{"paperId":"17dd3555fd1ccf1141cf984347fa1b3fd6b009ca","title":"Multitask Prompted Training Enables Zero-Shot Task Generalization"},{"paperId":"847a153286d7f6f496f1ff61089831c267d68e30","title":"Ego4D: Around the World in 3,000 Hours of Egocentric Video"},{"paperId":"2556e820cba6bda75f6f31b76bc74d9e36d72cb3","title":"Protein complex prediction with AlphaFold-Multimer"},{"paperId":"0b500aa5fcc175f07aecf26c0e8ddc4f0c6a931d","title":"GLoRIA: A Multimodal Global-Local Representation Learning Framework for Label-efficient Medical Image Recognition"},{"paperId":"69ee9b3a915951cc84b74599a3a2699a66d4004f","title":"CLIPort: What and Where Pathways for Robotic Manipulation"},{"paperId":"d6a1e9699d4e3571ab1eb74ab9eaba75095b809c","title":"PlaTe: Visually-Grounded Planning With Transformers in Procedural Tasks"},{"paperId":"5e00596fa946670d894b1bdaeff5a98e3867ef13","title":"SimVLM: Simple Visual Language Model Pretraining with Weak Supervision"},{"paperId":"4f68e07c6c3173480053fd52391851d6f80d651b","title":"On the Opportunities and Risks of Foundation Models"},{"paperId":"0da93a948211b63c462582d7e5dbf8da9cdfbcf1","title":"ColabFold: making protein folding accessible to all"},{"paperId":"ae73f99c5de069c0b41c7472397b64631fee281b","title":"Extracting Predictive Representations from Hundreds of Millions of Molecules."},{"paperId":"ebe259796870ebccf26577044d0087884209b884","title":"w2v-BERT: Combining Contrastive Learning and Masked Language Modeling for Self-Supervised Speech Pre-Training"},{"paperId":"f4d845c40176c349ef9701cb379f7959ca2a47f2","title":"Efficient Medical Image Segmentation Based on Knowledge Distillation"},{"paperId":"dc32a984b651256a8ec282be52310e6bd33d9815","title":"Highly accurate protein structure prediction with AlphaFold"},{"paperId":"86f01b405b0bc6269b409fb88f7d9473c1fd9010","title":"Egocentric Image Captioning for Privacy-Preserved Passive Dietary Intake Monitoring"},{"paperId":"4fffa5245d3972077c83614c2a08a47cb578631e","title":"HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units"},{"paperId":"979a9f247700d00ff2c3f0612d5eb001379f93c8","title":"The Medical Segmentation Decathlon"},{"paperId":"1f47d68fe87d0b1317b34a71f98548df3f5da5ff","title":"Algebraic graph-assisted bidirectional transformers for molecular property prediction"},{"paperId":"2a805d0e1b067444a554c5169d189fa1f649f411","title":"Scaling Vision Transformers"},{"paperId":"d4f6ef636e16b001986b541aa2afc76eed42ae34","title":"Considering the possibilities and pitfalls of Generative Pre-trained Transformer 3 (GPT-3) in healthcare delivery"},{"paperId":"c1ad5f9b32d80f1c65d67894e5b8c2fdf0ae4500","title":"Decision Transformer: Reinforcement Learning via Sequence Modeling"},{"paperId":"ea7cfe7f2340584cbe653da6077ee7c213e49b92","title":"Swin-Unet: Unet-like Pure Transformer for Medical Image Segmentation"},{"paperId":"1928290565ed842ffc2f5c3e9e8cefc6c26f269f","title":"MolGPT: Molecular Generation Using a Transformer-Decoder Model"},{"paperId":"3d5b29a3e6f05c6030366e5395918469543d0a7e","title":"MG-BERT: leveraging unsupervised atomic representation learning for molecular property prediction"},{"paperId":"e87c4248e031a846e6a4486b818a37a1b2233b36","title":"An effective self-supervised framework for learning expressive molecular global representations to drug discovery."},{"paperId":"27ade5114d61e26efdc4d35acc83a234ade3bd94","title":"ADMETlab 2.0: an integrated online platform for accurate and comprehensive predictions of ADMET properties"},{"paperId":"0d5406775fab3e71848908327fb5504df5f60f92","title":"ImageNet-21K Pretraining for the Masses"},{"paperId":"76d79b5956f1a1a75de52c9e32a84d03f504c97f","title":"EfficientNetV2: Smaller Models and Faster Training"},{"paperId":"fb37f2259f63bb0bffd6a001cdf0168b928dc488","title":"Federated deep learning for detecting COVID-19 lung abnormalities in CT: a privacy-preserving multinational validation study"},{"paperId":"7519a1e9e7371df79bd8a21cee871feb0ec597a5","title":"UNETR: Transformers for 3D Medical Image Segmentation"},{"paperId":"8356d155d730e374f4db6dfd03d19a7b66c348a8","title":"CoTr: Efficiently Bridging CNN and Transformer for 3D Medical Image Segmentation"},{"paperId":"0f8aa47ff8c6c49a347e192debe20ce4e5a4caea","title":"Self-supervised Pretraining of Visual Features in the Wild"},{"paperId":"98e565fa06f6c7bf7c46833b5106b26dc45130c4","title":"WIT: Wikipedia-based Image Text Dataset for Multimodal Multilingual Machine Learning"},{"paperId":"0ae67202f0584afccefa770865d14a46655d2975","title":"Transformer in Transformer"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"2cd605106b88c85d7d8b865b1ef0f8c8293debf1","title":"Zero-Shot Text-to-Image Generation"},{"paperId":"1e5e8106700c8dbdfa036a5a9be5e61e06c0ed02","title":"Medical Transformer: Gated Axial-Attention for Medical Image Segmentation"},{"paperId":"394be105b87e9bfe72c20efe6338de10604e1a11","title":"Conceptual 12M: Pushing Web-Scale Image-Text Pre-Training To Recognize Long-Tail Visual Concepts"},{"paperId":"20db2a2fadcf563a2d522aabc440b6b4f3ee46f4","title":"TransFuse: Fusing Transformers and CNNs for Medical Image Segmentation"},{"paperId":"deee48c5e0ac0407a1e002905caaf2b174bdb0e6","title":"MSA Transformer"},{"paperId":"141a5033d9994242b18bb3b217e79582f1ee9306","title":"Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision"},{"paperId":"24b8a0b02bcb7934967757fc59d273a71ba67e30","title":"TransUNet: Transformers Make Strong Encoders for Medical Image Segmentation"},{"paperId":"197ec03481b5e845fb4d34dd99a4b8e844fdabcc","title":"Big Self-Supervised Models Advance Medical Image Classification"},{"paperId":"e52bb04d8ecad05d0cde8a57af94478c0b904712","title":"Learn molecular representations from large-scale unlabeled molecules for drug discovery"},{"paperId":"3dfd924ad26e737db805ed29af61cc827e876bd9","title":"IPN-V2 and OCTA-500: Methodology and Dataset for Retinal Image Segmentation"},{"paperId":"43cb4886a8056d5005702edbc51be327542b2124","title":"Pre-Trained Image Processing Transformer"},{"paperId":"e4c5e81e6e337bb94af3eb719df5f029b40434fa","title":"Molecular representation learning with language models and domain-relevant auxiliary tasks"},{"paperId":"9f7626c7af925b7b69f1ba86ceb916d21bc03dbe","title":"Pfam: The protein families database in 2021"},{"paperId":"8c2b647a4ddd5d490ada2ed21e28aed460640926","title":"ZINC20 - A Free Ultralarge-Scale Chemical Database for Ligand Discovery"},{"paperId":"06a1958dbd5c94cdb1cfb164d57bf89e4a5dd788","title":"RNAcentral 2021: secondary structure integration, improved sequence search and new member databases"},{"paperId":"8c56b85571933e7520bdd07ed3946126005a4f10","title":"DrugSpaceX: a large screenable and synthetically tractable database extending drug space"},{"paperId":"268d347e8a55b5eb82fb5e7d2f800e33c75ab18a","title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"},{"paperId":"6d6595766a35f12a6ad671d05634b5e2159d4f3e","title":"Bio-Megatron: Larger Biomedical Domain Language Model"},{"paperId":"5985e77cce2a6a3720682c81dbeaef7b417e0459","title":"MoCo Pretraining Improves Representation and Transferability of Chest X-ray Models"},{"paperId":"6dd9f99cecd38504b667d320eb2a6267a9fee35d","title":"Contrastive Learning of Medical Visual Representations from Paired Images and Text"},{"paperId":"df70f6a46e6cf21a1c83db9fc9a4e16129393fee","title":"CMNPD: a comprehensive marine natural products database towards facilitating drug discovery from the ocean"},{"paperId":"54523ff961a1ac57a86696ef9a53b3a630b482c0","title":"Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing"},{"paperId":"1ce7901245879ef2bf371c4339507d2ea666eb2a","title":"Comparing to Learn: Surpassing ImageNet Pretraining on Radiographs By Comparing Image Representations"},{"paperId":"bc022dbb37b1bbf3905a7404d19c03ccbf6b81a8","title":"Generative Pretraining From Pixels"},{"paperId":"ca9b4fc03ad3ea4680ab2204ecf215f333c616a4","title":"ProtTrans: Towards Cracking the Language of Life’s Code Through Self-Supervised Deep Learning and High Performance Computing"},{"paperId":"a9a4e8e631890a14257539948e1813b5214c60dd","title":"Self-Supervised Graph Transformer on Large-Scale Molecular Data"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"5d4de0fa45aeddc31142e6a24666d06ed7923f1e","title":"Med-BERT: pretrained contextualized embeddings on large-scale structured electronic health records for disease prediction"},{"paperId":"39262e9ef11c77e0a7ebaffe8bc7afd2945b75bb","title":"A new paradigm for drug development"},{"paperId":"3eb85dbadeb5074325a404304313bed536fb6157","title":"MolTrans: Molecular Interaction Transformer for drug–target interaction prediction"},{"paperId":"e816f788767eec6a8ef0ea9eddd0e902435d4271","title":"Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks"},{"paperId":"8d4b4d9ecb25b84528d73b1495928f4295902cdf","title":"Autonomous task planning and situation awareness in robotic surgery*"},{"paperId":"2709167f1c3a03fa5b970a665ea48ed243aab582","title":"Designing Network Design Spaces"},{"paperId":"235b9812305cff3df849394ac7b70a2c04a4685c","title":"COVID-Net: a tailored deep convolutional neural network design for detection of COVID-19 cases from chest X-ray images"},{"paperId":"346edcc4be774240946a0d0fd242586ad745ac68","title":"Tracking Social Media Discourse About the COVID-19 Pandemic: Development of a Public Coronavirus Twitter Data Set"},{"paperId":"c5f7074a264356c9a022a8dff24df79d1db8c3d3","title":"ProGen: Language Modeling for Protein Generation"},{"paperId":"34733eaf66007516347a40ad5d9bbe1cc9dacb6b","title":"A Simple Framework for Contrastive Learning of Visual Representations"},{"paperId":"0495d9df8eb84dcdab4e5536179823cd26279949","title":"Big Transfer (BiT): General Visual Representation Learning"},{"paperId":"110930cd8e47f6cc801c38523ea100e276795882","title":"Dynamics of the double burden of malnutrition and the changing nutrition reality"},{"paperId":"4925d50efc261cd9f251a1106f109f98ed2eb85a","title":"Transformer neural network for protein-specific de novo drug generation as a machine translation problem"},{"paperId":"add2f205338d70e10ce5e686df4a690e2851bdfc","title":"Momentum Contrast for Unsupervised Visual Representation Learning"},{"paperId":"6289471f2eca01dbde71e4832f93891f54b91cfe","title":"SMILES Transformer: Pre-trained Molecular Fingerprint for Low Data Drug Discovery"},{"paperId":"3cfb319689f06bf04c2e28399361f414ca32c4b3","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"3d99747cc3e13d22f21e02c35e82b57d2e351e2a","title":"SMILES-BERT: Large Scale Unsupervised Pre-Training for Molecular Property Prediction"},{"paperId":"222baa4e9e7ce691fdfddbc826a70e027daed70d","title":"Reinforcement Learning in Healthcare: A Survey"},{"paperId":"75dd6e09e2a44c363bb50e463c812fd3393cbf1c","title":"Models Genesis: Generic Autodidactic Models for 3D Medical Image Analysis"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"57633ff5c6f0708be25e651f51eef29d2fbfe48b","title":"BEHRT: Transformer for Electronic Health Records"},{"paperId":"6447deb18d6d2510c147afcf1b04408250f0d7c6","title":"A Model to Search for Synthesizable Molecules"},{"paperId":"4f2eda8077dc7a69bb2b4e0a1a086cf054adb3f9","title":"EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks"},{"paperId":"18a93dc1558bf9d7534d0b416633cebaf75c1145","title":"Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences"},{"paperId":"96bd1cd9b37cc9eea6ecc1b46afc29f95a10d424","title":"wav2vec: Unsupervised Pre-training for Speech Recognition"},{"paperId":"2a567ebd78939d0861d788f0fedff8d40ae62bf2","title":"Publicly Available Clinical BERT Embeddings"},{"paperId":"fa795716fb68ccbe52a8479c4d28da340d932222","title":"PanglaoDB: a web server for exploration of mouse and human single-cell RNA sequencing data"},{"paperId":"5bcda431e0b615e094562bf038f1ef4df1865088","title":"Med3D: Transfer Learning for 3D Medical Image Analysis"},{"paperId":"1ab7f7c1d328589f25c79515b9a5d824d7ffbbd1","title":"GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering"},{"paperId":"d9ad92812d61709a9bf35b09078361d8bffd3f7a","title":"Autonomous Tissue Manipulation via Surgical Robot Using Learning Based Model Predictive Control"},{"paperId":"1e43c7084bdcb6b3102afaf301cce10faead2702","title":"BioBERT: a pre-trained biomedical language representation model for biomedical text mining"},{"paperId":"f1a17b7c4cae4513731f6d81b433e338cf4114eb","title":"PadChest: A large chest x-ray image dataset with multi-label annotated reports"},{"paperId":"89a816719613e220a64ab2590c938c23bbfe187e","title":"CheXpert: A Large Chest Radiograph Dataset with Uncertainty Labels and Expert Comparison"},{"paperId":"c18663fea10c8a303d045fd2c1f33cacf9b73ca3","title":"GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism"},{"paperId":"64d025132b34770bfa43d16c1e36662af687607d","title":"DeepConv-DTI: Prediction of drug-target interactions via deep learning with convolution on protein sequences"},{"paperId":"92a79ba7bb783b64069ca006206b05458a696b36","title":"PubChem 2019 update: improved access to chemical data"},{"paperId":"4828821fde83a09ceeca764d9369ec6d67333dae","title":"Observed Antibody Space: A Resource for Data Mining Next-Generation Sequencing of Antibody Repertoires"},{"paperId":"b4df354db88a70183a64dbc9e56cf14e7669a6c0","title":"Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning"},{"paperId":"98a68e8148d380120924d5c2810e3e0f513a75b2","title":"DeepLesion: automated mining of large-scale lesion annotations and universal lesion detection with deep learning"},{"paperId":"0f885fd46064d271d4404cf9bb3d758e1a6f8d55","title":"Exploring the Limits of Weakly Supervised Pretraining"},{"paperId":"f95f5b715eb0441ca4ee1b0fac6b4bcaaba65556","title":"The HAM10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions"},{"paperId":"d667b71d056ed1c407d456a31982b45f63dbf264","title":"Where is crystallography going?"},{"paperId":"20414786dead96380e11979537d8a27668d51636","title":"Clustering huge protein sequence sets in linear time"},{"paperId":"784ed2daacbc7d721fe599e71ef48ab733700716","title":"MURA: Large Dataset for Abnormality Detection in Musculoskeletal Radiographs."},{"paperId":"d0611891b9e8a7c5731146097b6f201578f47b2f","title":"Learning Transferable Architectures for Scalable Image Recognition"},{"paperId":"dce6f9d4017b1785979e7520fd0834ef8cf02f4b","title":"Proximal Policy Optimization Algorithms"},{"paperId":"35f7b928a5ed86b3a480a71846c3dfb19f3104fd","title":"Robot Autonomy for Surgery"},{"paperId":"8760bc7631c0cb04e7138254e9fd6451b7def8ca","title":"Revisiting Unreasonable Effectiveness of Data in Deep Learning Era"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd","title":"Deep Reinforcement Learning from Human Preferences"},{"paperId":"2ca9bd07ed18aadf5c532d0bf3cce25149260a89","title":"Medical robotics—Regulatory, ethical, and legal considerations for increasing levels of autonomy"},{"paperId":"7e232313a59d735ef7c8a9f4cc7bc980a29deb5e","title":"Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering"},{"paperId":"f6e0856b4a9199fa968ac00da612a9407b5cb85c","title":"Aggregated Residual Transformations for Deep Neural Networks"},{"paperId":"95cd83603a0d2b6918a8e34a5637a8f382da96f5","title":"MIMIC-III, a freely accessible critical care database"},{"paperId":"77f0a39b8e02686fd85b01971f8feb7f60971f80","title":"Identity Mappings in Deep Residual Networks"},{"paperId":"afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d","title":"Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations"},{"paperId":"2c03df8b48bf3fa39054345bafabfeff15bfd11d","title":"Deep Residual Learning for Image Recognition"},{"paperId":"def584565d05d6a8ba94de6621adab9e301d375d","title":"Visual7W: Grounded Question Answering in Images"},{"paperId":"c657ab339517fb8def7ce7f83bb81e746d558218","title":"Data Resource Profile: Clinical Practice Research Datalink (CPRD)"},{"paperId":"6364fdaa0a0eccd823a779fcdd489173f938e91a","title":"U-Net: Convolutional Networks for Biomedical Image Segmentation"},{"paperId":"696ca58d93f6404fea0fc75c62d1d7b378f47628","title":"Microsoft COCO Captions: Data Collection and Evaluation Server"},{"paperId":"354c029c88be2bbc27dfd2e2e729c0ae622511e6","title":"YFCC100M: the new data in multimedia research"},{"paperId":"66cdc28dc084af6507e979767755e99fe0b46b39","title":"Trust Region Policy Optimization"},{"paperId":"d6f2f611da110b5b5061731be3fc4c7f45d8ee23","title":"Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification"},{"paperId":"795dc87b4727b303d3672539e4578d41dfd3aeb3","title":"UniRef clusters: a comprehensive and scalable alternative for improving sequence similarity searches"},{"paperId":"081651b38ff7533550a3adfc1c00da333a8fe86c","title":"How transferable are features in deep neural networks?"},{"paperId":"e15cf50aa89fee8535703b9f9512fca5bfc43327","title":"Going deeper with convolutions"},{"paperId":"eb42cf88027de515750f230b23b1a057dc782108","title":"Very Deep Convolutional Networks for Large-Scale Image Recognition"},{"paperId":"44040913380206991b1991daf1192942e038fe31","title":"From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions"},{"paperId":"251c39c1b9372f3055cd53d0001fc122bfb2e418","title":"The Cancer Imaging Archive (TCIA): Maintaining and Operating a Public Information Repository"},{"paperId":"2df9a391c1a8124be63cd2bc71624b80d4e6af1f","title":"UniChem: a unified chemical structure cross-referencing and identifier tracking system"},{"paperId":"abd1c342495432171beb7ca8fd9551ef13cbd0ff","title":"ImageNet classification with deep convolutional neural networks"},{"paperId":"8e080b98efbe65c02a116439205ca2344b9f7cd4","title":"Im2Text: Describing Images Using 1 Million Captioned Photographs"},{"paperId":"a8db50edfe26a6ae33a6787e2049de5bacd18666","title":"ChEMBL: a large-scale bioactivity database for drug discovery"},{"paperId":"d2c733e34d48784a37d717fe43d9e93277a8c53e","title":"ImageNet: A large-scale hierarchical image database"},{"paperId":"3d9fbcf35f53bd84c75fd99daa6b2c69397b0a01","title":"The Universal Protein Resource (UniProt)"},{"paperId":"0784e90fd935a5e81135f80b35daa8d0d0a562c9","title":"The way to NMR structures of proteins"},{"paperId":"df239785e6d26a45e9c8e06551cfecba92d1ecad","title":"Exploring AI Ethics of ChatGPT: A Diagnostic Analysis"},{"paperId":"ae7f67c705c12391f7198527a0b962340ac8d39c","title":"ChatAug: Leveraging ChatGPT for Text Data Augmentation"},{"paperId":"11f42721f56f36a64638677ccde7784040829656","title":"Uni-Mol: A Universal 3D Molecular Representation Learning Framework"},{"paperId":null,"title":"Bionemo"},{"paperId":"0ba581718f294db1d7b3dbc159cc3d3380f74606","title":"ChatGPT for Robotics: Design Principles and Model Abilities"},{"paperId":null,"title":"Be my eyes"},{"paperId":null,"title":"Attentionsitedti: an interpretable graphbased model for drug-target interaction prediction using nlp sentencelevel relation classification"},{"paperId":null,"title":"Chatgpt: Optimizing language models for dialogue"},{"paperId":null,"title":"Language models of protein sequences at the scale of evolution enable accurate structure prediction"},{"paperId":null,"title":"xtrimoabfold: Improving antibody structure prediction without multiple sequence alignments"},{"paperId":"c8b25fab5608c3e033d34b4483ec47e68ba109b7","title":"Swin Transformer: Hierarchical Vision Transformer using Shifted Windows"},{"paperId":"ee6564e2e0f47c6ac131b093f057ca75907958c9","title":"BioELECTRA:Pretrained Biomedical text Encoder using Discriminators"},{"paperId":null,"title":"Combined scaling for openvocabulary image classification"},{"paperId":null,"title":"Graphdta: predicting drug–target binding affinity with graph neural networks"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":null,"title":"Mimic-cxr, a deidentified publicly available database of chest radiographs with freetext reports"},{"paperId":"cd18800a0fe0b668a1cc19f2ec95b5003d0a5035","title":"Improving Language Understanding by Generative Pre-Training"},{"paperId":null,"title":"Nih chest x-rays"},{"paperId":"0f2c1cf2b49ef19b076421550597a09d81b3eda9","title":"PubMed Central"},{"paperId":"18567e254d078ba4d03b1ba81a741f1b1405d369","title":"How cryo-EM is revolutionizing structural biology."},{"paperId":"955fe5b0eb5e9fefab7bf21dede20e1285dca410","title":"UniProt: the Universal Protein knowledgebase"},{"paperId":null,"title":"2023, accessed on Month Day, Year"},{"paperId":null,"title":"The national library of medicine presents medpix"}],"id":"994e08ac813028601907516aee9c4699234a6b4d","summary":"An up-to-date comprehensive review of large AI models, from background to their applications, is presented, including seven key sectors that largeAI models are applicable and might have substantial influence, including 1) molecular biology and drug discovery; 2) medical diagnosis and decision-making; 3) medical imaging and vision; 4) medical informatics; 5) medical education; 6) public health; and 7) medical robotics."},{"url":"https://www.semanticscholar.org/paper/c7a9c7302a72301ed79a7c0696d5af2e03ad3ac4","title":"MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action","venue":"arXiv.org","year":2023,"referenceCount":44,"citationCount":54,"influentialCitationCount":3,"publicationDate":"20/03/2023","authors":"Zhengyuan Yang,Linjie Li,Jianfeng Wang,Kevin Lin,E. Azarnasab,Faisal Ahmed,Zicheng Liu,Ce Liu,Michael Zeng,Lijuan Wang","citations":[{"paperId":"2e52178416b93a0a7dc1cb7d21d04c7e72c2ce0f","title":"ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring Instruction Tuning"},{"paperId":"3d02e5503caa2f444cbd61778c7cdc00a5b2e98d","title":"GPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest"},{"paperId":"ef4b604fca0c62dcd0d5caf7ca24ad74e285632d","title":"MultiQG-TI: Towards Question Generation from Multi-modal Sources"},{"paperId":"53937df61d496572e90ee34c670ddd00337e558d","title":"What Matters in Training a GPT4-Style Language Model with Multimodal Inputs?"},{"paperId":"ea566f87f6253bb2d32cf7b61cd3e2535a0c3f42","title":"mPLUG-DocOwl: Modularized Multimodal Large Language Model for Document Understanding"},{"paperId":"ebedc4d7a2356090904baba4104ef0832bc236df","title":"A Survey on Multimodal Large Language Models"},{"paperId":"966852963a88a28786b798c91b6662d6e501e590","title":"AssistGPT: A General Multi-modal Assistant that can Plan, Execute, Inspect, and Learn"},{"paperId":"051549d8ef56937b2f4d113afdcf8c7586d3770b","title":"Towards AGI in Computer Vision: Lessons Learned from GPT and Large Language Models"},{"paperId":"fe50667e1bea4c6f63909b90986231240818c1d6","title":"ChatGPT vs Human-authored Text: Insights into Controllable Text Summarization and Sentence Style Transfer"},{"paperId":"e8cc6a50bc9c6a1e1ed7fdde0e7ccbb4efd4b505","title":"AVIS: Autonomous Visual Information Seeking with Large Language Models"},{"paperId":"74538984e72a26254697d4e7eeeb169000cf762a","title":"Valley: Video Assistant with Large Language model Enhanced abilitY"},{"paperId":"fd755dc7b5b206c17fd953db04e1c888d45b6e4e","title":"LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark"},{"paperId":"d47524cd5c3c4b57af2e5a29f6f91c420310f236","title":"MIMIC-IT: Multi-Modal In-Context Instruction Tuning"},{"paperId":"ccef05840870479632f9055479a1269d53033b7b","title":"Certified Reasoning with Language Models"},{"paperId":"615962d8969c8e0ffe43319689dce6c50cbf1f29","title":"Responsible Task Automation: Empowering Large Language Models as Responsible Task Automators"},{"paperId":"811115d36e1eabe2cef03b38a0809514e40b658e","title":"Chain-Of-Thought Prompting Under Streaming Batch: A Case Study"},{"paperId":"b458fc5261595f44b36325e5eaea1f874d65138f","title":"GPT4Tools: Teaching Large Language Model to Use Tools via Self-instruction"},{"paperId":"af705d648b5b16daa3dcc593bc593f2574d76c07","title":"Grammar Prompting for Domain-Specific Language Generation with Large Language Models"},{"paperId":"b595b55ed27935d306b0a5e0b06a3b0a771275b1","title":"SheetCopilot: Bringing Software Productivity to the Next Level through Large Language Models"},{"paperId":"50c1414fe41d0cb9db6f0933c9319aa124beac5d","title":"Contextual Object Detection with Multimodal Large Language Models"},{"paperId":"1a28e9c62eeb76a1a77dc152197027c15310927b","title":"ANPL: Compiling Natural Programs with Interactive Decomposition"},{"paperId":"f8f8267a2acd7598de6c15327f3953241901a62d","title":"On Evaluating Adversarial Robustness of Large Vision-Language Models"},{"paperId":"32dcd0887537cece54e214f531d2c384470b023f","title":"Large Language Models as Tool Makers"},{"paperId":"c6ac708b65b24c20f80831d518c1795ce8133ad5","title":"ChatBridge: Bridging Modalities with Large Language Model as a Language Catalyst"},{"paperId":"c5c5b9c6660bb2089b781d851cb3fd0ba271d742","title":"Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models"},{"paperId":"6238fd213197ccf0d79191662828e38118a06d79","title":"ECHo: Event Causality Inference via Human-centric Reasoning"},{"paperId":"9837349417e36ef5be06da0fd6c74042148bdaa2","title":"Visual Programming for Text-to-Image Generation and Evaluation"},{"paperId":"86cbd30d1096b0c7e4ac6b03d97a8df12fd21457","title":"PathAsst: Redefining Pathology through Generative Foundation AI Assistant for Pathology"},{"paperId":"405bc18b9d2f783f22f50d5feb02c51b4b34655f","title":"LayoutGPT: Compositional Visual Planning and Generation with Large Language Models"},{"paperId":"00fcc983728346a5f3f8f005f1365be54456728e","title":"EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought"},{"paperId":"7cf64070fd3d7e53d80f260c10e6bd7018d580e1","title":"IdealGPT: Iteratively Decomposing Vision and Language Reasoning via Large Language Models"},{"paperId":"90027ca7802645671a69b00b65e1fa94e6b63544","title":"ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models"},{"paperId":"f9bfc6d9ba1665b73af3323d46c7642b852759ef","title":"VideoLLM: Modeling Video Sequence with Large Language Models"},{"paperId":"6fe2a4f7594abadafc9feb05e96ab7e9e634ce8f","title":"Album Storytelling with Iterative Story-aware Captioning and Large Language Models"},{"paperId":"ca055cfb9d4d47124cc035c346f38577825fcacf","title":"Enhance Reasoning Ability of Visual-Language Models via Large Language Models"},{"paperId":"6a5525c316b9be7909c433a79e090ed731425083","title":"What Makes for Good Visual Tokenizers for Large Language Models?"},{"paperId":"42a30dc5470f54ec249f25d3c31e05d7c376c8e3","title":"VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks"},{"paperId":"f71ce6b09756ea26df283640b4c1c1c125411d2e","title":"Towards Generalist Robots: A Promising Paradigm via Generative Simulation"},{"paperId":"d48cb91b9e555194f7494c4d4bb9815021d3ee45","title":"VideoChat: Chat-Centric Video Understanding"},{"paperId":"54a8b153ed04a872da878d695239bdc413dc782c","title":"InternGPT: Solving Vision-Centric Tasks by Interacting with Chatbots Beyond Language"},{"paperId":"d6d3604f369bb0415cbe814e43ca3131323b03e2","title":"Otter: A Multi-Modal Model with In-Context Instruction Tuning"},{"paperId":"570079bbdd8758dfe865097e05719313c9c1301a","title":"LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model"},{"paperId":"7e32aac43e9f1df49e116add03327ee6f365dbf3","title":"mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality"},{"paperId":"f44ad7ad67ddd5fe74598fe491ca75c5221380df","title":"MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models"},{"paperId":"170c97c7215f42edfb20c2248f954879e91ef86e","title":"Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models"},{"paperId":"352420ee61a8da783ca7750170793613b18b8d9c","title":"Tool Learning with Foundation Models"},{"paperId":"1a8eb2cae1833df3bf12fe3b41b03d60b4a4a98d","title":"Visual Instruction Tuning"},{"paperId":"b61fd5f1661d9234fe85e48f34c701be75ae2de5","title":"ChemCrow: Augmenting large-language models with chemistry tools"},{"paperId":"0ebc861f5478561f12941e6b48aad30574e996d8","title":"Video ChatCaptioner: Towards Enriched Spatiotemporal Descriptions"},{"paperId":"ac7771c332da42b29a913b116bd6ef622cbf89cf","title":"TaskMatrix.AI: Completing Tasks by Connecting Foundation Models with Millions of APIs"},{"paperId":"53df959bcf6499c45e316086a96a624389a39a52","title":"Accountable Textual-Visual Chat Learns to Reject Human Instructions in Image Re-creation"},{"paperId":"887138f4c365b9d1325de41a522d27bec34e0d7e","title":"Reasoning with Language Model Prompting: A Survey"},{"paperId":"a3711dbf296b5ddd97ba93826660cd3995611625","title":"Towards A Foundation Model for Generalist Robots: Diverse Skill Learning at Scale via Automated Task and Scene Generation"},{"paperId":"0167f681bac9b7f6cf396e8a5fc6e46c62fd1896","title":"Towards Understanding the Spatial Literacy of ChatGPT ∗ Taking a Geographic Information Systems (GIS) Exam"}],"references":[{"paperId":"8ca62fdf4c276ea3052dc96dcfd8ee96ca425a48","title":"GPT-4 Technical Report"},{"paperId":"6e754273d54a91371efbc928cd6b156364d517da","title":"ViperGPT: Visual Inference via Python Execution for Reasoning"},{"paperId":"af997821231898a5f8d0fd78dad4eec526acabe5","title":"Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models"},{"paperId":"38fe8f324d2162e63a967a9ac6648974fc4c66f3","title":"PaLM-E: An Embodied Multimodal Language Model"},{"paperId":"fbfef4723d8c8467d7bd523e1d0b703cce0e0f9c","title":"Language Is Not All You Need: Aligning Perception with Language Models"},{"paperId":"61e721334296ebfbbf6443b5ed9eb8c83b708c95","title":"Scaling Vision Transformers to 22 Billion Parameters"},{"paperId":"53d128ea815bcc0526856eb5a9c42cc977cb36a7","title":"Toolformer: Language Models Can Teach Themselves to Use Tools"},{"paperId":"780a7f5e8ba9b4b451e3dfee1bcfb0f68aba5050","title":"Multimodal Chain-of-Thought Reasoning in Language Models"},{"paperId":"3f5b31c4f7350dc88002c121aecbdc82f86eb5bb","title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models"},{"paperId":"967907503b24423b9b74621051811fcf684e3957","title":"Generalized Decoding for Pixel, Image, and Language"},{"paperId":"f208ea909fa7f54fea82def9a92fd81dfc758c39","title":"Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions"},{"paperId":"6c1e1cc1e0e1f8fd026fe517607b2d4535565fa7","title":"PAL: Program-aided Language Models"},{"paperId":"d00ca5c49415d3a45bfcf3fabaf0a60a1c52a6ff","title":"PromptCap: Prompt-Guided Task-Aware Image Captioning"},{"paperId":"2d2ca2e54c54748557b8aac7d328ce32ebfe8944","title":"ReAct: Synergizing Reasoning and Acting in Language Models"},{"paperId":"d3135733aa39dec20ce72aa138589dda27c8406d","title":"Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering"},{"paperId":"60ee030773ba1b68eb222a265b052ca028353362","title":"GIT: A Generative Image-to-text Transformer for Vision and Language"},{"paperId":"e7ad08848d5d7c5c47673ffe0da06af443643bda","title":"Large Language Models are Zero-Shot Reasoners"},{"paperId":"c1ace33daf974d3d16752c7a8565f32a63b09c49","title":"Language Models with Image Descriptors are Strong Few-Shot Video-Language Learners"},{"paperId":"26218bdcc3945c7edae7aa2adbfba4cd820a2df3","title":"Flamingo: a Visual Language Model for Few-Shot Learning"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"cb5e3f085caefd1f3d5e08637ab55d39e61234fc","title":"Do As I Can, Not As I Say: Grounding Language in Robotic Affordances"},{"paperId":"ada81a4de88a6ce474df2e2446ad11fea480616e","title":"Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language"},{"paperId":"1b6e810ce0afd0dd093f789d2b2742d047e316d5","title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"92a8f7f09f3705cb5a6009a42220a6f01ea084e8","title":"Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents"},{"paperId":"2f3efe44083af91cef562c1a3451eee2f8601d22","title":"WebGPT: Browser-assisted question-answering with human feedback"},{"paperId":"21ec90872abd986c12afe39bebe807732ffa70c9","title":"Florence: A New Foundation Model for Computer Vision"},{"paperId":"2672777d25562c9df6fc13b653181db62d39bece","title":"An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA"},{"paperId":"01b5412f3d17e90e09226d7c40ad4d4468a1414d","title":"Multimodal Few-Shot Learning with Frozen Language Models"},{"paperId":"268d347e8a55b5eb82fb5e7d2f800e33c75ab18a","title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"962dc29fdc3fbdc5930a10aba114050b82fe5a3e","title":"End-to-End Object Detection with Transformers"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"2c03df8b48bf3fa39054345bafabfeff15bfd11d","title":"Deep Residual Learning for Image Recognition"},{"paperId":"6424b69f3ff4d35249c0bb7ef912fbc2c86f4ff4","title":"Deep Learning Face Attributes in the Wild"},{"paperId":"71b7178df5d2b112d07e45038cb5637208659ff7","title":"Microsoft COCO: Common Objects in Context"},{"paperId":"b80a580a6f2eca77524302acd944fd6edf0a0611","title":"NUS-WIDE: a real-world web image database from National University of Singapore"},{"paperId":"f79131806747fce087d0fe73d0867cc621547b2a","title":"The MIR flickr retrieval evaluation"},{"paperId":null,"title":"Case studies of MM-REACT's extensibility. With the new release of GPT-4 (Language-only), we can upgrade ChatGPT in MM-REACT to GPT-4 (Langauge-only) and further improve the results of MM-REACT"},{"paperId":null,"title":"Here's a breakdown of the start and end times for each step in the video: 1"},{"paperId":null,"title":"GPT-4 (Multimodal)"},{"paperId":null,"title":"Langchain. https://langchain. readthedocs.io/, 2023"},{"paperId":null,"title":"I am trying to create a short version of the video, can you suggest a shorter segment for each step? Here's a suggested shorter segment for each step: 1"},{"paperId":null,"title":"Ours w/ ChatGPT Ours w/ GPT-4 (Language-only)"},{"paperId":null,"title":"Can you write a template for this website in HTML? Replace the joke and punchline with a real one"}],"id":"c7a9c7302a72301ed79a7c0696d5af2e03ad3ac4","summary":"This paper defines and explores a comprehensive list of advanced vision tasks that are intriguing to solve, but may exceed the capabilities of existing vision and vision-language models and proposes MM-REACT, a system paradigm that integrates ChatGPT with a pool of vision experts to achieve multimodal reasoning and action."},{"url":"https://www.semanticscholar.org/paper/28ff0816f19a5e3e37eac5569de41872fd262f0a","title":"Align, Reason and Learn: Enhancing Medical Vision-and-Language Pre-training with Knowledge","venue":"ACM Multimedia","year":2022,"referenceCount":60,"citationCount":12,"influentialCitationCount":1,"publicationDate":"15/09/2022","authors":"Zhihong Chen,Guanbin Li,Xiang Wan","citations":[{"paperId":"9b348715d0311056eee850dd1cce1cdd3c64eec8","title":"Knowledge Boosting: Rethinking Medical Contrastive Vision-Language Pre-Training"},{"paperId":"238c33aa23774562cb45918c4917565f1b725044","title":"Multi-modal Pre-training for Medical Vision-language Understanding and Generation: An Empirical Study with A New Benchmark"},{"paperId":"8700c5af25450bf8e84b94783344b054d268738b","title":"Bi-VLGM : Bi-Level Class-Severity-Aware Vision-Language Graph Matching for Text Guided Medical Image Segmentation"},{"paperId":"ac4d13b6a4f9fb67337099f4602135a0351f5c99","title":"Towards Medical Artificial General Intelligence via Knowledge-Enhanced Multimodal Pretraining"},{"paperId":"3d45e69557f0c6a54ec698304c2e27ec29bc1c2b","title":"PMC-CLIP: Contrastive Language-Image Pre-training using Biomedical Documents"},{"paperId":"4d4a96708fc67403176bb2b891b564af7a20c148","title":"RAMM: Retrieval-augmented Biomedical Visual Question Answering with Multi-modal Pre-training"},{"paperId":"5d937b7811d8fd4208b2810971cb2e33f64bcfa2","title":"Knowledge-enhanced Visual-Language Pre-training on Chest Radiology Images"},{"paperId":"da9579539385daedd33a0de0f814e2977ad0d1f5","title":"Towards Unifying Medical Vision-and-Language Pre-training via Soft Prompts"},{"paperId":"e3c70b0b71b51872bbdaa0f4bf2b56908f97abec","title":"MedKLIP: Medical Knowledge Enhanced Language-Image Pre-Training"},{"paperId":"508d9b43832790b4d35f4ae1fa76e9712859d6aa","title":"Vision and Structured-Language Pretraining for Cross-Modal Food Retrieval"},{"paperId":"ffe59a414a4016dbfdeabb9441e5794a5e6d2f43","title":"Pre-trained Language Models in Biomedical Domain: A Systematic Survey"},{"paperId":"10a8e7a7e07256178665f90074c5c41b071e73d3","title":"MDF-Net: Multimodal Dual-Fusion Network for Abnormality Detection using CXR Images and Clinical Data"}],"references":[{"paperId":"4ef3d9e492479e28fa57d107e52acc6a0c803de2","title":"Does CLIP Benefit Visual Question Answering in the Medical Domain as Much as it Does in the General Domain?"},{"paperId":"6351ebb4a3287f5f3e1273464b3b91e5df5a16d7","title":"Masked Autoencoders Are Scalable Vision Learners"},{"paperId":"94ff111c4d81bd03f159321728ceec8b4711c89d","title":"An Empirical Study of Training End-to-End Vision-and-Language Transformers"},{"paperId":"5e00596fa946670d894b1bdaeff5a98e3867ef13","title":"SimVLM: Simple Visual Language Model Pretraining with Weak Supervision"},{"paperId":"d0b59b3e34a79c8c79a31bf3944ded8ab7a803ae","title":"ROSITA: Enhancing Vision-and-Language Semantic Alignments via Cross- and Intra-modal Knowledge Integration"},{"paperId":"b82c5f9efdb2ae56baa084ca41aeddd8a665c1d1","title":"Align before Fuse: Vision and Language Representation Learning with Momentum Distillation"},{"paperId":"63c74d15940af1af9b386b5762e4445e54c73719","title":"VinVL: Revisiting Visual Representations in Vision-Language Models"},{"paperId":"d9317660e2a538d9c018028956fd114d55330f82","title":"Multi-Modal Understanding and Generation for Medical Images and Text via Vision-Language Pre-Training"},{"paperId":"3c83f80f06633ff4598d33c2959f8e4cdcad3e93","title":"Cross-Modal Self-Attention with Multi-Task Pre-Training for Medical Visual Question Answering"},{"paperId":"17c2bb358169541f2d0a769f80779f46d1cd3d37","title":"MMBERT: Multimodal BERT Pretraining for Improved Medical VQA"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"93b6b79b4ef6c345f31722ce7c829385c6dce0d6","title":"Slake: A Semantically-Labeled Knowledge-Enhanced Dataset For Medical Visual Question Answering"},{"paperId":"0839722fb5369c0abaff8515bfc08299efc790a1","title":"ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision"},{"paperId":"c9074d9719c5ce0dd3a7369dd0749cd08d7f67ed","title":"MELINDA: A Multimodal Dataset for Biomedical Experiment Method Classification"},{"paperId":"268d347e8a55b5eb82fb5e7d2f800e33c75ab18a","title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"},{"paperId":"5ba77a5bdeffb62aa0902ae68997bbc38db8a722","title":"MedICaT: A Dataset of Medical Images, Captions, and Textual References"},{"paperId":"b8d5b853f2212cbb48a43f1edec9b96d76d388ec","title":"Medical Visual Question Answering via Conditional Reasoning"},{"paperId":"7eda139d737eea10fc1d95364327a41ec0cee4a4","title":"CoLAKE: Contextualized Language and Knowledge Embedding"},{"paperId":"ff554f6228cf1f939a0e9e44ada06ef9cd28be15","title":"A Comparison of Pre-trained Vision-and-Language Models for Multimodal Representation Learning across Medical Images and Reports"},{"paperId":"bc996a4dbf9d4234eacdd0b930a94de1d158e256","title":"ERNIE-ViL: Knowledge Enhanced Vision-Language Representations Through Scene Graph"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"818e5cbc337e4e1b98e65a2d7c2d6d2a0318cd57","title":"Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks"},{"paperId":"598a2ee223e2949c3b28389e922c1892b4717d2a","title":"Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal Transformers"},{"paperId":"56cafbac34f2bb3f6a9828cd228ff281b810d6bb","title":"KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation"},{"paperId":"6007bd2a34385132a7885b934d90b519a1f65bba","title":"ZEN: Pre-training Chinese Text Encoder Enhanced by N-gram Representations"},{"paperId":"33301b25a297b701bdc287e985c006375cb7bb21","title":"Overcoming Data Limitation in Medical Visual Question Answering"},{"paperId":"d8a305b9366608d54452ac30459ee57b4f5cf1c9","title":"UNITER: UNiversal Image-TExt Representation Learning"},{"paperId":"06a73ad09664435f8b3cd90293f4e05a047cf375","title":"K-BERT: Enabling Language Representation with Knowledge Graph"},{"paperId":"bfeb827d06c1a3583b5cc6d25241203a81f6af09","title":"Knowledge Enhanced Contextual Word Representations"},{"paperId":"2527626c11a84f15709e943fbfa2356e19930e3b","title":"VL-BERT: Pre-training of Generic Visual-Linguistic Representations"},{"paperId":"79c93274429d6355959f1e4374c2147bb81ea649","title":"LXMERT: Learning Cross-Modality Encoder Representations from Transformers"},{"paperId":"5aec474c31a2f4b74703c6f786c0a8ff85c450da","title":"VisualBERT: A Simple and Performant Baseline for Vision and Language"},{"paperId":"65a9c7b0800c86a196bc14e7621ff895cc6ab287","title":"ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"5f994dc8cae24ca9d1ed629e517fcc652660ddde","title":"ERNIE: Enhanced Language Representation with Informative Entities"},{"paperId":"031e4e43aaffd7a479738dcea69a2d5be7957aa3","title":"ERNIE: Enhanced Representation through Knowledge Integration"},{"paperId":"156d217b0a911af97fa1b5a71dc909ccef7a8028","title":"SciBERT: A Pretrained Language Model for Scientific Text"},{"paperId":"de28ec1d7bd38c8fc4e8ac59b6133800818b4e29","title":"ScispaCy: Fast and Robust Models for Biomedical Natural Language Processing"},{"paperId":"74d8eb801c838d1dce814a1e9ce1074bd2c47721","title":"MIMIC-CXR: A large publicly available database of labeled chest radiographs"},{"paperId":"18f9a6045ba01cb079c4fa49a630d71bbd27cd92","title":"A dataset of clinically generated visual questions and answers about radiology images"},{"paperId":"a564fabf130ff6e2742cfba90c7a4018937d764d","title":"Radiology Objects in COntext (ROCO): A Multimodal Image Dataset"},{"paperId":"a5d10341717c0519cf63151b496a6d2ed67aa05f","title":"Bilinear Attention Networks"},{"paperId":"ff65e3bf34e892ef75d91c5e3d7294e0b64d867d","title":"Zero-Shot Recognition via Semantic Embeddings and Knowledge Graphs"},{"paperId":"d07284a6811f1b2745d91bdb06b040b57f226882","title":"Decoupled Weight Decay Regularization"},{"paperId":"33998aff64ce51df8dee45989cdca4b6b1329ec4","title":"Graph Attention Networks"},{"paperId":"8e9ad6f8b2bc97f0412fa0cc243ac6975864534a","title":"Multi-modal Factorized Bilinear Pooling with Co-attention Learning for Visual Question Answering"},{"paperId":"79baf8cf6be6510f69be8c515516136138678cf5","title":"The More You Know: Using Knowledge Graphs for Image Classification"},{"paperId":"dbde7dfa6cae81df8ac19ef500c42db96c3d1edd","title":"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"},{"paperId":"2c03df8b48bf3fa39054345bafabfeff15bfd11d","title":"Deep Residual Learning for Image Recognition"},{"paperId":"2c1890864c1c2b750f48316dc8b650ba4772adc5","title":"Stacked Attention Networks for Image Question Answering"},{"paperId":"2582ab7c70c9e7fcb84545944eba8f3a7f253248","title":"Translating Embeddings for Modeling Multi-relational Data"},{"paperId":"519799a9e2a72b808d81c6bcba5de263503de053","title":"Contrastive Pre-training and Representation Distillation for Medical Visual Question Answering Based on Radiology Images"},{"paperId":"e5d143ae82ede67726aa1a9aeac3de4bf53d8920","title":"KB-VLP: Knowledge Based Vision and Language Pretraining"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"9eeeb23546d3d2bbc73959bffc6819f2335f3c83","title":"VQA-Med: Overview of the Medical Visual Question Answering Task at ImageCLEF 2019"},{"paperId":"441281c07b5e5949aeb56375e25623ddbdab94f4","title":"Intravascular Imaging and Computer Assisted Stenting, and Large-Scale Annotation of Biomedical Data and Expert Label Synthesis"},{"paperId":"0adeb54d32925d200bb313a8e0f06116e49c67fb","title":"The Unified Medical Language System (UMLS): integrating biomedical terminology"},{"paperId":null,"title":"Neuro-Symbolic Visual Reasoning: Disentangling"},{"paperId":null,"title":"Long short-termmemory"},{"paperId":null,"title":"BERT-MK: Integrating Graph Contextualized Knowledge into Pre-trained Language Models"}],"id":"28ff0816f19a5e3e37eac5569de41872fd262f0a","summary":"A systematic and effective approach to enhance Med-VLP by structured medical knowledge from three perspectives is proposed, which align the representations of the vision encoder and the language encoder through knowledge."},{"url":"https://www.semanticscholar.org/paper/6ae700c89a9a9a3da7e55dd51c4710b5ed8c8d4e","title":"Caption-Aware Medical VQA via Semantic Focusing and Progressive Cross-Modality Comprehension","venue":"ACM Multimedia","year":2022,"referenceCount":45,"citationCount":2,"influentialCitationCount":0,"publicationDate":"10/10/2022","authors":"Fu'ze Cong,Shibiao Xu,Li Guo,Yinbing Tian","citations":[{"paperId":"bf40c9e7832e1b2887cbf5798455f91705ea11ba","title":"Masked Vision and Language Pre-training with Unimodal and Multimodal Contrastive Losses for Medical Visual Question Answering"},{"paperId":"d183cc170400e43535c5e2c37121c37ee0ba23dc","title":"Open-Ended Medical Visual Question Answering Through Prefix Tuning of Language Models"}],"references":[{"paperId":"6e7763ec04906726377953cc85f31a1a0c889001","title":"Anomaly Matters: An Anomaly-Oriented Model for Medical Visual Question Answering"},{"paperId":"4ef3d9e492479e28fa57d107e52acc6a0c803de2","title":"Does CLIP Benefit Visual Question Answering in the Medical Domain as Much as it Does in the General Domain?"},{"paperId":"c2d5426ee019d3d894b9d4416dc866b65fe64312","title":"Image Retrieval on Real-life Images with Pre-trained Vision-and-Language Models"},{"paperId":"5bd42c29a5ba8a6c39547db89023d879e98a6b32","title":"Multiple Meta-model Quantifying for Medical Visual Question Answering"},{"paperId":"17c2bb358169541f2d0a769f80779f46d1cd3d37","title":"MMBERT: Multimodal BERT Pretraining for Improved Medical VQA"},{"paperId":"93b6b79b4ef6c345f31722ce7c829385c6dce0d6","title":"Slake: A Semantically-Labeled Knowledge-Enhanced Dataset For Medical Visual Question Answering"},{"paperId":"b8d5b853f2212cbb48a43f1edec9b96d76d388ec","title":"Medical Visual Question Answering via Conditional Reasoning"},{"paperId":"b1b8ffe938f706a9416c319a34793a2389866773","title":"Visual Question Answering as a Multi-Task Problem"},{"paperId":"818e5cbc337e4e1b98e65a2d7c2d6d2a0318cd57","title":"Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks"},{"paperId":"667b88984df0c5c11ac07899ffb5509185abdf57","title":"Visual question answering: a state-of-the-art review"},{"paperId":"ed6ce80789889c0fd56c8117f85079c1c31fe426","title":"CGMVQA: A New Classification and Generative Model for Medical Visual Question Answering"},{"paperId":"6adb61121ca4560915ade532910acde56440b88f","title":"A Question-Centric Model for Visual Question Answering in Medical Imaging"},{"paperId":"3c8a456509e6c0805354bd40a35e3f2dbf8069b1","title":"PyTorch: An Imperative Style, High-Performance Deep Learning Library"},{"paperId":"27cb0b42e0573c4891ae2ca444776dee57bfe2ac","title":"Compact Trilinear Interaction for Visual Question Answering"},{"paperId":"33301b25a297b701bdc287e985c006375cb7bb21","title":"Overcoming Data Limitation in Medical Visual Question Answering"},{"paperId":"6648b4db5f12c30941ea78c695e77aded19672bb","title":"Unified Vision-Language Pre-Training for Image Captioning and VQA"},{"paperId":"1e43c7084bdcb6b3102afaf301cce10faead2702","title":"BioBERT: a pre-trained biomedical language representation model for biomedical text mining"},{"paperId":"18f9a6045ba01cb079c4fa49a630d71bbd27cd92","title":"A dataset of clinically generated visual questions and answers about radiology images"},{"paperId":"97add9744ae63c5e7af9d9861ecc18a2734d3f0c","title":"Examine before You Answer: Multi-task Learning with Adaptive-attentions for Multiple-choice VQA"},{"paperId":"a564fabf130ff6e2742cfba90c7a4018937d764d","title":"Radiology Objects in COntext (ROCO): A Multimodal Image Dataset"},{"paperId":"a5d10341717c0519cf63151b496a6d2ed67aa05f","title":"Bilinear Attention Networks"},{"paperId":"5d45cc9a3a2fc064eccc0c915dbdf73cce559ce7","title":"On the Automatic Generation of Medical Imaging Reports"},{"paperId":"8e9ad6f8b2bc97f0412fa0cc243ac6975864534a","title":"Multi-modal Factorized Bilinear Pooling with Co-attention Learning for Visual Question Answering"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"1a2118bed729579528deb51e745d58dd3629baf6","title":"Learning Important Features Through Propagating Activation Differences"},{"paperId":"2c03df8b48bf3fa39054345bafabfeff15bfd11d","title":"Deep Residual Learning for Image Recognition"},{"paperId":"2c1890864c1c2b750f48316dc8b650ba4772adc5","title":"Stacked Attention Networks for Image Question Answering"},{"paperId":"580062407427236ced45253a2ff7df2e147a81e2","title":"The Multimodal Brain Tumor Image Segmentation Benchmark (BRATS)"},{"paperId":"4d8f2d14af5991d4f0d050d22216825cac3157bd","title":"Show, Attend and Tell: Neural Image Caption Generation with Visual Attention"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"f37e1b62a767a307c046404ca96bc140b3e68cb5","title":"GloVe: Global Vectors for Word Representation"},{"paperId":"0b544dfe355a5070b60986319a3f51fb45d1348e","title":"Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation"},{"paperId":"d2c733e34d48784a37d717fe43d9e93277a8c53e","title":"ImageNet: A large-scale hierarchical image database"},{"paperId":"44d2abe2175df8153f465f6c39b68b76a0d40ab9","title":"Long Short-Term Memory"},{"paperId":"161ffb54a3fdf0715b198bb57bd22f910242eb49","title":"Multitask Learning"},{"paperId":"0d46f2f51836f6b455c60d08309d7c48ff354ef1","title":"Yunnan University at VQA-Med 2021: Pretrained BioBERT for Medical Domain Visual Question Answering"},{"paperId":"519799a9e2a72b808d81c6bcba5de263503de053","title":"Contrastive Pre-training and Representation Distillation for Medical Visual Question Answering Based on Radiology Images"},{"paperId":"20a52008ecd5a1396d1501545f96fe3c17f63863","title":"kdevqa at VQA-Med 2020: Focusing on GLU-based Classification"},{"paperId":"39dbb2e49fb33351044a9b8c152a173b31f4c405","title":"Overview of the VQA-Med Task at ImageCLEF 2021: Visual Question Answering and Generation in the Medical Domain"},{"paperId":"737806db2a06048433d4976f10625f98dcba815c","title":"HARENDRAKV at VQA-Med 2020: Sequential VQA with Attention for Medical Visual Question Answering"},{"paperId":"e5d250b1d69d36131e4848969f1ff9dd69486c44","title":"Shengyan at VQA-Med 2020: An Encoder-Decoder Model for Medical Domain Visual Question Answering Task"},{"paperId":"791c950feebeae3130e04fa008419ac99afb41f0","title":"NLM at VQA-Med 2020: Visual Question Answering and Generation in the Medical Domain"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"441281c07b5e5949aeb56375e25623ddbdab94f4","title":"Intravascular Imaging and Computer Assisted Stenting, and Large-Scale Annotation of Biomedical Data and Expert Label Synthesis"},{"paperId":"fdbb252f29ee0b72fc5467c0ae11f7cb30149f46","title":"Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)"}],"id":"6ae700c89a9a9a3da7e55dd51c4710b5ed8c8d4e","summary":"A caption-aware VQA method that can read the summary information of image content and clinic diagnoses from plenty of medical images and answer the medical question with richer multimodality features is proposed."},{"url":"https://www.semanticscholar.org/paper/da9579539385daedd33a0de0f814e2977ad0d1f5","title":"Towards Unifying Medical Vision-and-Language Pre-training via Soft Prompts","venue":"arXiv.org","year":2023,"referenceCount":67,"citationCount":1,"influentialCitationCount":0,"publicationDate":"17/02/2023","authors":"Zhihong Chen,Shizhe Diao,Benyou Wang,Guanbin Li,Xiang Wan","citations":[{"paperId":"385376b8aa48c25403f17d6206db7c09b67e1314","title":"Prompt Engineering for Healthcare: Methodologies and Applications"}],"references":[{"paperId":"84729ec815f0607a4a2370c0969e8c3ba82a9411","title":"Learning to Exploit Temporal Structure for Biomedical Vision-Language Processing"},{"paperId":"e3c70b0b71b51872bbdaa0f4bf2b56908f97abec","title":"MedKLIP: Medical Knowledge Enhanced Language-Image Pre-Training"},{"paperId":"cdd9c1d23f9e89d5113f3e31821bb174c6a6afed","title":"MedCLIP: Contrastive Learning from Unpaired Medical Images and Text"},{"paperId":"4867e6c7d190e37b9199a67ebeac62180b59aa32","title":"Multi-Granularity Cross-modal Alignment for Generalized Medical Visual Representation Learning"},{"paperId":"81adb80e390f25d4a2d764b1063eeaa2c334d441","title":"Multi-modal Masked Autoencoders for Medical Vision-and-Language Pre-training"},{"paperId":"28ff0816f19a5e3e37eac5569de41872fd262f0a","title":"Align, Reason and Learn: Enhancing Medical Vision-and-Language Pre-training with Knowledge"},{"paperId":"02251886950770e82b3d68564d60cdfe15e73199","title":"Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks"},{"paperId":"db2bd466953f3ea49280988e1659b6ac3f639e45","title":"Cross-modal Memory Networks for Radiology Report Generation"},{"paperId":"e9581d9758062f76e029bd19a58c4ae976cfb414","title":"SLIP: Self-supervision meets Language-Image Pre-training"},{"paperId":"008721e4f9cb9b2d3242bc31af48db6fb3f8727d","title":"Word Graph Guided Summarization for Radiology Findings"},{"paperId":"2fd6f77540c1cc8e70b96208ccf9971b4251fc02","title":"FLAVA: A Foundational Language And Vision Alignment Model"},{"paperId":"2fca2821ac2beb60fa0e26866e8f063261713951","title":"Joint Learning of Localized Representations from Medical Images and Reports"},{"paperId":"f675c62abfa788ea0be85d3124eba15a14d5e9d6","title":"FILIP: Fine-grained Interactive Language-Image Pre-Training"},{"paperId":"cf7c2e0e4fb2af689aaf4b7a7cddf7b1f4d5e3f0","title":"VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts"},{"paperId":"94ff111c4d81bd03f159321728ceec8b4711c89d","title":"An Empirical Study of Training End-to-End Vision-and-Language Transformers"},{"paperId":"681b16ed7258bf28622f9c835cfe94f195bb5395","title":"MedFuseNet: An attention-based multimodal deep learning model for visual question answering in the medical domain"},{"paperId":"0b500aa5fcc175f07aecf26c0e8ddc4f0c6a931d","title":"GLoRIA: A Multimodal Global-Local Representation Learning Framework for Label-efficient Medical Image Recognition"},{"paperId":"28692beece311a90f5fa1ca2ec9d0c2ce293d069","title":"Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"},{"paperId":"b82c5f9efdb2ae56baa084ca41aeddd8a665c1d1","title":"Align before Fuse: Vision and Language Representation Learning with Momentum Distillation"},{"paperId":"63c74d15940af1af9b386b5762e4445e54c73719","title":"VinVL: Revisiting Visual Representations in Vision-Language Models"},{"paperId":"d9317660e2a538d9c018028956fd114d55330f82","title":"Multi-Modal Understanding and Generation for Medical Images and Text via Vision-Language Pre-Training"},{"paperId":"17c2bb358169541f2d0a769f80779f46d1cd3d37","title":"MMBERT: Multimodal BERT Pretraining for Improved Medical VQA"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"93b6b79b4ef6c345f31722ce7c829385c6dce0d6","title":"Slake: A Semantically-Labeled Knowledge-Enhanced Dataset For Medical Visual Question Answering"},{"paperId":"a6ca91afe845ef5294c40c2029e0c1cba19ba40b","title":"Unifying Vision-and-Language Tasks via Text Generation"},{"paperId":"0839722fb5369c0abaff8515bfc08299efc790a1","title":"ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision"},{"paperId":"19adf1af8daa9551328226fc6c0140e955bf5689","title":"Generating Radiology Reports via Memory-driven Transformer"},{"paperId":"268d347e8a55b5eb82fb5e7d2f800e33c75ab18a","title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"},{"paperId":"9e90c17ef40404b79ad0f12d9b9c94656f12dfcd","title":"Improving Factual Completeness and Consistency of Image-to-Text Radiology Report Generation"},{"paperId":"5ba77a5bdeffb62aa0902ae68997bbc38db8a722","title":"MedICaT: A Dataset of Medical Images, Captions, and Textual References"},{"paperId":"ff554f6228cf1f939a0e9e44ada06ef9cd28be15","title":"A Comparison of Pre-trained Vision-and-Language Models for Multimodal Representation Learning across Medical Images and Reports"},{"paperId":"bc996a4dbf9d4234eacdd0b930a94de1d158e256","title":"ERNIE-ViL: Knowledge Enhanced Vision-Language Representations Through Scene Graph"},{"paperId":"818e5cbc337e4e1b98e65a2d7c2d6d2a0318cd57","title":"Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks"},{"paperId":"598a2ee223e2949c3b28389e922c1892b4717d2a","title":"Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal Transformers"},{"paperId":"34733eaf66007516347a40ad5d9bbe1cc9dacb6b","title":"A Simple Framework for Contrastive Learning of Visual Representations"},{"paperId":"add2f205338d70e10ce5e686df4a690e2851bdfc","title":"Momentum Contrast for Unsupervised Visual Representation Learning"},{"paperId":"33301b25a297b701bdc287e985c006375cb7bb21","title":"Overcoming Data Limitation in Medical Visual Question Answering"},{"paperId":"d8a305b9366608d54452ac30459ee57b4f5cf1c9","title":"UNITER: UNiversal Image-TExt Representation Learning"},{"paperId":"2527626c11a84f15709e943fbfa2356e19930e3b","title":"VL-BERT: Pre-training of Generic Visual-Linguistic Representations"},{"paperId":"79c93274429d6355959f1e4374c2147bb81ea649","title":"LXMERT: Learning Cross-Modality Encoder Representations from Transformers"},{"paperId":"5aec474c31a2f4b74703c6f786c0a8ff85c450da","title":"VisualBERT: A Simple and Performant Baseline for Vision and Language"},{"paperId":"65a9c7b0800c86a196bc14e7621ff895cc6ab287","title":"ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks"},{"paperId":"63748e59f4e106cbda6b65939b77589f40e48fcb","title":"Text Summarization with Pretrained Encoders"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"2a567ebd78939d0861d788f0fedff8d40ae62bf2","title":"Publicly Available Clinical BERT Embeddings"},{"paperId":"74d8eb801c838d1dce814a1e9ce1074bd2c47721","title":"MIMIC-CXR: A large publicly available database of labeled chest radiographs"},{"paperId":"89a816719613e220a64ab2590c938c23bbfe187e","title":"CheXpert: A Large Chest Radiograph Dataset with Uncertainty Labels and Expert Comparison"},{"paperId":"95b19e31af5385800855f245744aabfb0b0ee74e","title":"Augmenting the National Institutes of Health Chest Radiograph Dataset with Expert Annotations of Possible Pneumonia."},{"paperId":"18f9a6045ba01cb079c4fa49a630d71bbd27cd92","title":"A dataset of clinically generated visual questions and answers about radiology images"},{"paperId":"34b313a9923e4edeb9fda1865e162d2e8e4fa87c","title":"A dataset of clinically generated visual questions and answers about radiology images."},{"paperId":"a564fabf130ff6e2742cfba90c7a4018937d764d","title":"Radiology Objects in COntext (ROCO): A Multimodal Image Dataset"},{"paperId":"f2588de5173fb047192dbb93d62ce6636bdf46bd","title":"Lessons from Natural Language Inference in the Clinical Domain"},{"paperId":"d07284a6811f1b2745d91bdb06b040b57f226882","title":"Decoupled Weight Decay Regularization"},{"paperId":"2e10560579f2bdeae0143141f26bd9f0a195b4b7","title":"Mixed Precision Training"},{"paperId":"dbde7dfa6cae81df8ac19ef500c42db96c3d1edd","title":"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"},{"paperId":"258986132bf17755fe8263e42429fe73218c1534","title":"CIDEr: Consensus-based image description evaluation"},{"paperId":"0f8992ee6418d367d8e50ecbb59b08ea15e8431f","title":"Meteor 1.3: Automatic Metric for Reliable Optimization and Evaluation of Machine Translation Systems"},{"paperId":"60b05f32c32519a809f21642ef1eb3eaf3848008","title":"ROUGE: A Package for Automatic Evaluation of Summaries"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"},{"paperId":"97da3aaa0dfbd32942ca99c60329d590b1234937","title":"Prefix Language Models are Unified Modal Learners"},{"paperId":"53d8b356551a2361020a948f64454a6d599af69f","title":"Prefix-Tuning: Optimizing Continuous Prompts for Generation"},{"paperId":"519799a9e2a72b808d81c6bcba5de263503de053","title":"Contrastive Pre-training and Representation Distillation for Medical Visual Question Answering Based on Radiology Images"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"9eeeb23546d3d2bbc73959bffc6819f2335f3c83","title":"VQA-Med: Overview of the Medical Visual Question Answering Task at ImageCLEF 2019"},{"paperId":null,"title":"This dataset consists of 315 images and 3,515 questions. We adopt the commonly used version pre-processed by MEVF"},{"paperId":null,"title":"We follow previous studies [50] to prepare and pre-process the dataset by keeping the main three categories of questions: Modality, Plane, and Organ system"},{"paperId":null,"title":"Ofa: Unifying architectures, tasks, and modalities through a simple sequence-tosequence learning framework"}],"id":"da9579539385daedd33a0de0f814e2977ad0d1f5","summary":"Experimental results show that the proposed PTUnifier approach achieves state-of-the-art results on a broad range of tasks, spanning uni-modal tasks (\\textit{i.e.}, image-to-text generation and image-text/text-image retrieval), and multi- modal tasks, demonstrating the effectiveness of the approach."},{"url":"https://www.semanticscholar.org/paper/2ea26b243171e37ef20af269942ffde414f9f8cc","title":"UnICLAM: Contrastive Representation Learning with Adversarial Masking for Unified and Interpretable Medical Vision Question Answering","venue":"arXiv.org","year":2022,"referenceCount":42,"citationCount":0,"influentialCitationCount":0,"publicationDate":"21/12/2022","authors":"Chenlu Zhan,Peng Peng,Hongsen Wang,Tao Chen,Hongwei Wang","citations":[],"references":[{"paperId":"cfca7eedc6ede9d363d1662280a74d78dcdc9d4a","title":"Scaling Language-Image Pre-training via Masking"},{"paperId":"e9480d62e216f77d5556b7eda769daa4c92d004d","title":"VQAMix: Conditional Triplet Mixup for Medical Visual Question Answering"},{"paperId":"8af37e55e7994860e6eeb839fd06ec271619a241","title":"SemMAE: Semantic-Guided Masking for Learning Masked Autoencoders"},{"paperId":"004b97aea43f9f62cc49dec20f449abfbae28811","title":"Masked Autoencoders As Spatiotemporal Learners"},{"paperId":"213738a30bc7283cc4447ac87fe783a03a7aae5d","title":"UTC: A Unified Transformer with Inter-Task Contrastive Learning for Visual Dialog"},{"paperId":"4b8d8d36a18fd61a6eda3322d8dd3baad2819600","title":"Unified Contrastive Learning in Image-Text-Label Space"},{"paperId":"2d439ec2c301d058bd4a8b4743328e3d9939625e","title":"Should You Mask 15% in Masked Language Modeling?"},{"paperId":"76d3b9d8685b88866abd19615ac0868061ced7e6","title":"Adversarial Masking for Self-Supervised Learning"},{"paperId":"a3b42a83669998f65df60d7c065a70d07ca95e99","title":"BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation"},{"paperId":"4ef3d9e492479e28fa57d107e52acc6a0c803de2","title":"Does CLIP Benefit Visual Question Answering in the Medical Domain as Much as it Does in the General Domain?"},{"paperId":"6351ebb4a3287f5f3e1273464b3b91e5df5a16d7","title":"Masked Autoencoders Are Scalable Vision Learners"},{"paperId":"cf7c2e0e4fb2af689aaf4b7a7cddf7b1f4d5e3f0","title":"VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts"},{"paperId":"b82c5f9efdb2ae56baa084ca41aeddd8a665c1d1","title":"Align before Fuse: Vision and Language Representation Learning with Momentum Distillation"},{"paperId":"722ad6ac92286507437b31486f47987d6ece05c9","title":"BEiT: BERT Pre-Training of Image Transformers"},{"paperId":"5bd42c29a5ba8a6c39547db89023d879e98a6b32","title":"Multiple Meta-model Quantifying for Medical Visual Question Answering"},{"paperId":"3c83f80f06633ff4598d33c2959f8e4cdcad3e93","title":"Cross-Modal Self-Attention with Multi-Task Pre-Training for Medical Visual Question Answering"},{"paperId":"f0524b3005720bcff886bcb0227f7f0dd924ff07","title":"VATT: Transformers for Multimodal Self-Supervised Learning from Raw Video, Audio and Text"},{"paperId":"166e98317ed9c4687e71bef55a6800431e00b8fa","title":"SiT: Self-supervised vIsion Transformer"},{"paperId":"17c2bb358169541f2d0a769f80779f46d1cd3d37","title":"MMBERT: Multimodal BERT Pretraining for Improved Medical VQA"},{"paperId":"93b6b79b4ef6c345f31722ce7c829385c6dce0d6","title":"Slake: A Semantically-Labeled Knowledge-Enhanced Dataset For Medical Visual Question Answering"},{"paperId":"141a5033d9994242b18bb3b217e79582f1ee9306","title":"Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision"},{"paperId":"5e5fbc41106db9acaaf3a365801051e477f0e984","title":"UNIMO: Towards Unified-Modal Understanding and Generation via Cross-Modal Contrastive Learning"},{"paperId":"0e23d2f14e7e56e81538f4a63e11689d8ac1eb9d","title":"Exploring Simple Siamese Representation Learning"},{"paperId":"b8d5b853f2212cbb48a43f1edec9b96d76d388ec","title":"Medical Visual Question Answering via Conditional Reasoning"},{"paperId":"a1b8a8df281bbaec148a897927a49ea47ea31515","title":"Improved Baselines with Momentum Contrastive Learning"},{"paperId":"6adb61121ca4560915ade532910acde56440b88f","title":"A Question-Centric Model for Visual Question Answering in Medical Imaging"},{"paperId":"34733eaf66007516347a40ad5d9bbe1cc9dacb6b","title":"A Simple Framework for Contrastive Learning of Visual Representations"},{"paperId":"6d38bdd172cdbccb11745f5f031f848679117f25","title":"CHAOS Challenge - Combined (CT-MR) Healthy Abdominal Organ Segmentation"},{"paperId":"33301b25a297b701bdc287e985c006375cb7bb21","title":"Overcoming Data Limitation in Medical Visual Question Answering"},{"paperId":"4654aa505e5bcdb089d0df202cd7ceabc9d2d41f","title":"A large annotated medical image dataset for the development and evaluation of segmentation algorithms"},{"paperId":"18f9a6045ba01cb079c4fa49a630d71bbd27cd92","title":"A dataset of clinically generated visual questions and answers about radiology images"},{"paperId":"a564fabf130ff6e2742cfba90c7a4018937d764d","title":"Radiology Objects in COntext (ROCO): A Multimodal Image Dataset"},{"paperId":"a5d10341717c0519cf63151b496a6d2ed67aa05f","title":"Bilinear Attention Networks"},{"paperId":"8e9ad6f8b2bc97f0412fa0cc243ac6975864534a","title":"Multi-modal Factorized Bilinear Pooling with Co-attention Learning for Visual Question Answering"},{"paperId":"c889d6f98e6d79b89c3a6adf8a921f88fa6ba518","title":"Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks"},{"paperId":"e7eef2ac4136ec93bd306d2c9c353a13729a4553","title":"Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization"},{"paperId":"fddc15480d086629b960be5bff96232f967f2252","title":"Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding"},{"paperId":"2c1890864c1c2b750f48316dc8b650ba4772adc5","title":"Stacked Attention Networks for Image Question Answering"},{"paperId":"a6cb366736791bcccc5c8639de5a8f9636bf87e8","title":"Adam: A Method for Stochastic Optimization"},{"paperId":"f37e1b62a767a307c046404ca96bc140b3e68cb5","title":"GloVe: Global Vectors for Word Representation"},{"paperId":"519799a9e2a72b808d81c6bcba5de263503de053","title":"Contrastive Pre-training and Representation Distillation for Medical Visual Question Answering Based on Radiology Images"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}],"id":"2ea26b243171e37ef20af269942ffde414f9f8cc","summary":"Experimental results on VQA-RAD and SLAKE public benchmarks demonstrate that UnICLAM outperforms existing 11 state-of-the-art Medical-VQA models and makes an additional discussion about the performance of UnicLAM in diagnosing heart failure, verifying that it exhibits superior few-shot adaption performance in practical disease diagnosis."},{"url":"https://www.semanticscholar.org/paper/3d45e69557f0c6a54ec698304c2e27ec29bc1c2b","title":"PMC-CLIP: Contrastive Language-Image Pre-training using Biomedical Documents","venue":"arXiv.org","year":2023,"referenceCount":32,"citationCount":2,"influentialCitationCount":0,"publicationDate":"13/03/2023","authors":"Weixiong Lin,Ziheng Zhao,Xiaoman Zhang,Chaoyi Wu,Ya Zhang,Yanfeng Wang,Weidi Xie","citations":[{"paperId":"86cbd30d1096b0c7e4ac6b03d97a8df12fd21457","title":"PathAsst: Redefining Pathology through Generative Foundation AI Assistant for Pathology"},{"paperId":"f4793adffd6f67ffcb93ccfc5672ab301b8a2b96","title":"PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering"}],"references":[{"paperId":"fa988afd9889451e5fcc46a316bd1e2a6abda367","title":"A Self-Adaptive Discriminative Autoencoder for Medical Applications"},{"paperId":"81adb80e390f25d4a2d764b1063eeaa2c334d441","title":"Multi-modal Masked Autoencoders for Medical Vision-and-Language Pre-training"},{"paperId":"28ff0816f19a5e3e37eac5569de41872fd262f0a","title":"Align, Reason and Learn: Enhancing Medical Vision-and-Language Pre-training with Knowledge"},{"paperId":"02251886950770e82b3d68564d60cdfe15e73199","title":"Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks"},{"paperId":"75bb9eda70751c63fc54dbe63377c673b7dbdb15","title":"CogView2: Faster and Better Text-to-Image Generation via Hierarchical Transformers"},{"paperId":"c57293882b2561e1ba03017902df9fc2f289dea2","title":"Hierarchical Text-Conditional Image Generation with CLIP Latents"},{"paperId":"38d089e36d630189eb6c5274066d57efd48a187d","title":"DWT-CV: Dense weight transfer-based cross validation strategy for model selection in biomedical data analysis"},{"paperId":"d766bffc357127e0dc86dd69561d5aeb520d6f4c","title":"Training language models to follow instructions with human feedback"},{"paperId":"94ff111c4d81bd03f159321728ceec8b4711c89d","title":"An Empirical Study of Training End-to-End Vision-and-Language Transformers"},{"paperId":"a66606a0bb11a1d5a7a14ec09df8a3481121ad6c","title":"MedMNIST v2 - A large-scale lightweight benchmark for 2D and 3D biomedical image classification"},{"paperId":"b82c5f9efdb2ae56baa084ca41aeddd8a665c1d1","title":"Align before Fuse: Vision and Language Representation Learning with Momentum Distillation"},{"paperId":"ef39d344161d2af825b650168aa332f2217c406a","title":"EXSCLAIM! - An automated pipeline for the construction of labeled materials imaging datasets from literature"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"93b6b79b4ef6c345f31722ce7c829385c6dce0d6","title":"Slake: A Semantically-Labeled Knowledge-Enhanced Dataset For Medical Visual Question Answering"},{"paperId":"0839722fb5369c0abaff8515bfc08299efc790a1","title":"ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision"},{"paperId":"5ba77a5bdeffb62aa0902ae68997bbc38db8a722","title":"MedICaT: A Dataset of Medical Images, Captions, and Textual References"},{"paperId":"54523ff961a1ac57a86696ef9a53b3a630b482c0","title":"Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"962dc29fdc3fbdc5930a10aba114050b82fe5a3e","title":"End-to-End Object Detection with Transformers"},{"paperId":"d1f407b16fb8d99487baee37ed0805676c58e7ac","title":"MIMIC-CXR, a de-identified publicly available database of chest radiographs with free-text reports"},{"paperId":"33301b25a297b701bdc287e985c006375cb7bb21","title":"Overcoming Data Limitation in Medical Visual Question Answering"},{"paperId":"da076089117ca1e8bce65dfa848d23da914b63c5","title":"DocFigure: A Dataset for Scientific Document Figure Classification"},{"paperId":"18f9a6045ba01cb079c4fa49a630d71bbd27cd92","title":"A dataset of clinically generated visual questions and answers about radiology images"},{"paperId":"a564fabf130ff6e2742cfba90c7a4018937d764d","title":"Radiology Objects in COntext (ROCO): A Multimodal Image Dataset"},{"paperId":"b227f3e4c0dc96e5ac5426b85485a70f2175a205","title":"Representation Learning with Contrastive Predictive Coding"},{"paperId":"d07284a6811f1b2745d91bdb06b040b57f226882","title":"Decoupled Weight Decay Regularization"},{"paperId":"2c03df8b48bf3fa39054345bafabfeff15bfd11d","title":"Deep Residual Learning for Image Recognition"},{"paperId":"f7609414ae3a117cd2828c7dae19ad34ff7d72e6","title":"PubMed Central: The GenBank of the published literature."},{"paperId":"519799a9e2a72b808d81c6bcba5de263503de053","title":"Contrastive Pre-training and Representation Distillation for Medical Visual Question Answering Based on Radiology Images"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"0adeb54d32925d200bb313a8e0f06116e49c67fb","title":"The Unified Medical Language System (UMLS): integrating biomedical terminology"},{"paperId":null,"title":"Authors Suppressed Due to Excessive Length"}],"id":"3d45e69557f0c6a54ec698304c2e27ec29bc1c2b","summary":"PMC-OA, a biomedical dataset with 1.6M image-caption pairs collected from PubMedCentral's OpenAccess subset, is built and released, which is 8 times larger than before and achieves state-of-the-art results on various downstream tasks."},{"url":"https://www.semanticscholar.org/paper/170667a96f04adf3b3b83526f75fe8d1063e0f7a","title":"Self-supervised vision-language pretraining for Medical visual question answering","venue":"arXiv.org","year":2022,"referenceCount":25,"citationCount":1,"influentialCitationCount":0,"publicationDate":"24/11/2022","authors":"Pengfei Li,Gang Liu,Lin Tan,Jinying Liao,Shenjun Zhong","citations":[{"paperId":"0f8d12775a4685575f1489796b5dee9e11fbdfb5","title":"OphGLM: Training an Ophthalmology Large Language-and-Vision Assistant based on Instructions and Dialogue"}],"references":[{"paperId":"e9480d62e216f77d5556b7eda769daa4c92d004d","title":"VQAMix: Conditional Triplet Mixup for Medical Visual Question Answering"},{"paperId":"ed1d78633567b82e2626c4af6f3bbbf9c420d5aa","title":"Type-Aware Medical Visual Question Answering"},{"paperId":"4ef3d9e492479e28fa57d107e52acc6a0c803de2","title":"Does CLIP Benefit Visual Question Answering in the Medical Domain as Much as it Does in the General Domain?"},{"paperId":"6351ebb4a3287f5f3e1273464b3b91e5df5a16d7","title":"Masked Autoencoders Are Scalable Vision Learners"},{"paperId":"b82c5f9efdb2ae56baa084ca41aeddd8a665c1d1","title":"Align before Fuse: Vision and Language Representation Learning with Momentum Distillation"},{"paperId":"5bd42c29a5ba8a6c39547db89023d879e98a6b32","title":"Multiple Meta-model Quantifying for Medical Visual Question Answering"},{"paperId":"3c83f80f06633ff4598d33c2959f8e4cdcad3e93","title":"Cross-Modal Self-Attention with Multi-Task Pre-Training for Medical Visual Question Answering"},{"paperId":"17c2bb358169541f2d0a769f80779f46d1cd3d37","title":"MMBERT: Multimodal BERT Pretraining for Improved Medical VQA"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"93b6b79b4ef6c345f31722ce7c829385c6dce0d6","title":"Slake: A Semantically-Labeled Knowledge-Enhanced Dataset For Medical Visual Question Answering"},{"paperId":"268d347e8a55b5eb82fb5e7d2f800e33c75ab18a","title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"},{"paperId":"ed6ce80789889c0fd56c8117f85079c1c31fe426","title":"CGMVQA: A New Classification and Generative Model for Medical Visual Question Answering"},{"paperId":"add2f205338d70e10ce5e686df4a690e2851bdfc","title":"Momentum Contrast for Unsupervised Visual Representation Learning"},{"paperId":"87f6a7c014ce206ac5b57299c07e10667d194b39","title":"Randaugment: Practical automated data augmentation with a reduced search space"},{"paperId":"33301b25a297b701bdc287e985c006375cb7bb21","title":"Overcoming Data Limitation in Medical Visual Question Answering"},{"paperId":"18f9a6045ba01cb079c4fa49a630d71bbd27cd92","title":"A dataset of clinically generated visual questions and answers about radiology images"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"e7eef2ac4136ec93bd306d2c9c353a13729a4553","title":"Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization"},{"paperId":"2c03df8b48bf3fa39054345bafabfeff15bfd11d","title":"Deep Residual Learning for Image Recognition"},{"paperId":"eb42cf88027de515750f230b23b1a057dc782108","title":"Very Deep Convolutional Networks for Large-Scale Image Recognition"},{"paperId":"d2c733e34d48784a37d717fe43d9e93277a8c53e","title":"ImageNet: A large-scale hierarchical image database"},{"paperId":"44d2abe2175df8153f465f6c39b68b76a0d40ab9","title":"Long Short-Term Memory"},{"paperId":"26b5140294488937996cf1a9038cafacd54f7deb","title":"Overview of ImageCLEFmedical 2022 - Caption Prediction and Concept Detection"},{"paperId":"2551990a1ccdffb1a4d1d9040b2d493ba6d26dd1","title":"Towards Visual Question Answering on Pathology Images"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"}],"id":"170667a96f04adf3b3b83526f75fe8d1063e0f7a","summary":"A self-supervised method that applies Masked image modeling, Masked language modeling, Image text matching and Image text alignment via contrastive learning (M2I2) for pretraining on medical image caption dataset, and finetunes to downstream medical VQA tasks is proposed."}]}
{"papers":[{"url":"https://www.semanticscholar.org/paper/93b6b79b4ef6c345f31722ce7c829385c6dce0d6","title":"Slake: A Semantically-Labeled Knowledge-Enhanced Dataset For Medical Visual Question Answering","venue":"IEEE International Symposium on Biomedical Imaging","year":2021,"referenceCount":15,"citationCount":73,"influentialCitationCount":8,"publicationDate":"18/02/2021","authors":"Bo Liu,Li-Ming Zhan,Li Xu,Lin Ma,Y. Yang,Xiao-Ming Wu","citations":[{"paperId":"a3d418b4e35a02e4306505ab660a6bcd44c3c752","title":"Asclepius: A Spectrum Evaluation Benchmark for Medical Multi-Modal Large Language Models"},{"paperId":"bc431ad78e91a73a69580b7d256d18777dcda313","title":"Prompt-based Personalized Federated Learning for Medical Visual Question Answering"},{"paperId":"0fd27cba73c1af107a32b3c6138b643f22cf8749","title":"MultiMedEval: A Benchmark and a Toolkit for Evaluating Medical Vision-Language Models"},{"paperId":"7580327ffc9bd5daef83fe8285c0476ca074051d","title":"OmniMedVQA: A New Large-Scale Comprehensive Evaluation Benchmark for Medical LVLM"},{"paperId":"61cadcfa555cbef120df7c017ef02e87f19900b7","title":"Free Form Medical Visual Question Answering in Radiology"},{"paperId":"63de69245502d9a22de04581a4b5c0168d596aa3","title":"Generalizing Visual Question Answering from Synthetic to Human-Written Questions via a Chain of QA with a Large Language Model"},{"paperId":"1e0d21dc2caf7b58342ddc8609fb30cdc1e27cd5","title":"MISS: A Generative Pretraining and Finetuning Approach for Med-VQA"},{"paperId":"1e1230ef1de1ba9c4f6cb4789184a295133afac0","title":"Visual Instruction Tuning towards General-Purpose Multimodal Model: A Survey"},{"paperId":"352252231462c24440bc0016638ea5fe8d4c6f7e","title":"UniDCP: Unifying Multiple Medical Vision-language Tasks via Dynamic Cross-modal Learnable Prompts"},{"paperId":"d48fa3ed73817563130ef217d85011ce1fbe7470","title":"BESTMVQA: A Benchmark Evaluation System for Medical Visual Question Answering"},{"paperId":"2c7e346aa311fec4dda04bdf3a214ce2026d8807","title":"Medical Vision Language Pretraining: A survey"},{"paperId":"d77bc1a237b67c57b0c1b99b4802e703747a9688","title":"BenchLMM: Benchmarking Cross-style Visual Capability of Large Multimodal Models"},{"paperId":"1f280fe1f800bae53ada94b30f244fab0cd9f795","title":"KI-MAG: A knowledge-infused abstractive question answering system in medical domain"},{"paperId":"8d2709ed1788a67e64425fb410bb49f3ee49e088","title":"Large Language Models Illuminate a Progressive Pathway to Artificial Healthcare Assistant: A Review"},{"paperId":"749104d1a207f5bc192c7d95a12856b5e7f84d1f","title":"Mapping medical image-text to a joint space via masked modeling"},{"paperId":"18c48b42941d16eeaf053ae18b1fe671934af134","title":"Scaling-up medical vision-and-language representation learning with federated learning"},{"paperId":"cb886bc9674d689d3a1f23713826374279894557","title":"EHRXQA: A Multi-Modal Question Answering Dataset for Electronic Health Records with Chest X-ray Images"},{"paperId":"da9134f694959b68027c33c8e998ffb3d41305da","title":"Exploring Question Decomposition for Zero-Shot VQA"},{"paperId":"c7492913370b5726eaa6ced163a60de6c9d4bb7f","title":"A Survey of Large Language Models for Healthcare: from Data, Technology, and Applications to Accountability and Ethics"},{"paperId":"910e82a2a825d3d1939908094c01134c2cf30dcc","title":"MITER: Medical Image-TExt joint adaptive pretRaining with multi-level contrastive learning"},{"paperId":"a0476578761e983d5ab2083abab07b81236c1d58","title":"Asymmetric cross-modal attention network with multimodal augmented mixup for medical visual question answering"},{"paperId":"f0950a3f27c0fefffba60ae1c9a8ee360d5eb55f","title":"Instruction Tuning for Large Language Models: A Survey"},{"paperId":"df0ddb588a200d095743e9d26fc4a9318619766e","title":"Towards Generalist Foundation Model for Radiology"},{"paperId":"304f8b4edea01fdb5a2f7f8b998c83188deeccff","title":"Towards Generalist Biomedical AI"},{"paperId":"1a78aee4ba07c3b2486083c3a500bc8b3bd4df7a","title":"MDKG: Graph-Based Medical Knowledge-Guided Dialogue Generation"},{"paperId":"a4e3f9a3f8c875679c7806d846853b4530843799","title":"A Medical Domain Visual Question Generation Model via Large Language Model"},{"paperId":"baa1dc079d98ca76b0173c8d653fed759fd0a371","title":"A scoping review on multimodal deep learning in biomedical images and texts"},{"paperId":"bf40c9e7832e1b2887cbf5798455f91705ea11ba","title":"Masked Vision and Language Pre-training with Unimodal and Multimodal Contrastive Losses for Medical Visual Question Answering"},{"paperId":"e0e9ba0c01d441e1fdcb8628d3f743d387b0b017","title":"UIT-Saviors at MEDVQA-GI 2023: Improving Multimodal Learning with Image Enhancement for Gastrointestinal Visual Question Answering"},{"paperId":"8ca86bc84bf332352e22837de2d001d4a93414a1","title":"Localized Questions in Medical Visual Question Answering"},{"paperId":"534675abb9d72fc0c08d080d4f73335ceb75902c","title":"Multimodal Prompt Retrieval for Generative Visual Question Answering"},{"paperId":"ebedc4d7a2356090904baba4104ef0832bc236df","title":"A Survey on Multimodal Large Language Models"},{"paperId":"f57afb6c8addfc7a32f9be5916a374a542d1a026","title":"ECG-QA: A Comprehensive Question Answering Dataset Combined With Electrocardiogram"},{"paperId":"64fa56962dd0f4bbe206be6142fbe0315c4e7c2f","title":"Multi-modal Pre-training for Medical Vision-language Understanding and Generation: An Empirical Study with A New Benchmark"},{"paperId":"f22d71c7ce9720ba1f717a4f1181488200e78198","title":"LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day"},{"paperId":"07d45ce7de598ef03b400f8ddba7d2e055e77a08","title":"BiomedGPT: A Unified and Generalist Biomedical Generative Pre-trained Transformer for Vision, Language, and Multimodal Tasks"},{"paperId":"f4793adffd6f67ffcb93ccfc5672ab301b8a2b96","title":"PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering"},{"paperId":"3b508a48a4b48d2a16dd790a2a04ffcf51c0b4a6","title":"SUR-adapter: Enhancing Text-to-Image Pre-trained Diffusion Models with Large Language Models"},{"paperId":"ac4d13b6a4f9fb67337099f4602135a0351f5c99","title":"Towards Medical Artificial General Intelligence via Knowledge-Enhanced Multimodal Pretraining"},{"paperId":"f7ea746cd2cc25628a7a553ac27d228198be42cb","title":"Pre-trained multilevel fuse network based on vision-conditioned reasoning and bilinear attentions for medical image visual question answering"},{"paperId":"8f3138f7ee5127faab265793be8ae278bc49d9b1","title":"PMC-CLIP: Contrastive Language-Image Pre-training using Biomedical Documents"},{"paperId":"785650a805851c7e945523e495c5a523c60f72a4","title":"Open-Ended Medical Visual Question Answering Through Prefix Tuning of Language Models"},{"paperId":"5814bd146b37e13115af4330caf3a751159a156f","title":"BiomedCLIP: a multimodal biomedical foundation model pretrained from fifteen million scientific image-text pairs"},{"paperId":"8200be2e8b9af243ee72a9d919a4f7fbe82a17d2","title":"Medical knowledge-based network for Patient-oriented Visual Question Answering"},{"paperId":"4d4a96708fc67403176bb2b891b564af7a20c148","title":"RAMM: Retrieval-augmented Biomedical Visual Question Answering with Multi-modal Pre-training"},{"paperId":"9259c41695c4451f1ca3e6bdc9829623b43f9a69","title":"Interpretable Medical Image Visual Question Answering via Multi-Modal Relationship Graph Learning"},{"paperId":"da9579539385daedd33a0de0f814e2977ad0d1f5","title":"Towards Unifying Medical Vision-and-Language Pre-training via Soft Prompts"},{"paperId":"940f303c2530a52c5fd3c52c9c64ceea4b53ab05","title":"Diversity Learning Based on Multi-Latent Space for Medical Image Visual Question Generation"},{"paperId":"2580d3fc39fed3989f10665559a955b847b7eb7f","title":"Medical Visual Question Answering via Conditional Reasoning and Contrastive Learning"},{"paperId":"90cd86b3c157e40cbaf1076f69cbd38d9c0781b9","title":"UnICLAM: Contrastive Representation Learning with Adversarial Masking for Unified and Interpretable Medical Vision Question Answering"},{"paperId":"5942335fdd35d1651aaabd7af4db129a29ed2a85","title":"How Well Apply Multimodal Mixup and Simple MLPs Backbone to Medical Visual Question Answering?"},{"paperId":"560e0114a023bdfd99eb60eb4d9d555a348600a0","title":"PiggyBack: Pretrained Visual Question Answering Environment for Backing up Non-deep Learning Professionals"},{"paperId":"170667a96f04adf3b3b83526f75fe8d1063e0f7a","title":"Self-Supervised Vision-Language Pretraining for Medial Visual Question Answering"},{"paperId":"6ae700c89a9a9a3da7e55dd51c4710b5ed8c8d4e","title":"Caption-Aware Medical VQA via Semantic Focusing and Progressive Cross-Modality Comprehension"},{"paperId":"28ff0816f19a5e3e37eac5569de41872fd262f0a","title":"Align, Reason and Learn: Enhancing Medical Vision-and-Language Pre-training with Knowledge"},{"paperId":"81adb80e390f25d4a2d764b1063eeaa2c334d441","title":"Multi-modal Masked Autoencoders for Medical Vision-and-Language Pre-training"},{"paperId":"0cbd644254462341a897d4bfa0134637662c3ab5","title":"A Transformer-based Medical Visual Question Answering Model"},{"paperId":"ef2edea434e487f288d4eed6f9b1dc480b917211","title":"Adversarial Learning to Improve Question Image Embedding in Medical Visual Question Answering"},{"paperId":"67f992f43cc777a3e1aedc14cf3a11582ccfa570","title":"OVQA: A Clinically Generated Visual Question Answering Dataset"},{"paperId":"e9480d62e216f77d5556b7eda769daa4c92d004d","title":"VQAMix: Conditional Triplet Mixup for Medical Visual Question Answering"},{"paperId":"22f2f53e7474af620268318ac3ff4bb5a4fe3ab4","title":"MED-GPVS: A Deep Learning-Based Joint Biomedical Image Classification and Visual Question Answering System for Precision e-Health"},{"paperId":"ab2ba04580edb4340a896b37543e77fdc2ec6bbf","title":"Hybrid deep learning model for answering visual medical questions"},{"paperId":"e678898301a66faab85dfa4c84e51118e434b8f2","title":"Vision-Language Transformer for Interpretable Pathology Visual Question Answering"},{"paperId":"4ef3d9e492479e28fa57d107e52acc6a0c803de2","title":"Does CLIP Benefit Visual Question Answering in the Medical Domain as Much as it Does in the General Domain?"},{"paperId":"e34b699cef0a711a8cb9c39ecea20ac2df1578f5","title":"Medical Visual Question Answering: A Survey"},{"paperId":"3c83f80f06633ff4598d33c2959f8e4cdcad3e93","title":"Cross-Modal Self-Attention with Multi-Task Pre-Training for Medical Visual Question Answering"},{"paperId":"b88f6aa65a4e1faf963494a76d28cc12112c9543","title":"A Critical Analysis of Benchmarks, Techniques, and Models in Medical Visual Question Answering"},{"paperId":"ba067bb339ce3d28c0f084a029485374b3773aba","title":"Reducing Knowledge Noise for Improved Semantic Analysis in Biomedical Natural Language Processing Applications"},{"paperId":"61c0b6a5e7aea48a1376b61a4a737137d602b242","title":"PubMedCLIP: How Much Does CLIP Benefit Visual Question Answering in the Medical Domain?"},{"paperId":"31a7d8c4a5ab6bab522494b57270249105c8748e","title":"BiomedGPT: A Unified and Generalist Biomedical Generative Pre-trained Transformer for Vision, Language, and Multimodal Tasks"},{"paperId":"801e653b5b8bfc43571861c633b2a46c455b7d1e","title":"Multistep Question-Driven Visual Question Answering for Remote Sensing"},{"paperId":"99267305914a44ad6d626b7a6406fd0079b5508d","title":"Incorporating Medical Knowledge to Transformer-based Language Models for Medical Dialogue Generation"},{"paperId":"357fc385caf2e5b9898c9140fa3ac9955e6bb3c6","title":"TeamS at VQA-Med 2021: BBN-Orchestra for Long-tailed Medical Visual Question Answering"}],"references":[{"paperId":"b8d5b853f2212cbb48a43f1edec9b96d76d388ec","title":"Medical Visual Question Answering via Conditional Reasoning"},{"paperId":"33301b25a297b701bdc287e985c006375cb7bb21","title":"Overcoming Data Limitation in Medical Visual Question Answering"},{"paperId":"4654aa505e5bcdb089d0df202cd7ceabc9d2d41f","title":"A large annotated medical image dataset for the development and evaluation of segmentation algorithms"},{"paperId":"03eb382e04cca8cca743f7799070869954f1402a","title":"CLEVR: A Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning"},{"paperId":"b60630911d7746fba06de7c34abe98c9a61c6bcc","title":"FVQA: Fact-Based Visual Question Answering"},{"paperId":"8e759195eb4b4f0f480a8a2cf1c629bfd881d4e5","title":"Analyzing the Behavior of Visual Question Answering Models"},{"paperId":"5fa973b8d284145bf0ced9acf2913a74674260f6","title":"Yin and Yang: Balancing and Answering Binary Visual Questions"},{"paperId":"2c1890864c1c2b750f48316dc8b650ba4772adc5","title":"Stacked Attention Networks for Image Question Answering"},{"paperId":"97ad70a9fa3f99adf18030e5e38ebe3d90daa2db","title":"VQA: Visual Question Answering"},{"paperId":"eb42cf88027de515750f230b23b1a057dc782108","title":"Very Deep Convolutional Networks for Large-Scale Image Recognition"},{"paperId":"2582ab7c70c9e7fcb84545944eba8f3a7f253248","title":"Translating Embeddings for Modeling Multi-relational Data"},{"paperId":"038b582cccb00c54589c5563d9a00ee28dad83b0","title":"User-guided 3D active contour segmentation of anatomical structures: Significantly improved efficiency and reliability"},{"paperId":"63f560006ba47e9b163f45ef46122a5f85b686e6","title":"ChestX-ray: Hospital-Scale Chest X-ray Database and Benchmarks on Weakly Supervised Classification and Localization of Common Thorax Diseases"},{"paperId":"18f9a6045ba01cb079c4fa49a630d71bbd27cd92","title":"Descriptor : A dataset of clinically generated visual questions and answers about radiology images"},{"paperId":null,"title":"CHAOS - Combined (CT- MR) Healthy Abdominal Organ Segmentation Challenge Data"}],"id":"93b6b79b4ef6c345f31722ce7c829385c6dce0d6","summary":"A large bilingual dataset, SLAKE, with comprehensive semantic labels annotated by experienced physicians and a new structural medical knowledge base for Med-VQA is presented, which includes richer modalities and covers more human body parts than the currently available dataset."},{"url":"https://www.semanticscholar.org/paper/18f9a6045ba01cb079c4fa49a630d71bbd27cd92","title":"A dataset of clinically generated visual questions and answers about radiology images","venue":"Scientific Data","year":2018,"referenceCount":14,"citationCount":151,"influentialCitationCount":31,"publicationDate":"20/11/2018","authors":"J. Lau,Soumya Gayen,Asma Ben Abacha,Dina Demner-Fushman","citations":[{"paperId":"a3d418b4e35a02e4306505ab660a6bcd44c3c752","title":"Asclepius: A Spectrum Evaluation Benchmark for Medical Multi-Modal Large Language Models"},{"paperId":"bc431ad78e91a73a69580b7d256d18777dcda313","title":"Prompt-based Personalized Federated Learning for Medical Visual Question Answering"},{"paperId":"7580327ffc9bd5daef83fe8285c0476ca074051d","title":"OmniMedVQA: A New Large-Scale Comprehensive Evaluation Benchmark for Medical LVLM"},{"paperId":"0fd27cba73c1af107a32b3c6138b643f22cf8749","title":"MultiMedEval: A Benchmark and a Toolkit for Evaluating Medical Vision-Language Models"},{"paperId":"61cadcfa555cbef120df7c017ef02e87f19900b7","title":"Free Form Medical Visual Question Answering in Radiology"},{"paperId":"6ed96d6822a06ad9a735bc09e301bf41df61c534","title":"CheXagent: Towards a Foundation Model for Chest X-Ray Interpretation"},{"paperId":"63de69245502d9a22de04581a4b5c0168d596aa3","title":"Generalizing Visual Question Answering from Synthetic to Human-Written Questions via a Chain of QA with a Large Language Model"},{"paperId":"1e0d21dc2caf7b58342ddc8609fb30cdc1e27cd5","title":"MISS: A Generative Pretraining and Finetuning Approach for Med-VQA"},{"paperId":"420087f314633a381e61e6c5cd73ccc2070a749e","title":"PeFoMed: Parameter Efficient Fine-tuning on Multimodal Large Language Models for Medical Visual Question Answering"},{"paperId":"accf39ab42d8d1157e5541ae7dcd63ea2be7b32d","title":"From image to language: A critical analysis of Visual Question Answering (VQA) approaches, challenges, and opportunities"},{"paperId":"1e1230ef1de1ba9c4f6cb4789184a295133afac0","title":"Visual Instruction Tuning towards General-Purpose Multimodal Model: A Survey"},{"paperId":"352252231462c24440bc0016638ea5fe8d4c6f7e","title":"UniDCP: Unifying Multiple Medical Vision-language Tasks via Dynamic Cross-modal Learnable Prompts"},{"paperId":"d48fa3ed73817563130ef217d85011ce1fbe7470","title":"BESTMVQA: A Benchmark Evaluation System for Medical Visual Question Answering"},{"paperId":"2c7e346aa311fec4dda04bdf3a214ce2026d8807","title":"Medical Vision Language Pretraining: A survey"},{"paperId":"cb5e4157bc37affe59d105fb14e5581ca18d5caf","title":"A Weak Supervision-based Robust Pretraining Method for Medical Visual Question Answering"},{"paperId":"290c8a0dde6fede48afcabb6de3b34fca5b370a2","title":"Complex Organ Mask Guided Radiology Report Generation"},{"paperId":"8d2709ed1788a67e64425fb410bb49f3ee49e088","title":"Large Language Models Illuminate a Progressive Pathway to Artificial Healthcare Assistant: A Review"},{"paperId":"749104d1a207f5bc192c7d95a12856b5e7f84d1f","title":"Mapping medical image-text to a joint space via masked modeling"},{"paperId":"18c48b42941d16eeaf053ae18b1fe671934af134","title":"Scaling-up medical vision-and-language representation learning with federated learning"},{"paperId":"88bddfb7d1e0462be8fe99fdbd71c658140cb17b","title":"From Image to Language: A Critical Analysis of Visual Question Answering (VQA) Approaches, Challenges, and Opportunities"},{"paperId":"1f5e1a036b24b9dd34c006ba3bb61119624f4fdb","title":"A Comprehensive Study of GPT-4V's Multimodal Capabilities in Medical Imaging"},{"paperId":"2010e5fb3a804ac376412b4fa65ee83f34d5e1d9","title":"A Systematic Evaluation of GPT-4V's Multimodal Capability for Medical Image Analysis"},{"paperId":"ad083672bc110b2c86d1461bb4cda3cd3eededee","title":"Multimodal ChatGPT for Medical Applications: an Experimental Study of GPT-4V"},{"paperId":"cb886bc9674d689d3a1f23713826374279894557","title":"EHRXQA: A Multi-Modal Question Answering Dataset for Electronic Health Records with Chest X-ray Images"},{"paperId":"da9134f694959b68027c33c8e998ffb3d41305da","title":"Exploring Question Decomposition for Zero-Shot VQA"},{"paperId":"181cd4973db6170d72a438db54db8e52ccbbe87a","title":"Multi-modal multi-head self-attention for medical VQA"},{"paperId":"c7492913370b5726eaa6ced163a60de6c9d4bb7f","title":"A Survey of Large Language Models for Healthcare: from Data, Technology, and Applications to Accountability and Ethics"},{"paperId":"ecd29b6980c52b5c9af25063dec46221931dff5e","title":"FGCVQA: Fine-Grained Cross-Attention for Medical VQA"},{"paperId":"8946891e94831adc8cddb0d32311cce2445c96d2","title":"MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts"},{"paperId":"910e82a2a825d3d1939908094c01134c2cf30dcc","title":"MITER: Medical Image-TExt joint adaptive pretRaining with multi-level contrastive learning"},{"paperId":"837b03421a6e0314b46ed24a5ae889aaafe2ec2a","title":"Event-Oriented Visual Question Answering: The E-VQA Dataset and Benchmark"},{"paperId":"a0476578761e983d5ab2083abab07b81236c1d58","title":"Asymmetric cross-modal attention network with multimodal augmented mixup for medical visual question answering"},{"paperId":"f0950a3f27c0fefffba60ae1c9a8ee360d5eb55f","title":"Instruction Tuning for Large Language Models: A Survey"},{"paperId":"df0ddb588a200d095743e9d26fc4a9318619766e","title":"Towards Generalist Foundation Model for Radiology"},{"paperId":"f660250f9fcd465acdf2e727d309acf1cc64c780","title":"ELIXR: Towards a general purpose X-ray artificial intelligence system through alignment of large language models and radiology vision encoders"},{"paperId":"9b6779fe8805abae18dcafcdf80f45109d61a7d7","title":"Medical visual question answering with symmetric interaction attention and cross-modal gating"},{"paperId":"304f8b4edea01fdb5a2f7f8b998c83188deeccff","title":"Towards Generalist Biomedical AI"},{"paperId":"8e51781ef23930913d1d6b9157fb310103aa9f82","title":"Expert Knowledge-Aware Image Difference Graph Representation Learning for Difference-Aware Medical Visual Question Answering"},{"paperId":"a4e3f9a3f8c875679c7806d846853b4530843799","title":"A Medical Domain Visual Question Generation Model via Large Language Model"},{"paperId":"baa1dc079d98ca76b0173c8d653fed759fd0a371","title":"A scoping review on multimodal deep learning in biomedical images and texts"},{"paperId":"bf40c9e7832e1b2887cbf5798455f91705ea11ba","title":"Masked Vision and Language Pre-training with Unimodal and Multimodal Contrastive Losses for Medical Visual Question Answering"},{"paperId":"e0e9ba0c01d441e1fdcb8628d3f743d387b0b017","title":"UIT-Saviors at MEDVQA-GI 2023: Improving Multimodal Learning with Image Enhancement for Gastrointestinal Visual Question Answering"},{"paperId":"534675abb9d72fc0c08d080d4f73335ceb75902c","title":"Multimodal Prompt Retrieval for Generative Visual Question Answering"},{"paperId":"ebedc4d7a2356090904baba4104ef0832bc236df","title":"A Survey on Multimodal Large Language Models"},{"paperId":"64fa56962dd0f4bbe206be6142fbe0315c4e7c2f","title":"Multi-modal Pre-training for Medical Vision-language Understanding and Generation: An Empirical Study with A New Benchmark"},{"paperId":"8213492345c67d2b0e692b6bb5c814d4f1aef8d2","title":"Benchmarking Robustness of Adaptation Methods on Pre-trained Vision-Language Models"},{"paperId":"f22d71c7ce9720ba1f717a4f1181488200e78198","title":"LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day"},{"paperId":"a563f823a887b75dd61adc96c556a8bd83c6e4c3","title":"HaVQA: A Dataset for Visual Question Answering and Multimodal Research in Hausa Language"},{"paperId":"07d45ce7de598ef03b400f8ddba7d2e055e77a08","title":"BiomedGPT: A Unified and Generalist Biomedical Generative Pre-trained Transformer for Vision, Language, and Multimodal Tasks"},{"paperId":"f4793adffd6f67ffcb93ccfc5672ab301b8a2b96","title":"PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering"},{"paperId":"ad2ad450f1ee6a0df46bc6fe6916a797c90b68f1","title":"Multi-task Paired Masking with Alignment Modeling for Medical Vision-Language Pre-training"},{"paperId":"c5bcc78ae708b29edb03481e12213eca53c28963","title":"A multi-modal model based on transformers for medical visual question answering"},{"paperId":"ac4d13b6a4f9fb67337099f4602135a0351f5c99","title":"Towards Medical Artificial General Intelligence via Knowledge-Enhanced Multimodal Pretraining"},{"paperId":"2f9b344158e40d4af8391fc7e79d400bebba39ce","title":"Assertiveness-based Agent Communication for a Personalized Medicine on Medical Imaging Diagnosis"},{"paperId":"2587cb7b1c02fde76e3c23c13f1bd40d6a199c57","title":"Q2ATransformer: Improving Medical VQA via an Answer Querying Decoder"},{"paperId":"46a0f57d5376ad1b9fb94b894931a5419d67dbf5","title":"A reinforcement learning approach for VQA validation: An application to diabetic macular edema grading"},{"paperId":"f7ea746cd2cc25628a7a553ac27d228198be42cb","title":"Pre-trained multilevel fuse network based on vision-conditioned reasoning and bilinear attentions for medical image visual question answering"},{"paperId":"9e8936a131ee0c765a6749d93bc58ad9522b7d6d","title":"Logical Implications for Visual Question Answering Consistency"},{"paperId":"8f3138f7ee5127faab265793be8ae278bc49d9b1","title":"PMC-CLIP: Contrastive Language-Image Pre-training using Biomedical Documents"},{"paperId":"17ca48ad1b944c897863f04ba9ffa72674dce1ce","title":"Parallel multi-head attention and term-weighted question embedding for medical visual question answering"},{"paperId":"5814bd146b37e13115af4330caf3a751159a156f","title":"BiomedCLIP: a multimodal biomedical foundation model pretrained from fifteen million scientific image-text pairs"},{"paperId":"20fb06a4aa4010470d388098618af5d1bea224ad","title":"Vision–Language Model for Visual Question Answering in Medical Imagery"},{"paperId":"8200be2e8b9af243ee72a9d919a4f7fbe82a17d2","title":"Medical knowledge-based network for Patient-oriented Visual Question Answering"},{"paperId":"4d4a96708fc67403176bb2b891b564af7a20c148","title":"RAMM: Retrieval-augmented Biomedical Visual Question Answering with Multi-modal Pre-training"},{"paperId":"ffeeb60b76d18e1e36dee0f87c95bab1bc65aa79","title":"Medical visual question answering using joint self-supervised learning"},{"paperId":"9259c41695c4451f1ca3e6bdc9829623b43f9a69","title":"Interpretable Medical Image Visual Question Answering via Multi-Modal Relationship Graph Learning"},{"paperId":"da9579539385daedd33a0de0f814e2977ad0d1f5","title":"Towards Unifying Medical Vision-and-Language Pre-training via Soft Prompts"},{"paperId":"f4b9ec310ef9aff69066e3e482cc113c8ae8d9dd","title":"PathNarratives: Data annotation for pathological human-AI collaborative diagnosis"},{"paperId":"940f303c2530a52c5fd3c52c9c64ceea4b53ab05","title":"Diversity Learning Based on Multi-Latent Space for Medical Image Visual Question Generation"},{"paperId":"e0b4ca7bffb64b4bbd95c9f5ee7a610e35fe95d8","title":"A comprehensive interpretation for medical VQA: Datasets, techniques, and challenges"},{"paperId":"2580d3fc39fed3989f10665559a955b847b7eb7f","title":"Medical Visual Question Answering via Conditional Reasoning and Contrastive Learning"},{"paperId":"90cd86b3c157e40cbaf1076f69cbd38d9c0781b9","title":"UnICLAM: Contrastive Representation Learning with Adversarial Masking for Unified and Interpretable Medical Vision Question Answering"},{"paperId":"83caa8f9ec66a9ebae68d0e963ac7ca2396c94c2","title":"Is Unimodal Bias Always Bad for Visual Question Answering? A Medical Domain Study with Dynamic Attention"},{"paperId":"5942335fdd35d1651aaabd7af4db129a29ed2a85","title":"How Well Apply Multimodal Mixup and Simple MLPs Backbone to Medical Visual Question Answering?"},{"paperId":"56d8d9fff399f798da97a69e891de4eeb4568d4f","title":"MHKD-MVQA: Multimodal Hierarchical Knowledge Distillation for Medical Visual Question Answering"},{"paperId":"b60711d89c34d8902d4b2768f01770473cf0adfc","title":"Medical image enhancement strategy based on morphologically processing of residuals using a special kernel"},{"paperId":"170667a96f04adf3b3b83526f75fe8d1063e0f7a","title":"Self-Supervised Vision-Language Pretraining for Medial Visual Question Answering"},{"paperId":"9d79f3601b0d73a2b64784cad2738c0fcd030824","title":"MF2-MVQA: A Multi-Stage Feature Fusion Method for Medical Visual Question Answering"},{"paperId":"6ae700c89a9a9a3da7e55dd51c4710b5ed8c8d4e","title":"Caption-Aware Medical VQA via Semantic Focusing and Progressive Cross-Modality Comprehension"},{"paperId":"d4d07180764fc30cf31261ddd072175a4daee10b","title":"A Dual-Attention Learning Network With Word and Sentence Embedding for Medical Visual Question Answering"},{"paperId":"8f93076f8e060eec0c058edb3de05f62886fffdf","title":"RepsNet: Combining Vision with Language for Automated Medical Reports"},{"paperId":"28ff0816f19a5e3e37eac5569de41872fd262f0a","title":"Align, Reason and Learn: Enhancing Medical Vision-and-Language Pre-training with Knowledge"},{"paperId":"81adb80e390f25d4a2d764b1063eeaa2c334d441","title":"Multi-modal Masked Autoencoders for Medical Vision-and-Language Pre-training"},{"paperId":"8cee548aa6a8d31dcac830695e4b72960ff45ecb","title":"MMCN: Multi-Modal Co-attention Network for Medical Visual Question Answering"},{"paperId":"0cbd644254462341a897d4bfa0134637662c3ab5","title":"A Transformer-based Medical Visual Question Answering Model"},{"paperId":"0976f5e6e7c0481f5f44c981d9f676e9ea7fa4d0","title":"AMAM: An Attention-based Multimodal Alignment Model for Medical Visual Question Answering"},{"paperId":"ef7dd87e8bfd11878e88ec3f0795ebda8aaf1690","title":"A Bi-level representation learning model for medical visual question answering"},{"paperId":"ef2edea434e487f288d4eed6f9b1dc480b917211","title":"Adversarial Learning to Improve Question Image Embedding in Medical Visual Question Answering"},{"paperId":"2ac3bacbbee520b701707ebcf7b9ca7a3f233129","title":"Medical visual question answering via corresponding feature fusion combined with semantic attention."},{"paperId":"67f992f43cc777a3e1aedc14cf3a11582ccfa570","title":"OVQA: A Clinically Generated Visual Question Answering Dataset"},{"paperId":"c98eafbef6fa40010fc3b78d96a04c41699e2c1b","title":"EBMs vs. CL: Exploring Self-Supervised Visual Pretraining for Visual Question Answering"},{"paperId":"6e7763ec04906726377953cc85f31a1a0c889001","title":"Anomaly Matters: An Anomaly-Oriented Model for Medical Visual Question Answering"},{"paperId":"e9480d62e216f77d5556b7eda769daa4c92d004d","title":"VQAMix: Conditional Triplet Mixup for Medical Visual Question Answering"},{"paperId":"d1f25ff0b282486acf6ae225e5fd18a82673eb35","title":"Multi-Modal Alignment of Visual Question Answering Based on Multi-Hop Attention Mechanism"},{"paperId":"8c9a9a1bbba2a3e3bab34bce533b3b2acfda32b0","title":"Medical visual question answering based on question-type reasoning and semantic space constraint"},{"paperId":"38cbcaab9387c9c08df2f89fe93792c3dfe46a01","title":"BreastScreening-AI: Evaluating medical intelligent agents for human-AI interactions"},{"paperId":"4ef3d9e492479e28fa57d107e52acc6a0c803de2","title":"Does CLIP Benefit Visual Question Answering in the Medical Domain as Much as it Does in the General Domain?"},{"paperId":"e34b699cef0a711a8cb9c39ecea20ac2df1578f5","title":"Medical Visual Question Answering: A Survey"},{"paperId":"39528ef1de5a6c1b4fba44071591e9f12167769c","title":"MVQAS: A Medical Visual Question Answering System"},{"paperId":"681b16ed7258bf28622f9c835cfe94f195bb5395","title":"MedFuseNet: An attention-based multimodal deep learning model for visual question answering in the medical domain"},{"paperId":"933242d263859a12c058979163f58035047d78b5","title":"Fine-grained Hand Gesture Recognition in Multi-viewpoint Hand Hygiene"},{"paperId":"ecf3163157d477d1a2188a3f8cf75c697a303708","title":"Goal-Driven Visual Question Generation from Radiology Images"},{"paperId":"e4f99837e02e7fbcccec1bf15cececacaaabbe32","title":"MuVAM: A Multi-View Attention-based Model for Medical Visual Question Answering"},{"paperId":"3795b18bb223ee70c6c4347ea371b28fffb671e8","title":"Automatic Generation of Structured Radiology Reports for Volumetric Computed Tomography Images Using Question-Specific Deep Feature Extraction and Learning"},{"paperId":"5bd42c29a5ba8a6c39547db89023d879e98a6b32","title":"Multiple Meta-model Quantifying for Medical Visual Question Answering"},{"paperId":"3c83f80f06633ff4598d33c2959f8e4cdcad3e93","title":"Cross-Modal Self-Attention with Multi-Task Pre-Training for Medical Visual Question Answering"},{"paperId":"17c2bb358169541f2d0a769f80779f46d1cd3d37","title":"MMBERT: Multimodal BERT Pretraining for Improved Medical VQA"},{"paperId":"a4d21d620a6cb7a8e6f06b996463172478562a0a","title":"Visual Question Answering using Data Mining Techniques for Skeletal Scintigraphy in medical domain - VQADMSS"},{"paperId":"3fb9e014d52a2082141acefdeaddbd81fd422b24","title":"Visual Question Answering: which investigated applications?"},{"paperId":"93b6b79b4ef6c345f31722ce7c829385c6dce0d6","title":"Slake: A Semantically-Labeled Knowledge-Enhanced Dataset For Medical Visual Question Answering"},{"paperId":"a627232a97a7a63f8399d157f0b022eb1ccd547c","title":"Biomedical Question Answering: A Survey of Approaches and Challenges"},{"paperId":"b8d5b853f2212cbb48a43f1edec9b96d76d388ec","title":"Medical Visual Question Answering via Conditional Reasoning"},{"paperId":"72e0dccf59f126a64f970fe9f4712b3221a3be8c","title":"Pathological Visual Question Answering"},{"paperId":"9fe3eeafbe022de014aeb54d0b55502e2a2e46fe","title":"Hierarchical Deep Multi-modal Network for Medical Visual Question Answering"},{"paperId":"ed2a06388dd14b052f33bac5e3bfc0fa26243b55","title":"A Comparison of Pre-trained Vision-and-Language Models for Multimodal Representation Learning across Medical Images and Reports"},{"paperId":"d77a71c94e688d92a3fa10fb7f7feda2c306b9dc","title":"Visual Question Generation from Radiology Images"},{"paperId":"6609489a0f800a9ef411efdcfca4c014c4e86aa8","title":"Towards Visual Dialog for Radiology"},{"paperId":"30f86e15b4fd7936b9812d476976a6ff579b9036","title":"Toward General Scene Graph: Integration of Visual Semantic Knowledge with Entity Synset Alignment"},{"paperId":"ed6ce80789889c0fd56c8117f85079c1c31fe426","title":"CGMVQA: A New Classification and Generative Model for Medical Visual Question Answering"},{"paperId":"fc0b46a0f3720e6c29c1a913aaa3de4a0699f713","title":"PathVQA: 30000+ Questions for Medical Visual Question Answering"},{"paperId":"099d7a1b4f80318c4b6090bfba4b60d1ff81220f","title":"A database for using machine learning and data mining techniques for coronary artery disease diagnosis"},{"paperId":"e11ec81062d591b8bd97362d671db80394e391f6","title":"MoBVQA: A Modality based Medical Image Visual Question Answering System"},{"paperId":"33301b25a297b701bdc287e985c006375cb7bb21","title":"Overcoming Data Limitation in Medical Visual Question Answering"},{"paperId":"f59ae732612ce8c42035adfb47bd5739c6288ad6","title":"Answering Questions about Data Visualizations using Efficient Bimodal Fusion"},{"paperId":"46699dd04d40efd34ae9088f945f672e50f9ec62","title":"Concept-Centric Visual Turing Tests for Method Validation"},{"paperId":"821bb93c7638bf722bea8444059502cbc4674a04","title":"Artificial intelligence, regenerative surgery, robotics? What is realistic for the future of surgery?"},{"paperId":"9ae30ac3609a90b487df3beec10eeface021c7c5","title":"Machine Learning in Computer Vision: A Review"},{"paperId":"b88f6aa65a4e1faf963494a76d28cc12112c9543","title":"A Critical Analysis of Benchmarks, Techniques, and Models in Medical Visual Question Answering"},{"paperId":"61c0b6a5e7aea48a1376b61a4a737137d602b242","title":"PubMedCLIP: How Much Does CLIP Benefit Visual Question Answering in the Medical Domain?"},{"paperId":"31a7d8c4a5ab6bab522494b57270249105c8748e","title":"BiomedGPT: A Unified and Generalist Biomedical Generative Pre-trained Transformer for Vision, Language, and Multimodal Tasks"},{"paperId":"76f8bad5aff7c21cc0049e8d97ff53c34e8311f8","title":"A Dual of Stacked Attention Networks (SAN's) and VGG-16 Model-Based Visual Question Answering Evaluation"},{"paperId":"5c07fe90fb8e1fe4bee7ff40190c3f1e83667310","title":"Visual Cropping Improves Zero-Shot Question Answering of Multimodal Large Language Models"},{"paperId":"b047b3b7d76b79958e23b0fcab985be22b1ce42d","title":"Alternating Cross-attention Vision-Language Model for Efficient Learning with Medical Image and Report without Curation"},{"paperId":"79478a2ac67b9fdbeadcde13faa2d84eb239e080","title":"Vision-Language Pretraining Enables Radiographs and Reports to be Learned without Curation"},{"paperId":"fae23fc97a31bf66563dd033faf311eaaaa05911","title":"Exploratory analysis of different metaheuristic optimization methods for medical image enhancement"},{"paperId":"2441230bd2f3cca924d597b3044ad63aaff269ec","title":"Self-supervised Co-learning of Uncurated Images and Reports Enables Oversight AI in Radiology"},{"paperId":"1f96539c083d60fa83f7548bc6996cdede1026ee","title":"Biomedical Question Answering: A Comprehensive Review"},{"paperId":"2551990a1ccdffb1a4d1d9040b2d493ba6d26dd1","title":"Towards Visual Question Answering on Pathology Images"},{"paperId":"411c7a1fb951a1420013c0af56f9d142565112aa","title":"Chabbiimen at VQA-Med 2021: Visual Generation of Relevant Natural Language Questions from Radiology Images for Anomaly Detection"},{"paperId":"357fc385caf2e5b9898c9140fa3ac9955e6bb3c6","title":"TeamS at VQA-Med 2021: BBN-Orchestra for Long-tailed Medical Visual Question Answering"},{"paperId":"31acfba3a19f780a3239925ff12a7a4047d6a705","title":"MLEC-QA: A Chinese Multi-Choice Biomedical Question Answering Dataset"},{"paperId":"519799a9e2a72b808d81c6bcba5de263503de053","title":"Contrastive Pre-training and Representation Distillation for Medical Visual Question Answering Based on Radiology Images"},{"paperId":"f2ff49a399468c005a7cf689ff882b4198587b1b","title":"MedFuseNet: An attention-based multimodal deep learning model for visual question answering in the medical domain"},{"paperId":"83fabf18e4b67f148cc5f85fda417f6a98d31bf4","title":"Overview of the VQA-Med Task at ImageCLEF 2021: Visual Question Answering and Generation in the Medical Domain"},{"paperId":"e05e4ab677e93939adf901945987ef7493dfbe37","title":"The Inception Team at VQA-Med 2020: Pretrained VGG with Data Augmentation for Medical VQA and VQG"},{"paperId":"cc71a905ca132999a158857823606cd979b9080e","title":"Visual Dialog for Radiology: Data Curation and FirstSteps"},{"paperId":"b7c9e854c1e9b964c8abe0cb80a744e2739ddf40","title":"Tlemcen University at ImageCLEF 2019 Visual Question Answering Task"},{"paperId":"1526501f1939311106f72c128a189bbb6487ca6a","title":"SMAC: An Interpretable Reasoning Network for Visual Question Answering"},{"paperId":"9eeeb23546d3d2bbc73959bffc6819f2335f3c83","title":"VQA-Med: Overview of the Medical Visual Question Answering Task at ImageCLEF 2019"},{"paperId":"4634bf44a0c994e2bed89686225f8cef601a0224","title":"NLM at ImageCLEF 2018 Visual Question Answering in the Medical Domain"},{"paperId":"a7930f1ce6085f5bb6301cad958fd6501002eda0","title":"Medical Image Analysis"}],"references":[{"paperId":"7e4b638e028498e900747b600f46cd723f1f231e","title":"Data Augmentation for Visual Question Answering"},{"paperId":"6ff909c6fe089fc8ebfc64eca0f0c3cc34ba277f","title":"A survey on deep learning in medical image analysis"},{"paperId":"7e232313a59d735ef7c8a9f4cc7bc980a29deb5e","title":"Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering"},{"paperId":"12f7de07f9b00315418e381b2bd797d21f12b419","title":"Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding"},{"paperId":"0460d3497490fa8332c5ff2ecdab88fb7dff4755","title":"Learning to Read Chest X-Rays: Recurrent Neural Cascade Model for Automated Image Annotation"},{"paperId":"afcf4dbd2ef300e5c4b35043d4fbe516807cdf7d","title":"Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations"},{"paperId":"2c03df8b48bf3fa39054345bafabfeff15bfd11d","title":"Deep Residual Learning for Image Recognition"},{"paperId":"def584565d05d6a8ba94de6621adab9e301d375d","title":"Visual7W: Grounded Question Answering in Images"},{"paperId":"2c1890864c1c2b750f48316dc8b650ba4772adc5","title":"Stacked Attention Networks for Image Question Answering"},{"paperId":"580062407427236ced45253a2ff7df2e147a81e2","title":"The Multimodal Brain Tumor Image Segmentation Benchmark (BRATS)"},{"paperId":"2fcd5cff2b4743ea640c4af68bf4143f4a2cccb1","title":"Are You Talking to a Machine? Dataset and Methods for Multilingual Image Question"},{"paperId":"97ad70a9fa3f99adf18030e5e38ebe3d90daa2db","title":"VQA: Visual Question Answering"},{"paperId":"71b7178df5d2b112d07e45038cb5637208659ff7","title":"Microsoft COCO: Common Objects in Context"},{"paperId":"d7da009f457917aa381619facfa5ffae9329a6e9","title":"Bleu: a Method for Automatic Evaluation of Machine Translation"}],"id":"18f9a6045ba01cb079c4fa49a630d71bbd27cd92","summary":"This work introduces VQA-RAD, the first manually constructed dataset where clinicians asked naturally occurring questions about radiology images and provided reference answers and demonstrates the rich quality of this dataset over other automatically constructed ones."},{"url":"https://www.semanticscholar.org/paper/6edcb09a09c8df43cb62119133df9bb2eb75e5cf","title":"From Images to Textual Prompts: Zero-shot VQA with Frozen Large Language Models","venue":"arXiv.org","year":2022,"referenceCount":73,"citationCount":56,"influentialCitationCount":0,"publicationDate":"21/12/2022","authors":"Jiaxian Guo,Junnan Li,Dongxu Li,A. M. H. Tiong,Boyang Li,Dacheng Tao,Steven Hoi","citations":[{"paperId":"86188727c4d4f3eb064ae7ff0d9a3483b4ef47c1","title":"Typographic Attacks in Large Multimodal Models Can be Alleviated by More Informative Prompts"},{"paperId":"fed3376de52d70ba83050182e79466dddde45746","title":"On the Safety Concerns of Deploying LLMs/VLMs in Robotics: Highlighting the Risks and Vulnerabilities"},{"paperId":"3ca5d2f2483e658c2810d57d5ee00d85d00aa3db","title":"Large Language Models for Captioning and Retrieving Remote Sensing Images"},{"paperId":"deb1f198eea208fa637a8b4d3934f9ad4c8ed1b6","title":"CIC: A framework for Culturally-aware Image Captioning"},{"paperId":"59f3cf13401b9cc45d0e5ad7ea525e1eec84cce1","title":"Real-World Robot Applications of Foundation Models: A Review"},{"paperId":"fccc3cbe91ef873b6cf5929c3971c2218972826b","title":"Patient Centric Summarization of Radiology Findings using Large Language Models"},{"paperId":"d57dea679fae7cd5bca45adb882f2d334b495cfb","title":"GeReA: Question-Aware Prompt Captions for Knowledge-based Visual Question Answering"},{"paperId":"0be1c71b1710f01fb5d321e9b1459a7d2a7cdaf2","title":"Q&A Prompts: Discovering Rich Visual Clues through Mining Question-Answer Prompts for VQA requiring Diverse World Knowledge"},{"paperId":"a0807e4c9c5c67673a547f8fecb409ad33cb1d0c","title":"Quantifying Similarity: Text-Mining Approaches to Evaluate ChatGPT and Google Bard Content in Relation to BioMedical Literature"},{"paperId":"c8575881b4a30d89a20bd58401f202cd7ca6c69b","title":"Consolidating Trees of Robotic Plans Generated Using Large Language Models to Improve Reliability"},{"paperId":"d5a7abc154fce6086e65c09e6621cf1d46814738","title":"Enhancing Multimodal Understanding With LIUS: A Novel Framework for Visual Question Answering in Digital Marketing"},{"paperId":"63de69245502d9a22de04581a4b5c0168d596aa3","title":"Generalizing Visual Question Answering from Synthetic to Human-Written Questions via a Chain of QA with a Large Language Model"},{"paperId":"af5f256e9771bf9cd02451195e3a7ac693fde3ed","title":"Exploring the Reasoning Abilities of Multimodal Large Language Models (MLLMs): A Comprehensive Survey on Emerging Trends in Multimodal Reasoning"},{"paperId":"8b55f7386a6f68df678cd791b29a23ab6c5515e4","title":"Teach Large Language Models to Forget Privacy"},{"paperId":"30843f8b40e387f3a8b510277b1bbb464deafd9b","title":"LLM4VG: Large Language Models Evaluation for Video Grounding"},{"paperId":"c2f1a101eb1c54cab321c47ce4f3b4b416f4786a","title":"M3DBench: Let's Instruct Large Models with Multi-modal 3D Prompts"},{"paperId":"c42c25b81dfe9b6745a25b01396e3e2309ba4dae","title":"Shot2Story20K: A New Benchmark for Comprehensive Understanding of Multi-shot Videos"},{"paperId":"cd6402234500f37286e52811dbdbfb87b557a437","title":"Large Scale Foundation Models for Intelligent Manufacturing Applications: A Survey"},{"paperId":"4c92bd73698cf25e7a16c694f8c278ab3f9bfd08","title":"Effectively Fine-tune to Improve Large Multimodal Models for Radiology Report Generation"},{"paperId":"9b45bd2221b1d91e72939019bfdf3aba0310e6e1","title":"Large Language Models Are Zero-Shot Text Classifiers"},{"paperId":"4df34ebf827d21decc8de2742daa10accf5e6168","title":"Plug-and-Play, Dense-Label-Free Extraction of Open-Vocabulary Semantic Segmentation from Vision-Language Models"},{"paperId":"52941cadbd340344f3e0a6f50719fe55b3de5088","title":"Multimodal Large Language Models: A Survey"},{"paperId":"cf193b5b34178a444cb9bd9f51beb4124b753935","title":"HalluciDoctor: Mitigating Hallucinatory Toxicity in Visual Instruction Data"},{"paperId":"13d12b26db345f62e8e512db181b96a7f8763b47","title":"An Embodied Generalist Agent in 3D World"},{"paperId":"bddb9d818b73de0a06197a6966673c7eb63c9146","title":"Think-in-Memory: Recalling and Post-thinking Enable LLMs with Long-Term Memory"},{"paperId":"88bddfb7d1e0462be8fe99fdbd71c658140cb17b","title":"From Image to Language: A Critical Analysis of Visual Question Answering (VQA) Approaches, Challenges, and Opportunities"},{"paperId":"589e34ea39a418b7d2e291f66e379de4b55a7edd","title":"Harvest Video Foundation Models via Efficient Post-Pretraining"},{"paperId":"696ec1f8d25de41a082ff6c059e296c8fc6d9af2","title":"Challenges and Opportunities in Neuro-Symbolic Composition of Foundation Models"},{"paperId":"2b6a3cad4e4cbc2f2ae2a9f2d5b9e349071f24c2","title":"Open Visual Knowledge Extraction via Relation-Oriented Multimodality Model Prompting"},{"paperId":"da9134f694959b68027c33c8e998ffb3d41305da","title":"Exploring Question Decomposition for Zero-Shot VQA"},{"paperId":"29b3ce4de9dd9d784ca1d876957950f4b2d3796a","title":"Towards Perceiving Small Visual Details in Zero-shot Visual Question Answering with Multimodal LLMs"},{"paperId":"96d104dfe727f78a35faaafe81481f3672b485ee","title":"Large Language Models are Visual Reasoning Coordinators"},{"paperId":"123785bba81f243f9ed177ff66aff556dace5a12","title":"RSAdapter: Adapting Multimodal Models for Remote Sensing Visual Question Answering"},{"paperId":"beb3e8acd816bac1a5b7fccfd073f79048877e33","title":"Frozen Transformers in Language Models Are Effective Visual Encoder Layers"},{"paperId":"ac2e5bf716aed246ca8914a6816ef73e00286099","title":"Beyond Segmentation: Road Network Generation with Multi-Modal LLMs"},{"paperId":"6f2f20c78d311c4ce8be0bb6855177c5169bb6cd","title":"MuseChat: A Conversational Music Recommendation System for Videos"},{"paperId":"28fbbf98bac1bb941162df553ca034d600cb59a6","title":"Rephrase, Augment, Reason: Visual Grounding of Questions for Vision-Language Models"},{"paperId":"61bbdbf481a6d3519c22513ebe8d6c3cd381851e","title":"Language Models as Knowledge Bases for Visual Word Sense Disambiguation"},{"paperId":"a9f016b31daf2896510ada413e54e62440d61ed0","title":"Learning to Prompt CLIP for Monocular Depth Estimation: Exploring the Limits of Human Language"},{"paperId":"0b9ab24684d275c355248c54bcac5d44cf6b1999","title":"LANCAR: Leveraging Language for Context-Aware Robot Locomotion in Unstructured Environments"},{"paperId":"93183f050e0ce0aff8ba0aa850c8353e24fb169d","title":"Fine-grained Late-interaction Multi-modal Retrieval for Retrieval Augmented Visual Question Answering"},{"paperId":"c74e9642ec71c6dfaadd3b8638c110d4048ff53e","title":"Suspicion-Agent: Playing Imperfect Information Games with Theory of Mind Aware GPT-4"},{"paperId":"a8f05b5ef3d60fb310f8366aa775118df5b700c3","title":"Tackling VQA with Pretrained Foundation Models without Further Training"},{"paperId":"1df754afa6d27b630ecf91d15bb1ae4ed12a194e","title":"A Survey on Interpretable Cross-modal Reasoning"},{"paperId":"a5cddee937d7d2f005e781e453833cd64d3cf343","title":"Learning to Model the World with Language"},{"paperId":"838422b5ddaaa5637ba86056d1e964409bb2f016","title":"Robust Visual Question Answering: Datasets, Methods, and Future Challenges"},{"paperId":"16ea13d8a88b3d65596f36aac22561dd663bec22","title":"ClipSitu: Effectively Leveraging CLIP for Conditional Predictions in Situation Recognition"},{"paperId":"efc694164312006c543ef745611348ef64e68dda","title":"Towards Language Models That Can See: Computer Vision Through the LENS of Natural Language"},{"paperId":"8efc20988021ce3b4b05dd44b13e27260ee9b99b","title":"Investigating Prompting Techniques for Zero- and Few-Shot Visual Question Answering"},{"paperId":"8ecdbfe011b7189fa0ee49ffc4e42a93d728a371","title":"On Evaluating Adversarial Robustness of Large Vision-Language Models"},{"paperId":"3b508a48a4b48d2a16dd790a2a04ffcf51c0b4a6","title":"SUR-adapter: Enhancing Text-to-Image Pre-trained Diffusion Models with Large Language Models"},{"paperId":"20fcc01d12a50f1da2af71d85f0a269b3ba48b77","title":"LMEye: An Interactive Perception Network for Large Language Models"},{"paperId":"95430a76264a9be7d64633e56831c60041fb2948","title":"SurgicalGPT: End-to-End Language-Vision GPT for Visual Question Answering in Surgery"},{"paperId":"197022486b2e2584302bd9b6442e44d15bf3e351","title":"ICL-D3IE: In-Context Learning with Diverse Demonstrations Updating for Document Information Extraction"},{"paperId":"0f19e94f30b99d6c4b349900057cdae9262034f9","title":"The Contribution of Knowledge in Visiolinguistic Learning: A Survey on Tasks and Challenges"},{"paperId":"2859de53b8309a1389715f54b50bc84bab9893d3","title":"Towards Explainable Automatic Knowledge Graph Construction with Human-in-the-Loop"}],"references":[{"paperId":"bb15f3727f827a3cb88b5d3ca48415c09b40a88f","title":"What Language Model to Train if You Have One Million GPU Hours?"},{"paperId":"26fd105d0b5a458979c012cddb3ba2de943388c4","title":"Plug-and-Play VQA: Zero-shot VQA by Conjoining Large Pretrained Models with Zero Training"},{"paperId":"50502d0de333902d90ec0b68ce7339281281eeb9","title":"LAVIS: A Library for Language-Vision Intelligence"},{"paperId":"dac3a172b504f4e33c029655e9befb3386e5f63a","title":"Emergent Abilities of Large Language Models"},{"paperId":"47a67e76ed84260ff19f7a948d764005d1edf1c9","title":"A-OKVQA: A Benchmark for Visual Question Answering using World Knowledge"},{"paperId":"57c64f233a0db4d17e0e750c12516364ca009fb2","title":"REVIVE: Regional Visual Representation Matters in Knowledge-Based Visual Question Answering"},{"paperId":"e7ad08848d5d7c5c47673ffe0da06af443643bda","title":"Large Language Models are Zero-Shot Reasoners"},{"paperId":"5437e8adab596d7294124c0e798708e050e25321","title":"Least-to-Most Prompting Enables Complex Reasoning in Large Language Models"},{"paperId":"7f5170b8ec68629164a98f8dfa1d2cbef5bbe5f5","title":"All You May Need for VQA are Image Captions"},{"paperId":"13a0d8bb38f739990c8cd65a44061c6534f17221","title":"OPT: Open Pre-trained Transformer Language Models"},{"paperId":"26218bdcc3945c7edae7aa2adbfba4cd820a2df3","title":"Flamingo: a Visual Language Model for Few-Shot Learning"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"de20efa062cb54ce06beab24ac70be9501423f6a","title":"Pseudo-Q: Generating Pseudo Language Queries for Visual Grounding"},{"paperId":"7f71875f8214dffa4f3276da123c4990a6d437cc","title":"Enabling Multimodal Generation on CLIP via Vision-Language Knowledge Distillation"},{"paperId":"79956ac2a4164c298387546fc10139c3d5192842","title":"Webly Supervised Concept Expansion for General Purpose Vision Models"},{"paperId":"a3b42a83669998f65df60d7c065a70d07ca95e99","title":"BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation"},{"paperId":"1b6e810ce0afd0dd093f789d2b2742d047e316d5","title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"ab8ee8d06ed581c876da3a2e7a5fdb1cbec21a45","title":"KAT: A Knowledge Augmented Transformer for Vision-and-Language"},{"paperId":"2fd6f77540c1cc8e70b96208ccf9971b4251fc02","title":"FLAVA: A Foundational Language And Vision Alignment Model"},{"paperId":"21ec90872abd986c12afe39bebe807732ffa70c9","title":"Florence: A New Foundation Model for Computer Vision"},{"paperId":"a7aa150b55d64d339b1c154d6d88455fc3cbc44f","title":"ClipCap: CLIP Prefix for Image Captioning"},{"paperId":"118962f61df9ab6c8310d5a3eb0ab61f22802360","title":"Language bias in Visual Question Answering: A Survey and Taxonomy"},{"paperId":"32d59ab951be74be351f9777da2cbc71bb68c3c1","title":"A Good Prompt Is Worth Millions of Parameters: Low-resource Prompt-based Learning for Vision-Language Models"},{"paperId":"467e5a2164cf78c5be70c91129e1c6e843685fb3","title":"Discovering the Unknown Knowns: Turning Implicit Knowledge in the Dataset into Explicit Training Examples for Visual Question Answering"},{"paperId":"2672777d25562c9df6fc13b653181db62d39bece","title":"An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA"},{"paperId":"4e92fec0a61972ae076707d0630d1333affccdfc","title":"Weakly-Supervised Visual-Retriever-Reader for Knowledge-based Question Answering"},{"paperId":"f5a76442659066434b1bdf480cf11f4f549411ab","title":"QACE: Asking Questions to Evaluate an Image Caption"},{"paperId":"5e00596fa946670d894b1bdaeff5a98e3867ef13","title":"SimVLM: Simple Visual Language Model Pretraining with Weak Supervision"},{"paperId":"b82c5f9efdb2ae56baa084ca41aeddd8a665c1d1","title":"Align before Fuse: Vision and Language Representation Learning with Momentum Distillation"},{"paperId":"01b5412f3d17e90e09226d7c40ad4d4468a1414d","title":"Multimodal Few-Shot Learning with Frozen Language Models"},{"paperId":"63c74d15940af1af9b386b5762e4445e54c73719","title":"VinVL: Revisiting Visual Representations in Vision-Language Models"},{"paperId":"57ed901be5d1b4d853d4f8998dadc1b60e2151f9","title":"On Attention Redundancy: A Comprehensive Study"},{"paperId":"8dce342a435034fa0521b24b61393397df95c095","title":"Multi-Modal Answer Validation for Knowledge-Based VQA"},{"paperId":"7e5008713c404445dd8786753526f1a45b93de12","title":"GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow"},{"paperId":"cb596bffc5c5042c254058b62317a57fa156fea4","title":"Unifying Vision-and-Language Tasks via Text Generation"},{"paperId":"1a9015e511ec3da873f6114eeb542905a92d7d62","title":"KRISP: Integrating Implicit and Symbolic Knowledge for Open-Domain Knowledge-Based VQA"},{"paperId":"1a575075ba357723009a9a8905d5dccf9115ae6c","title":"WeaQA: Weak Supervision via Captions for Visual Question Answering"},{"paperId":"9958887e8dd5f84595818c50fb734b566996541a","title":"ConceptBert: Concept-Aware Representation for Visual Question Answering"},{"paperId":"0030605bfa0a11e7474a8c5ff5b00f3ccdb22b22","title":"Boosting Visual Question Answering with Context-aware Knowledge Aggregation"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"ad5970584754cc7a1d91c95ab84a1e210258183a","title":"UnifiedQA: Crossing Format Boundaries With a Single QA System"},{"paperId":"64a548ca02c8d647358cac809d9c059d34dc4f3a","title":"Radial Graph Convolutional Network for Visual Question Generation"},{"paperId":"818e5cbc337e4e1b98e65a2d7c2d6d2a0318cd57","title":"Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks"},{"paperId":"9915315f5cae822e98c94382ce3b0a6f9a7f8e5e","title":"12-in-1: Multi-Task Vision and Language Representation Learning"},{"paperId":"6c4b76232bb72897685d19b3d264c6ee3005bc2b","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"79c93274429d6355959f1e4374c2147bb81ea649","title":"LXMERT: Learning Cross-Modality Encoder Representations from Transformers"},{"paperId":"65a9c7b0800c86a196bc14e7621ff895cc6ab287","title":"ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks"},{"paperId":"9ecd13dff7ce478d36b2390e28bbf3990a24c751","title":"Generating Question Relevant Captions to Aid Visual Question Answering"},{"paperId":"28ad018c39d1578bea84e7cedf94459e3dbe1e70","title":"OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge"},{"paperId":"1536e8958697c5364f68b2e2448905dbbeb3a0ca","title":"Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering"},{"paperId":"36c3972569a6949ecca90bfa6f8e99883e092845","title":"Pythia v0.1: the Winning Entry to the VQA Challenge 2018"},{"paperId":"4d1c856275744c0284312a3a50efb6ca9dc4cd4c","title":"Know What You Don’t Know: Unanswerable Questions for SQuAD"},{"paperId":"99ad0533f84c110da2d0713d5798e6e14080b159","title":"Looking Beyond the Surface: A Challenge Set for Reading Comprehension over Multiple Sentences"},{"paperId":"29de7c0fb3c09eaf55b20619bceaeafe72fd87a6","title":"Hierarchical Neural Story Generation"},{"paperId":"90873a97aa9a43775e5aeea01b03aea54b28bfbd","title":"Don't Just Assume; Look and Answer: Overcoming Priors for Visual Question Answering"},{"paperId":"7e4b638e028498e900747b600f46cd723f1f231e","title":"Data Augmentation for Visual Question Answering"},{"paperId":"a82c1d1ccaa3a3d1d6ee6677de0eed2e93ddb6e8","title":"Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering"},{"paperId":"915b5b12f9bdebc321e970ecd713458c3479d70e","title":"An Analysis of Visual Question Answering Algorithms"},{"paperId":"26aa6fe2028b5eefbaa40ab54ef725bbbe7d9810","title":"ConceptNet 5.5: An Open Multilingual Graph of General Knowledge"},{"paperId":"7e232313a59d735ef7c8a9f4cc7bc980a29deb5e","title":"Making the V in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering"},{"paperId":"5582bebed97947a41e3ddd9bd1f284b73f1648c2","title":"Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization"},{"paperId":"2c1890864c1c2b750f48316dc8b650ba4772adc5","title":"Stacked Attention Networks for Image Question Answering"},{"paperId":"97ad70a9fa3f99adf18030e5e38ebe3d90daa2db","title":"VQA: Visual Question Answering"},{"paperId":"e6a6b66eeb506dc326e3c3f7f49a1f260469c281","title":"VC-GPT: Visual Conditioned GPT for End-to-End Generative Vision-and-Language Pre-training"},{"paperId":null,"title":"Pseudo-q: Generating pseudo language queries 10875 Authorized licensed use limited to the terms of the applicable license agreement with IEEE. Restrictions apply. for visual grounding"},{"paperId":"4593c88fb33023bec84f8f443d16262810b9047a","title":"CrossVQA: Scalably Generating Benchmarks for Systematically Testing VQA Generalization"},{"paperId":null,"title":"GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"c21a4d70d83e0f6eb2a9e1c41d034842dd561e47","title":"CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge"},{"paperId":null,"title":"Sand-hini 9"},{"paperId":null,"title":"From Images to Textual Prompts: Zero-shot VQA with Frozen Large Language Models A"},{"paperId":null,"title":"If you use this software, please cite it using these metadata"},{"paperId":null,"title":"Success case analysis for VQAv2. Green color indicates answer cues and correct prediction"}],"id":"6edcb09a09c8df43cb62119133df9bb2eb75e5cf","summary":"Img2Prompt is a plug-and-play module that provides the prompts that can bridge the aforementioned modality and task disconnections, so that LLMs can perform zero-shot VQA tasks without end-to-end training."},{"url":"https://www.semanticscholar.org/paper/1e43c7084bdcb6b3102afaf301cce10faead2702","title":"BioBERT: a pre-trained biomedical language representation model for biomedical text mining","venue":"Bioinform.","year":2019,"referenceCount":45,"citationCount":3974,"influentialCitationCount":536,"publicationDate":"25/01/2019","authors":"Jinhyuk Lee,Wonjin Yoon,Sungdong Kim,Donghyeon Kim,Sunkyu Kim,Chan Ho So,Jaewoo Kang","citations":[{"paperId":"d31819413e4eb1b743bf087b61d7914812e66986","title":"A Token-based transition-aware joint framework for multi-span question answering"},{"paperId":"c36399f00993a30d4e7cbc821ff53a50ea05cc97","title":"BioEmoDetector: A flexible platform for detecting emotions from health narratives"},{"paperId":"2f0a28af4cc6b52e2c25246f6c40f2f718ddbb14","title":"Online biomedical named entities recognition by data and knowledge-driven model"},{"paperId":"b0a73d18abcdb250885d02d9f2149b9a65c2f3f5","title":"The landscape of biomedical research"},{"paperId":"5220bc52d5bec334315724f059106b6b9e364fd3","title":"Information bottleneck based knowledge selection for commonsense reasoning"},{"paperId":"f4c24e085cbd115c0ec7223812d388acbf0b2103","title":"Sequence Labelling with 2 Level Segregation (SL2LS): A framework to extract COVID-19 vaccine adverse drug reactions from twitter data"},{"paperId":"6a15aeed098e21cebfff8dfc94a59aaec156b579","title":"Exploring COVID-related relationship extraction: Contrasting data sources and analyzing misinformation"},{"paperId":"775f5deb25a932a5522a94c4518494556c5ea81c","title":"Improving Legal Judgement Prediction in Romanian with Long Text Encoders"},{"paperId":"ca19aaca6adee5877bc49821abb9f9e8a570da1b","title":"Prompt Tuning in Biomedical Relation Extraction"},{"paperId":"839c1c2c9f112dc9040f843e3d65e7a1f13f36d6","title":"VerifiNER: Verification-augmented NER via Knowledge-grounded Reasoning with Large Language Models"},{"paperId":"b0633ccf235e467c35b963ad012f6b8c54aba19f","title":"Editing Factual Knowledge and Explanatory Ability of Medical Large Language Models"},{"paperId":"b09debf8d28fea3214a72dad72e6706ac0fb9dd8","title":"STRING-ing together protein complexes: corpus and methods for extracting physical protein interactions from the biomedical literature"},{"paperId":"af9d159801ab4de2418f2627d7c44070acb35dea","title":"exKidneyBERT: a language model for kidney transplant pathology reports and the crucial role of extended vocabularies"},{"paperId":"2046b2da23eb2f79744eb391d902da9cedf87947","title":"Large Language Models(LLMs) on Tabular Data: Prediction, Generation, and Understanding -- A Survey"},{"paperId":"0c57cb87fcfb345ddc13bab3f20e9c203bac52e9","title":"JMLR: Joint Medical LLM and Retrieval Training for Enhancing Reasoning and Professional Question Answering Capability"},{"paperId":"22871540170409bfef8bbca6b304e98214a8cf5a","title":"ENQUIRE RECONSTRUCTS AND EXPANDS CONTEXT-SPECIFIC CO-OCCURRENCE NETWORKS FROM BIOMEDICAL LITERATURE"},{"paperId":"d28d082520a274b3a845e4ce1007df0327b14223","title":"Leveraging Large Language Models for Clinical Abbreviation Disambiguation"},{"paperId":"26a35eefcbf2d0598e5d9b020030d9dd48d572eb","title":"Enhancing Mental Health Condition Detection on Social Media through Multi-Task Learning"},{"paperId":"e692672bc299848a1c35d953bc025091f371e070","title":"Adaptation of Biomedical and Clinical Pretrained Models to French Long Documents: A Comparative Study"},{"paperId":"a45512e25d9d2bb7412fda77099af46ca2e4a546","title":"Machine learning for antimicrobial peptide identification and design"},{"paperId":"9d4bd6f057fde94e2956a14711655e9044257044","title":"From Text to Transformation: A Comprehensive Review of Large Language Models' Versatility"},{"paperId":"c3b82c397e3bc30942a214b8aca7d861d453a74f","title":"NuNER: Entity Recognition Encoder Pre-training via LLM-Annotated Data"},{"paperId":"5137ea664b392af5eaef8b4c76d7143c69c7ebb0","title":"Fine-tuning BERT models to extract transcriptional regulatory interactions of bacteria from biomedical literature"},{"paperId":"ad9f223ce6ee93ec5042492583ae9ab913ff020c","title":"How Important Is Tokenization in French Medical Masked Language Models?"},{"paperId":"5fb0f16d1a661c3997d3ed3f71fc95f8b8161f1e","title":"How Important is Domain Specificity in Language Models and Instruction Finetuning for Biomedical Relation Extraction?"},{"paperId":"d4cc89b1c38e014b8889ec5eb44734728ffc1784","title":"LocalTweets to LocalHealth: A Mental Health Surveillance Framework Based on Twitter Data"},{"paperId":"b26f66c9bb59811c7d67cdab91a0d65d15364836","title":"An Evaluation of Large Language Models in Bioinformatics Research"},{"paperId":"2d968cbfbf8d5bd40041dc50b9d490642ce47e98","title":"On Leveraging Encoder-only Pre-trained Language Models for Effective Keyphrase Generation"},{"paperId":"f3ababca2fe194bfc7414bbe8ccd74bf23524dc8","title":"Neurosurgical literature classification – Evaluation of three automated methods and time trend analysis of the literature"},{"paperId":"b798cf6af813638fab09a8af6ad0f3df6c241485","title":"Benchmarking Retrieval-Augmented Generation for Medicine"},{"paperId":"2db51d392ba762af3703240325dc74df2c112b8f","title":"BiMediX: Bilingual Medical Mixture of Experts LLM"},{"paperId":"0a1b6d53f66733b584c95885564bfd6aacf22529","title":"InMD-X: Large Language Models for Internal Medicine Doctors"},{"paperId":"3b494b3b7b0b2013c0a036c9aa1868af807f8714","title":"HunFlair2 in a cross-corpus evaluation of biomedical named entity recognition and normalization tools"},{"paperId":"97cabfed8f83f49df8e524fa718d70fa661c3658","title":"A simple but effective span-level tagging method for discontinuous named entity recognition"},{"paperId":"73dd39b7bab172fc01980c966bb2746597bb012d","title":"Efficiency at Scale: Investigating the Performance of Diminutive Language Models in Clinical Tasks"},{"paperId":"13b8934468665ecb586f491d7f9f6c460cb095e5","title":"BioMistral: A Collection of Open-Source Pretrained Large Language Models for Medical Domains"},{"paperId":"fed3376de52d70ba83050182e79466dddde45746","title":"On the Safety Concerns of Deploying LLMs/VLMs in Robotics: Highlighting the Risks and Vulnerabilities"},{"paperId":"3ed112214b8345738d6cb7146c3198f911eb2620","title":"Evaluating Knowledge Fusion Models on Detecting Adverse Drug Events in Text"},{"paperId":"15aad9b72ad278397a467a2f7a2576fff6557c5f","title":"Chinese MentalBERT: Domain-Adaptive Pre-training on Social Media for Chinese Mental Health Text Analysis"},{"paperId":"ee9eb03d95158bc405945abb7070ab45ecc647d7","title":"Integrating ChatGPT into Secure Hospital Networks: A Case Study on Improving Radiology Report Analysis"},{"paperId":"e99766a0aefbac7c9b8b998584c2ca86883055f9","title":"Academic Surgery in the Era of Large Language Models: A Review."},{"paperId":"208f3df65812326b983abe3bf12c84799e4c1709","title":"Named Entity Recognition of Pharmacokinetic parameters in the scientific literature"},{"paperId":"727b058bf0c3c3b5d18f1937783e8c7bbddcd03d","title":"Utilizing BERT for Information Retrieval: Survey, Applications, Resources, and Challenges"},{"paperId":"eca8ea2ed71ea0a2851881a748b0fd9a8540a692","title":"Artificial Intelligence for Literature Reviews: Opportunities and Challenges"},{"paperId":"2dc2d0273903f47e592eb9e693b763c13b2f8818","title":"Deep learning for drug‐drug interaction prediction: A comprehensive review"},{"paperId":"0c2f30064341bd59f25ece5bf6562722a01412e4","title":"A bibliometric analysis of autism spectrum disorder signaling pathways research in the past decade"},{"paperId":"7566981189099487b9a78db4496eacab177eaf27","title":"Crop-GPA: an integrated platform of crop gene-phenotype associations"},{"paperId":"5ca5e2efb0f7bb8111999d6d0d4ee6885aae6e10","title":"Limitations of Language Models in The Oil & Gas Upstream Operations"},{"paperId":"f8b12781543b4d6ade2a16739c87d5b9afce0bd6","title":"A Combined Manual Annotation and Deep-Learning Natural Language Processing Study on Accurate Entity Extraction in Hereditary Disease Related Biomedical Literature."},{"paperId":"310a858da26bca61bc78a9f860304731e63604a2","title":"DAEDRA: A language model for predicting outcomes in passive pharmacovigilance reporting"},{"paperId":"e5481d7b936569040220867f533696af46017b3a","title":"Comparison of Prompt Engineering and Fine-Tuning Strategies in Large Language Models in the Classification of Clinical Notes"},{"paperId":"e708a73ea5f54cd944fb1e4ab36d947bc35aca2c","title":"Pretrained Generative Language Models as General Learning Frameworks for Sequence-Based Tasks"},{"paperId":"88e7ef0fe62ccf6c92d3c3bc8b5f5f66767e2a84","title":"Text-to-Code Generation with Modality-relative Pre-training"},{"paperId":"f2d2220ee6d015bfac03dabd1d8e4d19ec088d85","title":"Advances in materials informatics: a review"},{"paperId":"604200318cc7337ad0f6978ad4976603ce35e7ef","title":"Exploring the performance and explainability of fine-tuned BERT models for neuroradiology protocol assignment"},{"paperId":"ad3c7ec457de8ed0bc1144a8460cc3b2dd9ae323","title":"Drug-drug interaction relation extraction based on deep learning: A review"},{"paperId":"877226d28578f2e7a01bf1ad6c617f919b273d51","title":"Navigating Data Privacy and Analytics: The Role of Large Language Models in Masking conversational data in data platforms"},{"paperId":"3dd1c7e4d30999a51188b7ef960c2380021c2547","title":"Progress and Opportunities of Foundation Models in Bioinformatics"},{"paperId":"3cb5827e5e613fd0a3cd4cc515c2149743a8df79","title":"Leveraging Semantic Text Analysis to Improve the Performance of Transformer-Based Relation Extraction"},{"paperId":"186c7e80de21eda4272687c2ee1542af0e66f20a","title":"KnowLog: Knowledge Enhanced Pre-trained Language Model for Log Understanding"},{"paperId":"361e9549940fc75daeeef3d9b31183cc1cf417da","title":"A Survey on Challenges and Advances in Natural Language Processing with a Focus on Legal Informatics and Low-Resource Languages"},{"paperId":"a8e1c215722f46d3af27310c2819ea509e9db6bc","title":"Large language models assisted multi-effect variants mining on cerebral cavernous malformation familial whole genome sequencing"},{"paperId":"43b8600d11d98449bf7d81fa6b3f8f1c8beb8320","title":"EHR-KnowGen: Knowledge-enhanced multimodal learning for disease diagnosis generation"},{"paperId":"f3edc5a87051be2b4b249c4896c8ab14462c659c","title":"Knowledge-based dynamic prompt learning for multi-label disease diagnosis"},{"paperId":"3c68185557e2c649c7bf4805bad358946f6d11fc","title":"MED-Prompt: A novel prompt engineering framework for medicine prediction on free-text clinical notes"},{"paperId":"2848dfa7d0ae090fcfe4bda208c84510654c4296","title":"SA-MDKIF: A Scalable and Adaptable Medical Domain Knowledge Injection Framework for Large Language Models"},{"paperId":"76b48eb9719e8efa8b1215c6b04d0e57de7de72c","title":"Overview of temporal action detection based on deep learning"},{"paperId":"7ce9de3c54a725f7ed5b3eaa7d8d25d67a68dbda","title":"PolyAMiner-Bulk is a deep learning-based algorithm that decodes alternative polyadenylation dynamics from bulk RNA-seq data"},{"paperId":"3ddd2ac706eceeafe791bfe11dbc0a6ce74e8f11","title":"Natural Language Processing for Radiation Oncology: Personalizing Treatment Pathways"},{"paperId":"3957d4c46e0949907c86d9ef4486ffebbd95fd69","title":"TCGA-Reports: A machine-readable pathology report resource for benchmarking text-based AI models"},{"paperId":"fe810f953db6e83f99e05e8d77fe54d7697ad964","title":"AssistMed project: Transforming cardiology cohort characterisation from electronic health records through natural language processing – Algorithm design, preliminary results, and field prospects"},{"paperId":"e59797967a757420d16e086fcba4be5c7cef4748","title":"Artificial Intelligence in Endodontic Education."},{"paperId":"e2273a2d981cc54b6329d078e4ccc6ab7bf1576a","title":"A label information fused medical image report generation framework"},{"paperId":"cdc8aa048b518cd615adbf1bd81207d17f2f6027","title":"Path-BigBird: An AI-Driven Transformer Approach to Classification of Cancer Pathology Reports"},{"paperId":"1b7cf9e153955d3aaf5a78323c24be8c6d3bef29","title":"FEUDA: Frustratingly Easy Prompt Based Unsupervised Domain Adaptation"},{"paperId":"f6ad16768ea5db9de78bd08a373c3295c016aabd","title":"Are my answers medically accurate? Exploiting medical knowledge graphs for medical question answering"},{"paperId":"bdcc5b4701af025652dec2feab2fe67c3530e2b6","title":"NNOSE: Nearest Neighbor Occupational Skill Extraction"},{"paperId":"8e1c1ef7743f624515fa59528a1dc8bf020c2162","title":"NanoNER: Named Entity Recognition for Nanobiology Using Experts’ Knowledge and Distant Supervision"},{"paperId":"af782b96b72cda6ff057a0d2ef4218676177eb87","title":"Knowledge-Aware Code Generation with Large Language Models"},{"paperId":"ef91db28bc3c301e3d8ef91b361178dbad54c1c1","title":"Credit Risk Meets Large Language Models: Building a Risk Indicator from Loan Descriptions in P2P Lending"},{"paperId":"16625e774006dc55d733c8b6660a01cf6a6ace9f","title":"Mitophagy in Alzheimer’s Disease: A Bibliometric Analysis from 2007 to 2022"},{"paperId":"89abfcec27ae484c368c25b0a1441aaa3bf6e408","title":"What is the Consumer Attitude toward Healthcare Services? A Transfer Learning Approach for Detecting Emotions from Consumer Feedback"},{"paperId":"392024c233bc1ca3a2cf66a2e5185b3d4554060e","title":"The Impact of Snippet Reliability on Misinformation in Online Health Search"},{"paperId":"f4478d04731b4323f6db7f143188f18f2087929e","title":"CERM: Context-Aware Literature-Based Discovery via Sentiment Analysis"},{"paperId":"edb3f38e434b33c360c6c1e1b419f4ecab016c5a","title":"Transfer Learning for the Prediction of Entity Modifiers in Clinical Text: Application to Opioid Use Disorder Case Detection"},{"paperId":"1f98af8fdca3bc61da466ef88fdc64a86de6d422","title":"Almanac - Retrieval-Augmented Language Models for Clinical Medicine."},{"paperId":"0adafc770a0497832fbdd8a5e02aba6f9c984c1b","title":"KIMedQA: towards building knowledge-enhanced medical QA models"},{"paperId":"a0e5697f4250a20d79fd8f033ef105512d4bbd32","title":"Question answering systems for health professionals at the point of care - a systematic review"},{"paperId":"61cadcfa555cbef120df7c017ef02e87f19900b7","title":"Free Form Medical Visual Question Answering in Radiology"},{"paperId":"d376872de9d2420ce1f4a2d4a5a472334d26bdd2","title":"Clinical Information Retrieval: A Literature Review"},{"paperId":"67335a676ae3b4e17a4494edf19b2101484cd5b4","title":"AI-assisted Blockchain-enabled Smart and Secure E-prescription Management Framework"},{"paperId":"d034845314490c8a30ce8d801f87e00c853098fc","title":"Quality of Answers of Generative Large Language Models vs Peer Patients for Interpreting Lab Test Results for Lay Patients: Evaluation Study"},{"paperId":"6ed96d6822a06ad9a735bc09e301bf41df61c534","title":"CheXagent: Towards a Foundation Model for Chest X-Ray Interpretation"},{"paperId":"ec65cc0c0f3186c43ee839e61a4f1ae30acd50e3","title":"Generative AI-Driven Human Digital Twin in IoT-Healthcare: A Comprehensive Survey"},{"paperId":"2fd7d9923014c12477d1673f0f85c144d9502ba9","title":"Evaluating the ChatGPT family of models for biomedical reasoning and classification."},{"paperId":"1c10df8351a6efee2b87b16d405d73ab373f210f","title":"Automated stratification of trauma injury severity across multiple body regions using multi-modal, multi-class machine learning models"},{"paperId":"331c0d54e02b03b72e70b5058c86969ca392e71b","title":"MedLM: Exploring Language Models for Medical Question Answering Systems"},{"paperId":"b2e393581361fb47c6859be832ada7391dc8d4aa","title":"Dynamic Q&A of Clinical Documents with Large Language Models"},{"paperId":"c458eff1b087caa53feaf5c86b912e61fc0f2a97","title":"CrossU-Net: Dual-modality cross-attention U-Net for segmentation of precancerous lesions in gastric cancer."},{"paperId":"ede7ad7eba55bc0e292619e058097040784f8e8d","title":"NeighBERT: Medical Entity Linking Using Relation-Induced Dense Retrieval"},{"paperId":"6202d2a94b29d6602707eac98a17490eb50b2f89","title":"BibSonomy Meets ChatLLMs for Publication Management: From Chat to Publication Management: Organizing your related work using BibSonomy & LLMs"},{"paperId":"cc82790342e20f943f0b47f80175e0e3100a5503","title":"Optimizing classification of diseases through language model analysis of symptoms"},{"paperId":"6c273bde12952c8d58de56ba0a43405c0681bfbc","title":"Unfolding the Transitions in Sustainability Reporting"},{"paperId":"d47a0adfeee0c83299391d6bc205a1adb70d792e","title":"Leveraging External Knowledge Resources to Enable Domain-Specific Comprehension"},{"paperId":"c1046a57e416f6785069f5fa4f7b24b02cd66f7a","title":"Taec: a Manually annotated text dataset for trait and phenotype extraction and entity linking in wheat breeding literature"},{"paperId":"0c281160b7cfba2d652fea297b68317c7539cd37","title":"Assessing Large Language Models in Mechanical Engineering Education: A Study on Mechanics-Focused Conceptual Understanding"},{"paperId":"4be46ae53206440cfa6cab48f7523df5a701a65f","title":"Zero-shot Generative Large Language Models for Systematic Review Screening Automation"},{"paperId":"505aea68b6b9438d98ce0dfa228f69c84bcb9757","title":"Knowledge-Informed Machine Learning for Cancer Diagnosis and Prognosis: A review"},{"paperId":"94fd434cd601d842893af02c68c911c9dd312a5b","title":"Lightweight transformers for clinical natural language processing"},{"paperId":"2a0295009215387aa85add9b629b5ec2e3b9c7d6","title":"Hybrid architecture based intelligent diagnosis assistant for GP"},{"paperId":"07e6b56936fbbc83675344f2ec85ed6442e7433c","title":"TwinBooster: Synergising Large Language Models with Barlow Twins and Gradient Boosting for Enhanced Molecular Property Prediction"},{"paperId":"8551dc1842deb251f183fc9ebe38a726b1f1665b","title":"Empirical Analysis of Efficient Fine-Tuning Methods for Large Pre-Trained Language Models"},{"paperId":"fcb232cc3c7b4b1aaee9cce538f5ca8247d4df17","title":"A Span-based Model for Extracting Overlapping PICO Entities from RCT Publications"},{"paperId":"e6590f833cbccded3002467f8f0ec8479062bead","title":"Taxonomy Mining from a Smart City CMS using the Multidimensional Knowledge Representation Approach"},{"paperId":"18d9b13e3383d98c181f4d7a2b3ca1503ed707a0","title":"No-boundary thinking for artificial intelligence in bioinformatics and education"},{"paperId":"c9c322288f06976b11879baf566a8de040cf5e35","title":"German Text Embedding Clustering Benchmark"},{"paperId":"1347323c30180b5d5528abace9d36ba0d2dbf3c1","title":"ACP-ESM: A novel framework for classification of anticancer peptides using protein-oriented transformer approach"},{"paperId":"00b68a19a60a0611df19da97365b2cf40c6224b0","title":"BLINKtextsubscriptLSTM: BioLinkBERT and LSTM based approach for extraction of PICO frame from Clinical Trial Text"},{"paperId":"a76fdc630ccd8275ee48475c0224f307c48a7aba","title":"A patient safety knowledge graph supporting vaccine product development"},{"paperId":"93886752191db25efd096a65af7b09df5c0a64e0","title":"Data-Centric Foundation Models in Computational Healthcare: A Survey"},{"paperId":"a1c04034df40cbe6f0d0004d456fdab807ae63ec","title":"Generalist embedding models are better at short-context clinical semantic search than specialized embedding models"},{"paperId":"053487dd273f9bd03ab42afd8d713db579dcbeee","title":"Contextual Word Embedding for Biomedical Knowledge Extraction: a Rapid Review and Case Study"},{"paperId":"7db68d2282d03690c28a24004f0b5a88f23d4de7","title":"Freeze the backbones: A Parameter-Efficient Contrastive Approach to Robust Medical Vision-Language Pre-training"},{"paperId":"15da03bb202a9504d13c9e106d3b630d6392cacc","title":"VALD-MD: Visual Attribution via Latent Diffusion for Medical Diagnostics"},{"paperId":"595dd3553e3233f461b8ef4f71d4023f723f3376","title":"Artificial intelligence generated content (AIGC) in medicine: A narrative review."},{"paperId":"938dde7822cc914610c2f8681396c2c90bf3a081","title":"Entity Recognition from Colloquial Text"},{"paperId":"0f02e3c514a57bdeb224d1a24d206f7b3cc27885","title":"A hierarchical convolutional model for biomedical relation extraction"},{"paperId":"78a67ff8654a08a9c36e66190bb1474234e4e151","title":"Revealing the technology development of natural language processing: A Scientific entity-centric perspective"},{"paperId":"23170f794747145d3c56baddd9584e061e474f8b","title":"A study on pharmaceutical text relationship extraction based on heterogeneous graph neural networks."},{"paperId":"0d9bfeab1a38aa770b4237ebf89ba76bf9206ca3","title":"Predicting Anti-microbial Resistance using Large Language Models"},{"paperId":"f815ad5532d79a6bd0126826a09055750fec4447","title":"BactInt: A domain driven transfer learning approach for extracting inter-bacterial associations from biomedical text."},{"paperId":"6b3acd49f16b2a5d68337b7160955e2d226fbb0b","title":"An empirical assessment of different word embedding and deep learning models for bug assignment"},{"paperId":"e5d7159adb4458df938181d0e40edc1cbeea4551","title":"A topical review on AI-interlinked biodomain sensors for multi-purpose applications"},{"paperId":"ab0eb6cab97ee63ca1ab16a2e468cd1326449088","title":"Industry-sensitive language modeling for business"},{"paperId":"518f9b3aba34bb08a4f20b42423e16403cc1bf02","title":"Semantics-enabled Biomedical Literature Analytics."},{"paperId":"50cf2ce0ba9b33fbe3bee22921ceefcc98ab4c12","title":"T4SEpp: A pipeline integrating protein language models to predict bacterial type IV secreted effectors"},{"paperId":"5fdd837b9ee9e7a6507dc30a0bc0d6d4b8d4d7c1","title":"Child-Sum (N2E2N)Tree-LSTMs: An Interactive Child-Sum Tree-LSTMs to Extract Biomedical Event"},{"paperId":"f47125522490eb5dfe38d4b35289af27b4f9c9a1","title":"Validation of a Zero-shot Learning Natural Language Processing Tool to Facilitate Data Abstraction for Urologic Research."},{"paperId":"032f11733df3d4aa517911fd4e74d0611feda846","title":"Do Managers Always Make Optimal Investment Decisions: Relationship between Managerial Optimism Measured by BERT and Investment Decision-making"},{"paperId":"3fad6e45cf9d3c063079cd4042d698448b383cf7","title":"HSC-GPT: A Large Language Model for Human Settlements Construction"},{"paperId":"a4715887bd5a4c328ebf1d6ebb18ab94e71ee8d8","title":"MosaicBERT: A Bidirectional Encoder Optimized for Fast Pretraining"},{"paperId":"73757c0e4b2f28eaa73ac036694468fd6c811e22","title":"One Model to Rule them All: Towards Universal Segmentation for Medical Images with Text Prompts"},{"paperId":"c21c59740ad75e0d8164fa4355ba0317f9c65530","title":"Empowering Transformers for Evidence-Based Medicine"},{"paperId":"d93f1475212f14bd3c778b9f7f82f9dc0ee6c740","title":"T cell receptor binding prediction: A machine learning revolution"},{"paperId":"e2960506f5e69fe2b8e5bbf67025580e54431e39","title":"Safeguarding Mail-Order DNA Synthesis in the Age of Artificial Intelligence"},{"paperId":"fba6329fc9b1da747892efae8b89f4ef6d69fe21","title":"Mining literature and pathway data to explore the relations of ketamine with neurotransmitters and gut microbiota using a knowledge-graph"},{"paperId":"efd916a6e1f320a11715b2fbadd1dd7b9abcc9f7","title":"Improved Weighting in the Automated Texts Classification using Fuzzy Method"},{"paperId":"92aabfa1e0e174686c1c5190518136eb4d40f1e2","title":"Inference of Dependency Knowledge Graph for Electronic Health Records"},{"paperId":"11df06751c4d03c5cb70faccc4c3c25f415d6900","title":"Multi-level biomedical NER through multi-granularity embeddings and enhanced labeling"},{"paperId":"9b996cdf31e1078cc38dedb4980f42a5ca8fd10a","title":"Robust Knowledge Extraction from Large Language Models using Social Choice Theory"},{"paperId":"c7809b89c648012f28b272ed1bb91175f08426c8","title":"Collaborative Synthesis of Patient Records through Multi-Visit Health State Inference"},{"paperId":"99973c3830ba8fc8f4915f6fc4859e024c189a68","title":"The Influence of Teacher Professional Competence on Student Learning Motivation in PAI Subjects"},{"paperId":"fb8a1050c3a281fcaf20eb4526918503a6da5b0f","title":"Cross-Lingual Sentiment Analysis: Comparative Study of Opinion Expression Across Different Languages"},{"paperId":"97a868f12c123d637f667eb99633591db189b2d9","title":"Diversifying Knowledge Enhancement of Biomedical Language Models using Adapter Modules and Knowledge Graphs"},{"paperId":"8596a700705502563f5549fa005014cfff2b675d","title":"Domain-Specific Code Language Models: Unraveling the Potential for HPC Codes and Tasks"},{"paperId":"0a0b121dcc127be734c5199d121946bbe8b1ae5d","title":"Quantifying confidence shifts in a BERT-based question answering system evaluated on perturbed instances"},{"paperId":"744e0fa91081389e1249ccd04ce0d6289a6dd25f","title":"BioEGRE: a linguistic topology enhanced method for biomedical relation extraction based on BioELECTRA and graph pointer neural network"},{"paperId":"b87906b53933a8e442c54bd448821f9a869c6332","title":"PowerPulse: Power energy chat model with LLaMA model fine‐tuned on Chinese and power sector domain knowledge"},{"paperId":"352252231462c24440bc0016638ea5fe8d4c6f7e","title":"UniDCP: Unifying Multiple Medical Vision-language Tasks via Dynamic Cross-modal Learnable Prompts"},{"paperId":"4739e96fb40091b0d15bd22750b143e1fcef05c7","title":"Using BERT to Identify Causal Structure in Students’ Scientific Explanations"},{"paperId":"8bbcc26b4b8edf730f1b19997b318e9e63e80159","title":"LLM Instruction-Example Adaptive Prompting (LEAP) Framework for Clinical Relation Extraction"},{"paperId":"b667b389647f3bbccaedfa6a60b1bc50af246795","title":"A compressed large language model embedding dataset of ICD 10 CM descriptions"},{"paperId":"9d0cd8d210b5f9032837b04b173975397a1a31b2","title":"ConfliBERT-Spanish: A Pre-trained Spanish Language Model for Political Conflict and Violence"},{"paperId":"6cb3e86b3b518fb179887e49ae7e5b73153c17c3","title":"Enabling Dataspaces Using Foundation Models: Technical, Legal and Ethical Considerations and Future Trends"},{"paperId":"43a7ed002d1a1b822a0f3a0361cdc37d0d4c9a8e","title":"A study of deep active learning methods to reduce labelling efforts in biomedical relation extraction"},{"paperId":"0f49334fc5de373dfdb22d12a5c62a7732c5b511","title":"BUILD-KG: Integrating Heterogeneous Data Into Analytics-Enabling Knowledge Graphs"},{"paperId":"a6c8af2e21439ce62efc3b21e2b0c05bf178db62","title":"TreeBERT: Advanced Representation Learning for Relation Extraction"},{"paperId":"931623f332af66b84527f92648dbb5cd0a334a8b","title":"Towards automatic identification of self-reported COVID-19 tweets: Introducing a multilingual manually annotated dataset, baseline systems and exploratory evaluations"},{"paperId":"a3039c1780bc06c259637190e630025c6a87532e","title":"Integrating a PICO Clinical Questioning to the QL4POMR Framework for Building Evidence-Based Clinical Case Reports"},{"paperId":"7d0638ce1f0980360840b2f9c51fc23d5e8ba154","title":"CovPTM : COVID-19 PreTrained Model Generic Model vs Biomedical Model Adaptation"},{"paperId":"9bdb2c918b91500356898a2432b0746b017f4613","title":"Achieving Seamless Semantic Interoperability and Enhancing Text Embedding in Healthcare IoT: A Deep Learning Approach with Survey"},{"paperId":"1b6ae238f48514288ffc6cb5716e09ababeec791","title":"Detecting Signs of Depression from Social Media Texts using Generalized Autoregressive Pretraining Transformer Model"},{"paperId":"d9403061e3a3392652d037138532fab111c845c1","title":"Large language models in healthcare and medical domain: A review"},{"paperId":"33ac04c55ebcfa7a6dbc47514922bfb5cfeecbab","title":"Large Language Models are Clinical Reasoners: Reasoning-Aware Diagnosis Framework with Prompt-Generated Rationales"},{"paperId":"6bdfffbf92d01c8b543088d40d46233610e469a8","title":"CLIP in Medical Imaging: A Comprehensive Survey"},{"paperId":"21b3df7a067ed40c2f325df020846ed42bfd7ffc","title":"GWAS From Spoken Phenotypic Descriptions: A Proof of Concept From Maize Field Studies"},{"paperId":"9ba61be98dfbd865dc0a1bb501b55c152a46570d","title":"BioKG: a comprehensive, large-scale biomedical knowledge graph for AI-powered, data-driven biomedical research"},{"paperId":"8081cadfdd6c34627ec7584f8718a11a0ddf499b","title":"Bidirectional Encoder Representations from Transformers-like large language models in patient safety and pharmacovigilance: A comprehensive assessment of causal inference implications."},{"paperId":"91353067c8f4e0ef81a031ab8e8c66bb8eef0080","title":"Improving dictionary-based named entity recognition with deep learning"},{"paperId":"338de438ce922fc154163d26ae761194855f69a3","title":"Ensemble Text Summarization Model for COVID-19-Associated Datasets"},{"paperId":"eb5a139538e6e74591be9d613ab8b625c7562ee6","title":"Knowledge-Based Intelligent Text Simplification for Biological Relation Extraction"},{"paperId":"fa427de71ff8602c9f4b766320fd8e340fbc1553","title":"Advancing Clinical Text Summarization through Extractive Methods using BERT-Based Models on the NBME Dataset"},{"paperId":"baa0ebbe528e035e05b1bb4bc620ef1e8b234353","title":"Integrating PubMed Label Hierarchy Knowledge into a Complex Hierarchical Deep Neural Network"},{"paperId":"f18560a6aee793bb9b4e4a1974cf4d061cedbbaf","title":"Labrador: Exploring the Limits of Masked Language Modeling for Laboratory Data"},{"paperId":"1bd7b90c9d92b3714e77d5bb1bcc9a190757afb2","title":"Dictionary-based matching graph network for biomedical named entity recognition"},{"paperId":"7e55d8701785818776323b4147cb13354c820469","title":"PaperQA: Retrieval-Augmented Generative Agent for Scientific Research"},{"paperId":"7ae88d10358347e4258269184f78991d18cad71d","title":"Deep Multimodal Fusion for Surgical Feedback Classification"},{"paperId":"1aa49b3622668d0500aa783e9a28bfb08e4b9ce6","title":"OpenDeID Pipeline for Unstructured Electronic Health Record Text Notes Based on Rules and Transformers: Deidentification Algorithm Development and Validation Study"},{"paperId":"413880f17759ffefbc15f0fdf81a83ffa15aa684","title":"Corporate Bankruptcy Prediction with BERT Model"},{"paperId":"c40717f5527c82acb9eb63634ce3aa67e2c9ad96","title":"Medical Extractive Question-Answering Based on Fusion of Hierarchical Features"},{"paperId":"e8915fb8c2d3e06b705e0ee5c3afe61420e529b8","title":"Fine-Grained and Complex Food Entity Recognition Benchmark for Ingredient Substitution"},{"paperId":"67239d6e9c2c5f8a6d19cb35154e5aa7eaa00f51","title":"Large Language Models on Graphs: A Comprehensive Survey"},{"paperId":"539ef391f944818b1158ea0e3f839ba43ccdccd2","title":"Topic-BiGRU-U-Net for Document-level Relation Extraction from Biomedical Literature"},{"paperId":"544ff0ab5255a1b4dfdbe2c3bf80faeec676adc9","title":"EGDE: A Framework for Bridging the Gap in Medical Zero-shot Relation Triplet Extraction"},{"paperId":"c07a58f82e6b3ee99af1e253fb1b69fa1b512c35","title":"Multimodal reasoning for nutrition and human health via knowledge graph embedding"},{"paperId":"ec3ef9bbbdf11d08f27fab35d1487acb6c2bc221","title":"Cascade Decoding for Antibiotic Resistance Event Extraction Based on Contrastive Learning"},{"paperId":"ceec4e2f08777997ba41132a5c6b397f54c3a569","title":"Joint Learning-based Multiple Documents Heterogeneous Graph Inference for Biomedical Entity Linking"},{"paperId":"20ae8ff81d9d87b7842272c6bff4233515167b4b","title":"Radiology Report Generation via Structured Knowledge-Enhanced Multi-modal Attention and Contrastive Learning"},{"paperId":"6eab13697a1a6f6c584d65c3c772d935d27f2b11","title":"Mining disease-associated genes based on heterogeneous graph transformer"},{"paperId":"397a11d4f5b31424d1ac5cf49732521c0b66d620","title":"Fine-tuning a pre-trained Transformers-based model for gene name entity recognition in biomedical text using a customized dataset: case of Desulfovibrio vulgaris Hildenborough"},{"paperId":"0dfbd9adbef108ab33e244f1ebc168835db2b235","title":"Machine learning based system for the automation of systematic literature reviews"},{"paperId":"9bddee9564053de6ab0a547fc46afc10eb7fe035","title":"Enhancing Clinical Outcome Predictions through Auxiliary Loss and Sentence-Level Self-Attention"},{"paperId":"6c09cb19cf424673911bc97bdaf6a17eda4012a2","title":"Biomedical Causal Relation Extraction via Data Augmentation and Multi-source Knowledge Fusion"},{"paperId":"6c8c36a790d2dded8b0cf8e1ecb8b44403204333","title":"An Effective Microbial–drug Relation Extraction Model Based on Dual Graph Convolutional Networks"},{"paperId":"bb4d212304221089b49612bdc4b1b9d898f77996","title":"A Tree-structured Neural Network Model for Joint Extraction of Adverse Drug Events"},{"paperId":"979ffb184d0b2945d1dfedff6157e531e98e7abe","title":"Survey and Experiments on Biomedical Pre-Trained Language Models for Named Entity Recognition"},{"paperId":"f87e82627e4c683fd49fec5c158e716722b9dfa7","title":"An Early Depression Detection Model on Social Media using Emotional and Causal Features"},{"paperId":"d0c05cc8cc062eae3d4671361d3762bbf41052f5","title":"Disease Diagnosis based on Multiple Semantic Relationship Prompt Subgraph"},{"paperId":"f5cc6df1a7d00a5ed93bee3db2e5df00c338a936","title":"LLMs Accelerate Annotation for Medical Information Extraction"},{"paperId":"f2e5d96c745b29e5d062a89b6c43810bdcb69969","title":"A medical multimodal large language model for future pandemics"},{"paperId":"68b9033e42088b58c4ef85e68ac62db2e9b4cb02","title":"Beyond rating scales: With targeted evaluation, large language models are poised for psychological assessment"},{"paperId":"47e674257870f781d918d3c59cc172d1c86e7c97","title":"Deep learning for report generation on chest X-ray images"},{"paperId":"8c882a0c55a87ff4a120b13c052608868f3b9f28","title":"Enhancing phenotype recognition in clinical notes using large language models: PhenoBCBERT and PhenoGPT"},{"paperId":"45f0808ef12ef299038be04d4c45cc2589baa52d","title":"A natural language processing system for the efficient updating of highly curated pathophysiology mechanism knowledge graphs"},{"paperId":"ab8ee4af2f0f30fe1ca080278d2a333ae1c7d3f2","title":"Automated clinical knowledge graph generation framework for evidence based medicine"},{"paperId":"2bf0b1caa891803b899859248970f0a48f6b2c19","title":"HALD, a human aging and longevity knowledge graph for precision gerontology and geroscience analyses"},{"paperId":"51fb6598a3ebe36b371b096b4824d718e6e527fb","title":"The Efficiency Spectrum of Large Language Models: An Algorithmic Survey"},{"paperId":"a4f95e1bcb0f7fe57e02dae86b74a8af2da64656","title":"BERT based natural language processing for triage of adverse drug reaction reports shows close to human-level performance"},{"paperId":"e1da20e6da3786931e9b0084813967d954c6508a","title":"Context is not key: Detecting Alzheimer’s disease with both classical and transformer-based neural language models"},{"paperId":"db7f52834908e3cac56e8a010cf775b69bc7a08a","title":"Explanatory Argument Extraction of Correct Answers in Resident Medical Exams"},{"paperId":"f900b983ca0eb1540fff1cba37d6b80d0743db13","title":"Entity recognition method for airborne products metrological traceability knowledge graph construction"},{"paperId":"ebb5dccb2f00c918ec5a165de5f1c09413a84935","title":"SOAP classifier for free-text clinical notes with domain-specific pre-trained language models"},{"paperId":"52bc4893339de8e68daa29492d2500946646d535","title":"EMoDi: Entity-Enhanced Momentum-Difference Contrastive Learning for Semantic-Aware Verification of Scientific Information"},{"paperId":"11f7383957a274f0c4878099f5780014b481ceaa","title":"Building knowledge graphs from technical documents using named entity recognition and edge weight updating neural network with triplet loss for entity normalization"},{"paperId":"5faa163cbd1651e5325cacba93f6b4d5fbcda6f0","title":"Biomedical knowledge graph-enhanced prompt generation for large language models"},{"paperId":"0e733bbb87f4406800c13b2b5439641368937750","title":"Gene-MOE: A sparsely gated prognosis and classification framework exploiting pan-cancer genomic information"},{"paperId":"cde42399a756854ad0997ac39f9f84231533efdc","title":"Ascle: A Python Natural Language Processing Toolkit for Medical Text Generation"},{"paperId":"6481d1a88ff3a180758c619ec69366f75fda62d7","title":"A machine learning-enabled open biodata resource inventory from the scientific literature"},{"paperId":"2b3554a8fea6f123fc04bd3e120f2293f227e1b2","title":"InstructMol: Multi-Modal Integration for Building a Versatile and Reliable Molecular Assistant in Drug Discovery"},{"paperId":"99dd3d58f0074aa1221b660927327b345935e60a","title":"SCREENER: Streamlined collaborative learning of NER and RE model for discovering gene-disease relations"},{"paperId":"4ed3854943390b9cf991bd877496b3822a71c59d","title":"A survey of consumer health question answering systems"},{"paperId":"82009c0a288eb86eae72a4c1c14b72b3cba1afa4","title":"Leveraging deep active learning to identify low-resource mobility functioning information in public clinical notes"},{"paperId":"ff5f0c5b6905a8c4b361a625b450e9ab417fa854","title":"MEDITRON-70B: Scaling Medical Pretraining for Large Language Models"},{"paperId":"e9e47833edacd4dbb5d3b3272fee4c11c5d41aaf","title":"Solving the Right Problem is Key for Translational NLP: A Case Study in UMLS Vocabulary Insertion"},{"paperId":"9c586fe742c3820667a04e613a8b81430a6859a8","title":"AptaBERT: Predicting aptamer binding interactions"},{"paperId":"460be63bf6876b947168a7024d3cda73e4cf5459","title":"BIR: Biomedical Information Retrieval System for Cancer Treatment in Electronic Health Record Using Transformers"},{"paperId":"eab81517f393f0e9515f89b164910d8482cdd196","title":"Identifying the potential miRNA biomarkers based on multi-view networks and reinforcement learning for diseases"},{"paperId":"d733c2d6e08f5b59ccc0af9188f1f86d0aa7a4c5","title":"nach0: Multimodal Natural and Chemical Languages Foundation Model"},{"paperId":"0c8a630657a2cf5dea41472a9b5e20544ce2bd56","title":"Taiyi: A Bilingual Fine-Tuned Large Language Model for Diverse Biomedical Tasks"},{"paperId":"4a6c1ee10c448840f598a281374496f6ebe11b5c","title":"Adapt in Contexts: Retrieval-Augmented Domain Adaptation via In-Context Learning"},{"paperId":"29fa031268ffe0a80f7a4e3fb3a135db7fcd0b8d","title":"KBioXLM: A Knowledge-anchored Biomedical Multilingual Pretrained Language Model"},{"paperId":"cd370a4b149a3ffcebc8f2973952e82f723a973b","title":"Unifying Corroborative and Contributive Attributions in Large Language Models"},{"paperId":"ed9dfde95319125922166a6ea64fdcde226e09d3","title":"Automated classification of lay health articles using natural language processing: a case study on pregnancy health and postpartum depression"},{"paperId":"31a748f9edfc69054f879a94a63a2b350ee42231","title":"TarBase-v9.0 extends experimentally supported miRNA–gene interactions to cell-types and virally encoded miRNAs"},{"paperId":"d7b4420226d01fa05a72a9d4f813f94a7032576d","title":"Leveraging Generative AI for Clinical Evidence Summarization Needs to Ensure Trustworthiness"},{"paperId":"6a149970c5f5b69ff777dd5f5a8d632d94d88091","title":"Prompting Meta-Learned Hierarchical Graph Network for Molecular Property Prediction"},{"paperId":"6f77613b4ecfa5c96942ede6a9a38f8580df831a","title":"A Knowledge-Enhanced Medical Named Entity Recognition Method that Integrates Pre-Trained Language Models"},{"paperId":"aae2e3a53ec7cdcff9898b11a7f73859206b89a1","title":"Zero-Shot Construction of Chinese Medical Knowledge Graph with ChatGPT"},{"paperId":"5853a640b2e1453661e978072892f4602eb3f48f","title":"Advancing COVID-19 Inquiry Responses Through Transfer Learning-Based Question Entailment Methodology"},{"paperId":"55cc92db1633e9155ea70c88264bd7a5bf35a5f5","title":"ClotCatcher: a novel natural language model to accurately adjudicate venous thromboembolism from radiology reports"},{"paperId":"eaa55bc8b654b038ecec15c2dad64ce32ae99806","title":"Source Prompt: Coordinated Pre-training of Language Models on Diverse Corpora from Multiple Sources"},{"paperId":"91ae1eadf9336fb613cb6a27ac0a2bc6046dbd41","title":"German FinBERT: A German Pre-trained Language Model"},{"paperId":"a817263affeed3fc11937b8e3e3f3ef7f04680c8","title":"GO2Sum: Generating Human Readable Functional Summary of Proteins from GO Terms"},{"paperId":"99be42f807311159efeb19f89fe0f0a6c3e97861","title":"An Eye on Clinical BERT: Investigating Language Model Generalization for Diabetic Eye Disease Phenotyping"},{"paperId":"74fe68e7a89390fdbb29a8546139ef9341aa979c","title":"An Entity Extraction pipeline for Medical Text Records Utilizing Large Language Models: An Analytical Study (Preprint)"},{"paperId":"3f5b5613de10ec59c87cc3c7b2b512e8ff3a772e","title":"Natural Language Processing for Financial Regulation"},{"paperId":"ed3682e646acb412823d60f0b7c736398ecb9b38","title":"PolyIE: A Dataset of Information Extraction from Polymer Material Scientific Literature"},{"paperId":"d3f96583a2f441f627b372e9452466eafa89656f","title":"Learning Knowledge-Enhanced Contextual Language Representations for Domain Natural Language Understanding"},{"paperId":"0d2e5c45f7bc8c9f58f4bc48d5295665a2417e56","title":"ADHD-KG: a knowledge graph of attention deficit hyperactivity disorder"},{"paperId":"24c3375fb3e18faf2356e0f2234f18c029ec1188","title":"PECoP: Parameter Efficient Continual Pretraining for Action Quality Assessment"},{"paperId":"aef5bea83f7f000e7bbdd81483220baeb5393014","title":"Complementary and Integrative Health Information in the literature: its lexicon and named entity recognition"},{"paperId":"cf90f4d2bea741281db659946598842e7c344a92","title":"Automatic Report Generation for Histopathology images using pre-trained Vision Transformers"},{"paperId":"58cf5b3f5b6cef67f0f0a2a909eb2efcb09e27ac","title":"Biomedical Named Entity Recognition Based on Residual Network and Global Context Mechanism"},{"paperId":"4bf576ff688724c7d2fc0ee759969a1dc37eb9de","title":"Labor Space: A Unifying Representation of the Labor Market via Large Language Models"},{"paperId":"04c3f1debbba3a856f26c58edc585d8688139740","title":"Legal-HNet: Mixing Legal Long-Context Tokens with Hartley Transform"},{"paperId":"0a898a2772c7c2941b83a737f1bef806710b047a","title":"Evaluating multiple large language models in pediatric ophthalmology"},{"paperId":"ade0ab81403cd1dbe2a49d76c0393dbd615fd979","title":"Principles from Clinical Research for NLP Model Generalization"},{"paperId":"65f22498157efa395c6feef7d88f948519c1a9d2","title":"Adapting Pre-trained Generative Models for Extractive Question Answering"},{"paperId":"b26a44eac3fb2a583846c99e594d841525bacbe7","title":"Injecting Categorical Labels and Syntactic Information into Biomedical NER"},{"paperId":"d587027861ae2ed5049b3efc71f1a08eeeca93f9","title":"Spoken Dialogue System for Medical Prescription Acquisition on Smartphone: Development, Corpus and Evaluation"},{"paperId":"f8c86413b528815e0749d0d7f6791fb31a60ba0c","title":"Using natural language processing to extract plant functional traits from unstructured text"},{"paperId":"283fad39ca746f10d2bcc444c1d80c9bd2a4132c","title":"Fine-tuning Strategies for Domain Specific Question Answering under Low Annotation Budget Constraints"},{"paperId":"e15007216ee8c4c490c84e11b046b3efbf6e4b92","title":"Automatic literature screening using the PAJO deep-learning model for clinical practice guidelines"},{"paperId":"b74be6891cd7f6d81e346852494c08528dbffdc4","title":"TCM-GPT: Efficient Pre-training of Large Language Models for Domain Adaptation in Traditional Chinese Medicine"},{"paperId":"c33416dfb9a00224b30db3faa6f61a03c9119cdf","title":"Investigating Deep-Learning NLP for Automating the Extraction of Oncology Efficacy Endpoints from Scientific Literature"},{"paperId":"6cb53afdc2b21236c3957efd4bbeb2aa65338c50","title":"An Interdisciplinary Outlook on Large Language Models for Scientific Research"},{"paperId":"ba6da0b434a0ea8f6e8ff73ba03d83933e1e2478","title":"Heterogeneous deep graph convolutional network with iterative deep graph learning for Covid-19 inline recommendation"},{"paperId":"9c93f65f5bffad8362537b61310babbc5f1af3ac","title":"DeepSA: a deep-learning driven predictor of compound synthesis accessibility"},{"paperId":"4a2a300aee196aff6f490c148a3966eb9a1b20d4","title":"Better with Less: A Data-Active Perspective on Pre-Training Graph Neural Networks"},{"paperId":"18d44e0bad89b512de308dfedf3105d5a0775a9f","title":"pathCLIP: Detection of Genes and Gene Relations from Biological Pathway Figures through Image-Text Contrastive Learning"},{"paperId":"441dc2cf846ab6c9bc75e6e7e8de92c62735e754","title":"Preserving the knowledge of long clinical texts using aggregated ensembles of large language models"},{"paperId":"944bca64fdd8fd068bf59ecf5f99e692cbfa5ba1","title":"Low-Resource Named Entity Recognition: Can One-vs-All AUC Maximization Help?"},{"paperId":"74bab08a1aff7e0e55f6da47535774e079a232ef","title":"Hierarchical Classification System for Breast Cancer Specimen Report (HCSBC) - an end-to-end model for characterizing severity and diagnosis"},{"paperId":"7d1c642f4de2cbe5a43d31d2b1961b91239cd1be","title":"A survey of the recent trends in deep learning for literature based discovery in the biomedical domain"},{"paperId":"26b7e742fbad6bf67567039fcd3d0ec95d271708","title":"RxBERT: Enhancing drug labeling text mining and analysis with AI language modeling."},{"paperId":"4d5e44c257d92c49c51edd8bddff2c624c926fcf","title":"Deep learning-enabled natural language processing to identify directional pharmacokinetic drug–drug interactions"},{"paperId":"f5f8b3f79193d1302a3c7aa58696fd4d507d6cb3","title":"IDPpub: Illuminating the Dark Phosphoproteome Through PubMed Mining"},{"paperId":"ac3754287e8d2718d01f99ef83c48fb5a1331633","title":"Serial KinderMiner (SKiM) discovers and annotates biomedical knowledge using co-occurrence and transformer models"},{"paperId":"ed12a60f8c58319fc0a5536f62f193ac50e1ed84","title":"Deep purified feature mining model for joint named entity recognition and relation extraction"},{"paperId":"df22e98223d9c8adddfc7eee061c33042214cef9","title":"Multilingual bi‐encoder models for biomedical entity linking"},{"paperId":"a6432dca42d8c3d9627a65b3fc337f043e865746","title":"AdaSent: Efficient Domain-Adapted Sentence Embeddings for Few-Shot Classification"},{"paperId":"24928fa6efa495cc8828c015bc87c14f32ad6452","title":"Prediction of High-Risk Donors for Kidney Discard and Nonrecovery Using Structured Donor Characteristics and Unstructured Donor Narratives."},{"paperId":"77fc0ed29a5d9dfc99c00401c9ac920c4b73e07d","title":"A cross-modal clinical prediction system for intensive care unit patient outcome"},{"paperId":"ddd0e8917ee5ab020c6b63a1d55e09043b154adb","title":"One Hot (Up)Take: Vision, Language, and Domain Knowledge in PET/CT Reporting."},{"paperId":"63d2268d1c1bab53f59a378d0e63b113ee9d373a","title":"Integration of multiple terminology bases: a multi-view alignment method using the hierarchical structure"},{"paperId":"3e99d92eb2193a5bd94ab39fb2be9af709e88544","title":"BioBERT-based SNP-trait Associations Extraction from Biomedical Literature"},{"paperId":"1422ccf1d62a9c1192436e89487ae5d350a2a2fa","title":"Enhancing pre-trained contextual embeddings with triplet loss as an effective fine-tuning method for extracting clinical features from electronic health record derived mental health clinical notes"},{"paperId":"8b8b4ca8141f78f09962aa8a5a40691bd5acf4c8","title":"The SourceData-NLP dataset: integrating curation into scientific publishing for training large language models"},{"paperId":"49823523acaafbab64391a105a945045f1b0efd5","title":"Evidence-based clinical engineering: Health information technology adverse events identification and classification with natural language processing"},{"paperId":"8dd5ef8f047a2c74a30cc21d12b82c80232aa41d","title":"Interactive Multi-fidelity Learning for Cost-effective Adaptation of Language Model with Sparse Human Supervision"},{"paperId":"8a42f1d55656d1e8df9527747fefc4eacd6904ca","title":"Learning Meta Soft Prompt for Few-Shot Language Models"},{"paperId":"b7db959f3b7e6d0feffc0332b91400f0ac6cacb8","title":"Split-NER: Named Entity Recognition via Two Question-Answering-based Classifications"},{"paperId":"458915f659dd77f0306a53e6c5c7be30d1352f62","title":"Taking Off with AI: Lessons from Aviation for Healthcare"},{"paperId":"defe18a0a862b2024f1ea1671edd8b086a053c98","title":"BioInstruct: Instruction Tuning of Large Language Models for Biomedical Natural Language Processing"},{"paperId":"0340d6e882bb47674500d2304696f287bf89786a","title":"Semantic harmonization of Alzheimer’s disease datasets using AD-Mapper"},{"paperId":"d6bf6d570283423b968f195c7faa3fb80b62e405","title":"Sentence Bag Graph Formulation for Biomedical Distant Supervision Relation Extraction"},{"paperId":"ddbb30487dede4836aa9fd62acbee3247e2a260a","title":"Artificial Intelligence for Surface-Enhanced Raman Spectroscopy."},{"paperId":"14f18bed093405e9c80a920d679702c67a93d698","title":"Prediction of coronary heart disease risk based on multimodal EHRs"},{"paperId":"dd6458b69886ace4be33121804a4832e5f65f26b","title":"Contextualizing Answer Ranking: Embracing Multi-dimensional Enhanced Knowledge"},{"paperId":"b763f84fb29243120ecca5a5c9fb092da6d430c0","title":"Constructing a finer-grained representation of clinical trial results from ClinicalTrials.gov"},{"paperId":"be4a8ca1fb43fdbc1db1c2bc2695c5cc05a0a5ff","title":"Joint Constrained Learning for Causal Event-Event Relation Extraction of Brain Connectome"},{"paperId":"40183351b32f1432841d1dfaa262f827d759cadc","title":"Show from Tell: Audio-Visual Modelling in Clinical Settings"},{"paperId":"9ea8105a7bc03dbcde05bf953f3b90f1db61f6bd","title":"URL-BERT: Training Webpage Representations via Social Media Engagements"},{"paperId":"9c5609baff6175b0a2e436bb69e89737c4be3cf4","title":"Improving Biomedical Abstractive Summarisation with Knowledge Aggregation from Citation Papers"},{"paperId":"116eba970643613f5284b490f8c3c85319053763","title":"WojoodNER 2023: The First Arabic Named Entity Recognition Shared Task"},{"paperId":"426ac33636fd56ba80153e60fc59ef7f1db2d3af","title":"Multimodal Graph Learning for Modeling Emerging Pandemics with Big Data"},{"paperId":"468fc94845b52c6e96ba1f3c3884d0653d5421b4","title":"GeoLM: Empowering Language Models for Geospatially Grounded Language Understanding"},{"paperId":"876bfbe25cabe31d337e38d7e600b4d445581023","title":"Bi-Encoders based Species Normalization - Pairwise Sentence Learning to Rank"},{"paperId":"51232f0037e8ca83f30ba46b9f87e3565494063a","title":"NERetrieve: Dataset for Next Generation Named Entity Recognition and Retrieval"},{"paperId":"73325a6e15d940de861b115395f3c453608a4e50","title":"Continually-Adaptive Representation Learning Framework for Time-Sensitive Healthcare Applications"},{"paperId":"c6aab2f8dea7aaf650fc7df6a7fa067dac135e68","title":"PetBERT: automated ICD-11 syndromic disease coding for outbreak detection in first opinion veterinary electronic health records"},{"paperId":"39abce3268f309d5655247de0c442a28219df390","title":"MedEval: A Multi-Level, Multi-Task, and Multi-Domain Medical Benchmark for Language Model Evaluation"},{"paperId":"852355b71817c508a64593b65dc76bdf3f792dd5","title":"A Hybrid Named Entity Recognition System for Aviation Text"},{"paperId":"ec00637804494c881c3b208a6655665d127f7b71","title":"Exploring the Impact of Corpus Diversity on Financial Pretrained Language Models"},{"paperId":"2bd6d70978e92e960544329ef81cbe70668d7ffd","title":"CLIFT: Analysing Natural Distribution Shift on Question Answering Models in Clinical Domain"},{"paperId":"7dd4b7e6f5f1588eae707df8334589ffd503bc54","title":"A Predictive Factor Analysis of Social Biases and Task-Performance in Pretrained Masked Language Models"},{"paperId":"23f96db82ae02c5c3c0a861571e7aa8d27c91bc9","title":"Data Augmentations for Improved (Large) Language Model Generalization"},{"paperId":"53f1715758830e49722de332be18bc3184422ee9","title":"OphNER: Named Entity Recognition for Ophthalmology Newspapers"},{"paperId":"cee600d8e99fded496437a0358755b9fd2111c28","title":"Thyroidkeeper: a healthcare management system for patients with thyroid diseases"},{"paperId":"b44fbb613573f78d444361d7670c8ab130ef9174","title":"ViSoBERT: A Pre-Trained Language Model for Vietnamese Social Media Text Processing"},{"paperId":"468e84b71e8d71742a89bce870ee1727b94fd695","title":"Medical Specialty Classification Based on Semiadversarial Data Augmentation"},{"paperId":"b185e04bf51a50d23640160ae7c74b9b1a5747da","title":"Research on NER model for coal mine safety hazards based on BERT-CNN-BiGRUs-CRF"},{"paperId":"185f9b5d79cae6a2e20bbf0d935a26724192924f","title":"Semantic Template-based Convolutional Neural Network for Text Classification"},{"paperId":"5b038c1a93967072cc76689fd805e756f804cc42","title":"Large Models for Time Series and Spatio-Temporal Data: A Survey and Outlook"},{"paperId":"1efb48995732f58dbf2e251bfdf8571545033db9","title":"BioPlanner: Automatic Evaluation of LLMs on Protocol Planning in Biology"},{"paperId":"99bd4e2e25a0088c59342ba8047778fd9a8f9c22","title":"Learning entity-oriented representation for biomedical relation extraction"},{"paperId":"c2a707f093a58beb050892db9417e7cffd722213","title":"Interpretable Disease Prediction from Clinical Text by Leveraging Pattern Disentanglement"},{"paperId":"34f237a2cc90417e33ae70dd13636f60f056b8c5","title":"SiaKey: A Method for Improving Few-shot Learning with Clinical Domain Information"},{"paperId":"fff2a52ea684a50aae8b2d6e1077ce129be89498","title":"Surveying the Landscape of Text Summarization with Deep Learning: A Comprehensive Review"},{"paperId":"e95c17a6788dcd8aec1f4a99c1740ff0ee824243","title":"Prediction Of User Ratings For Drug Side Effects Using Deep Neural Network With Contextual Co-occurrence Based Word-Embedding Vector"},{"paperId":"d8605b80bd617a7b7b8ec9df625dd7972a9a2efa","title":"From Large Language Models to Knowledge Graphs for Biomarker Discovery in Cancer"},{"paperId":"d9ef89eab7f18bdd41588066b1987a4f91131fe0","title":"Question Answering for Electronic Health Records: A Scoping Review of datasets and models"},{"paperId":"9c4ebdcf3bdfed0ed9b2f0fea5ba6f8fea49c632","title":"Large Language Models for Scientific Synthesis, Inference and Explanation"},{"paperId":"bccc22f9f2f08e3e8490e341aea58b551592fedf","title":"MProto: Multi-Prototype Network with Denoised Optimal Transport for Distantly Supervised Named Entity Recognition"},{"paperId":"7fcb9ffcea0c6af04a2e7cfe7d0e750e77f8d311","title":"A Scientific Document Retrieval and Reordering Method by Incorporating HFS and LSD"},{"paperId":"d551073f0897fceccea43280d898bfcea6fbf0fc","title":"A Survey of Heterogeneous Transfer Learning"},{"paperId":"ddcc5bdd28355652e29393ee76d17601e5983b3f","title":"Effects of Human Adversarial and Affable Samples on BERT Generalization"},{"paperId":"b4a01f2c43481f4c956b1da384beedd90dc5e9e3","title":"DKEC: Domain Knowledge Enhanced Multi-Label Classification for Electronic Health Records"},{"paperId":"615c34193a26b8a5dc04407d777ea0ff81114fb5","title":"An Analysis on Large Language Models in Healthcare: A Case Study of BioBERT"},{"paperId":"b1ece182d4934dfc629ca409ef78e56c1762965c","title":"On the Impact of Cross-Domain Data on German Language Models"},{"paperId":"5bda9200b6466da0779116385a277be6dbc746f4","title":"Hierarchical Pretraining on Multimodal Electronic Health Records"},{"paperId":"c3382fd533b9dd7f8ed7ba7766159079bc1d3935","title":"BioT5: Enriching Cross-modal Integration in Biology with Chemical Knowledge and Natural Language Associations"},{"paperId":"23097f2d2deda9b92544fc2294d0c2f7d57cf12a","title":"Rethinking Model Selection and Decoding for Keyphrase Generation with Pre-trained Sequence-to-Sequence Models"},{"paperId":"72b06aef94f798ad9035b5775c186fd2fd6a8f38","title":"Multi-domain improves out-of-distribution and data-limited scenarios for medical image analysis"},{"paperId":"282c568302701bc163d454702eae10e43ca784a3","title":"The future landscape of large language models in medicine"},{"paperId":"b1edb49d90a1a652821084435ea1ec750aafcc54","title":"Domain-Specific Language Model Post-Training for Indonesian Financial NLP"},{"paperId":"732b1e2e57351342be4456b486bdfc39ed348df0","title":"Early Prediction of Sepsis Using Time Series Forecasting"},{"paperId":"da7bc9c47bc06ed2742142540cc94b918c1fe723","title":"Are Large Language Models Post Hoc Explainers?"},{"paperId":"99e6c90c0ed18030164a731250d547b5e5735055","title":"LLM for SoC Security: A Paradigm Shift"},{"paperId":"c7492913370b5726eaa6ced163a60de6c9d4bb7f","title":"A Survey of Large Language Models for Healthcare: from Data, Technology, and Applications to Accountability and Ethics"},{"paperId":"c957c50ead898bd76944a554f0bf02bdf59c67e5","title":"ShellGPT: Generative Pre-trained Transformer Model for Shell Language Understanding"},{"paperId":"a922ce01d2fadcad1bf7cd4bbf91cb8effadd3d4","title":"Recent advancement in targeted therapy and role of emerging technologies to treat cancer"},{"paperId":"e2a2ae2b97e838b905b1a928605e3a887ca7133a","title":"CnGeoPLM: Contextual knowledge selection and embedding with pretrained language representation model for the geoscience domain"},{"paperId":"7b5b7aefc727bdf1e73efdd65af539d79628230a","title":"A Comprehensive Evaluation of Large Language Models on Benchmark Biomedical Text Processing Tasks"},{"paperId":"02d7fe5ffc543b8d16e87fdd37bedcd00716117d","title":"Evaluation of a prototype machine learning tool to semi-automate data extraction for systematic literature reviews"},{"paperId":"020e370bf90c142b9ba5e83173986f1b9dc09628","title":"Automated Extraction and Classification of Drug Prescriptions in Electronic Health Records: Introducing the PRESNER Pipeline"},{"paperId":"b7f379f67f5e0777aa7a15d3d61950b519bdc589","title":"Deep Representations of First-person Pronouns for Prediction of Depression Symptom Severity"},{"paperId":"864c896c686d36ba84de69cbb646a56bfb1b7d44","title":"Know2BIO: A Comprehensive Dual-View Benchmark for Evolving Biomedical Knowledge Graphs"},{"paperId":"ad76cd056ce580d24cda52764fe06b926edfee41","title":"ChatGPT makes medicine easy to swallow: an exploratory case study on simplified radiology reports."},{"paperId":"0c924d41033eca9ea19e6418c8fcfcbf35a32a12","title":"A Multilabel Text Classifier of Cancer Literature at the Publication Level: Methods Study of Medical Text Classification"},{"paperId":"d4b44f9a783911c9222fe6975d0dc6ad99a4cb81","title":"Literature Based Discovery (LBD): Towards Hypothesis Generation and Knowledge Discovery in Biomedical Text Mining"},{"paperId":"d52e65ef8e909eb15c544a9e0ba0b551f768b578","title":"The Use of ICT as a Resource and Media for Modern 21st Century Learning in Primary Schools"},{"paperId":"fa874e7b66a5b936469872054986c8f340701146","title":"A Survey of GPT-3 Family Large Language Models Including ChatGPT and GPT-4"},{"paperId":"a8aef5a15dc2b486b3bf01205d2687a1140a41bb","title":"Conversational Health Agents: A Personalized LLM-Powered Agent Framework"},{"paperId":"2784e89fb178fdb13e47166a1e961eebd2b72a4a","title":"Evolution of research topics and paradigms in plant sciences"},{"paperId":"bd6d7122b871aa80dd22ff18985d435004851b5a","title":"Extraction of Medication and Temporal Relation from Clinical Text using Neural Language Models"},{"paperId":"9ee833ef54724cd292122e9901862af797cfe15a","title":"Large Language Models Trained on Equipment Maintenance Text"},{"paperId":"7c6b857a8c8c9e0874fd29e1a605a1edb6028d50","title":"Schema matching based on energy domain pre-trained language model"},{"paperId":"29a5c700b5844df30cd8c0ff01b59bd25b94c477","title":"GDPRxiv: Establishing the State of the Art in GDPR Enforcement"},{"paperId":"748194e591fa7689940d7d6a8766f1a4afa2dac0","title":"A literature-mining method of integrating text and table extraction for materials science publications"},{"paperId":"a0476578761e983d5ab2083abab07b81236c1d58","title":"Asymmetric cross-modal attention network with multimodal augmented mixup for medical visual question answering"},{"paperId":"bf176a1da55b30cc81ad309808c3d8c90b16456a","title":"Artificial Intelligence and Infectious Disease Imaging."},{"paperId":"ff0bbf1867f9f6b494ee79443c403d062fa23fd1","title":"Data augmentation via context similarity: An application to biomedical Named Entity Recognition"},{"paperId":"061ec64edce2632870bdf72d33ddc5c046193e0a","title":"SUSIE: Pharmaceutical CMC ontology-based information extraction for drug development using machine learning"},{"paperId":"a73f8fe4095cb0fe3408a15cd4f937094d02b1ab","title":"GERNERMED++: Semantic annotation in German medical NLP through transfer-learning, translation and word alignment"},{"paperId":"932be70db1e3d6fbe5fe4a305593f22a5f058049","title":"T 2 -NER: A Two-Stage Span-Based Framework for Unified Named Entity Recognition with Templates"},{"paperId":"e0aba21dc7f4cffdcda0a772153fdbc139681e5a","title":"Exploring named entity recognition and relation extraction for ontology and medical records integration"},{"paperId":"243e325812d4fc02dd380c5561b7a72954ba0834","title":"On the instability of further pre-training: Does a single sentence matter to BERT?"},{"paperId":"01a2fe143a05b9faeb481f86261dd1f0db3a57fa","title":"Multi-ontology embeddings approach on human-aligned multi-ontologies representation for gene-disease associations prediction"},{"paperId":"399cbcf0187197c8c371fcca1bd78cd3e529621c","title":"Named Entity Recognition in Electronic Health Records: A Methodological Review"},{"paperId":"7bc7f0929632b5b130d5626efd78ebdba28b2753","title":"Multi-Scale Bidirectional Recurrent Network with Hybrid Correlation for Point Cloud Based Scene Flow Estimation"},{"paperId":"1c0b7518e219d71ae682a838744c5c5bed8fce74","title":"A Small Claims Court for the NLP: Judging Legal Text Classification Strategies With Small Datasets"},{"paperId":"9d2a314b46ead6332ff2926f857df9537c839509","title":"Question-Answering Model for Schizophrenia Symptoms and Their Impact on Daily Life using Mental Health Forums Data"},{"paperId":"8c132843ba216536e8b0a1e13b212e6c1d429075","title":"How to Improve Student Understanding in Learning Science by Regulating Strategy in Language Education? Definition, Factors for Enhancing Students Comprehension, and Computational Bibliometric Review Analysis"},{"paperId":"0c75cda2bb0812217bf0e5460e910212ad512944","title":"An evaluation of GPT models for phenotype concept recognition"},{"paperId":"e94b1b868bf57f0243e42d4f51042bd1f1e621b3","title":"Medical Foundation Models are Susceptible to Targeted Misinformation Attacks"},{"paperId":"513cd3699a0518ba2119f36691d23ccd6de67c0f","title":"Knowledge Graphs for the Life Sciences: Recent Developments, Challenges and Opportunities"},{"paperId":"2380ea65a9e5d34df3ea2ee84a487e8263663e12","title":"Using Weak Supervision and Data Augmentation in Question Answering"},{"paperId":"3ba45f22238c9902557cae0dbb381cc2e562b1ad","title":"Deeply integrating unsupervised semantics and syntax into heterogeneous graphs for inductive text classification"},{"paperId":"c234381686e782987a556e44aed061aaedd8c2de","title":"MedEdit: Model Editing for Medical Question Answering with External Knowledge Bases"},{"paperId":"04ec6f22741b3d0d3e695486957d5e6cf94b3cfc","title":"Mapping Vaccine Names in Clinical Trials to Vaccine Ontology using Cascaded Fine-Tuned Domain-Specific Language Models"},{"paperId":"133b5d9959b7f1c2b094934eab36cdaa0e27f387","title":"CONORM: Context-Aware Entity Normalization for Adverse Drug Event Detection"},{"paperId":"504218f47f24a89aba246243c228fd3fa85b9643","title":"Global trends in PANoptosis research: bibliometrics and knowledge graph analysis"},{"paperId":"10eb81d069f3654fa234f769852173a9ddadfabd","title":"Comprehensive Overview of Named Entity Recognition: Models, Domain-Specific Applications and Challenges"},{"paperId":"1295a68542f23696f3fd02ffbf39a5fe107d6227","title":"Hybrid medical named entity recognition using document structure and surrounding context"},{"paperId":"c5fa173e6f52dbc2e5156e2e7c9e6d3c0f1c4aa0","title":"DRG-LLaMA : tuning LLaMA model to predict diagnosis-related group for hospitalized patients"},{"paperId":"784c4f49e4b70b8907f75fd9c5606f62913e40db","title":"To BERT or not to BERT: advancing non-invasive prediction of tumor biomarkers using transformer-based natural language processing (NLP)"},{"paperId":"e3b0944a445706addd7e12617308245034db0456","title":"Nested Event Extraction upon Pivot Element Recogniton"},{"paperId":"a54493bdca9b63c63468714e9b60fe89d56cd265","title":"Large Language Models and Control Mechanisms Improve Text Readability of Biomedical Abstracts"},{"paperId":"3e7c564b9da8ee4f2b1ed437f347aed59a68c529","title":"Improving VTE Identification through Adaptive NLP Model Selection and Clinical Expert Rule-based Classifier from Radiology Reports"},{"paperId":"f993edb9aca0239decf07bdbec0cef8ba4ca7c83","title":"Inspire the Large Language Model by External Knowledge on BioMedical Named Entity Recognition"},{"paperId":"61b48fb871bf184ab2e8137275e74639fdbd3b36","title":"A short review on recent developments in the computational techniques (2021) to mitigate SARS-CoV-19 disease"},{"paperId":"18a2295063d02204375488da5a926287cdcb77d0","title":"Ottoman Turkey the Sorrowful Period of Islam's Political Journey"},{"paperId":"991409eec5acfcad720073c91ae06441c438d14e","title":"Visual Question Answering in the Medical Domain"},{"paperId":"7aef8e81883e9ded80076b8d2002e29ec5555564","title":"BioBERTurk: Exploring Turkish Biomedical Language Model Development Strategies in Low-Resource Setting"},{"paperId":"e07e9a54f71c598e01f3d868bd02cfec7362eaa4","title":"Investigating Zero- and Few-shot Generalization in Fact Verification"},{"paperId":"6d180d3d54fdfb82149b03dd0cb28e808be957c2","title":"Prediction of hot spots towards drug discovery by protein sequence embedding with 1D convolutional neural network"},{"paperId":"df5a3ab06204449c314c2b7f45a41c2b08a82bfe","title":"Natural Language Processing to Classify Caregiver Strategies Supporting Participation Among Children and Youth with Craniofacial Microsomia and Other Childhood-Onset Disabilities"},{"paperId":"bed13c309108041de2d46321423146ac819ef610","title":"Adding Linguistic Information to Transformer Models Improves Biomedical Event Detection?"},{"paperId":"a94cd92bb3b0cca139198c4eedd494f9ece74c2a","title":"Reranking for a Polish Medical Search Engine"},{"paperId":"ea9735ac5ddf69823c44e360a7a68b5e5e71ad46","title":"A study of BERT-based methods for formal citation identification of scientific data"},{"paperId":"756a5a2387ae9c09704efc4757765d656576df07","title":"Analyzing research diversity of scholars based on multi-dimensional calculation of knowledge entities"},{"paperId":"c51fef8d716cbb7811fe0253e21ef8dac62cc78a","title":"HealthFC: A Dataset of Health Claims for Evidence-Based Medical Fact-Checking"},{"paperId":"e8c16838f342eb36d06a67ceece672f4879bd03a","title":"Discovering research data management trends from job advertisements using a text-mining approach"},{"paperId":"59fc922aac0f177a517d5656868c9c4334d863ef","title":"MAPLE: Mobile App Prediction Leveraging Large Language model Embeddings"},{"paperId":"9ff38398b5808fe3d26fbfa19f77eebbad177dc2","title":"Text Classification of Cancer Clinical Trial Eligibility Criteria"},{"paperId":"bcea96cdff75b703c366c8bb8f42e30d4f8a9f3b","title":"Benchmarking ChatGPT-4 on a radiation oncology in-training exam and Red Journal Gray Zone cases: potentials and challenges for ai-assisted medical education and decision making in radiation oncology"},{"paperId":"7ca7d39a9023dccab0925613cfe8c98927adcb92","title":"Sequence Labeling for Disambiguating Medical Abbreviations"},{"paperId":"ebcc994b32959bcdc2d091781178ca2872a9cbef","title":"Advanced Privacy Preserving Model for Smart Healthcare Using Deep Learning"},{"paperId":"22a52b4a74334bd17ae75ef9f0618e75c69e8986","title":"Hypert: hypernymy-aware BERT with Hearst pattern exploitation for hypernym discovery"},{"paperId":"0502ad3507b437af48afb3cd8bb4c2d1875bcbff","title":"Content Reduction, Surprisal and Information Density Estimation for Long Documents"},{"paperId":"5ca6e9a5029e4b2d5d718840c20d5853a8c5719e","title":"Leveraging Large Language Models and Weak Supervision for Social Media data annotation: an evaluation using COVID-19 self-reported vaccination tweets"},{"paperId":"3aed81250dcd924b601835aadff50cc6dfc0984e","title":"Collaborative and privacy-preserving workflows on a clinical data warehouse: an example developing natural language processing pipelines to detect medical conditions"},{"paperId":"211ee44814d9f5df4ffe86640cb2017a496d8641","title":"CrisisTransformers: Pre-trained language models and sentence encoders for crisis-related social media texts"},{"paperId":"6ecdea4b8c07a539092650be017a940138a9d8b2","title":"BugSigDB captures patterns of differential abundance across a broad range of host-associated microbial signatures."},{"paperId":"59018c9baafb9de3906e9fc413cba529179e46c6","title":"Evaluation of text summarization techniques in healthcare domain: Pharmaceutical drug feedback"},{"paperId":"aebc595bf89c201428c69d878b8fdc57784582fb","title":"IFM-RCNN: a hybrid text classifier with enhanced performance of binary drug classification from tweets using improved faster mask-recurrent convolutional neural network"},{"paperId":"1c3170932d55d61756916c5c33fedb51cdf3bb7e","title":"A large-scale evaluation of NLP-derived chemical-gene/protein relationships from the scientific literature: Implications for knowledge graph construction"},{"paperId":"1fb2bde5c2f3a3c4d7b810b29ec3f21f60e75d35","title":"Knowledge-tuning Large Language Models with Structured Medical Knowledge Bases for Reliable Response Generation in Chinese"},{"paperId":"46bdc5cf1e1472fb6f58687a93fca19370e823ff","title":"Extracting Mutant-Affected Protein-Protein Interactions via Gaussian-Enhanced Representation and Contrastive Learning"},{"paperId":"a570d3d8f1b66a8ab3fff7876dc9bba3bcdc789b","title":"Aligning Large Language Models for Clinical Tasks"},{"paperId":"82736c46c788744b8cf3428441e27b86bb8480f6","title":"Fine-Tuning Pre-Trained Language Model for Urgency Classification on Food Safety Feedback"},{"paperId":"35408ebad5f1acb9d6e9dd8591c62bf75bffd6b1","title":"Predictive Food Safety Risk Monitoring"},{"paperId":"2aa5b00af1431abc1fdedd3f0bdf136d7c055c92","title":"A multi-stage recognizer for nested named entity with weakly labeled data"},{"paperId":"5e761e9f5cd9672a181b256299cd2916a8079461","title":"Augmenting Black-box LLMs with Medical Textbooks for Clinical Question Answering"},{"paperId":"ccab431bf6a154c7c4048eed723a46aef93891fc","title":"DiscoverPath: A Knowledge Refinement and Retrieval System for Interdisciplinarity on Biomedical Research"},{"paperId":"1af7ae5c0a476fa9ee7b8ae0d76012a73c47c63f","title":"Incorporating Dictionaries into a Neural Network Architecture to Extract COVID-19 Medical Concepts From Social Media"},{"paperId":"2acd6f474567fec34cd2e46f7dd049f8ff2dc9dd","title":"Efficient Machine Reading Comprehension for Healthcare Applications: A Context Extraction Approach (Preprint)"},{"paperId":"d0929ad54e151b4bd7f0577a78bbba6c25292459","title":"Into the Single Cell Multiverse: an End-to-End Dataset for Procedural Knowledge Extraction in Biomedical Texts"},{"paperId":"8f968ce745d65f8929aaf809b4a7bc252a65598d","title":"Large-scale Dataset and Effective Model for Variant-Disease Associations Extraction"},{"paperId":"41135453dd1d09ec7d2942b649d5145ecb9b9113","title":"Exploring Pair-Aware Triangular Attention for Biomedical Relation Extraction"},{"paperId":"cbd7b827c541ed6a5d944c21c2bff2887c273413","title":"Knowledge Graph Embeddings for Multi-Lingual Structured Representations of Radiology Reports"},{"paperId":"1a66c290beef2d992fc55137f9e8fd5a67cf09ae","title":"Empirical evaluation of language modeling to ascertain cancer outcomes from clinical text reports"},{"paperId":"f9fc7e1d942900a5c589602c9cefefacdbafb05f","title":"Datasets Construction and Development of QSAR Models for Predicting Micronucleus In Vitro and In Vivo Assay Outcomes"},{"paperId":"bffbfd88a9c4465d56d2535683f5ca1cbff26588","title":"Disambiguation of medical abbreviations for knowledge organization"},{"paperId":"735035035a8b81cffb7b982d88d5ff753517fbce","title":"SPS-LCNN: A Significant Point Sampling-based Lightweight Convolutional Neural Network for point cloud processing"},{"paperId":"9812785093330fb6882e243e1c5051a3be1f9ee6","title":"Weak-PMLC: A large-scale framework for multi-label policy classification based on extremely weak supervision"},{"paperId":"3e5e5c2098de36a144728e766c58eed3e77a1eff","title":"A survey on Relation Extraction"},{"paperId":"01ca6da9b3756c178e43358e31528a6aea472eba","title":"Improving Log-Based Anomaly Detection by Pre-Training Hierarchical Transformers"},{"paperId":"2bc4c5d853225ba1194c45d2c84bac822f81b6d0","title":"Automatic classification of experimental models in biomedical literature to support searching for alternative methods to animal experiments"},{"paperId":"937a6c992dff0b02dd0d0e30ebf2ef49d6989d4f","title":"TGRA-P: Task-driven model predicts 90-day mortality from ICU clinical notes on mechanical ventilation"},{"paperId":"5878a3a90c9c31f68defabaf9cff49acbe7b797d","title":"Biomaterials Text Mining: A Hands-On Comparative Study of Methods on Polydioxanone Biocompatibility."},{"paperId":"28f462b3241751446edc8915868c446476103612","title":"Reconstruction of temperature field in nanofluid-filled annular receiver with fins using deep hybrid transformer-convolutional neural network"},{"paperId":"d07ce3d87e98ce804c0aebf430f402bc8c67806d","title":"Generative Artificial Intelligence Through ChatGPT and Other Large Language Models in Ophthalmology"},{"paperId":"37008d6aadecd12aa52f330fe00bfe0c7e655d2c","title":"An interpretable deep learning framework for predicting liver metastases in postoperative colorectal cancer patients using natural language processing and clinical data integration"},{"paperId":"386c6e609bb700587fb8d675f0f53a3b3ebd31e4","title":"Towards precise PICO extraction from abstracts of randomized controlled trials using a section-specific learning approach"},{"paperId":"d859cf170e60ffa7f109e4f9e3a88fd2519f5b73","title":"A self-supervised language model selection strategy for biomedical question answering"},{"paperId":"b8bb40305a6432af86048e93929aeb28c187f306","title":"Literature-based predictions of Mendelian disease therapies."},{"paperId":"5289937ca9bbd4dfefa5e90e5b5857f404a09ffc","title":"DEED: DEep Evidential Doctor"},{"paperId":"e20766f84713eef324cfcd6e3ddf9e6c47915836","title":"Tell me your position: Distantly supervised biomedical entity relation extraction using entity position marker"},{"paperId":"2ccdfdbf3bb408a1c74dde2dbb2cf8492b3591db","title":"Large language models in medicine: the potentials and pitfalls"},{"paperId":"55c5b37a2089781a264553ec7d1b96fe89675132","title":"Can ChatGPT explain it? Use of artificial intelligence in multiple sclerosis communication"},{"paperId":"84fe94bad3f2adff4b021e11641ba44cffb835d8","title":"Automatic retrieval of health case reports for public needs using deep learning techniques"},{"paperId":"437b0c6218bd06a0730e1b29829fa299e72b4a0b","title":"Towards More Generalizable and Accurate Sentence Classification in Medical Abstracts with Less Data"},{"paperId":"50da73ae5d7f2dae59a3fc5b2a00d160b8bd437d","title":"Multimodal Foundation Models For Echocardiogram Interpretation"},{"paperId":"236a88ec400bcbc91c235cd56638e13e31c03fbb","title":"A decision support system in precision medicine: contrastive multimodal learning for patient stratification"},{"paperId":"1ce155c917b5c7d800d8a677840cf0336508bf90","title":"Stock trend prediction based on feature embedding and stock similarity"},{"paperId":"3d4980522a7641b1244aeedaf7cd9e2d0b0f6af4","title":"Medical Relation Extraction Based On Feature Fusion Forest"},{"paperId":"125081b729bee38708e218cba101f21bb5288c8a","title":"Exploring the Transfer Learning Capabilities of CLIP in Domain Generalization for Diabetic Retinopathy"},{"paperId":"f97ec1d8b221bf548d314a71257197f642e1eedb","title":"T4SEpp: a pipeline integrated with protein language models effectively predicting bacterial type IV secreted effectors"},{"paperId":"f190aeb5ff68a51d963680403c3d160a72e47e63","title":"Multimodal Data Hybrid Fusion and Natural Language Processing for Clinical Prediction Models"},{"paperId":"23efe9b99b5f0e79d7dbd4e3bfcf1c2d8b23c1ff","title":"Marie and BERT—A Knowledge Graph Embedding Based Question Answering System for Chemistry"},{"paperId":"13f8afd11e7267f3c59ee0f9b1b58bfe50d5a85f","title":"Improving Image Classification of Knee Radiographs: An Automated Image Labeling Approach"},{"paperId":"b6d64d2f3464fea548731693ed194edfcf58af77","title":"Knowledge-injected Prompt Learning for Chinese Biomedical Entity Normalization"},{"paperId":"b9e0fdb5af49a2219ec8743d5daab7df9946ce1b","title":"BELB: a biomedical entity linking benchmark"},{"paperId":"35d2e90278212678dc0ee66b08f4bce01bb221ed","title":"An extensible point-based method for data chart value detection"},{"paperId":"8beaf2a71e44e0c9e6be254f50decc574733c4a2","title":"Joint representation learning for retrieval and annotation of genomic interval sets"},{"paperId":"1c486c97e49c53dd0350131da6d962a4ab43845d","title":"Identifying COVID-19 cases and extracting patient reported symptoms from Reddit using natural language processing"},{"paperId":"591d15dadf10e60b3cb94586879663ac6d74612d","title":"Leveraging Language Models for Inpatient Diagnosis Coding"},{"paperId":"116eef204491b82b4bfa23345f1645935faedbad","title":"Software Entity Recognition with Noise-Robust Learning"},{"paperId":"2cfda2581748c2a47b3460c7255685b8c07740bf","title":"Unlocking Hardware Security Assurance: The Potential of LLMs"},{"paperId":"e2d9af5557b69a84fdfb3fa845f4fec75f5f2c4b","title":"Biomedical Named Entity Recognition from Malaria Literature using BioBERT"},{"paperId":"73c948bf410db6f4482214a4e39abb18745236c5","title":"OralMedNER: A Named Entity Recognition System for Oral Medicine and Radiology"},{"paperId":"a6cbafb46fb6a58321c440829c359675a2c892ae","title":"Named Entity Recognition in Power Marketing Domain Based on Whole Word Masking and Dual Feature Extraction"},{"paperId":"308db6f428a20fbef0a72f9ee73798d60fe9ff50","title":"BIOptimus: Pre-training an Optimal Biomedical Language Model with Curriculum Learning for Named Entity Recognition"},{"paperId":"479018820d4ddeab58297224f0462497668f57bd","title":"Multi-label topic classification model of COVID-19 literature"},{"paperId":"98db49e5fbda7198f7946da1d6fda2829bdfa603","title":"Leveraging Explainable AI to Analyze Researchers' Aspect-Based Sentiment about ChatGPT"},{"paperId":"0db5edaaacb1f3b240b2eea596297419ca9cb88e","title":"Finding Stakeholder-Material Information from 10-K Reports using Fine-Tuned BERT and LSTM Models"},{"paperId":"3958152c43888a0a9461de2b952df4a39491442b","title":"Informed Named Entity Recognition Decoding for Generative Language Models"},{"paperId":"a232471542bf3f6c7a5a23c8d96acf065b830ce7","title":"Advanced intelligent health advice with informative summaries to facilitate treatment decision-making"},{"paperId":"6ddef11311fc716b3d87e4d6814202b3df985cb5","title":"Artificial intelligence can dynamically adjust strategies for auxiliary diagnosing respiratory diseases and analyzing potential pathological relationships"},{"paperId":"ccf993557d41fed8b92a17bbd84a7a8a3247706b","title":"KBMQA: medical question and answering model based on Knowledge Graph and BERT"},{"paperId":"64e802ea8e9dbe247c31fb06184c04dbf9e55e4e","title":"EcomGPT: Instruction-tuning Large Language Models with Chain-of-Task Tasks for E-commerce"},{"paperId":"745e015bbe917a7a3dbd5f50c2198a027d67475d","title":"MC-DRE: Multi-Aspect Cross Integration for Drug Event/Entity Extraction"},{"paperId":"3eff0e1187dbd60f12dd06c5f3291b1eb6858c1a","title":"Bio-SIEVE: Exploring Instruction Tuning Large Language Models for Systematic Review Automation"},{"paperId":"70980c12cdb463caedd0441f061bd9103e5a039f","title":"Demonstration-based learning for few-shot biomedical named entity recognition under machine reading comprehension"},{"paperId":"bd1b4a8181b75574f7afa11a1f3404232e1ebee8","title":"Natural language processing to predict isocitrate dehydrogenase genotype in diffuse glioma using MR radiology reports"},{"paperId":"cee0fabddbb9d84ff2f5aa139bc57055082525c1","title":"Enhancing Phenotype Recognition in Clinical Notes Using Large Language Models: PhenoBCBERT and PhenoGPT"},{"paperId":"4fc9c2ff8aef5d75b3e3ab7b7897c71fd2fe9332","title":"DEBBIE: The Open Access Database of Experimental Scaffolds and Biomaterials Built Using an Automated Text Mining Pipeline"},{"paperId":"e965883e850d4a9599f541bf0cdedb9785973d0d","title":"RadGraph2: Modeling Disease Progression in Radiology Reports via Hierarchical Information Extraction"},{"paperId":"72cd71582f212002ad73d0bdfd6319114d4cb0e4","title":"MediBERT: A Medical Chatbot Built Using KeyBERT, BioBERT and GPT-2"},{"paperId":"cd9a1e785f3f2ec85256decec0986a2e24fd4db1","title":"A framework for multi-faceted content analysis of social media chatter regarding non-medical use of prescription medications"},{"paperId":"2e783a3fe650c8eeff81cc867fa9921729422c1d","title":"PaniniQA: Enhancing Patient Education Through Interactive Question Answering"},{"paperId":"ce3a4278ec0459f5186b2d8c5b5fd8d86b5847e9","title":"Decision Knowledge Graphs: Construction of and Usage in Question Answering for Clinical Practice Guidelines"},{"paperId":"53947b7f42b7bf6c59d2f72bab2829342154d2ac","title":"A Survey of Spanish Clinical Language Models"},{"paperId":"28cf32dfe7f5f4532d02ed0b864e9ecbc9c09111","title":"Improving Biomedical Claim Detection using Prompt Learning Approaches"},{"paperId":"a3a43f60f004b698306a0ef398ae8a34bb9c1748","title":"BioBERT Based SNP-traits Associations Extraction from Biomedical Literature"},{"paperId":"2090f208a05344fd15a995fcbc9d539508841347","title":"Local Large Language Models for Complex Structured Medical Tasks"},{"paperId":"81a2d4ee13955a4fdbfc178d6df64cf9e786ff52","title":"Toward a stable and low-resource PLM-based medical diagnostic system via prompt tuning and MoE structure"},{"paperId":"b7ced95a2f6984bd0bc4ae90420a59f55151c834","title":"Searching for explanations of black-box classifiers in the space of semantic queries"},{"paperId":"2feaff53b1aa498d09dc5dba2f08ab6a60385e9c","title":"Can ChatGPT provide intelligent diagnoses? A comparative study between predictive models and ChatGPT to define a new medical diagnostic bot"},{"paperId":"210ecef8b8add99b90191db37206c6811f6475fd","title":"Few-shot biomedical named entity recognition via knowledge-guided instance generation and prompt contrastive learning"},{"paperId":"2147a8b64f406945d0d670a6dbab99b536d6e45f","title":"Document-level relation extraction with multi-layer heterogeneous graph attention network"},{"paperId":"68469d83ebd14069e8173e5bf830ed925d13bea2","title":"Control, coordination, and adaptation functions in construction contracts: A machine-coding model"},{"paperId":"08063c57e0ff13dfff0f895ca67a37b52953e0e6","title":"Extracting and structuring information from the electronic medical text: state of the art and trendy directions"},{"paperId":"d2bd31f8034c86da3598d94a05e878dcd2374f6b","title":"Recycle-BERT: Extracting Knowledge about Plastic Waste Recycling by Natural Language Processing"},{"paperId":"5ca39d6d6ae510609f9608e8d49b421bd522326d","title":"Supplementing domain knowledge to BERT with semi-structured information of documents"},{"paperId":"368fe791d80d954400fa52cd630f745adfe02e6b","title":"Systematic review of natural language processing for recurrent cancer detection from electronic medical records"},{"paperId":"648a2ef5c1e3be331c77e86e6b99b70c2909153b","title":"Associating biological context with protein-protein interactions through text mining at PubMed scale"},{"paperId":"639207980804bae07bc6651da27acbe7bd5c7015","title":"ChatGPT and Artificial Intelligence in Medical Writing: Concerns and Ethical Considerations"},{"paperId":"8a71f4c8f6c3a1780a4aa8f6f1f2f7f5ca05e109","title":"MOOC-BERT: Automatically Identifying Learner Cognitive Presence From MOOC Discussion Data"},{"paperId":"3c71960195b00baa4b77b9ed464155736e25be77","title":"ODEE: A One-Stage Object Detection Framework for Overlapping and Nested Event Extraction"},{"paperId":"c2200f2a69ae1e7665adfd74ec617443558b51c5","title":"FedBFPT: An Efficient Federated Learning Framework for Bert Further Pre-training"},{"paperId":"f6a2087b7b6dbf051d53e1b330b497026124e36d","title":"Using transfer learning-based causality extraction to mine latent factors for Sjögren's syndrome from biomedical literature"},{"paperId":"8aceb1e4a5ebac9dd42a83456909a9a67dd57b5e","title":"An efficient circRNA-miRNA interaction prediction model by combining biological text mining and wavelet diffusion-based sparse network structure embedding"},{"paperId":"c95290d74d01cb29e8eee3ac238739513b9551fb","title":"Year 2022 in Medical Natural Language Processing: Availability of Language Models as a Step in the Democratization of NLP in the Biomedical Area"},{"paperId":"6bd919086940f84a60046f4bfe45c195eaa8168e","title":"AsdKB: A Chinese Knowledge Base for the Early Screening and Diagnosis of Autism Spectrum Disorder"},{"paperId":"05fac7020ef3782ea91191fb6b77d6db00d8840f","title":"Prompt Learning with Structured Semantic Knowledge Makes Pre-Trained Language Models Better"},{"paperId":"21737406f448844220dde7c711565bd44b0c27f0","title":"ChatHome: Development and Evaluation of a Domain-Specific Language Model for Home Renovation"},{"paperId":"6eb3dd2b64db74f9743802acbb9875bdffb9d246","title":"Text-guided Foundation Model Adaptation for Pathological Image Classification"},{"paperId":"90bba6971cb04ee035e869e77fb471016868ab35","title":"Matching Patients to Clinical Trials with Large Language Models"},{"paperId":"c9dbdae8146b9f97e254f5d26fd6efde96eaa703","title":"Med-Flamingo: a Multimodal Medical Few-shot Learner"},{"paperId":"7a97e7bd7c7be5331bfe66f673a5f086d1954750","title":"Mining drug-target interactions from biomedical literature using chemical and gene descriptions based ensemble transformer model"},{"paperId":"152d9bb7be8721578e2d6c4b3ebe9641924295eb","title":"An Improved Model for Medical Forum Question Classification Based on CNN and BiLSTM"},{"paperId":"3b59dc8c98371458d4a5ab6a04e8381858552edb","title":"Biomedical relation extraction with knowledge base–refined weak supervision"},{"paperId":"de7e5fee8cf03bd485b1104d3e40e8ab45d76c0a","title":"Prot2Text: Multimodal Protein's Function Generation with GNNs and Transformers"},{"paperId":"bacd25f6d593fc65edf8287d79bfb1318f5df913","title":"Knowledge-Grounded Dialogue Generation for Medical Conversations: A Survey"},{"paperId":"e87ebf5f69f349b861af0aa0b967f6bd4c039188","title":"DeepGATGO: A Hierarchical Pretraining-Based Graph-Attention Model for Automatic Protein Function Prediction"},{"paperId":"85f8d6fe82b9e98bc5e40c9d640856d7fc59729c","title":"An attention‐based deep learning model for credibility assessment of online health information"},{"paperId":"33112b58e3eb4a6506fa537d892dc6742c5e794d","title":"Large language models in health care: Development, applications, and challenges"},{"paperId":"089f6328085066263fedc083952624ca121ebbf3","title":"CohortGPT: An Enhanced GPT for Participant Recruitment in Clinical Study"},{"paperId":"4623a828f776c029dfa26b77c75cc21379461259","title":"GIST: Generating Image-Specific Text for Fine-grained Object Classification"},{"paperId":"3c98209db032328c5eed95e923783cff09314156","title":"An Evidential Classifier with Multiple Pre-trained Language Models for Nested Named Entity Recognition"},{"paperId":"3d915391a46593524eb47189b47d52334f13a9a4","title":"UMLS-KGI-BERT: Data-Centric Knowledge Integration in Transformers for Biomedical Entity Recognition"},{"paperId":"9f4017de7deded49c032a83d7844efcd9ea1aa21","title":"Embroid: Unsupervised Prediction Smoothing Can Improve Few-Shot Classification"},{"paperId":"6def4b34c904ccd59589229584489d291252fe01","title":"An In-Depth Evaluation of Federated Learning on Biomedical Natural Language Processing"},{"paperId":"d0edff5402edeab721b7681643e1ff7c2354de4a","title":"Leveraging pre-trained language models for mining microbiome-disease relationships"},{"paperId":"233015efe02ceaf3fe97d5893cdf210b05b36861","title":"Improving the Reusability of Pre-trained Language Models in Real-world Applications"},{"paperId":"42555e01ba20ea0ef970ff7d5d6266992e50e233","title":"Investigating drug translational research using PubMed articles"},{"paperId":"d210cb3f64ab2f5fa106fac60f2f5a94387a3b29","title":"RegEMR: a natural language processing system to automatically identify premature ovarian decline from Chinese electronic medical records"},{"paperId":"0379cfb16c1678bde9b889bb1c0ca39db2cb564a","title":"Few-shot Named Entity Recognition: Definition, Taxonomy and Research Directions"},{"paperId":"91c5e43f6ed8d13057eaead21c4f6dd0ed8a1112","title":"Multimodal Machine Learning for Extraction of Theorems and Proofs in the Scientific Literature"},{"paperId":"f11d58e16c3eee7d85ccf5ae99b774894341aaa0","title":"SciMine: An Efficient Systematic Prioritization Model Based on Richer Semantic Information"},{"paperId":"8c881afce8ba9ff0eb430d7d6018965453b60435","title":"BioSift: A Dataset for Filtering Biomedical Abstracts for Drug Repurposing and Clinical Meta-Analysis"},{"paperId":"5b97c7ba8dc762d917af5a486ca76e352c8f2a33","title":"Assessment of the E3C corpus for the recognition of disorders in clinical texts"},{"paperId":"94ce1d5924e05e8d75e43ce70044293ddcef850a","title":"Large language models in medicine"},{"paperId":"5d7bef7691811a080a38b7852f2bed942a42797e","title":"Improved prediction of drug-induced liver injury literature using natural language processing and machine learning methods"},{"paperId":"04626331422e2d0480cbf88ec9ad087e47b9ee9c","title":"Generalizable and explainable prediction of potential miRNA-disease associations based on heterogeneous graph learning"},{"paperId":"4b84cb46a704cc3a978758d8bf09fff25ed71a5a","title":"Inferring cancer disease response from radiology reports using large language models with data augmentation and prompting"},{"paperId":"7502ce27b86263dafd791ff9d4934fafce7bd02d","title":"Do not Mask Randomly: Effective Domain-adaptive Pre-training by Masking In-domain Keywords"},{"paperId":"29954a8c7e43b9d96bf968298c8dbd37aedbb887","title":"Leveraging artificial intelligence in the fight against infectious diseases"},{"paperId":"7e2e093009d63e3cd1f3152fa11d8a75bf937c31","title":"Generalizability of machine learning methods in detecting adverse drug events from clinical narratives in electronic medical records"},{"paperId":"2003dcf311ccab2426818113964e59ca0cfde2c9","title":"FDAPT: Federated Domain-adaptive Pre-training for Language Models"},{"paperId":"313e928295fcf9a33186155c53a9acda98e4f945","title":"Artificial Intelligence in Orthopaedic Surgery: Can a Large Language Model \"Write\" a Believable Orthopaedic Journal Article?"},{"paperId":"8a1c61b9364f0ae19331b1ea753ff892c635c59b","title":"Enhancing Biomedical Text Summarization and Question-Answering: On the Utility of Domain-Specific Pre-Training"},{"paperId":"8dd053ea38e09cd8d311b6b40fbc3d99420e6c5e","title":"Sentiment Analysis with the Use of Transformers and BERT"},{"paperId":"9bf8cb128b5e5a13122152bb34617a393520d02c","title":"Advancements in Scientific Controllable Text Generation Methods"},{"paperId":"6651eb8205e3d90c420fbdf8a2740c74e590e545","title":"Parameter-Efficient Fine-Tuning of LLaMA for the Clinical Domain"},{"paperId":"e0e9ba0c01d441e1fdcb8628d3f743d387b0b017","title":"UIT-Saviors at MEDVQA-GI 2023: Improving Multimodal Learning with Image Enhancement for Gastrointestinal Visual Question Answering"},{"paperId":"a2d765c0563aaf802cf3b70bb69fe6361bb9e316","title":"DeepOnto: A Python Package for Ontology Engineering with Deep Learning"},{"paperId":"0734964fbeaab1da5b992fd5e88a6fe1b8c6e97a","title":"Enhancing multiple-choice question answering through sequential fine-tuning and Curriculum Learning strategies"},{"paperId":"ed26016878ec049ae8b3179cc7a5fdd896a69d08","title":"ChatGPT, a powerful language model and its potential uses in bioinformatics"},{"paperId":"5651bcb92660a2a376b36a08647549b0305c7c58","title":"ODD: A Benchmark Dataset for the NLP-based Opioid Related Aberrant Behavior Detection"},{"paperId":"1e3ef48abeef882e12f9553a1baf8944f3782c88","title":"Several categories of Large Language Models (LLMs): A Short Survey"},{"paperId":"1b8cdff9e5eefc614cd8940c386b26ce215d32b6","title":"DeBEIR: A Python Package for Dense Bi-Encoder Information Retrieval"},{"paperId":"9ecf184dd657640cbd1c4cc0f3c801ebd9d53162","title":"ReactIE: Enhancing Chemical Reaction Extraction with Weak Supervision"},{"paperId":"fd42031baa3fe8690a00767c2fdf52dbcf945713","title":"Exploring the In-context Learning Ability of Large Language Model for Biomedical Concept Linking"},{"paperId":"bffe2951b1ec9ebde3677a7aadc8b91212a9fdfc","title":"BioCPT: Contrastive Pre-trained Transformers with Large-scale PubMed Search Logs for Zero-shot Biomedical Information Retrieval"},{"paperId":"5e791fa8bd5ebe8b31957345112cb03ceec23b9e","title":"Automated Recognition of Visual Acuity Measurements in Ophthalmology Clinical Notes Using Deep Learning"},{"paperId":"316a845b449b64bed6f2aeab94045e5d77c25a8b","title":"Biomedical extractive question answering based on dynamic routing and answer voting"},{"paperId":"ec23e0a536e0c2d09cfb115e11842fc4575043a2","title":"A social media event detection framework based on transformers and swarm optimization for public notification of crises and emergency management"},{"paperId":"3ec26a84129f1dfd758563d116a4c2b8976732e2","title":"Counterfactual can be strong in medical question and answering"},{"paperId":"e9388e699a7ef4d12ae425e341ff610c67cbf64b","title":"How far is Language Model from 100% Few-shot Named Entity Recognition in Medical Domain"},{"paperId":"d935f18e9942bc6ecaefad2c739a5c4a83f3b121","title":"SenRev: Measurement of Personal Information Disclosure in Online Health Communities"},{"paperId":"cb3fc8d828f5c7628e5bc5f7427dc567a4043788","title":"ShortMail: An email summarizer system"},{"paperId":"7d9fbd1ce90c2e6b64ba054c9f78cc5c45756cac","title":"Self-prediction of relations in GO facilitates its quality auditing"},{"paperId":"d798b65bb57f54c62e6f54b074c580fb7bc360d3","title":"RDKG-115: Assisting drug repurposing and discovery for rare diseases by trimodal knowledge graph embedding"},{"paperId":"bc67c4134b772e0af693d33cd27d7fd033d61253","title":"SPBERE: Boosting span-based pipeline biomedical entity and relation extraction via entity information."},{"paperId":"77bb65ee2e7d5cb0515c6d77a565c15e06b10f0c","title":"MEGACare: Knowledge-guided multi-view hypergraph predictive framework for healthcare"},{"paperId":"d71b646b9d159996866b9436a75a85c31a7ffce0","title":"Biomedical document relation extraction with prompt learning and KNN"},{"paperId":"f6974a11315fa698dc584e9f09316d61f414e952","title":"Learning Representations on Logs for AIOps"},{"paperId":"2fa9f26c26ae7872cd9a6e5833e563b80b25d5fe","title":"Transfer Learning Improves Unsupervised Assignment of ICD codes with Clinical Notes"},{"paperId":"94a443efe70955abf9831e0afd6cc44b18e9af39","title":"Information Flow in Graph Neural Networks: A Clinical Triage Use Case"},{"paperId":"be6cf83660d71b6ee6dd7c231fc57dd6fffb774d","title":"MIKA: Manager for Intelligent Knowledge Access Toolkit for Engineering Knowledge Discovery and Information Retrieval"},{"paperId":"92eab493a403adb72cca77ef645b9173d5cc0523","title":"Hybrid approach combining deep learning and a rule based expert system for concept extraction from prescriptions"},{"paperId":"b39c7419f505ba9f2c801dd3ea46a83d5b77c541","title":"ChatGPT for phenotypes extraction: one model to rule them all?"},{"paperId":"b44865672b3896e249b81a39cbe850286f8140c0","title":"Transformers in Healthcare: A Survey"},{"paperId":"30f36f68265823c7f9945f902451fe0b1fac790b","title":"Biomedical Language Models are Robust to Sub-optimal Tokenization"},{"paperId":"ebb3d299213bae89b5d302cc3dfc36573ec83956","title":"SummQA at MEDIQA-Chat 2023: In-Context Learning with GPT-4 for Medical Summarization"},{"paperId":"ee29656917619452360fe5dd848251a5081944bf","title":"Machine learning for potion development at Hogwarts"},{"paperId":"439c2a5c4883b421ca316617b1306583cc1d706c","title":"Automated Extraction and Visualization of Metabolic Networks from Biomedical Literature Using a Large Language Model"},{"paperId":"4f20a597685bb1554c318d0b75cf71f608708166","title":"Knowledge and skills extraction from the job requirements texts"},{"paperId":"06c235c27db3e6f43413ccf99490f9b22cc83aaf","title":"Is ChatGPT a Biomedical Expert?"},{"paperId":"ce6c1514a75619996e9fd686e53ff1f480b87d51","title":"Biomedical Entity Recognition by Detection and Matching"},{"paperId":"668eb1a3c4878363b56037af0beb262d9e08e09d","title":"CamemBERT-bio: a Tasty French Language Model Better for your Health"},{"paperId":"d8b8d0109f9b02442b366b10517e99f15e08a277","title":"Nested Named Entity Recognition as Building Local Hypergraphs"},{"paperId":"a1306e40b8d0c89150967c6b63b4bf4cbf67c8a3","title":"EASAL: Entity-Aware Subsequence-Based Active Learning for Named Entity Recognition"},{"paperId":"593434904a5fe302aa876d7736a737c86e795724","title":"A Simple Yet Effective Subsequence-Enhanced Approach for Cross-Domain NER"},{"paperId":"2a8fe3f0c6d39efcd5e0c1a8d186fb736b0ab414","title":"Extracting Periodontitis Diagnosis in Clinical Notes with RoBERTa and Regular Expression"},{"paperId":"fe3aed731150fc0a2ad2019050de88e0311381dd","title":"Identifying Major Depressive Disorder From Clinical Notes Using Neural Language Models with Distant Supervision"},{"paperId":"a28b42541034d0e18d925f03b1958a247787c322","title":"Annotate French Clinical Data Using Large Language Model Predictions"},{"paperId":"bf758359f5f9bbf3201ec1cf2444fd231f56d01a","title":"Classification of Patient Portal Messages with BERT-based Language Models"},{"paperId":"920953238e203dbfc507ec6a1d103ff49ddabf09","title":"Multimodal Deep Learning Methods on Image and Textual Data to Predict Radiotherapy Structure Names"},{"paperId":"0c4a1d3bb96df512037c6f0ae7630aa696e09180","title":"Vocabulary Matters: An Annotation Pipeline and Two Deep Learning Algorithms for Enzyme Named Entity Recognition"},{"paperId":"ba95eca1bc4b99c1b95246d09efd8bf46de2a6e9","title":"Stress Testing BERT Anaphora Resolution Models for Reaction Extraction in Chemical Patents"},{"paperId":"234f618b5ac7ecae8228009280e95c2f74a113dd","title":"A Literature Mining Method Based on Curve Graph Information in Literature"},{"paperId":"554b6817197821d006fd60aa8515712f30c71e75","title":"Supervised Text Classification System Detects Fontan Patients in Electronic Records With Higher Accuracy Than ICD Codes"},{"paperId":"aed522d7ea6a05349d0f6e5365b9284d463b8a52","title":"Explainable online health information truthfulness in Consumer Health Search"},{"paperId":"e96d3f85aa56f027e028189346e043e346f3acea","title":"LMFlow: An Extensible Toolkit for Finetuning and Inference of Large Foundation Models"},{"paperId":"1d633934bb4164c48f1c29bf2632492fe31b629b","title":"Bidirectional End-to-End Learning of Retriever-Reader Paradigm for Entity Linking"},{"paperId":"c7288139fa83a54c6bbc3680535256371678ff1e","title":"Exploring New Frontiers in Agricultural NLP: Investigating the Potential of Large Language Models for Food Applications"},{"paperId":"4ca6b4cd66c418478461f6e741b5195dd9aae763","title":"Provision and Characterization of a Corpus for Pharmaceutical, Biomedical Named Entity Recognition for Pharmacovigilance: Evaluation of Language Registers and Training Data Sufficiency"},{"paperId":"e623fc12e03d599de94d6384601723416bedd41c","title":"Enhancing Documents with Multidimensional Relevance Statements in Cross-encoder Re-ranking"},{"paperId":"a6418c9f0135a4d1489aa4cf9e67564c57d14706","title":"BioREx: Improving Biomedical Relation Extraction by Leveraging Heterogeneous Datasets"},{"paperId":"e36b3a9b6071a237ab110219d61107f14c7275e2","title":"No Labels?: No Problem! Experiments with active learning strategies for multi-class classification in imbalanced low-resource settings"},{"paperId":"3d50bc44a45a5d9ee98104e45c71c4f072a1d2cd","title":"Legal Holding Extraction from Italian Case Documents using Italian-LEGAL-BERT Text Summarization"},{"paperId":"b0984b2572cfe393d842114e559da1496fc8ce6f","title":"Uncovering Trauma in Genocide Tribunals: An NLP Approach Using the Genocide Transcript Corpus"},{"paperId":"13b4243fa5658b363c75d8faffaab8021bcb1c18","title":"EMSAssist: An End-to-End Mobile Voice Assistant at the Edge for Emergency Medical Services"},{"paperId":"26def038af58e714b23b1fb875c53f09ce388692","title":"A Joint Entity and Relation Extraction Model based on Efficient Sampling and Explicit Interaction"},{"paperId":"8453a04c5a7b41675fd4c6748ce9bc3ae16ecdb9","title":"Automatic Extraction of Comprehensive Drug Safety Information from Adverse Drug Event Narratives in the Korea Adverse Event Reporting System Using Natural Language Processing Techniques"},{"paperId":"67c6295d6af7fe79b5f7e0309e85cfc76495f123","title":"SCALE: Scaling up the Complexity for Advanced Language Model Evaluation"},{"paperId":"aff0fe00ed7892d1185e8c5b66d318d3892abe6e","title":"Opportunities and Challenges for ChatGPT and Large Language Models in Biomedicine and Health"},{"paperId":"d2c4d1b7643e16ccfc0f6975ec84d0ca86a8d791","title":"SoyDNGP: A Web-Accessible Deep Learning Framework for Genomic Prediction in Soybean Breeding"},{"paperId":"ace34d08c8600ddd364d09347295f4529d662a30","title":"Acute stroke CDS: automatic retrieval of thrombolysis contraindications from unstructured clinical letters"},{"paperId":"be26d2b940eac33a9989d1a6bf2316934c6e1837","title":"Building a Corpus for Biomedical Relation Extraction of Species Mentions"},{"paperId":"405c420a5c5753d0d7168e67809b67096c862a0a","title":"Large Language Models and Medical Education: Preparing for a Rapid Transformation in How Trainees Will Learn to Be Doctors"},{"paperId":"e92d9b002849d9f9350b2337194836ec64cef3af","title":"Learning to Summarize Chinese Radiology Findings With a Pre-Trained Encoder"},{"paperId":"9f3778d51275a912a46bd05ace20b101c3f8f0d7","title":"A detailed library perspective on nearly unsupervised information extraction workflows in digital libraries"},{"paperId":"86804135859c64b37b0dac7e41fe4b9a0e8b6023","title":"Contextualized medication event extraction with striding NER and multi-turn QA"},{"paperId":"1bfe35520140dbd1a21508ed3ff814e7ae218464","title":"Weakly supervised information extraction from inscrutable handwritten document images"},{"paperId":"b17e827549b75376a2e56e4324b28e96bb13e576","title":"EriBERTa: A Bilingual Pre-Trained Language Model for Clinical Natural Language Processing"},{"paperId":"6294f078e79828cac21e717813e8f3d02b18a97c","title":"The importance of resource awareness in artificial intelligence for healthcare"},{"paperId":"09734bb2b7da3f7dfc0eb1c093a949e855794d6a","title":"MedKPL: A heterogeneous knowledge enhanced prompt learning framework for transferable diagnosis"},{"paperId":"116c19f5cdf2bf7884fd25ff2a7683ede6eaaa8a","title":"QUERT: Continual Pre-training of Language Model for Query Understanding in Travel Domain Search"},{"paperId":"a866be3528023e70ba4a6e3abf4c1151d3a7eff5","title":"ECGBERT: Understanding Hidden Language of ECGs with Self-Supervised Representation Learning"},{"paperId":"b239648a07d44d71185e48ab79bf5a2bc347e52f","title":"Medical Data Augmentation via ChatGPT: A Case Study on Medication Identification and Medication Event Classification"},{"paperId":"1da7013f004bc2107d58d094cb9e868152b255c9","title":"Using BERT models for breast cancer diagnosis from Turkish radiology reports"},{"paperId":"30987ef379e52b8bf3d942a2458d6774b8580853","title":"$FastDoc$: Domain-Specific Fast Pre-training Technique using Document-Level Metadata and Taxonomy"},{"paperId":"542308ad5c1c0b5ad88e76e6c8d941a6d08ccd01","title":"Interpretable Medical Diagnostics with Structured Data Extraction by Large Language Models"},{"paperId":"bb17c5fb339e758feeed1bd080bf49ba1f097900","title":"Comprehensive evaluation of deep and graph learning on drug-drug interactions prediction"},{"paperId":"db1aa71314016e12e115fbe449a688f523e52e77","title":"CUED at ProbSum 2023: Hierarchical Ensemble of Summarization Models"},{"paperId":"9618aa98729670f74418d2087f5e47ab137856b4","title":"Mixture-of-Domain-Adapters: Decoupling and Injecting Domain Knowledge to Pre-trained Language Models’ Memories"},{"paperId":"5bec96fe13fb1ff9bb860b326bab076fdd1f52e9","title":"Aviation-BERT: A Preliminary Aviation-Specific Natural Language Model"},{"paperId":"23de28dfa8173679f78cd068a860c9c102eaa2ec","title":"Advancing Italian biomedical information extraction with transformers-based models: Methodological insights and multicenter practical application"},{"paperId":"a3f5fa0897ba260aa3d89ba6fc358f742f379a55","title":"Multi-task bioassay pre-training for protein-ligand binding affinity prediction"},{"paperId":"9821928eee1ed1f36a2f5f935a5d31a71eede6b7","title":"Leveraging Knowledge Graph Embeddings to Enhance Contextual Representations for Relation Extraction"},{"paperId":"ea7e6df5b48f36dd13c838fd56744aae6189ee8b","title":"Evaluation of ChatGPT on Biomedical Tasks: A Zero-Shot Comparison with Fine-Tuned Generative Transformers"},{"paperId":"9b11f5e8b40b109cb774e29e5cf5a5baa8beeed8","title":"Generative Text-Guided 3D Vision-Language Pretraining for Unified Medical Image Segmentation"},{"paperId":"9121b7e4cb8d8d241574e7066a8aadcda93b4828","title":"ModuleFormer: Modularity Emerges from Mixture-of-Experts"},{"paperId":"28b07dd3c43238b2de2d7aaa64cb06d0c88411b4","title":"PyTrial: Machine Learning Software and Benchmark for Clinical Trial Applications"},{"paperId":"a7f55cf136f6a24eaa0772006de92979acfc3a58","title":"BioBLP: a modular framework for learning on multimodal biomedical knowledge graphs"},{"paperId":"3846d296a938dcc0a92784da76f9ef90d9de2e29","title":"Integrating domain knowledge for biomedical text analysis into deep learning: A survey"},{"paperId":"7c8254f6d95863fffeef2ba3d0d07f42b0f72e21","title":"MolFM: A Multimodal Molecular Foundation Model"},{"paperId":"f63a02601c7c3fdabcfff118d98e815697c42e0f","title":"shs-nlp at RadSum23: Domain-Adaptive Pre-training of Instruction-tuned LLMs for Radiology Report Impression Generation"},{"paperId":"264fd0af8b0906e3deb5df7f0ec0a7845b8ba213","title":"CoSiNES: Contrastive Siamese Network for Entity Standardization"},{"paperId":"8d277d9264fb6f8bf7cc99f8d91f2e64c12091aa","title":"When does Continuous Learning for BERT make sense?"},{"paperId":"4791aacf07f8ed425003d32ad3138416ffa44ae2","title":"Meta Learning for Domain Agnostic Soft Prompt"},{"paperId":"3f18227b9b23115253e582610b41d864264fe7f7","title":"RadLing: Towards Efficient Radiology Report Understanding"},{"paperId":"cd9c33a2b221d9af7a2da8cd05655db3ca3d18a2","title":"Impact of translation on biomedical information extraction: an experiment on real-life clinical notes."},{"paperId":"93856a907207c770a68f5c91c67854b366b3c1f7","title":"Impact of translation on biomedical information extraction from real-life clinical notes"},{"paperId":"362d4e00506f9bb39d42185a0b128f8602e139a8","title":"Utilizing ChatGPT to Enhance Clinical Trial Enrollment"},{"paperId":"b4f0059c9a706f963e0ab6d82163a7762dafb176","title":"Aci-bench: a Novel Ambient Clinical Intelligence Dataset for Benchmarking Automatic Visit Note Generation"},{"paperId":"4b78ad9179428a746908d05313a160d220e97750","title":"A Comprehensive Survey on Deep Learning for Relation Extraction: Recent Advances and New Frontiers"},{"paperId":"80cee5037d01470edc7fbd20c564f2e1fc2c6b85","title":"MultiLegalPile: A 689GB Multilingual Legal Corpus"},{"paperId":"edb428f00899810457892faaecdbcfbd04a4b42f","title":"Contextual Representation in NLP to Improve Success in Accident Classification of Mine Safety Narratives"},{"paperId":"4a4511d367113da3d3febf813bcb857023665533","title":"An analysis of entity normalization evaluation biases in specialized domains"},{"paperId":"7de4761f76af92762f88e6fa877ce5ea70db445c","title":"HealthE: Recognizing Health Advice & Entities in Online Health Communities"},{"paperId":"120c3c98818ce29dbb9847f221050b2a2a82d4ed","title":"Extensive Evaluation of Transformer-based Architectures for Adverse Drug Events Extraction"},{"paperId":"ac9d0362acd4cb37a0226cc29cedde336a740bb3","title":"EMS-BERT: A Pre-Trained Language Representation Model for the Emergency Medical Services (EMS) Domain"},{"paperId":"34b1220687a8bf25ece402aa3a1053bce959c49c","title":"CMTN: A Convolutional Multi-Level Transformer to Identify Suicidal Behaviors Using Clinical Notes"},{"paperId":"1681b374734559fe476aed69afaa2887f3576ad9","title":"BERT-based natural language processing analysis of French CT reports: Application to the measurement of the positivity rate for pulmonary embolism"},{"paperId":"08d5d3f67cb783ebe6fcb4274116335607a4b3ba","title":"Performance Comparison of Transformer-Based Models on Twitter Health Mention Classification"},{"paperId":"ea537354fc2e2e9c53253aad1dc752f7bf715805","title":"CAISA at SemEval-2023 Task 8: Counterfactual Data Augmentation for Mitigating Class Imbalance in Causal Claim Identification"},{"paperId":"f22d71c7ce9720ba1f717a4f1181488200e78198","title":"LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day"},{"paperId":"ea4cf43b0c4990a2a262cf86c62e3a522e60861f","title":"MTMG: A multi-task model with multi-granularity information for drug-drug interaction extraction"},{"paperId":"dd6ed104420b314ac3b4f3556f35fd07274bf453","title":"Quality of word and concept embeddings in targetted biomedical domains"},{"paperId":"e6c1422edf59b77da143c44e29f0020352e85d65","title":"Applications of cutting-edge artificial intelligence technologies in biomedical literature and document mining"},{"paperId":"c0f1ca1ed0f031ffaaf91a00eb16049917d395af","title":"Probing the EHR for Standardized Nursing Data"},{"paperId":"b5f437115d6c763218f186962811185613d64908","title":"Large-scale neural biomedical entity linking with layer overwriting"},{"paperId":"ee28be89eea46d2d46ea39efc670f08089241bf2","title":"Pre-Training MLM Using Bert for the Albanian Language"},{"paperId":"8952bdaac612090c99a8430e79b26c7059f97350","title":"FindZebra online search delving into rare disease case reports using natural language processing"},{"paperId":"858be2842122005b5a1aabce844676423ab6ca01","title":"CardioBERTpt: Transformer-based Models for Cardiology Language Representation in Portuguese"},{"paperId":"718ab6f71db78fd9ce756901b3321f27cb27d437","title":"Transformer-based Automatic Mapping of Clinical Notes to Specific Clinical Concepts"},{"paperId":"103f454cbcb1b54c77f27b38662138d85983e7fc","title":"ChatGPT in glioma adjuvant therapy decision making: ready to assume the role of a doctor in the tumour board?"},{"paperId":"ee390b0a29d65373aa0f4c64558ed8b02925bec6","title":"A Novel Method for Medical Semantic Word Sense Disambiguation by Using Graph Neural Network"},{"paperId":"1a1f7d3376cb4773aff852bf6bd7f6df229bd5f3","title":"Research on the System of Blockchain Data Sharing and Early-warning Decision for Public Health Emergency"},{"paperId":"6b38963d45e15a83244aafec08438bb9bf8f3831","title":"Serial KinderMiner (SKiM) Discovers and Annotates Biomedical Knowledge Using Co-Occurrence and Transformer Models"},{"paperId":"53707e7d7c5c95495b68fed893069c3457509630","title":"Named Entity Recognition and Normalization for Alzheimer’s Disease Eligibility Criteria"},{"paperId":"6e69fe3f69249dffc90ef2336b842ca7f4724c82","title":"MedNgage: A Dataset for Understanding Engagement in Patient-Nurse Conversations"},{"paperId":"96f006da556061e74751a598c5ff185999efa240","title":"DyGen: Learning from Noisy Labels via Dynamics-Enhanced Generative Modeling"},{"paperId":"8b5d8d852a0b924fe285ed70c3add4a8ff14713c","title":"Shuo Wen Jie Zi: Rethinking Dictionaries and Glyphs for Chinese Language Pre-training"},{"paperId":"5f1d51ce3bc9823e000af9c4ebfdd5d993a53e50","title":"W-procer: Weighted Prototypical Contrastive Learning for Medical Few-Shot Named Entity Recognition"},{"paperId":"d3060876d9ad4e4e50e1c88a8c04186df00f24e2","title":"A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets"},{"paperId":"55971443f3ca7f26056682ecb3da4491a1e4810d","title":"Understanding Breast Cancer Survival: Using Causality and Language Models on Multi-omics Data"},{"paperId":"ebf3a59aacdd9982283d7f41229ee2a93800d6ef","title":"Knowledge-Augmented Reasoning Distillation for Small Language Models in Knowledge-Intensive Tasks"},{"paperId":"04b62536270e27efe280f904f7e27e394f6d0192","title":"Complementary and Integrative Health Lexicon (CIHLex) and Entity Recognition in the Literature"},{"paperId":"a893a42e32651a0d6d1fee327c285765216809ae","title":"Diagnosing Transformers: Illuminating Feature Spaces for Clinical Decision-Making"},{"paperId":"2def8709b2a75ed533663ec7676601ceebdd23c3","title":"Constructing a disease database and using natural language processing to capture and standardize free text clinical information"},{"paperId":"0133c1128f2036ecb6b65ab15c562b71bf4f18a0","title":"Scientific Fact-Checking: A Survey of Resources and Approaches"},{"paperId":"f45bee9da1655320b7fc290d2abc20903bd12545","title":"Leveraging Domain Knowledge for Inclusive and Bias-aware Humanitarian Response Entry Classification"},{"paperId":"51b169701290cd129e0781fc9f3a9918604c89b5","title":"Improving accuracy of GPT-3/4 results on biomedical data using a retrieval-augmented language model"},{"paperId":"26de21346c5b88566a873fa76325673c917ff3a7","title":"W&G-BERT: A Pretrained Language Model for Automotive Warranty and Goodwill Text"},{"paperId":"0cd9a76d89ad2ffdd8ecaa314e141c2b66d05e5e","title":"Extracting Textual Information from Website Using Mixed Rules"},{"paperId":"07d45ce7de598ef03b400f8ddba7d2e055e77a08","title":"BiomedGPT: A Unified and Generalist Biomedical Generative Pre-trained Transformer for Vision, Language, and Multimodal Tasks"},{"paperId":"d8eb6b0535ae6c96921bfc2c08318902c08a3e63","title":"Efficient Document Embeddings via Self-Contrastive Bregman Divergence Learning"},{"paperId":"bb07ac7a94ab5cefc6d40df46fe20b71382ef09d","title":"Neural Natural Language Processing for Long Texts: A Survey of the State-of-the-Art"},{"paperId":"0fbf7ea1a3bd1754ed9aa12ed25906b731ece589","title":"Training Data Extraction From Pre-trained Language Models: A Survey"},{"paperId":"06091944b864d6dc473cab63321a95fb9c4067cc","title":"ChatCAD+: Towards a Universal and Reliable Interactive CAD using LLMs"},{"paperId":"f31b89d216f3c60773e11228fd0b60c37ccfefdf","title":"Ensemble of deep learning language models to support the creation of living systematic reviews for the COVID-19 literature"},{"paperId":"30fd5cad61b20dd39a49fb50c3dbc300d146c049","title":"Injecting Knowledge into Biomedical Pre-trained Models via Polymorphism and Synonymous Substitution"},{"paperId":"4fd05237af737c58c60e4ca8a745f85013681604","title":"Lawyer LLaMA Technical Report"},{"paperId":"180a5bfb5459538127f90db0a44445353175b052","title":"A publication-wide association study (PWAS), historical language models to prioritise novel therapeutic drug targets"},{"paperId":"29bf34941a5f8c33f2262356cb18ffe4f555fbc5","title":"A-BBL: A Risk Prediction Model for Patient Readmission based on Electronic Medical Records"},{"paperId":"3fabdd81cae936132d996c5d7534cc26c9b250cc","title":"Unleashing the Power of NLP and Transformers: A Game-Changer in Medical Research and Clinical Practice and a revolution of Medical Text Analysis.: Case Study: Cancer report classification by priority"},{"paperId":"ca4c88f57a1914024ccfd2e98d59e343c340fb01","title":"Challenges to sharing sample metadata in computational genomics"},{"paperId":"b17969990b3745a494f8fcadae6c8cd0426dc3ec","title":"Pre-training Multi-task Contrastive Learning Models for Scientific Literature Understanding"},{"paperId":"124a3c12d35c4f40d5ce4fb48355cd17af1aca89","title":"BAND: Biomedical Alert News Dataset"},{"paperId":"8d0c37eee7162f33178979b4183f0211e2dcae0d","title":"Difference-Masking: Choosing What to Mask in Continued Pretraining"},{"paperId":"3351c60442e8bd9e109fe2d9cd7fbf806a42dd33","title":"Partial Annotation Learning for Biomedical Entity Recognition"},{"paperId":"12d4cbeea33a6a61d8461648d070fe358c2ac879","title":"Biomedical Named Entity Recognition via Dictionary-based Synonym Generalization"},{"paperId":"1e4c49c9c93678dec95326ce25715fd2a1e64192","title":"Farewell to Aimless Large-scale Pretraining: Influential Subset Selection for Language Model"},{"paperId":"9bd30b7626fb4c229abe41c2c0d19c90dd87e168","title":"Drug–disease association prediction with literature based multi-feature fusion"},{"paperId":"d62ddd64f841cd5c263003a426adb3909b6311bc","title":"Evaluating Prompt-based Question Answering for Object Prediction in the Open Research Knowledge Graph"},{"paperId":"74b94891f8f7ac8d73d9df817b6720e1cb792bcc","title":"Enhancing Small Medical Learners with Privacy-preserving Contextual Prompting"},{"paperId":"de24a9888816de03dfabd9215ceb3443981b8e7e","title":"TADA: Efficient Task-Agnostic Domain Adaptation for Transformers"},{"paperId":"508730e9b10bf7fb048677248b53d82144d63666","title":"ARCH: Large-scale Knowledge Graph via Aggregated Narrative Codified Health Records Analysis"},{"paperId":"fe090a804a6d229034823dde035e4c0655b6665c","title":"Understanding the Effect of Data Augmentation on Knowledge Distillation"},{"paperId":"21c3343148290cf3f3d7739ca6d6f191fc66f613","title":"Gene Set Summarization using Large Language Models"},{"paperId":"4eeade5b60ad6495dfe9f4fb4cd6183a861520af","title":"EduNER: a Chinese named entity recognition dataset for education research"},{"paperId":"c07618042c9ad4ae4b296cc307f21d6b28d3dcdd","title":"ESCOXLM-R: Multilingual Taxonomy-driven Pre-training for the Job Market Domain"},{"paperId":"60b7c6913bead7636cba9aec55b1428c466771e1","title":"MediTab: Scaling Medical Tabular Data Predictors via Data Consolidation, Enrichment, and Refinement"},{"paperId":"8949f157d5e4483f1e9ca29b6ea80f3fe1ad699d","title":"Word Embedding of Dimensionality Reduction for Document Clustering"},{"paperId":"4280acb444242ab708e36c817d4d6682dad70373","title":"PlugMed: Improving Specificity in Patient-Centered Medical Dialogue Generation using In-Context Learning"},{"paperId":"784335a19e41dc0cedc5e030cba85b74ba142eff","title":"Appraising the Potential Uses and Harms of LLMs for Medical Systematic Reviews"},{"paperId":"6783b17fe4328f48403f57009a73f784de09f645","title":"XuanYuan 2.0: A Large Chinese Financial Chat Model with Hundreds of Billions Parameters"},{"paperId":"b396232b0b32212a94d46b9d30253eab3e97dfc5","title":"Decouple knowledge from parameters for plug-and-play language modeling"},{"paperId":"5ecfadb0211dd89a81d066a35a0e2312e991bc4c","title":"BioAug: Conditional Generation based Data Augmentation for Low-Resource Biomedical NER"},{"paperId":"cb5f53cd7d43ac6f48abf4dde6fc58e53a083b0e","title":"Accelerating the integration of ChatGPT and other large‐scale AI models into biomedical research and healthcare"},{"paperId":"cd72278471f49a3e667d6eab56ba9b538b42403b","title":"A domain semantics-enhanced relation extraction model for identifying the railway safety risk"},{"paperId":"a03b0ca43b5b687a6c38789157c3b803c9e02694","title":"Echoes of Biases: How Stigmatizing Language Affects AI Performance"},{"paperId":"80e972c82df37c60970552fb262c68cc24114964","title":"Review on Query-focused Multi-document Summarization (QMDS) with Comparative Analysis"},{"paperId":"393262e1345e71fbd6eb454a4f76eecf2da6634f","title":"Can Publicly Available Models Understand NATO Language? A Named Entity Recognition Case Study"},{"paperId":"dff0efaa388f546249976e8f5c6cee8eb4f12633","title":"An answer recommendation framework for an online cancer community forum"},{"paperId":"44cf14ea05ea9e7bc7d92de1d2228f514671bc44","title":"Use of Machine Learning Tools in Evidence Synthesis of Tobacco Use Among Sexual and Gender Diverse Populations: Algorithm Development and Validation"},{"paperId":"963cc6bddbdd140aa972327be5c04e2b451e45d5","title":"Question-Answering System Extracts Information on Injection Drug Use from Clinical Notes"},{"paperId":"fded78529a6c853bb97ce7755000ff217756bd0d","title":"From language models to large-scale food and biomedical knowledge graphs"},{"paperId":"049288e68caeadf7842df6977e140b47a8a2f89d","title":"MatSci-NLP: Evaluating Scientific Language Models on Materials Science Language Tasks Using Text-to-Schema Modeling"},{"paperId":"2e4e00164e5fb1a6786cc45d39ce95063dca2e70","title":"Deployment of machine learning algorithms to predict sepsis: systematic review and application of the SALIENT clinical AI implementation framework"},{"paperId":"0f7e4790f0b225c4b677e0424631f015cebe364b","title":"Understanding the Clinical Context of Medication Change Events in Clinical Narratives using Pre-trained Clinical Language Models"},{"paperId":"33d9ceeb9bf8e6e8a7ed724e8f2f98d05fc9d941","title":"Optimizing Medical Service Request Processes through Language Modeling and Semantic Search"},{"paperId":"891fed52e58af8150de46f0b7abbcbad4e6f9dcb","title":"INGENIOUS: Using Informative Data Subsets for Efficient Pre-Training of Language Models"},{"paperId":"39831c8c222a8d78fff4d67e7e56f5eeb90fdd7f","title":"Generative Pre-trained Transformer: A Comprehensive Review on Enabling Technologies, Potential Applications, Emerging Challenges, and Future Directions"},{"paperId":"3f0212d72d1f35ef69e18bfaf580fd7456114879","title":"Applications of the Natural Language Processing Tool ChatGPT in Clinical Practice: Comparative Study and Augmented Systematic Review"},{"paperId":"4c4722d3767dae6bc00b7de3e3fa160caaffe483","title":"Privacy-Preserving Prompt Tuning for Large Language Model Services"},{"paperId":"fc19a3b7364cc9ee1092df99cb74426843d92af1","title":"Large language models in biomedical natural language processing: benchmarks, baselines, and recommendations"},{"paperId":"9dc245029868ef9f63c07a2cac57791f2f055ecd","title":"Applying BioBERT to Extract Germline Gene-Disease Associations for Building a Knowledge Graph from the Biomedical Literature"},{"paperId":"c7d4f0e2cd91ea8b66563e43152fa82808b00443","title":"Representation Learning for Person or Entity-Centric Knowledge Graphs: An Application in Healthcare"},{"paperId":"ce7d9f1fab20e8121a6a86107eccbf0064405003","title":"Large language models for oncological applications"},{"paperId":"7445ca3b53cf597b1c81b347b3e954e70b71dee9","title":"GersteinLab at MEDIQA-Chat 2023: Clinical Note Summarization from Doctor-Patient Conversations through Fine-tuning and In-context Learning"},{"paperId":"bf09ea12856f2e055d24a1e41612c2a70e4e24f2","title":"Enhancing Knowledge Management in Healthcare: An Embedding Fusion Approach to Business Rule Representation"},{"paperId":"aa95fa8f677b07d32446de78dca300aae13d0a36","title":"MIReAD: Simple Method for Learning High-quality Representations from Scientific Documents"},{"paperId":"9e005b62ac48fae400028bcc00378d9bb1c04a7b","title":"NER-to-MRC: Named-Entity Recognition Completely Solving as Machine Reading Comprehension"},{"paperId":"86a55cd501409c129441456c4425547f3b981134","title":"SANTA: Separate Strategies for Inaccurate and Incomplete Annotation Noise in Distantly-Supervised Named Entity Recognition"},{"paperId":"802c74851cfd607fde830932f4b46d58f2b79e85","title":"Algorithmic Bias, Generalist Models, and Clinical Medicine"},{"paperId":"76c26e1a30d665c62fe78b7e9c31ed8358915dcc","title":"From Zero to Hero: Harnessing Transformers for Biomedical Named Entity Recognition in Zero- and Few-shot Contexts"},{"paperId":"35d2276749c2c31290d2ff410a305112e742da71","title":"Low-Resource Multi-Granularity Academic Function Recognition Based on Multiple Prompt Knowledge"},{"paperId":"3922365b7b40a447ecc57e027530cc90131e171e","title":"NLI4CT: Multi-Evidence Natural Language Inference for Clinical Trial Reports"},{"paperId":"42b6c3f3ab62311d503e6d30233aa96beac689fe","title":"Harnessing the Power of BERT in the Turkish Clinical Domain: Pretraining Approaches for Limited Data Scenarios"},{"paperId":"f11ea0fe307ce40fee1f18dfc3eef946a1c7a769","title":"SemEval-2023 Task 7: Multi-Evidence Natural Language Inference for Clinical Trial Data"},{"paperId":"75b04f49dd5685fc639c4511a4793a8d7b416abb","title":"Learning Missing Modal Electronic Health Records with Unified Multi-modal Data Embedding and Modality-Aware Attention"},{"paperId":"452fa2baafdfedaa93c6cbe9a28d3927864b185d","title":"SnorkelPlus: A Novel Approach for Identifying Relationships Among Biomedical Entities Within Abstracts"},{"paperId":"ae15f13135748fae111c8cfdeb18b35eb9239cc6","title":"Creating an Ignorance-Base: Exploring Known Unknowns in the Scientific Literature"},{"paperId":"c3b516c152c02bee1896ba6205d71f852ec9b236","title":"Heart disease risk factors detection from electronic health records using advanced NLP and deep learning techniques"},{"paperId":"fcd1d26d443a982ea79e1351bfaf791209e7c74d","title":"Europe PMC annotated full-text corpus for gene/proteins, diseases and organisms"},{"paperId":"60d309178cd324de7980008d768707ba1682b8ea","title":"Siamese BERT Architecture Model with attention mechanism for Textual Semantic Similarity"},{"paperId":"1c8703b93f28db2c748627e176f280b138354c12","title":"Knowledge-graph-enabled biomedical entity linking: a survey"},{"paperId":"28751205d16e6058798a6b2a2a5cf2d63f76f93e","title":"Bloom’s Taxonomy-based exam question classification: The outcome of CNN and optimal pre-trained word embedding technique"},{"paperId":"912ef861b37cfd95d631daefb57a21832efb8380","title":"Biomedical Relation Extraction Using Dependency Graph and Decoder-Enhanced Transformer Model"},{"paperId":"8ff8b3173c6b684d361209d7493568c3588d3daa","title":"Affect Analysis in Arabic Text: Further Pre-Training Language Models for Sentiment and Emotion"},{"paperId":"1227c8f40b62153c7068e4a134711f6ebb402e64","title":"Exploring the knowledge certainty shift: Metaknowledge analysis on drugs via assertion uncertainty burstiness"},{"paperId":"e5d51e257299cc3996b4dc10a3ca55f4934a38df","title":"Deep learning to refine the identification of high-quality clinical research articles from the biomedical literature: Performance evaluation"},{"paperId":"e4d6df552ec096000e459cecfe936ab4d6781c3b","title":"Enhancing early autism prediction based on electronic records using clinical narratives"},{"paperId":"69e3769724b91aa19f852786c04a9d079097ae64","title":"Active learning with feature matching for clinical named entity recognition"},{"paperId":"033cd044b41868c4d1713d917b6ff73f919783c5","title":"KEBLM: Knowledge-Enhanced Biomedical Language Models"},{"paperId":"cd5a1c434fc47892f68a4d0f58d88dc3b6218e05","title":"FooDis: A food-disease relation mining pipeline"},{"paperId":"6486f0b6e443cb864639d4a85277d71cf69f78e0","title":"Embracing Large Language Models for Medical Applications: Opportunities and Challenges"},{"paperId":"618f9c581f6003a79eff5ca2410e3be70bf4abd1","title":"Towards electronic health record-based medical knowledge graph construction, completion, and applications: A literature study"},{"paperId":"662a0ffbbb108e6d51882c788a949e73ff56fe7f","title":"A co-adaptive duality-aware framework for biomedical relation extraction"},{"paperId":"b9f7947227210e44012f806c28510dbf761dacc0","title":"A Joint Extraction System Based on Conditional Layer Normalization for Health Monitoring"},{"paperId":"df8740034b68e4250d0ceefa9fcbdf42c83af25d","title":"A Comparative Study of Cross-Sentence Features for Named Entity Recognition"},{"paperId":"ae495ee34fa97614c79949d528de9dec182f5365","title":"RepresentThemAll: A Universal Learning Representation of Bug Reports"},{"paperId":"e18590ebcd56755fb19d8c1ff3f8ed46a66fe1a8","title":"An integrated explicit and implicit offensive language taxonomy"},{"paperId":"d2685ecfb612493b44b6eaecb5533e8da7b4b7c3","title":"Lessons learned to enable question answering on knowledge graphs extracted from scientific publications: A case study on the coronavirus literature"},{"paperId":"afccd7c756ef9616af588bbd04e1946dbcfdae85","title":"MSQ-BioBERT: Ambiguity Resolution to Enhance BioBERT Medical Question-Answering"},{"paperId":"309275a127536f4efca4a5cda1d47ee7bb0368c3","title":"Contextual Response Interpretation for Automated Structured Interviews: A Case Study in Market Research"},{"paperId":"a0dca1c35f698b7b7af91449427ed035d9e4e049","title":"A Review of ChatGPT Applications in Education, Marketing, Software Engineering, and Healthcare: Benefits, Drawbacks, and Research Directions"},{"paperId":"39c486150cc8179e30ac8bab5935c7990167b5c0","title":"BactInt: A domain driven transfer learning approach and a corpus for extracting inter-bacterial interactions from biomedical text"},{"paperId":"04ee9597be4d6d2457214334e495e591000b5542","title":"PMC-LLaMA: Towards Building Open-source Language Models for Medicine"},{"paperId":"131c6f328c11706de2c43cd16e0b7c5d5e610b6a","title":"Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond"},{"paperId":"37ebfbc27e9a9a9dd772a472c94c2ae664152508","title":"MasonNLP+ at SemEval-2023 Task 8: Extracting Medical Questions, Experiences and Claims from Social Media using Knowledge-Augmented Pre-trained Language Models"},{"paperId":"32fdf76e571089ac463e356694a90599b99c6214","title":"Extracting social determinants of health from clinical note text with classification and sequence-to-sequence approaches"},{"paperId":"d15009785d3449d16244b50ef570922e463ba3ba","title":"A Compressed Language Model Embedding Dataset of ICD 10 CM Descriptions"},{"paperId":"12594b6afe01461384d2856d2bf44f1cf8533e3e","title":"ChatGPT and the rise of large language models: the new AI-driven infodemic threat in public health"},{"paperId":"6377f7a529549af725908e8eaee9d24aef9879cc","title":"Sebis at SemEval-2023 Task 7: A Joint System for Natural Language Inference and Evidence Retrieval from Clinical Trial Reports"},{"paperId":"7480bf28ce20a03f8296925ec3eb6e71ee71935b","title":"Information Extraction Network Based on Multi-Granularity Attention and Multi-Scale Self-Learning"},{"paperId":"dbd1162ad79cf153bad482a5dc63b9430b6e593e","title":"FineEHR: Refine Clinical Note Representations to Improve Mortality Prediction"},{"paperId":"da226d6e7007b39e7f5f0878894419858e3133cb","title":"Improving Model Transferability for Clinical Note Section Classification Models Using Continued Pretraining"},{"paperId":"1a45858d84a3baece50bb690235323ed8579b4c7","title":"Benchmarking ChatGPT-4 on ACR Radiation Oncology In-Training (TXIT) Exam and Red Journal Gray Zone Cases: Potentials and Challenges for AI-Assisted Medical Education and Decision Making in Radiation Oncology"},{"paperId":"89cbb16aea55469e56a7b5051196e14055511a00","title":"DDI-MuG: Multi-aspect graphs for drug-drug interaction extraction"},{"paperId":"286756b2b02d6a7bc49a7ad66686f30831f26c25","title":"Differentiating ChatGPT-Generated and Human-Written Medical Texts: Quantitative Study"},{"paperId":"2d31cf90218fbf3ce4b220ab3bdff74e17d1b4f5","title":"Web Interface of NER and RE with BERT for Biomedical Text Mining"},{"paperId":"ccbc308ff7775af21c3922b6a92faf3fe21f0261","title":"CPK-Adapter: Infusing Medical Knowledge into K-Adapter with Continuous Prompt"},{"paperId":"40ae90005ba612cff567a96b7d0cedeca0d2635a","title":"On the Use of Transformer-Based Models for Intent Detection Using Clustering Algorithms"},{"paperId":"7e8ce82692f8f32c584de37ff2294f3bc7a1acfa","title":"Extracting Drug-Drug Interactions from Biomedical Texts Using BioBERT with Improved Focal Loss"},{"paperId":"a564daa5ffdd4ea4cbb28b6ea459da9f9f65428d","title":"Faithful AI in Medicine: A Systematic Review with Large Language Models and Beyond"},{"paperId":"a0ec9f110ea172aa863862929cc0338934ff93c6","title":"Harnessing Biomedical Literature to Calibrate Clinicians’ Trust in AI Decision Support Systems"},{"paperId":"8e4558c878ecdac1091486727634c1ed5c8c38a8","title":"A Systematic survey on automated text generation tools and techniques: application, evaluation, and challenges"},{"paperId":"be545c9ec5e757513988a11cb7024e026616d8b4","title":"Multi-task learning for few-shot biomedical relation extraction"},{"paperId":"258605dc5b00fe66b72091f947642a554e472aee","title":"Exploring the Trade-Offs: Unified Large Language Models vs Local Fine-Tuned Models for Highly-Specific Radiology NLI Task"},{"paperId":"b3c1fad1f5f8f0213b6d3f3458fa86205a3434f7","title":"A Survey for Biomedical Text Summarization: From Pre-trained to Large Language Models"},{"paperId":"a7f8fd45fbcdd81449cb7a1a6a2b2c18b38f8151","title":"ImpressionGPT: An Iterative Optimizing Framework for Radiology Report Summarization with ChatGPT"},{"paperId":"7e97c05a1374082c49b69c8a19461490d6452efa","title":"EasyNER: A Customizable Easy-to-Use Pipeline for Deep Learning- and Dictionary-based Named Entity Recognition from Medical Text"},{"paperId":"9ec07ad77267af6c304bdf0c2b9c914d296b468b","title":"EDAD: Efficient Domain Adaptive Distillation by Integrating Domain Adaptation and Knowledge Distillation"},{"paperId":"c7705944c22a3d95413bc1a1950521662a25f1b3","title":"Bridging the Gap between Medical Tabular Data and NLP Predictive Models: A Fuzzy-Logic-Based Textualization Approach"},{"paperId":"965e0d4bfe8097baab1947fc23263ae790620e23","title":"AGI for Agriculture"},{"paperId":"90616bb932b345c83b5b70dffc76a75b14805315","title":"From benchmark to bedside: transfer learning from social media to patient-provider text messages for suicide risk prediction"},{"paperId":"30fec23437cf9aaf3e9cb7d0c076483c14893abf","title":"An NLP approach to identify SDoH-related circumstance and suicide crisis from death investigation narratives"},{"paperId":"22c6d46e88dd289d700be2767622cc9e9391a0aa","title":"Large-Scale Biomedical Relation Extraction Across Diverse Relation Types: Model Development and Usability Study on COVID-19"},{"paperId":"b8c9bcec47ac62105e32549e77aac979df8ad481","title":"DisGeReExT: a knowledge discovery system for exploration of disease–gene associations through large-scale literature-wide analysis study"},{"paperId":"989c337316e25f8e5dadf3847f8bac36d4ed0e3c","title":"Drug–drug interaction extraction‐based system: An\n natural language processing\n approach"},{"paperId":"42780f9c7f73d73d7a887e2f787af0e079703d40","title":"Are Large Language Models Ready for Healthcare? A Comparative Study on Clinical Language Understanding"},{"paperId":"4ce68478791bd4cfcdf883d75fa31fc1ebc6c7cc","title":"FrenchMedMCQA: A French Multiple-Choice Question Answering Dataset for Medical domain"},{"paperId":"994b335b0e42955df63c3e963de47655f5aa8b8d","title":"How Does Imperfect Automatic Indexing Affect Semantic Search Performance?"},{"paperId":"744cbd5c2e6edff0584104d196ef45128ac12800","title":"Transfer Learning Approach to Multilabel Biomedical Literature Classification using Transformer Models"},{"paperId":"0940155bec999067aba536d80e37f720ce91c4d0","title":"Automatic ICD-10 Code Association: A Challenging Task on French Clinical Texts"},{"paperId":"1774405ae834c0e3c1f7af2b1e8f963fc23bd4a1","title":"Machine learning for synergistic network pharmacology: a comprehensive overview"},{"paperId":"020e473d8c987dcfb03fcfffeb87b17812447031","title":"Evaluation of ChatGPT Family of Models for Biomedical Reasoning and Classification"},{"paperId":"8f4e198467de15fdbb305d0982ff6f15565ab601","title":"To Asymmetry and Beyond: Structured Pruning of Sequence to Sequence Models for Improved Inference Efficiency"},{"paperId":"4d1d673b0184d910f9b3d182825fc1ae256800ff","title":"G2PTL: A Pre-trained Model for Delivery Address and its Applications in Logistics System"},{"paperId":"677529c13cfebc273e13ae036c09658033b0afee","title":"DrBERT: A Robust Pre-trained Model in French for Biomedical and Clinical domains"},{"paperId":"bce55193d9a887ad00774a9134df08cd521a85ae","title":"DoctorGLM: Fine-tuning your Chinese Doctor is not a Herculean Task"},{"paperId":"538680e08812fadc22ac4a7eefa6b40ae9179b28","title":"Identifying Mentions of Pain in Mental Health Records Text: A Natural Language Processing Approach"},{"paperId":"f822aa4c0a026e67566689e6012b0446b58ab7cd","title":"A marker-based neural network system for extracting social determinants of health"},{"paperId":"771921b829b04d149bb0e06112a731e4a43ce666","title":"Using language models and ontology topology to perform semantic mapping of traits between biomedical datasets"},{"paperId":"a41552d69ca566bfc5c7f1b7e0e97fe61cadb7af","title":"Robust Identification of Figurative Language in Personal Health Mentions on Twitter"},{"paperId":"304b640c1b2559ac1442ac9be54853ac80ec248c","title":"Multimodal data fusion for cancer biomarker discovery with deep learning"},{"paperId":"2da5b68b89d1d53373d122ac8e6fb2d23668c22f","title":"Toward structuring real-world data: Deep learning for extracting oncology information from clinical text with patient-level supervision"},{"paperId":"80652b8e0c03ebfabb6255f097a9704dcb2b79ff","title":"Disto-TRP: An approach for identifying transient receptor potential (TRP) channels using structural information generated by AlphaFold."},{"paperId":"07bfaf2d713040efe69d0c61bcdfd9870bfbaf5a","title":"A novel self-attention enriching mechanism for biomedical question answering"},{"paperId":"4283021777631cbdc0e9a84218d37d2fe0e9f828","title":"Vision-knowledge fusion model for multi-domain medical report generation"},{"paperId":"1cdce64d3832e465adb1151044153c687f8c819d","title":"Analysis of ‘One in a Million’ primary care consultation conversations using natural language processing"},{"paperId":"b56c53936a436ebb7d9e79a0e0da1760184cd3c5","title":"Artificial Intelligence in Pharmaceutical Sciences"},{"paperId":"3b99f8c18dfe1ba61144cfacaf8c22357455024c","title":"Review: A Roadmap to Use Nonstructured Data to Discover Multitarget Cancer Therapies."},{"paperId":"e68d738c6c769fc28d790bfad57b9ce804961f18","title":"Modeling semantic business trajectories of territories for multidisciplinary studies through controlled vocabularies"},{"paperId":"f9ccd94fbe3946b47e07540d232b084ef805fcef","title":"The Future of AI in Ovarian Cancer Research: The Large Language Models Perspective"},{"paperId":"f88e3426f752c0712b87af29aa1bfc811adff2b5","title":"RoBERTa-Assisted Outcome Prediction in Ovarian Cancer Cytoreductive Surgery Using Operative Notes"},{"paperId":"4be694b101230da39a035c0cf4ebb90aa569879c","title":"Quick Dense Retrievers Consume KALE: Post Training KullbackLeibler Alignment of Embeddings for Asymmetrical dual encoders"},{"paperId":"00366a22d89a1c580ecd21108d44e9d8042fd9d8","title":"GPT-4 can pass the Korean National Licensing Examination for Korean Medicine Doctors"},{"paperId":"e3048287564e965203d22837ad7b2c488c19d451","title":"Identifying Functional Status Impairment in People Living With Dementia Through Natural Language Processing of Clinical Documents: Cross-Sectional Study"},{"paperId":"19cd2250f419666d4df441bae7ade1dd9a2f6bf9","title":"ChatGPT in Healthcare: A Taxonomy and Systematic Review"},{"paperId":"83edcfbb206ddad38a971d605da09390604248ea","title":"BloombergGPT: A Large Language Model for Finance"},{"paperId":"1ad9a295ea841599383e5ae3e88381438d4a7db3","title":"Evaluation of GPT and BERT-based models on identifying protein-protein interactions in biomedical text"},{"paperId":"1f78eb8e03774209cff65dba4bfd3b95aef77ded","title":"oBERTa: Improving Sparse Transfer Learning via improved initialization, distillation, and pruning regimes"},{"paperId":"06cf8da98925503cd2f4186dc48d8ef454d5a6f6","title":"Methods of extracting biomedical information from patents and scientific publications (on the example of chemical compounds)"},{"paperId":"de6fd09ed7783b95af7e5f4088a70d5b0244f5aa","title":"Improving large language models for clinical named entity recognition via prompt engineering."},{"paperId":"eaadcb852955ac2b664f7b7cd111a73661aa90df","title":"Biomedical named entity recognition based on fusion multi-features embedding"},{"paperId":"0d7a0b53f65929776b851933d869fa753798bca3","title":"Carolina: a General Corpus of Contemporary Brazilian Portuguese with Provenance, Typology and Versioning Information"},{"paperId":"df6ce09e914f278a95c5c2b6dfaecfaf6e81c8c7","title":"Identifying Reasons for Statin Nonuse in Patients With Diabetes Using Deep Learning of Electronic Health Records"},{"paperId":"a7e80501734132b784aa8866ce1994d56611acd8","title":"Enhancing Biomedical ReQA With Adversarial Hard In-Batch Negative Samples"},{"paperId":"0f8f20ef4bc90a2b210bc5be08a7f326a214556f","title":"An Information Extraction Study: Take In Mind the Tokenization!"},{"paperId":"02634d754f4898ffd68623f8ff6f7861e700ef88","title":"Accurate and Reliable Classification of Unstructured Reports on Their Diagnostic Goal Using BERT Models"},{"paperId":"620facb14edcd4897c00e335569932392894778f","title":"A Disease-Prediction Protocol Integrating Triage Priority and BERT-Based Transfer Learning for Intelligent Triage"},{"paperId":"ea2504a6ca0a520af5ea0c96d00fb28cccc5d410","title":"A Biomedical Entity Extraction Pipeline for Oncology Health Records in Portuguese"},{"paperId":"42c2507cf28070785b92342804aed1eba4380400","title":"Bias Amplification in Intersectional Subpopulations for Clinical Phenotyping by Large Language Models"},{"paperId":"3494e10a099ffef4e87f0a84d64af8f1a527b80c","title":"ChatGPT in glioma patient adjuvant therapy decision making: ready to assume the role of a doctor in the tumour board?"},{"paperId":"812b6f4dd78edb52959a660c1ac3cdcf5f8e13c6","title":"Bat4RCT: A suite of benchmark data and baseline methods for text classification of randomized controlled trials"},{"paperId":"2d0b9af28c80cfa5c87c08a249af7393c6b4695f","title":"Enabling Early Health Care Intervention by Detecting Depression in Users of Web-Based Forums using Language Models: Longitudinal Analysis and Evaluation"},{"paperId":"9305fd6d87007c7b90d2e0db579ff40467352969","title":"Natural language processing to automatically extract the presence and severity of esophagitis in notes of patients undergoing radiotherapy"},{"paperId":"ec13360ba5820b228333bc21d12f4871250546e8","title":"Lay Text Summarisation Using Natural Language Processing: A Narrative Literature Review"},{"paperId":"44caf26949b4799e21f7b0754fe61315e8b71542","title":"Compositional Zero-Shot Domain Transfer with Text-to-Text Models"},{"paperId":"3f11e89e7f19e931adf31b91f15302d7539c809d","title":"A Joint Domain-Specific Pre-Training Method Based on Data Enhancement"},{"paperId":"92bf19978212365fd372d99e946ae86f36fb20b0","title":"Deep Learning Based Structural Reliability Finite Element Analysis Surrogate for Hydro-Generator Lower Frame"},{"paperId":"256979852e0e0a5fe5cc8ddbf54fa1af2a843722","title":"ChatGPT for shaping the future of dentistry: the potential of multi-modal large language model"},{"paperId":"6fdcc152422c64de86c859b53669c0548261ec09","title":"GrapeQA: GRaph Augmentation and Pruning to Enhance Question-Answering"},{"paperId":"e9e52a94f1cd46da9ae47e40ba2981ba87cb92ab","title":"Analyzing the Generalizability of Deep Contextualized Language Representations For Text Classification"},{"paperId":"443d898928eb8e32d2e6f8f287beaa63f5b00eb9","title":"JaCoText: A Pretrained Model for Java Code-Text Generation"},{"paperId":"0096f13f307a5bfae8cb92f722f9811e10d16ee5","title":"ExBEHRT: Extended Transformer for Electronic Health Records to Predict Disease Subtypes & Progressions"},{"paperId":"23684a07517870cffd1f97fafbaae16ba22bd2b7","title":"Large AI Models in Health Informatics: Applications, Challenges, and the Future"},{"paperId":"3611674ccd820efc0b59981038bc4161d46b3add","title":"A Systematic Literature Review of the Use of Computational Text Analysis Methods in Intimate Partner Violence Research"},{"paperId":"cff26bda86237d113ed01c812ad8bedd0afbe070","title":"DeID-GPT: Zero-shot Medical Text De-Identification by GPT-4"},{"paperId":"79545d9d30b924df293ee103e46f78aaf3249e51","title":"Leveraging Foundation Models for Clinical Text Analysis"},{"paperId":"9f105fdbe301eb23371f35f697164a19e6c45ed5","title":"OpticalBERT and OpticalTable-SQA: Text- and Table-Based Language Models for the Optical-Materials Domain"},{"paperId":"01caa1fbe78ee2234ade9228c772cbb6d5e47458","title":"Exploring Partial Knowledge Base Inference in Biomedical Entity Linking"},{"paperId":"689e0dfcec660611c1f84490b3055b020b7bd0e1","title":"Public Awareness and Sentiment Analysis of COVID-Related Discussions Using BERT-Based Infoveillance"},{"paperId":"701e61977155143529e44264ccdb8443f07c4660","title":"IK-DDI: a novel framework based on instance position embedding and key external text for DDI extraction"},{"paperId":"0a438980ac42451d6d32dd2ad8ead7b55520408d","title":"A Systematic Review of Transformer-Based Pre-Trained Language Models through Self-Supervised Learning"},{"paperId":"eea77daf238730dd7e3686b33cf31bb771f058ff","title":"B-LBConA: a medical entity disambiguation model based on Bio-LinkBERT and context-aware mechanism"},{"paperId":"0429e9343424ded011eaa46547780c5c17f66fec","title":"A cross-modal deep metric learning model for disease diagnosis based on chest x-ray images"},{"paperId":"171598987de38aeac08ffa338df9e0bbd78d58ca","title":"Applying unsupervised keyphrase methods on concepts extracted from discharge sheets"},{"paperId":"8641c70c106c4f7485e613888b91a58e9812a5a7","title":"MEDBERT.de: A Comprehensive German BERT Model for the Medical Domain"},{"paperId":"c7ccb92677afc2738868cf95d38af79a1adf7e9d","title":"Potential of natural language processing for metadata extraction from environmental scientific publications"},{"paperId":"7d66ec6870d6773d559df20642bb2f30b106edc0","title":"Input-length-shortening and text generation via attention values"},{"paperId":"1fcd70f19c05c37f75bbf856cbb3b8bbef73373a","title":"Contextualized Medication Information Extraction Using Transformer-based Deep Learning Architectures"},{"paperId":"6e96773bac534c87cf0eeaf11c5ba2a596b3380e","title":"Attention is not all you need: the complicated case of ethically using large language models in healthcare and medicine"},{"paperId":"d3d9021293fd0716466306b4a4ec9dbbd1932886","title":"Self-Supervised Learning-Based General Laboratory Progress Pretrained Model for Cardiovascular Event Detection"},{"paperId":"9556d7ad415cadb2c2d9b34e5fa5d9c2192a26b7","title":"Generating multiple-choice questions for medical question answering with distractors and cue-masking"},{"paperId":"17ca48ad1b944c897863f04ba9ffa72674dce1ce","title":"Parallel multi-head attention and term-weighted question embedding for medical visual question answering"},{"paperId":"656e8c5f8bced540425c12d854b2911dddefff14","title":"Multimodal Data Integration for Oncology in the Era of Deep Neural Networks: A Review"},{"paperId":"3440687e1fc734baeab1abee4a86c81347d1422a","title":"aeroBERT-Classifier: Classification of Aerospace Requirements Using BERT"},{"paperId":"fa9482793ab5f9ad6f7429476db6e11f502e2440","title":"Math Function Recognition with Fine-Tuning Pre-Trained Models"},{"paperId":"e5174aeda1baa67c17f4ac630ae2e44453954cc3","title":"Personalisation within bounds: A risk taxonomy and policy framework for the alignment of large language models with personalised feedback"},{"paperId":"758e94d65a10783c2a64e478a2103224d295ab13","title":"Enhanced disease-disease association with information enriched disease representation."},{"paperId":"1475905e3687c21428ef3cad902d465093072fd1","title":"Deep multi-modal intermediate fusion of clinical record and time series data in mortality prediction"},{"paperId":"bdf7bf9e81a6c12e22323d0402885b2ba62f623e","title":"Does Synthetic Data Generation of LLMs Help Clinical Text Mining?"},{"paperId":"38311d8f52e4fef86a121dd91923a1df798f79fd","title":"A Study of Text Summarization in the Medical Domain using BERT and its Variants"},{"paperId":"0606bb9a541ce7e57bd78ac680a7df0225ece30c","title":"Can large language models build causal graphs?"},{"paperId":"914f807de0eaf055aded977419d5d22bb6078d90","title":"Document-level Relation Extraction with Cross-sentence Reasoning Graph"},{"paperId":"b388002a68143f94a6efb12ea75a2d18af64da0d","title":"The named entity recognition of vessel power equipment fault using the multi-details embedding model"},{"paperId":"30809168fff23c852867ad359baaebfae532f0a7","title":"Enhancing Activity Prediction Models in Drug Discovery with the Ability to Understand Human Language"},{"paperId":"62245ea7a34afb2613b7fbffbc3578e12e459f3c","title":"Hybrid Graph Convolutional Network With Online Masked Autoencoder for Robust Multimodal Cancer Survival Prediction"},{"paperId":"7f193c6302b2eed4779354f6763fa1217b123d05","title":"Rad-Former: Structuring Radiology Reports using Transformers*"},{"paperId":"883a5338dee175ae61f323202a5aba80b2458e0f","title":"Deep Learning Based Code Generation Methods: A Literature Review"},{"paperId":"31f32b26e6b997f9d5691a8edc6ea86e434865d8","title":"Constructing and analyzing domain-specific language model for financial text mining"},{"paperId":"1419dea18f214029959097c265024e9e9bc598f3","title":"WeakMeSH: Leveraging provenance information for weakly supervised classification of biomedical articles with emerging MeSH descriptors"},{"paperId":"4d4a96708fc67403176bb2b891b564af7a20c148","title":"RAMM: Retrieval-augmented Biomedical Visual Question Answering with Multi-modal Pre-training"},{"paperId":"a68ba8496d7c7125ac470057f7b2e8fb13845e3a","title":"Domain-adapted large language models for classifying nuclear medicine reports"},{"paperId":"57c14d250f231bf56b3c68441aaa36d389281b0d","title":"Assessment of Natural Language Processing of Electronic Health Records to Measure Goals-of-Care Discussions as a Clinical Trial Outcome"},{"paperId":"6c48f1a429e8371ebdac412602629cb6472db1a8","title":"Domain Word Extension Using Curriculum Learning"},{"paperId":"b169f2ce55e14584d2db6f64eeb5ad2702d39d40","title":"Artificial intelligence foundation and pre-trained models: Fundamentals, applications, opportunities, and social impacts"},{"paperId":"9991fe75b5c3d47699d7aa3b44f2a26a9b29eecd","title":"Extraction and analysis of risk factors from Chinese Chemical Accident Reports"},{"paperId":"10916b9df058a076238f6520435d0961419a4308","title":"Knowledge graph assisted end-to-end medical dialog generation"},{"paperId":"eb78b34409a323fb36af93b5c252ee99a9c036b5","title":"Planarized sentence representation for nested named entity recognition"},{"paperId":"430aa6966c15c4a20a4fb2d8383e136b9cb6cde7","title":"Almanac: Retrieval-Augmented Language Models for Clinical Medicine"},{"paperId":"efa9fc7d5b6b244d8ae3a9c3db98418ec39aa7a3","title":"Precision information extraction for rare disease epidemiology at scale"},{"paperId":"d8035d652a26bc96ceb9d0ba89460d11d4850e76","title":"Knowledge grounded medical dialogue generation using augmented graphs"},{"paperId":"8abee896e893dcf230c9e02de2bb435e33ecba76","title":"Survey on the Biomedical Text Summarization Techniques with an Emphasis on Databases, Techniques, Semantic Approaches, Classification Techniques, and Similarity Measures"},{"paperId":"236445f0a3b1e30b2542e5e64616ff6a8af7e3ea","title":"Language Models are Few-shot Learners for Prognostic Prediction"},{"paperId":"7480d1e33e14ff113413de8dc09b7664dad1c0da","title":"Improving Clinical Decision Making with a Two-Stage Recommender System Based on Language Models: A Case Study on MIMIC-III Dataset"},{"paperId":"fefe6c2eb25da9f9f7982b8718f3abc1de2ada03","title":"Modelling Temporal Document Sequences for Clinical ICD Coding"},{"paperId":"5527c8aca3e9f30b2f6382ab66a83eec2757051e","title":"Construction of Knowledge Graphs: State and Challenges"},{"paperId":"114655441607fbf58f5b174f2905a006b3853d91","title":"FiTs: Fine-grained Two-stage Training for Knowledge-aware Question Answering"},{"paperId":"ef959f7212091d0e9aa7502d15ef7d87dd70b902","title":"Identification of Thermophilic Proteins Based on Sequence-Based Bidirectional Representations from Transformer-Embedding Features"},{"paperId":"5c7353fac22a8fdc43fc2f5c006b5d6902c47e75","title":"On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective"},{"paperId":"b48adefb1d6a2da907f602c1d572704f4599792e","title":"S1000: a better taxonomic name corpus for biomedical information extraction"},{"paperId":"460dea7c62ca0fdc27f671f50b76f477942dea12","title":"Improving text mining in plant health domain with GAN and/or pre-trained language model"},{"paperId":"534be0d0603866e71f873d9940e1281a5a3045fb","title":"Boosting classification reliability of NLP transformer models in the long run"},{"paperId":"692f49d243a23c220b40cbd2b6b26b773d9b31c4","title":"Hashtag-Guided Low-Resource Tweet Classification"},{"paperId":"b58c2110655e950c36bc533fa81f143397a5fe2e","title":"Exploring the Limits of Transfer Learning with Unified Model in the Cybersecurity Domain"},{"paperId":"f221280799e5cc9f4e43f2a26754ff802e22a3f0","title":"Question Answering Chatbots for Biomedical Research Using Transformers"},{"paperId":"6b6ea46e57d026c546e45cbe25ea4be55523a6b6","title":"On the Use of Knowledge Transfer Techniques for Biomedical Named Entity Recognition"},{"paperId":"48de1a31cca6631bd73a5d0854acfda5e5195d66","title":"BORD: A Biomedical Ontology based method for concept Recognition using Distant supervision: Application to Phenotypes and Diseases"},{"paperId":"d0c5f901868f6e2cb126fd51b155f631372a9669","title":"Biomedical Text Classification Using Augmented Word Representation Based on Distributional and Relational Contexts"},{"paperId":"99ecb1ffe691f5414d737c4cb8e824f513c0bb31","title":"Uni-Fold MuSSe: De Novo Protein Complex Prediction with Protein Language Models"},{"paperId":"ef91c31d8aab9fe95fec29149e2fe4568ab2fb32","title":"SwitchPrompt: Learning Domain-Specific Gated Soft Prompts for Classification in Low-Resource Domains"},{"paperId":"fab384a1f667b75a1277244acdbc22625d20caf8","title":"Reveal the Unknown: Out-of-Knowledge-Base Mention Discovery with Entity Linking"},{"paperId":"629bc57782bb4326a3eb5f89314e350729c5f417","title":"AdapterSoup: Weight Averaging to Improve Generalization of Pretrained Language Models"},{"paperId":"5ef821267fa68d3231ed8135ff8ec09f25bb1398","title":"ChatCAD: Interactive Computer-Aided Diagnosis on Medical Image using Large Language Models"},{"paperId":"1929c070964bac55a1d57d13bcaae44b28eb97fb","title":"BLIAM: Literature-based Data Synthesis for Synergistic Drug Combination Prediction"},{"paperId":"83c3bef2a3d24c31bccb8cf638dcdea630567089","title":"Expediting Distributed DNN Training With Device Topology-Aware Graph Deployment"},{"paperId":"ce6f2d68b1a4029ff4a838fcf12d5ad1d47f0e68","title":"Multilingual translation for zero-shot biomedical classification using BioTranslator"},{"paperId":"dd4238d23fea6fc8e9232ca0fec4cad728f213ea","title":"Span-based Named Entity Recognition by Generating and Compressing Information"},{"paperId":"597c60202acd32e4ad7e3fc9f1db993fa25290d6","title":"TaughtNet: Learning Multi-Task Biomedical Named Entity Recognition From Single-Task Teachers"},{"paperId":"d7047f54c65fee3c61288b3c490a862e95ae5092","title":"Lightweight Transformers for Clinical Natural Language Processing"},{"paperId":"1e2839669f61fd99c524690e238f6cbe505e5ffd","title":"Real-Time Visual Feedback to Guide Benchmark Creation: A Human-and-Metric-in-the-Loop Workflow"},{"paperId":"292c3ca299362db1435ae8ea6a35929b430bdb17","title":"Zero-Shot Learning for Requirements Classification: An Exploratory Study"},{"paperId":"4562c122c523f7ea2b7c36ee524a47f59d7e74b2","title":"A Biomedical Knowledge Graph for Biomarker Discovery in Cancer"},{"paperId":"9d379e0e77a57bf0c9e33576c465afcedd13ec89","title":"A prefix and attention map discrimination fusion guided attention for biomedical named entity recognition"},{"paperId":"b3cbce144f18ba1bfbaacb17d6284ba8bb4dbb28","title":"A Review on Clinical Named Entity Recognition"},{"paperId":"fed2fab877ba1af72470d3dc061747d0ea9879d0","title":"Deep learning approach to detection of colonoscopic information from unstructured reports"},{"paperId":"2ad818c34b63aa2260542ac619b7098fb7745bac","title":"Machine learning and deep learning in medicine and neuroimaging"},{"paperId":"10c5079d2baa5a287054d9ddd7806a4c16fd7531","title":"UDAPTER - Efficient Domain Adaptation Using Adapters"},{"paperId":"e7dcdfb7734d59b97f825cce8b3105a2d9b14d10","title":"The Effect of Metadata on Scientific Literature Tagging: A Cross-Field Cross-Model Study"},{"paperId":"e3ec55e9e6720194a0ed5d4033d93a941c8a4f99","title":"Continual Pre-training of Language Models"},{"paperId":"2c6a6eb161c04d1f4149b38321b23878d24f2da3","title":"A survey on Transformers for long sentences and its application to medical notes"},{"paperId":"3b0424149731d10829015cb4ab6299d18162128e","title":"LIQUID: A Framework for List Question Answering Dataset Generation"},{"paperId":"627b6f7687e122b5578f095221f66583850f0ea5","title":"GLADIS: A General and Large Acronym Disambiguation Benchmark"},{"paperId":"4cfec8ec51a0ecd7efbc6e6622ae8f930935f714","title":"Bioformer: an efficient transformer language model for biomedical text mining"},{"paperId":"279cc657655eeb4e96a2eaf3d77f708edbf6a313","title":"Construction and evaluation of a domain-specific knowledge graph for knowledge discovery"},{"paperId":"47a1263ba21a72790334544f2a11b7c0ee4b5e76","title":"Multimodality Representation Learning: A Survey on Evolution, Pretraining and Its Applications"},{"paperId":"cb1a64200edaa038326a177538b6b0d5ba21558a","title":"Informing clinical assessment by contextualizing post-hoc explanations of risk prediction models in type-2 diabetes"}],"references":[{"paperId":"31fe8afa6531400e3b76982a3984c7e2605074f8","title":"Document-level attention-based BiLSTM-CRF incorporating disease dictionary for disease named entity recognition"},{"paperId":"2a567ebd78939d0861d788f0fedff8d40ae62bf2","title":"Publicly Available Clinical BERT Embeddings"},{"paperId":"fa82a552e9001ec95432f63fe4f3172dbba3beab","title":"The cell line ontology-based representation, integration and analysis of cell lines used in China"},{"paperId":"d08be34cb90718b331aa6574dc80b0370ca5895f","title":"A Silver Standard Corpus of Human Phenotype-Gene Relations"},{"paperId":"e65715d20baac11d0a53fd7107de18cb7f67e775","title":"MedMentions: A Large Biomedical Corpus Annotated with UMLS Concepts"},{"paperId":"fc33b11d0cbc6ec891aa5faa88e471bfa3dee361","title":"Clinical Concept Extraction with Contextual Word Embedding"},{"paperId":"f8b901c330e7f946ef93453b24682f294b8764a1","title":"In-domain Context-aware Token Embeddings Improve Biomedical Named Entity Recognition"},{"paperId":"14ad9d060c1e8f0449e697ee189ac346353fbfbc","title":"CollaboNet: collaboration of deep neural networks for biomedical named entity recognition"},{"paperId":"cc09d65183d3b986d1477df8584c96ecfee2b184","title":"Automatic extraction of gene-disease associations from literature using joint ensemble learning"},{"paperId":"2966e82ec5f89f23ec7636acc00c9ee74d491968","title":"Chemical–gene relation extraction using recursive neural network"},{"paperId":"fa4e9eb4020ef7152adbd62906d605d63236ef37","title":"Transfer learning for biomedical named entity recognition with neural networks"},{"paperId":"204a327e58a8594f701e0526fcc12d2c36aaaf4c","title":"An attention‐based BiLSTM‐CRF approach to document‐level chemical named entity recognition"},{"paperId":"3febb2bed8865945e7fddc99efd791887bb7e14f","title":"Deep Contextualized Word Representations"},{"paperId":"418cd8e078ed9b91b08e7915ac6753ed811053e4","title":"Cross-type Biomedical Named Entity Recognition with Deep Multi-Task Learning"},{"paperId":"d5400c4cc068eda5fd4c6f7c7f4bfcea41a5a1f8","title":"A Pilot Study of Biomedical Text Comprehension using an Attention-Based Deep Neural Reader: Design and Experimental Analysis"},{"paperId":"91c035165ee5f251c4a0c0b2af67d2891b404316","title":"NSML: A Machine Learning Platform That Enables You to Focus on Your Models"},{"paperId":"9223c95f0e600aee2dcf476094a5102adc386e0f","title":"Effective Use of Bidirectional Language Modeling for Transfer Learning in Biomedical Named Entity Recognition"},{"paperId":"c54e8c7a4f9c2ebd8787aecafa4cfdb35bfd49e0","title":"Effective Use of Bidirectional Language Modeling for Medical Named Entity Recognition"},{"paperId":"bc8fa64625d9189f5801837e7b133e7fe3c581f7","title":"Learned in Translation: Contextualized Word Vectors"},{"paperId":"db8562bb8dc69a7628c49c92ac8e08b5da91130e","title":"A transition‐based joint model for disease named entity recognition and normalization"},{"paperId":"91d4498849fe1966a629cddb187d3cd62eedb2ca","title":"Deep learning with word embeddings improves biomedical named entity recognition"},{"paperId":"4c16a6fd7b4aad8c1331e4753b30701fdf6d12f4","title":"Neural Domain Adaptation for Biomedical Question Answering"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"f27c51137c40940e2facc0ec932cf560967e1f5a","title":"ChimerDB 3.0: an enhanced database for fusion genes from cancer transcriptome and literature data mining"},{"paperId":"c6850869aa5e78a107c378d2e8bfa39633158c0c","title":"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"},{"paperId":"05dd7254b632376973f3a1b4d39485da17814df5","title":"SQuAD: 100,000+ Questions for Machine Comprehension of Text"},{"paperId":"61322ec6cfc54fe9723d4637239b8fb9938dc501","title":"BioCreative V CDR task corpus: a resource for chemical disease relation extraction"},{"paperId":"c4dd9a19d822c965ce8cde55ab23b8a0b628278a","title":"An overview of the BIOASQ large-scale biomedical semantic indexing and question answering competition"},{"paperId":"932117813af889104cafb457ebbc52a38d8b64eb","title":"The CHEMDNER corpus of chemicals and drugs and its annotation principles"},{"paperId":"f37e1b62a767a307c046404ca96bc140b3e68cb5","title":"GloVe: Global Vectors for Word Representation"},{"paperId":"cfb4edb7541fafcf593b466320c63ae32d27f57e","title":"Extraction of relations between genes and diseases from text and large-scale data analysis: implications for translational research"},{"paperId":"696753d59185436ec95ecf3021c413f353be4874","title":"NCBI disease corpus: A resource for disease name recognition and concept normalization"},{"paperId":"87f40e6f3022adbc1f1905e3e506abad05a9964f","title":"Distributed Representations of Words and Phrases and their Compositionality"},{"paperId":"c83d05b15797ade0f8dffb9a311a859682d43a27","title":"The SPECIES and ORGANISMS Resources for Fast and Accurate Identification of Taxonomic Names in Text"},{"paperId":"506c7e333efa0b31823c1b1914b1180c346773ee","title":"The EU-ADR corpus: Annotated drugs, diseases, targets, and their relationships"},{"paperId":"5e095981ebf4d389e9356bd56e59e0ade1b42e88","title":"2010 i2b2/VA challenge on concepts, assertions, and relations in clinical text"},{"paperId":"e1039346942874c59d89028bb934b72524827b8c","title":"LINNAEUS: A species name identification system for biomedical literature"},{"paperId":"f5a0c6593ba95d23c025608ce9280848da8b929f","title":"Overview of BioCreative II gene mention recognition"},{"paperId":"3bd4d2de49d8a092abb295b845dba14874f8787d","title":"Introduction to the Bio-entity Recognition Task at JNLPBA"},{"paperId":"28209ce8d0ac1cf4ceea3eeddf4630e1032fa0ef","title":"A neural probabilistic language model"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"afc317b098cd6744611049ff16f351032ab14f83","title":"A BERT-based Universal Model for Both Within- and Cross-sentence Clinical Temporal Relation Extraction"},{"paperId":"eed781f498b563df5a9e8a241c67d63dd1d92ad5","title":"Overview of the BioCreative VI chemical-protein interaction Track"},{"paperId":"2913c2bf3f92b5ae369400a42b2d27cc5bc05ecb","title":"Deep Learning"},{"paperId":"e2f28568031e1902d4f8ee818261f0f2c20de6dd","title":"Distributional Semantics Resources for Biomedical Text Processing"}],"id":"1e43c7084bdcb6b3102afaf301cce10faead2702","summary":"This article introduces BioBERT (Bidirectional Encoder Representations from Transformers for Biomedical Text Mining), which is a domain-specific language representation model pre-trained on large-scale biomedical corpora that largely outperforms BERT and previous state-of-the-art models in a variety of biomedical text mining tasks when pre- trained on biomedical Corpora."},{"url":"https://www.semanticscholar.org/paper/af997821231898a5f8d0fd78dad4eec526acabe5","title":"Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models","venue":"arXiv.org","year":2023,"referenceCount":61,"citationCount":330,"influentialCitationCount":6,"publicationDate":"08/03/2023","authors":"Chenfei Wu,Sheng-Kai Yin,Weizhen Qi,Xiaodong Wang,Zecheng Tang,Nan Duan","citations":[{"paperId":"ac8089bb7944090cf1de5df25aadf5e6356f3040","title":"TempCompass: Do Video LLMs Really Understand Videos?"},{"paperId":"86188727c4d4f3eb064ae7ff0d9a3483b4ef47c1","title":"Typographic Attacks in Large Multimodal Models Can be Alleviated by More Informative Prompts"},{"paperId":"933ece2cbaa5a023094ce43673c35af44695ab88","title":"SoK: Exploring the Potential of Large Language Models for Improving Digital Forensic Investigation Efficiency"},{"paperId":"ded3266d047b36a963c1324aa9f98705d598bdcf","title":"From Summary to Action: Enhancing Large Language Models for Complex Tasks with Open World APIs"},{"paperId":"789485978d69e248832df358ee0fb062012925b8","title":"Agent-Pro: Learning to Evolve via Policy-Level Reflection and Optimization"},{"paperId":"74c68aed85f2fe8019113bbdb533fcba7e3ce0bd","title":"ShapeLLM: Universal 3D Object Understanding for Embodied Interaction"},{"paperId":"a7059d44991cc98c75e86a915390dd3abd4fe5fb","title":"Multi-Bit Distortion-Free Watermarking for Large Language Models"},{"paperId":"2cea424c7dce71042c24d43317521abdc4c0ffb4","title":"Large Multimodal Agents: A Survey"},{"paperId":"8e090a67b25ec43765a6998836f38d031e7b47a4","title":"Label-efficient Multi-organ Segmentation Method with Diffusion Model"},{"paperId":"090e1f1efb36735171fa1d487d80e93d42b54c6f","title":"AutoMMLab: Automatically Generating Deployable Models from Language Instructions for Computer Vision Tasks"},{"paperId":"c44393114047e0f9c32e45b381970d50ed503260","title":"LLMs Meet Long Video: Advancing Long Video Comprehension with An Interactive Visual Adapter in LLMs"},{"paperId":"b6b5ffb9a1faee7b488eb46f2d68f62beaecb8c7","title":"Modality-Aware Integration with Large Language Models for Knowledge-based Visual Question Answering"},{"paperId":"83d201d503b863fec7d1225f00a141e722e03f17","title":"Using Left and Right Brains Together: Towards Vision and Language Planning"},{"paperId":"710b1e23b09e0b826f9d47e7cc23b5f4c0808c7e","title":"Multi-modal preference alignment remedies regression of visual instruction tuning on language model"},{"paperId":"5439a10e45bf1af937b1009348fb9582d9c3d640","title":"L3GO: Language Agents with Chain-of-3D-Thoughts for Generating Unconventional Objects"},{"paperId":"f4e4f2090b639372daf32d4b466752b0fb32b261","title":"Tell Me More! Towards Implicit User Intention Understanding of Language Model Driven Agents"},{"paperId":"d6fc25e3ccb0ea314eefe7c49f02985e4ddd48de","title":"Towards a Foundation Model for Brain Age Prediction using coVariance Neural Networks"},{"paperId":"b90ea57bf10f7ed1d50dd051604d68fb892c5633","title":"GraphTranslator: Aligning Graph Model to Large Language Model for Open-ended Tasks"},{"paperId":"930b5331056d6e543191866df530ce3d5061c2ed","title":"The Future of Cognitive Strategy-enhanced Persuasive Dialogue Agents: New Perspectives and Trends"},{"paperId":"eaad6e351ab7ddb5a31bce3c5fe8bf38cd08c7f2","title":"Multimodal Query Suggestion with Multi-Agent Reinforcement Learning from Human Feedback"},{"paperId":"4c98e18cf16395b95ffaaeeac3eceffa608dcf8d","title":"\"Task Success\" is not Enough: Investigating the Use of Video-Language Models as Behavior Critics for Catching Undesirable Agent Behaviors"},{"paperId":"7e281e8ab380affd3c5724feae038274df378511","title":"Understanding the planning of LLM agents: A survey"},{"paperId":"83ee82e62f2eae18cc3472120eb9004109895a31","title":"Delving into Multi-modal Multi-task Foundation Models for Road Scene Understanding: From Learning Paradigm Perspectives"},{"paperId":"33b5dd24169ff876420f56c19c0a65b6194cb49f","title":"User Intent Recognition and Satisfaction with Large Language Models: A User Study with ChatGPT"},{"paperId":"f1057758c7ca7fe2babeea4908c281be9d1075c4","title":"IMUGPT 2.0: Language-Based Cross Modality Transfer for Sensor-Based Human Activity Recognition"},{"paperId":"6b3d58d367b049532c0dd7a203eb0c17c94e734e","title":"Image Anything: Towards Reasoning-coherent and Training-free Multi-modal Image Generation"},{"paperId":"b5710193dc676e0b9043f00759d776d10d3f6ea5","title":"PlantoGraphy: Incorporating Iterative Design Process into Generative Artificial Intelligence for Landscape Rendering"},{"paperId":"c5db6c2726911b72d534f97bd4d1ed63f6431340","title":"Mobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual Perception"},{"paperId":"9cd6c6d85de6180dd92ba43e685663067cf3ab7f","title":"Grounded SAM: Assembling Open-World Models for Diverse Visual Tasks"},{"paperId":"a050c9b0c321839e4427ab9defa3463be7825ac4","title":"MM-LLMs: Recent Advances in MultiModal Large Language Models"},{"paperId":"35c2f6f40d54d578bcdcf9797c7f153088554572","title":"GraphiMind: LLM-centric Interface for Information Graphics Design"},{"paperId":"140cfda71bfff852c3e205b7ad61854b78c76982","title":"Mastering Text-to-Image Diffusion: Recaptioning, Planning, and Generating with Multimodal LLMs"},{"paperId":"23957040943f883542f47850c709b9e7f9d6fa55","title":"Prompting Large Vision-Language Models for Compositional Reasoning"},{"paperId":"4f2a56102bcbf0fe79379c4c27daecbccfb35a26","title":"MLLM-Tool: A Multimodal Large Language Model For Tool Agent Learning"},{"paperId":"e80252dabd94148ad959c42e0121795939a78198","title":"Remote Sensing ChatGPT: Solving Remote Sensing Tasks with ChatGPT and Visual Models"},{"paperId":"7f6b1bdebb0f772230412a4451e98a5ba77ba043","title":"Vlogger: Make Your Dream A Vlog"},{"paperId":"4a48d628e53f554eb6ef09a457ca855188b96171","title":"DoraemonGPT: Toward Understanding Dynamic Scenes with Large Language Models"},{"paperId":"ff61aef2fef3a235bfaa123158a990c4f5f27d1a","title":"Small LLMs Are Weak Tool Learners: A Multi-LLM Agent"},{"paperId":"79e729b0a05547486f2e450387022e9c9fe6d729","title":"PersianMind: A Cross-Lingual Persian-English Large Language Model"},{"paperId":"002d2c4569d070a55fe69c25ebccad8e9ddae572","title":"Incorporating Visual Experts to Resolve the Information Loss in Multimodal Large Language Models"},{"paperId":"4bebe389dfa85423e5cc089edf20b2c3f572f38c","title":"Self-Contrast: Better Reflection Through Inconsistent Solving Perspectives"},{"paperId":"9eab4104973f5de650544729a4a69d84c594da92","title":"A Vision Check-up for Language Models"},{"paperId":"7bfc47f93c5b48222cc553883d24a4f0be291328","title":"Self-supervision advances morphological profiling by unlocking powerful image representations"},{"paperId":"b33b8f7508f8a802e2e669ec16348bb789ae0470","title":"Taking the Next Step with Generative Artificial Intelligence: The Transformative Role of Multimodal Large Language Models in Science Education"},{"paperId":"a06d3e9e90008c64c45a0029d580541d5f646771","title":"If LLM Is the Wizard, Then Code Is the Wand: A Survey on How Code Empowers Large Language Models to Serve as Intelligent Agents"},{"paperId":"575f403261d5f99526f0b4dfc8644352d6c4467a","title":"DocLLM: A layout-aware generative language model for multimodal document understanding"},{"paperId":"5f58863dd6474d6f127be995b5871e7c60f2792f","title":"Video Understanding with Large Language Models: A Survey"},{"paperId":"b6ca4b1bee98f4317dd25c764a8057e6ef11c89e","title":"GitAgent: Facilitating Autonomous Agent with GitHub by Tool Extension"},{"paperId":"46f64681d7a0c7f80303bebb0d62a3b7acbcdcd9","title":"An Improved Baseline for Reasoning Segmentation with Large Language Model"},{"paperId":"3e525c72d29c0f6a86dc4ebab673f5b3db069a0c","title":"Gemini Pro Defeated by GPT-4V: Evidence from Education"},{"paperId":"82dc8edc49f9edf4a53056aedcdfb339be070166","title":"IQAGPT: Image Quality Assessment with Vision-language and ChatGPT Models"},{"paperId":"4599d5af850da482f591a02a3b17d56e0d358771","title":"Plan, Posture and Go: Towards Open-World Text-to-Motion Generation"},{"paperId":"c672ec79f55cef8f7a32cd8dddfa981b893f1567","title":"V*: Guided Visual Search as a Core Mechanism in Multimodal LLMs"},{"paperId":"6a33e58ef961a3a0a5657518b2be86395eb7c8d0","title":"InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks"},{"paperId":"24fc9ad715372358bd0108eeb7c944b915963293","title":"ASSISTGUI: Task-Oriented Desktop Graphical User Interface Automation"},{"paperId":"17a32c825bd746a2625eddc2728092171a9ef72a","title":"Jack of All Tasks, Master of Many: Designing General-purpose Coarse-to-Fine Vision-Language Model"},{"paperId":"35a17f896847614a71df772bbe2b66ae231cabc7","title":"CLOVA: A Closed-Loop Visual Assistant with Tool Usage and Update"},{"paperId":"95d791ad14db2c779daa67ca7fdc3a75214c42eb","title":"3DAxiesPrompts: Unleashing the 3D Spatial Task Capabilities of GPT-4V"},{"paperId":"ea6982a936a2b263bbf46ff6eb27fc0b63fddaf7","title":"VL-GPT: A Generative Pre-trained Transformer for Vision and Language Understanding and Generation"},{"paperId":"6d2ab31aa75468f5458b9d96192c3f4a28f55d73","title":"DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving"},{"paperId":"5a97f677ab02602f5f319ad4b3dcd3ed8c53238f","title":"Spatial Interpretation and LLMs"},{"paperId":"55c6d16b550c606d62dd85084f0d373d8f087966","title":"VLAP: Efficient Video-Language Alignment via Frame Prompting and Distilling for Video Question Answering"},{"paperId":"475a396daeb1972b078ef175f51f44231e7f6b21","title":"LDM²: A Large Decision Model Imitating Human Cognition with Dynamic Memory Enhancement"},{"paperId":"33c3dbf86dd6e338e2a49bba9c2e2193d1eca08a","title":"Vista-LLaMA: Reliable Video Narrator via Equal Distance to Visual Tokens"},{"paperId":"b240a1d8ec2860bdd7370daa3144268ce46ac018","title":"Vary: Scaling up the Vision Vocabulary for Large Vision-Language Models"},{"paperId":"6e8a92479981afec0610414d0501d8ef56c555dc","title":"NLLG Quarterly arXiv Report 09/23: What are the most influential current AI Papers?"},{"paperId":"e2a4369c0febdc3676179ea7a4a68491282c67ae","title":"Image and Data Mining in Reticular Chemistry Using GPT-4V"},{"paperId":"96a7b0fe722e6d2d5167ef25a6aff714a20233a0","title":"Exploring the Limits of ChatGPT in Software Security Applications"},{"paperId":"10578bc0bdb3ebf9232931dd4961f55ba470caad","title":"LaMPilot: An Open Benchmark Dataset for Autonomous Driving with Language Model Programs"},{"paperId":"b92289123a94f6076505487adfb4513bd3495c1d","title":"LEGO: Learning EGOcentric Action Frame Generation via Visual Instruction Tuning"},{"paperId":"06b2ac5153e3d8d05c13c82f93d7f4e13eee6d0f","title":"Mitigating Fine-Grained Hallucination by Fine-Tuning Large Vision-Language Models with Caption Rewrites"},{"paperId":"ef4e4e4b52d4379ab5387d8dc53da87e561e78db","title":"Good Questions Help Zero-Shot Image Reasoning"},{"paperId":"246017780386eba39d6cda760a1c2c70356baa50","title":"VIoTGPT: Learning to Schedule Vision Tools towards Intelligent Video Internet of Things"},{"paperId":"9e2bac2777eebe603a39f69221689493609d4149","title":"MLLMs-Augmented Visual-Language Representation Learning"},{"paperId":"7492ef863cd376420de462dd1f041abb91172530","title":"Exposing Limitations of Language Model Agents in Sequential-Task Compositions on the Web"},{"paperId":"8441c30ad4abdca9ee380aa6f22ffd731b10231b","title":"COLE: A Hierarchical Generation Framework for Graphic Design"},{"paperId":"486c2df78cbb770a90a55f7fa3fe19102fba2c24","title":"LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models"},{"paperId":"b6ddf3f76eef409b9b105be68511dcfb9ed993d0","title":"The Influence of ChatGPT on Student Learning and Academic Performance"},{"paperId":"8073f493f33842979d71c5b05bbbb276493d3c54","title":"Function-constrained Program Synthesis"},{"paperId":"769a924d0af014acec326f50c15c5d70d258a969","title":"LLMGA: Multimodal Large Language Model based Generation Assistant"},{"paperId":"7c261866e9d8ddc42f3c1f0b1c2c882182d47fc9","title":"Mitigating Hallucination in Visual Language Models with Visual Supervision"},{"paperId":"5eea245cc12c55905d4df827d0c9776c5ddfa743","title":"Compositional Chain-of-Thought Prompting for Large Multimodal Models"},{"paperId":"a756b584f8f8b4307e52895ae2120bc339580ad8","title":"See and Think: Embodied Agent in Virtual Environment"},{"paperId":"7b0a186b0140ee91fb13991c9c7187f3dc3b0670","title":"Visual Programming for Zero-shot Open-Vocabulary 3D Visual Grounding"},{"paperId":"437c7836d32c7f221aad466047130075c7cb5336","title":"Point Cloud Pre-training with Diffusion Models"},{"paperId":"8708914c567ecfc9c1ef4b2a921bbd3abf687737","title":"Benchmarking Robustness of Text-Image Composed Retrieval"},{"paperId":"ee2c769943f9e46c3bbee117d1ecf14566b7bf1f","title":"Boosting the Power of Small Multimodal Reasoning Models to Match Larger Models with Self-Consistency Training"},{"paperId":"52941cadbd340344f3e0a6f50719fe55b3de5088","title":"Multimodal Large Language Models: A Survey"},{"paperId":"da9e284c75334c660029c79f09ed371aaf2f1139","title":"AcademicGPT: Empowering Academic Research"},{"paperId":"c3b75d5bc1401eaeecb365049dcf9781c6f359af","title":"NERIF: GPT-4V for Automatic Scoring of Drawn Models"},{"paperId":"451539c0d0f5f5785ff58d09ca5e67a5f129f9de","title":"A Survey on Multimodal Large Language Models for Autonomous Driving"},{"paperId":"ab8169d6e4dfabfe7c30ebec1bb871bf3e1551cd","title":"GAIA: a benchmark for General AI Assistants"},{"paperId":"5f370f52e24d185ae44bb0ea18cbd4be2aab0d15","title":"VLM-Eval: A General Evaluation on Video Large Language Models"},{"paperId":"107fb6eec2febbae12db29bf3e311aaf5680027c","title":"Video-LLaVA: Learning United Visual Representation by Alignment Before Projection"},{"paperId":"e72f612f29d48010501f043cc2d4c1d0762c2d98","title":"Knowledge Plugins: Enhancing Large Language Models for Domain-Specific Recommendations"},{"paperId":"0f993809c1fe00403ecea66d8f572832f075cfe4","title":"MMC: Advancing Multimodal Chart Understanding with Large-scale Instruction Tuning"},{"paperId":"9574261b66fc006bb7cd9091504b519ce0ce27cf","title":"Defining the boundaries: challenges and advances in identifying cells in microscopy images"},{"paperId":"aad3d2e690f6c73f04a14622ceff51464bbc560e","title":"Chat-UniVi: Unified Visual Representation Empowers Large Language Models with Image and Video Understanding"},{"paperId":"2fb605f67fee79cad94952ddfe0f686e926f49f5","title":"GPT-4V in Wonderland: Large Multimodal Models for Zero-Shot Smartphone GUI Navigation"},{"paperId":"76a3f4a79ae9a00db2f2b5f6877021d8deb96ada","title":"SPHINX: The Joint Mixing of Weights, Tasks, and Visual Embeddings for Multi-modal Large Language Models"},{"paperId":"69bbd2dbb866a6d646b46046851bde4e913f43b7","title":"ChatAnything: Facetime Chat with LLM-Enhanced Personas"},{"paperId":"ecfff30e570498b6c87c5d1319a0cbcdf7a8b86d","title":"LLaVA-Plus: Learning to Use Tools for Creating Multimodal Agents"},{"paperId":"242600cd00d959a0a11096d24fa44ecbdefcac0c","title":"Foundation Models for Mining 5.0: Challenges, Frameworks, and Opportunities"},{"paperId":"33af85c1b2b9325e096a5b8ee36af8d957a45914","title":"PPTC Benchmark: Evaluating Large Language Models for PowerPoint Task Completion"},{"paperId":"990501f48725bf6f8e0a90f215e7426ced576317","title":"DECENTRALISED AUTONOMOUS SOCIETY THROUGH LARGE LANGUAGE MODELS’ BASED AGENTS: A PATHWAY TO EMPOWER SMALL COMMUNITIES"},{"paperId":"c020f15be1dee20f9e2e0c5a6f05f272b5508325","title":"LLaVA-Interactive: An All-in-One Demo for Image Chat, Segmentation, Generation and Editing"},{"paperId":"debe978e02b664fb7254b6c7b58a74c09ce897e3","title":"Plug-and-Play Policy Planner for Large Language Model Powered Dialogue Agents"},{"paperId":"88bddfb7d1e0462be8fe99fdbd71c658140cb17b","title":"From Image to Language: A Critical Analysis of Visual Question Answering (VQA) Approaches, Challenges, and Opportunities"},{"paperId":"e5bded12f9f9ae17ed1d29937967efe48684c47d","title":"Breaking the Trilemma of Privacy, Utility, Efficiency via Controllable Machine Unlearning"},{"paperId":"288e7224d53d68669eb67f2496e068dc965c639e","title":"ControlLLM: Augment Language Models with Tools by Searching on Graphs"},{"paperId":"d30f5e8cd863e89a0a84a9fa1bf84fddd428df7e","title":"MaTCR: Modality-Aligned Thought Chain Reasoning for Multimodal Task-Oriented Dialogue Generation"},{"paperId":"0212dca18cd0765deed0b6ba80a796f0ad46e066","title":"mPLUG-Octopus: The Versatile Assistant Empowered by A Modularized End-to-End Multimodal LLM"},{"paperId":"61a15fcd8c2c356994f56091e1c0af36d0f09d50","title":"Prompt Me Up: Unleashing the Power of Alignments for Multimodal Entity and Relation Extraction"},{"paperId":"f8b8f926bbfa327c86c40796131fe2695db81126","title":"DDCoT: Duty-Distinct Chain-of-Thought Prompting for Multimodal Reasoning in Language Models"},{"paperId":"807f336176070bd3f95b82a16f125ee99b7d2c80","title":"Woodpecker: Hallucination Correction for Multimodal Large Language Models"},{"paperId":"8e3e7deb95d2a984cba615ec847e64f354626cdf","title":"WebWISE: Web Interface Control and Sequential Exploration with Large Language Models"},{"paperId":"b1d2a29860e69c6ce9987ddefbe112feb1efa16a","title":"Large Language Models can Share Images, Too!"},{"paperId":"013719cdcc1cbf9861a661210d981a4f8bf24268","title":"Vision Language Models in Autonomous Driving and Intelligent Transportation Systems"},{"paperId":"f90c522b284a6c065fa5126216a26a7415a2b9fa","title":"MoqaGPT : Zero-Shot Multi-modal Open-domain Question Answering with Large Language Model"},{"paperId":"938d1028b3c3cd4e3d34eed20b622bdc33453f6e","title":"MarineGPT: Unlocking Secrets of Ocean to the Public"},{"paperId":"79e7ead8f59b17431de2b86af10dc0c30a1f5a2b","title":"ToolChain*: Efficient Action Space Navigation in Large Language Models with A* Search"},{"paperId":"58b77dc0603eb52559d98a383bf9649fd31d0bc5","title":"WordArt Designer: User-Driven Artistic Typography Synthesis using Large Language Models"},{"paperId":"cca4218dd7c10c1614bbd84aa7cd7e00027bdc7c","title":"Loop Copilot: Conducting AI Ensembles for Music Generation and Iterative Editing"},{"paperId":"beaf64df85f8204b8cd89a7f46827608e6d16922","title":"MusicAgent: An AI Agent for Music Understanding and Generation with Large Language Models"},{"paperId":"7451d756118628474dc022813eb952a21d34c5f6","title":"Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V"},{"paperId":"36b923d97d7cfaf73d11c55c15ea46605ba974a5","title":"BiLL-VTG: Bridging Large Language Models and Lightweight Visual Tools for Video-based Texts Generation"},{"paperId":"00c19d9818bf093a2eed323d1bd5c763c4f512b9","title":"Reading Books is Great, But Not if You Are Driving! Visually Grounded Reasoning about Defeasible Commonsense Norms"},{"paperId":"b217b6bc340af9a10bebbf8acc36ea30871769bd","title":"In-Context Learning with Iterative Demonstration Selection"},{"paperId":"a3c1f6809dd1455da73eec407bcb3be92e680112","title":"Penetrative AI: Making LLMs Comprehend the Physical World"},{"paperId":"a80546c9847710af1ba8d5f8dca9386e7a520d0a","title":"Precedent-Enhanced Legal Judgment Prediction with LLM and Domain-Model Collaboration"},{"paperId":"03bf1da1caa5f63203d43ed78c12c35a78fc6ed9","title":"EasyGen: Easing Multimodal Generation with a Bidirectional Conditional Diffusion Model and LLMs"},{"paperId":"e690af2f6aaef0ad3b534dd1323fa6f4db8fed5d","title":"NetDiffusion: Network Data Augmentation Through Protocol-Constrained Traffic Generation"},{"paperId":"1d14a708622917da4b9820ada6d32af24fc1651a","title":"Idea2Img: Iterative Self-Refinement with GPT-4V(ision) for Automatic Image Design and Generation"},{"paperId":"e5f8776db5801c61430cf18d3545806f822ef1cb","title":"Jigsaw: Supporting Designers in Prototyping Multimodal Applications by Assembling AI Foundation Models"},{"paperId":"a710efa9247207a72f06e0c9db302fd3ecab5fbb","title":"Towards Robust Multi-Modal Reasoning via Model Selection"},{"paperId":"b3e9f249dd2e09ec111496f6b533101e8217a5b0","title":"Multimodal Large Language Model for Visual Navigation"},{"paperId":"7f1ba5630c3baa09b11cc665b3f71cdb117e5ffb","title":"OpenLEAF: Open-Domain Interleaved Image-Text Generation and Evaluation"},{"paperId":"711e64934bc0bab2f82aee489cf0f8fdb837addc","title":"Pushing the Envelope: Investigating the Potential and Limitations of ChatGPT and Artificial Intelligence in Advancing Computer Science Research"},{"paperId":"33095b1334bed852e3652bd9d7da3f4df0cdf485","title":"ViCor: Bridging Visual Understanding and Commonsense Reasoning with Large Language Models"},{"paperId":"b9e8b62bcc019f47a0a015568f70039b3b7c1196","title":"Toolink: Linking Toolkit Creation and Using through Chain-of-Solving on Open-Source Model"},{"paperId":"10828be2eaa52ba7fd78356980afd0669e2f2879","title":"Large Language Model (LLM) as a System of Multiple Expert Agents: An Approach to solve the Abstraction and Reasoning Corpus (ARC) Challenge"},{"paperId":"84f9bc5f89dac53662fb467b6af8ff26415ca3e7","title":"InstructDET: Diversifying Referring Object Detection with Generalized Instructions"},{"paperId":"8918e3cc21ecaf81532e452d3b9518360d14860e","title":"Reinforced UI Instruction Grounding: Towards a Generic UI Task Automation API"},{"paperId":"97afc428341eca1011a142daf269c9b01230f410","title":"Reverse Chain: A Generic-Rule for LLMs to Master Multi-API Planning"},{"paperId":"bee68767debbdc96d6f75947e544a8be98b869e3","title":"Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model: Explorations with GPT4-Vision and Beyond"},{"paperId":"e7d09b6f2bc878cf2c993acf675f409d0b55f35a","title":"MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens"},{"paperId":"65bceaab98ed937854e5177fe4ff9c63d40b4685","title":"Semantically Enhanced Scene Captions with Physical and Weather Condition Changes"},{"paperId":"d67cff4079e5799dde2480575eafdeb50c525adb","title":"Unseen And Adverse Outdoor Scenes Recognition Through Event-based Captions"},{"paperId":"8892b0937b1e6f3892647a812841e9dccacd7a34","title":"Dynamic Texts From UAV Perspective Natural Images"},{"paperId":"bd074cc87fe3567b1f8d70825cc5249b167d1674","title":"ChatGPT's Epoch in Rheumatological Diagnostics: A Critical Assessment in the Context of Sjögren's Syndrome"},{"paperId":"a1426b13b74dbad17b34606d25aabe1d61f6e11a","title":"CRAFT: Customizing LLMs by Creating and Retrieving from Specialized Toolsets"},{"paperId":"092245d86b77181c36f972b1b7a17a59cd989c4a","title":"Guiding Instruction-based Image Editing via Multimodal Large Language Models"},{"paperId":"7b689adb8c156d6158660f90d1c86888ee281f63","title":"DreamLLM: Synergistic Multimodal Comprehension and Creation"},{"paperId":"f34b6b4f75fb4e6f2c5654ee13fb2479c6170b3a","title":"Kosmos-2.5: A Multimodal Literate Model"},{"paperId":"f39630035497c76e5b3ea3f96f5cd54b4911ecb3","title":"Integrating Visual Foundation Models for Enhanced Robot Manipulation and Motion Planning: A Layered Approach"},{"paperId":"46c435dbf3ae54d8ba1bacef9e424a46bfc2c643","title":"Large Language Models (LLMs): Representation Matters, Low-Resource Languages and Multi-Modal Architecture"},{"paperId":"bc9f29881c1d93d225f0a74fa700531202c7043a","title":"OpenBA: An Open-sourced 15B Bilingual Asymmetric seq2seq Model Pre-trained from Scratch"},{"paperId":"3ec464696db25acc2c39a6d967ec3df09e06f633","title":"Multimodal Multi-Hop Question Answering Through a Conversation Between Tools and Efficiently Finetuned Large Language Models"},{"paperId":"4eb87eaa193929dbef93fa2db9419245a8e8916f","title":"TextBind: Multi-turn Interleaved Multimodal Instruction-following in the Wild"},{"paperId":"8aab972b0c3a3d581536b0d74339794809dc1a64","title":"TrafficGPT: Viewing, Processing and Interacting with Traffic Foundation Models"},{"paperId":"d39182113cd4176ead48027b4fc05fe06ec6aaca","title":"Language Models as Black-Box Optimizers for Vision-Language Models"},{"paperId":"fa75a55760e6ea49b39b83cb85c99a22e1088254","title":"NExT-GPT: Any-to-Any Multimodal LLM"},{"paperId":"24d52678c887331b9da0368e8a2f58bec07f7203","title":"Exploring Large Language Models for Communication Games: An Empirical Study on Werewolf"},{"paperId":"e2f1f04f648a8863d11439aa4c80ee65d6caccda","title":"ModelScope-Agent: Building Your Customizable Agent System with Open-source Large Language Models"},{"paperId":"529ff7d6441d244212cf2becafd12a7e67ac56d9","title":"FederatedScope-LLM: A Comprehensive Package for Fine-tuning Large Language Models in Federated Learning"},{"paperId":"3cc896b9a9c4951510b6b9f28be930d65b24d29f","title":"A Comprehensive Survey of ChatGPT: Advancements, Applications, Prospects, and Challenges."},{"paperId":"a404da40ae0d188095b5b0a3a3385c1cc7cffe62","title":"VistaGPT: Generative Parallel Transformers for Vehicles With Intelligent Systems for Transport Automation"},{"paperId":"1e7a2f9f9441462e92ee349f00414aff49617caa","title":"Enhancing Subtask Performance of Multi-modal Large Language Model"},{"paperId":"c237a22698223e4060d83027f399f4fb2aa24291","title":"Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"},{"paperId":"6bcc6ab9c28805d4067e99b2cdc7524550fe80e1","title":"PointLLM: Empowering Large Language Models to Understand Point Clouds"},{"paperId":"3b36d16985286b03e06e8404a7be49a9713d37b9","title":"Confucius: Iterative Tool Learning from Introspection Feedback by Easy-to-Difficult Curriculum"},{"paperId":"894ed1aba8e42a4ec27ba53ecde383b14c5128ca","title":"Examining User-Friendly and Open-Sourced Large GPT Models: A Survey on Language, Multimodal, and Scientific GPT Models"},{"paperId":"d0a7f7fe31e0e0c42b471b4c47a313bd8c8e5206","title":"Empowering Dynamics-aware Text-to-Video Diffusion with Large Language Models"},{"paperId":"ef1ebdb24ce45fb57e271783a0c9a877fe0e79c3","title":"Position-Enhanced Visual Instruction Tuning for Multimodal Large Language Models"},{"paperId":"98a508f5a4d49490bf97f195238b4fa5c41b8088","title":"Rational Decision-Making Agent with Internalized Utility Judgment"},{"paperId":"da96ec9c32d63292e506ba8f8ea8e838df998c02","title":"StableLLaVA: Enhanced Visual Instruction Tuning with Synthesized Image-Dialogue Data"},{"paperId":"ecfe2becc1040d3810c10f006d4be43b4eef3c41","title":"Tree-of-Mixed-Thought: Combining Fast and Slow Thinking for Multi-hop Visual Reasoning"},{"paperId":"eb5cf10406a8ad31e0ebe56b36571d5db4758a62","title":"PUMGPT: A Large Vision-Language Model for Product Understanding"},{"paperId":"d53945d4afb4528590d79e20de52883d29037e86","title":"FashionLOGO: Prompting Multimodal Large Language Models for Fashion Logo Embeddings"},{"paperId":"2e3dcf5a5d58ac210d0d87e9f918540a8373211a","title":"GIT-Mol: A Multi-modal Large Language Model for Molecular Science with Graph, Image, and Text"},{"paperId":"d6c2523ab97416c2692cbbeab082ed1790e8e55e","title":"VisIT-Bench: A Benchmark for Vision-Language Instruction Following Inspired by Real-World Use"},{"paperId":"52bef3d7ed6d079772d5a6ddfc68304cf40122cf","title":"Progressive Spatio-temporal Perception for Audio-Visual Question Answering"},{"paperId":"4f2be887e991efa85f7b874e7ab871080a745c39","title":"CAESURA: Language Models as Multi-Modal Query Planners"},{"paperId":"dd0612ce863f64b0f69d0d9f708c52e829f6f859","title":"TPTU: Large Language Model-based AI Agents for Task Planning and Tool Usage"},{"paperId":"446fb5dead075a1a08862662738f462e9a0e91c8","title":"Tool Documentation Enables Zero-Shot Tool-Usage with Large Language Models"},{"paperId":"ad113d8b6f8f6dace7abd12dc88af520aaaf3fc7","title":"LISA: Reasoning Segmentation via Large Language Model"},{"paperId":"7d46a13a1edd02dd6ae2b9f713e6f91ea001dfb4","title":"When Large Language Models Meet Personalization: Perspectives of Challenges and Opportunities"},{"paperId":"0bfc804e31eecfd77f45e4ee7f4d629fffdcd628","title":"ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs"},{"paperId":"bbcd5cc4bf6c77282e88cae07f7f2adb1da818ca","title":"Testing the Depth of ChatGPT's Comprehension via Cross-Modal Tasks Based on ASCII-Art: GPT3.5's Abilities in Regard to Recognizing and Generating ASCII-Art Are Not Totally Lacking"},{"paperId":"584ca135b61482fd89247113da87d784f738dbfa","title":"Foundational Models Defining a New Era in Vision: A Survey and Outlook"},{"paperId":"ece8cc6de361afc27cddcb2a4113936a877d931c","title":"Fashion Matrix: Editing Photos by Just Talking"},{"paperId":"8fd72a8bc1ab8fa08259656fa617d1c861f27239","title":"Interpreting Art by Leveraging Pre-Trained Models"},{"paperId":"1b4012f38daa8f09299e16771973c91ce9464ee2","title":"DVPT: Dynamic Visual Prompt Tuning of Large Pre-trained Models for Medical Image Analysis"},{"paperId":"2e52178416b93a0a7dc1cb7d21d04c7e72c2ce0f","title":"ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring Instruction Tuning"},{"paperId":"962ccf1fc49c83817fb031e5b24b81b19cdfb89d","title":"BuboGPT: Enabling Visual Grounding in Multi-Modal LLMs"},{"paperId":"a68dc9208aae7578e8ee384caa8ccbcf34e539e8","title":"Mini-Giants: \"Small\" Language Models and Open Source Win-Win"},{"paperId":"65d7663b60d95f98e6281ecc4da9c7a975119b91","title":"GeoGPT: Understanding and Processing Geospatial Tasks through An Autonomous GPT"},{"paperId":"ca31b8584b6c022ef15ddfe994fe361e002b7729","title":"A Comprehensive Overview of Large Language Models"},{"paperId":"451a3f03aca4aa87b93981364842137417549e58","title":"SVIT: Scaling up Visual Instruction Tuning"},{"paperId":"094883e42bb9a41f602c0715c1059bc431e33fb2","title":"GPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest"},{"paperId":"ebddfdc5d845a788e8062eddbbf7a335737cb99b","title":"What Matters in Training a GPT4-Style Language Model with Multimodal Inputs?"},{"paperId":"ea566f87f6253bb2d32cf7b61cd3e2535a0c3f42","title":"mPLUG-DocOwl: Modularized Multimodal Large Language Model for Document Understanding"},{"paperId":"376f494126d1ea4f571ea0263c43ac2b6331800a","title":"SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs"},{"paperId":"fb4ed7d3f24c69d33dc0654cf91ed8409f8dafb3","title":"The Segment Anything Model (SAM) for Remote Sensing Applications: From Zero to One Shot"},{"paperId":"145be18b3e2e1baf3fed09e919da68da6e14a839","title":"Stone Needle: A General Multimodal Large-scale Model Framework towards Healthcare"},{"paperId":"15dfb06dab3162f4bb7939b0e54c3b68c2b34cc4","title":"Paradigm Shift in Sustainability Disclosure Analysis: Empowering Stakeholders with Chatreport, a Language Model-Based Tool"},{"paperId":"ebedc4d7a2356090904baba4104ef0832bc236df","title":"A Survey on Multimodal Large Language Models"},{"paperId":"697e0add95e880bd42e00bef838181e105f91981","title":"MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models"},{"paperId":"966852963a88a28786b798c91b6662d6e501e590","title":"AssistGPT: A General Multi-modal Assistant that can Plan, Execute, Inspect, and Learn"},{"paperId":"051549d8ef56937b2f4d113afdcf8c7586d3770b","title":"Towards AGI in Computer Vision: Lessons Learned from GPT and Large Language Models"},{"paperId":"d98536f24272e258b1d399074b64284d64786099","title":"AVIS: Autonomous Visual Information Seeking with Large Language Models"},{"paperId":"eaa7853facb9b49444b48a96192cb4be66b62671","title":"Synapse: Trajectory-as-Exemplar Prompting with Memory for Computer Control"},{"paperId":"473eb062612a17c965eaa62136322f0dec6b1f8e","title":"Data-Copilot: Bridging Billions of Data and Humans with Autonomous Workflow"},{"paperId":"79150cb420d15830c8d36f0e91eea1b02e177f0f","title":"Sticker820K: Empowering Interactive Retrieval with Stickers"},{"paperId":"4c4d176c6e28f48041f215d563f6ee8633534cff","title":"Valley: Video Assistant with Large Language model Enhanced abilitY"},{"paperId":"6294f078e79828cac21e717813e8f3d02b18a97c","title":"The importance of resource awareness in artificial intelligence for healthcare"},{"paperId":"fd755dc7b5b206c17fd953db04e1c888d45b6e4e","title":"LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark"},{"paperId":"ed30969f0e4811473144ffe83c1baa6d54f02202","title":"RestGPT: Connecting Large Language Models with Real-World RESTful APIs"},{"paperId":"fed150a219f9c31bdb4920e615c7c9264c634736","title":"On the Challenges and Perspectives of Foundation Models for Medical Image Analysis"},{"paperId":"2933acb28b7369c7ea5b8728f6d8cb55e1beef98","title":"Customizing General-Purpose Foundation Models for Medical Report Generation"},{"paperId":"dc0f9a4988fccd8cdd6d64e475306f8a639ac1af","title":"Decision Stacks: Flexible Reinforcement Learning via Modular Generative Models"},{"paperId":"d47524cd5c3c4b57af2e5a29f6f91c420310f236","title":"MIMIC-IT: Multi-Modal In-Context Instruction Tuning"},{"paperId":"2d338cdd12091814dec11155d3f6f848d7bab4d8","title":"Prompt Space Optimizing Few-shot Reasoning Success with Large Language Models"},{"paperId":"5bbc3b014f7c2dd151dc6b3cfb183889c44e772d","title":"Natural Language Commanding via Program Synthesis"},{"paperId":"42ea55edb46395469aee1b760829657e65ab6577","title":"Zero-Shot 3D Shape Correspondence"},{"paperId":"5d321194696f1f75cf9da045e6022b2f20ba5b9c","title":"Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding"},{"paperId":"486a8c8655b81c7f87ff257141466ec1186d4aea","title":"Prompt Sapper: LLM-Empowered Software Engineering Infrastructure for AI-Native Services"},{"paperId":"615962d8969c8e0ffe43319689dce6c50cbf1f29","title":"Responsible Task Automation: Empowering Large Language Models as Responsible Task Automators"},{"paperId":"c85c90ef9e9a71efe031c3f7d6e34561f91168fe","title":"Deliberate then Generate: Enhanced Prompting Framework for Text Generation"},{"paperId":"461437e165fa8236373bdcc4db5f09667863bc29","title":"Chatting Makes Perfect: Chat-based Image Retrieval"},{"paperId":"b458fc5261595f44b36325e5eaea1f874d65138f","title":"GPT4Tools: Teaching Large Language Model to Use Tools via Self-instruction"},{"paperId":"af705d648b5b16daa3dcc593bc593f2574d76c07","title":"Grammar Prompting for Domain-Specific Language Generation with Large Language Models"},{"paperId":"50c1414fe41d0cb9db6f0933c9319aa124beac5d","title":"Contextual Object Detection with Multimodal Large Language Models"},{"paperId":"828e27fd4fcd5e8982032b903950947b12afb6bb","title":"Diffusion Model is an Effective Planner and Data Synthesizer for Multi-Task Reinforcement Learning"},{"paperId":"5ff2f5212713ec424662ac3c9e4aa5a8790d40cf","title":"ANPL: Towards Natural Programming with Interactive Decomposition"},{"paperId":"7e72eb196b7c90b3a5d6385af536fe8e5934fb82","title":"ConvGenVisMo: Evaluation of Conversational Generative Vision Models"},{"paperId":"8ecdbfe011b7189fa0ee49ffc4e42a93d728a371","title":"On Evaluating Adversarial Robustness of Large Vision-Language Models"},{"paperId":"8199c9d55dd998f69f703e0ad250ca0697e3ad27","title":"NavGPT: Explicit Reasoning in Vision-and-Language Navigation with Large Language Models"},{"paperId":"51b169701290cd129e0781fc9f3a9918604c89b5","title":"Improving accuracy of GPT-3/4 results on biomedical data using a retrieval-augmented language model"},{"paperId":"c6ac708b65b24c20f80831d518c1795ce8133ad5","title":"ChatBridge: Bridging Modalities with Large Language Model as a Language Catalyst"},{"paperId":"06091944b864d6dc473cab63321a95fb9c4067cc","title":"ChatCAD+: Towards a Universal and Reliable Interactive CAD using LLMs"},{"paperId":"ef8c21e1f574495f0c80b8c1037dbdb886f0808d","title":"Towards Language-guided Interactive 3D Generation: LLMs as Layout Interpreter with Generative Feedback"},{"paperId":"f0888b9c0ef63e68c7758e6aec2370961c0eede9","title":"On the Tool Manipulation Capability of Open-source Large Language Models"},{"paperId":"9c3a9b4821daa03cb5369041d59d2714329a3811","title":"Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models"},{"paperId":"7cf64070fd3d7e53d80f260c10e6bd7018d580e1","title":"IdealGPT: Iteratively Decomposing Vision and Language Reasoning via Large Language Models"},{"paperId":"7c4f6fd4c7eadcc7189a6797db215895340f93c7","title":"ChatFace: Chat-Guided Real Face Editing via Diffusion Latent Space Manipulation"},{"paperId":"9837349417e36ef5be06da0fd6c74042148bdaa2","title":"Visual Programming for Text-to-Image Generation and Evaluation"},{"paperId":"abab79d1135684d039cfbebd0097e48ef4c1940c","title":"Vision + Language Applications: A Survey"},{"paperId":"66d755730f5d08a6f4fcc5e81f24982ba389dca9","title":"LayoutGPT: Compositional Visual Planning and Generation with Large Language Models"},{"paperId":"00cb69a9f280317d1c59ac5827551ee9b10642b8","title":"EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought"},{"paperId":"69335077fcacbff7a7cf25697da1949e6bdfa968","title":"The Art of SOCRATIC QUESTIONING: Recursive Thinking with Large Language Models"},{"paperId":"bd0488e0c1bb3ab375f28b3157058101350d3766","title":"ChipGPT: How far are we from natural language hardware design"},{"paperId":"90027ca7802645671a69b00b65e1fa94e6b63544","title":"ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models"},{"paperId":"e9ae0c76a71b8f302eb17b1c4462b9cc97d87cd0","title":"LLM-grounded Diffusion: Enhancing Prompt Understanding of Text-to-Image Diffusion Models with Large Language Models"},{"paperId":"8da9b1436212b233fc49c7daf1ba15c22874ff5a","title":"CREATOR: Tool Creation for Disentangling Abstract and Concrete Reasoning of Large Language Models"},{"paperId":"13a5140fc0b269c408ecfc666cb297410bc753c5","title":"Matcher: Segment Anything with One Shot Using All-Purpose Feature Matching"},{"paperId":"43a55dbd95c9d5cd82de8db276f41adeec4a937d","title":"Interactive Data Synthesis for Systematic Vision Adaptation via LLMs-AIGCs Collaboration"},{"paperId":"6fe2a4f7594abadafc9feb05e96ab7e9e634ce8f","title":"Album Storytelling with Iterative Story-aware Captioning and Large Language Models"},{"paperId":"ca055cfb9d4d47124cc035c346f38577825fcacf","title":"Enhance Reasoning Ability of Visual-Language Models via Large Language Models"},{"paperId":"7919cb1a1dcf70ed7803c43a71d43dba696ef149","title":"Making Language Models Better Tool Learners with Execution Feedback"},{"paperId":"205d2ed0906440f07a0275d7d6a63bced60951fc","title":"InstructVid2Vid: Controllable Video Editing with Natural Language Instructions"},{"paperId":"98d05533678a9a09a2053b7d974738c60c4894ea","title":"Empower Large Language Model to Perform Better on Industrial Domain-Specific Question Answering"},{"paperId":"f8cbcb106a48524edc39df23e2a95f1e6d4c739a","title":"Examining Inter-Consistency of Large Language Models Collaboration: An In-depth Analysis via Debate"},{"paperId":"2195676f111ad492c50f4d4c96abb2bd3d72f7fc","title":"Instruct2Act: Mapping Multi-modality Instructions to Robotic Actions with Large Language Model"},{"paperId":"42a30dc5470f54ec249f25d3c31e05d7c376c8e3","title":"VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks"},{"paperId":"1bdd5fc17cc580efe998304692639c57c857cc84","title":"Going Denser with Open-Vocabulary Part Segmentation"},{"paperId":"cb5f53cd7d43ac6f48abf4dde6fc58e53a083b0e","title":"Accelerating the integration of ChatGPT and other large‐scale AI models into biomedical research and healthcare"},{"paperId":"7787efaf502421eac9b6b0fd946a82e1ecf4c8c9","title":"Generating coherent comic with rich story using ChatGPT and Stable Diffusion"},{"paperId":"8dbb29f93292d8b1b861c322d232fe087b2ef7b1","title":"Small Models are Valuable Plug-ins for Large Language Models"},{"paperId":"0340c850e033abbf71c7214e403c8fe2be5ef91f","title":"Visual Tuning"},{"paperId":"d48cb91b9e555194f7494c4d4bb9815021d3ee45","title":"VideoChat: Chat-Centric Video Understanding"},{"paperId":"7a6dc7071891cb3d658c93418801942a4c6ed373","title":"Autonomous GIS: the next-generation AI-powered GIS"},{"paperId":"54a8b153ed04a872da878d695239bdc413dc782c","title":"InternGPT: Solving Vision-Centric Tasks by Interacting with Chatbots Beyond Language"},{"paperId":"e0dc8e113dbdd2896fb6420ac93e0b976c47f2a2","title":"Augmented Large Language Models with Parametric Knowledge Guiding"},{"paperId":"43e6e8d6663d83f1b74cf5a2be7b040b0928f867","title":"X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages"},{"paperId":"20fcc01d12a50f1da2af71d85f0a269b3ba48b77","title":"LMEye: An Interactive Perception Network for Large Language Models"},{"paperId":"d6d3604f369bb0415cbe814e43ca3131323b03e2","title":"Otter: A Multi-Modal Model with In-Context Instruction Tuning"},{"paperId":"6f8b9192b1f215254ee7625d752710182c05d2f9","title":"Caption Anything: Interactive Image Description with Diverse Multimodal Controls"},{"paperId":"c77d908ba29567445a9a4ad1bd4461d441cce174","title":"AutoML-GPT: Automatic Machine Learning with GPT"},{"paperId":"d473847dff63e3f5d238251cb23597f8205f72f2","title":"Generating Virtual On-body Accelerometer Data from Virtual Textual Descriptions for Human Activity Recognition"},{"paperId":"37ba1833e844f5178f91f50d82bfff616551e6ad","title":"The Role of Summarization in Generative Agents: A Preliminary Perspective"},{"paperId":"570079bbdd8758dfe865097e05719313c9c1301a","title":"LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model"},{"paperId":"c56a51728678e5b2e3ff95e51caf21d267439c36","title":"ChatVideo: A Tracklet-centric Multimodal and Versatile Video Understanding System"},{"paperId":"7e32aac43e9f1df49e116add03327ee6f365dbf3","title":"mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality"},{"paperId":"8bc617c9139648d7a92991d70c671230bac7b2e2","title":"AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head"},{"paperId":"f6654702479eddb01d6f1bd72d6d56102ebefbfc","title":"The Potential of Visual ChatGPT For Remote Sensing"},{"paperId":"4c8ef2db0c77aba453783f5211ebafc6695d3835","title":"ChatABL: Abductive Learning via Natural Language Interaction with ChatGPT"},{"paperId":"ca6a2bc279be5a3349a22bfd6866ed633d18734b","title":"MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models"},{"paperId":"5c0f3b0e46e6125bf2f454bcd8565a6f3430a54c","title":"Learning to Plan with Natural Language"},{"paperId":"170c97c7215f42edfb20c2248f954879e91ef86e","title":"Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models"},{"paperId":"352420ee61a8da783ca7750170793613b18b8d9c","title":"Tool Learning with Foundation Models"},{"paperId":"be2b0396de9431bae931642516a1d3e4906329f5","title":"Low-code LLM: Visual Programming over LLMs"},{"paperId":"a5036f31f0e629dc661f120b8c3b1f374d479ab8","title":"Visual Instruction Tuning"},{"paperId":"ba2f935d2578fbf77ec1aa79e26e3db396771e38","title":"Self-collaboration Code Generation via ChatGPT"},{"paperId":"0819c1e60c13b9797f937282d06b54d252d9d6ec","title":"Segment Everything Everywhere All at Once"},{"paperId":"6316cbb4f1e7dba5806a3310ec7f89f3571bc3db","title":"Boosting Cross-task Transferability of Adversarial Patches with Visual Relations"},{"paperId":"38179848e2d6a3ad373b1793848816111428ac36","title":"OpenAGI: When LLM Meets Domain Experts"},{"paperId":"0ebc861f5478561f12941e6b48aad30574e996d8","title":"Video ChatCaptioner: Towards Enriched Spatiotemporal Descriptions"},{"paperId":"16d83e930a4dab2d49f5d276838ddce79df3f787","title":"Should ChatGPT be Biased? Challenges and Risks of Bias in Large Language Models"},{"paperId":"51a0bba0c5fb4257e843040615bb23f712fed4e6","title":"Summary of ChatGPT-Related Research and Perspective Towards the Future of Large Language Models"},{"paperId":"c61d54644e9aedcfc756e5d6fe4cc8b78c87755d","title":"A Survey of Large Language Models"},{"paperId":"ac7771c332da42b29a913b116bd6ef622cbf89cf","title":"TaskMatrix.AI: Completing Tasks by Connecting Foundation Models with Millions of APIs"},{"paperId":"f8e77bd3d573d0daee0744443c65c40e3b5dc10f","title":"Unleashing the Power of Edge-Cloud Generative AI in Mobile Networks: A Survey of AIGC Services"},{"paperId":"23684a07517870cffd1f97fafbaae16ba22bd2b7","title":"Large AI Models in Health Informatics: Applications, Challenges, and the Future"},{"paperId":"c7a9c7302a72301ed79a7c0696d5af2e03ad3ac4","title":"MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action"},{"paperId":"9fe9af7cf3d54b707a7be3c53ce94b77dcc3bae5","title":"A Short Survey of Viewing Large Language Models in Legal Aspect"},{"paperId":"6e754273d54a91371efbc928cd6b156364d517da","title":"ViperGPT: Visual Inference via Python Execution for Reasoning"},{"paperId":"53df959bcf6499c45e316086a96a624389a39a52","title":"Accountable Textual-Visual Chat Learns to Reject Human Instructions in Image Re-creation"},{"paperId":"6845bea94b2fb17d4377b3bb2bd10f73a959f9cc","title":"Reasoning with Language Model Prompting: A Survey"},{"paperId":"419eb47fea3931c4098232f44ccbc216275d3f56","title":"Decoding Visual Neural Representations by Multimodal Learning of Brain-Visual-Linguistic Features"},{"paperId":"e342165a614588878ad0f4bc9bacf3905df34d08","title":"Diffusion Models: A Comprehensive Survey of Methods and Applications"},{"paperId":"5ce5eb5c4f4dc9e17e3548be931d48c96f5d14c9","title":"Priors in Deep Image Restoration and Enhancement: A Survey"},{"paperId":"09ca5072a76796c65e5936b6fb4968afead61944","title":"Semantics-Empowered Communications: A Tutorial-Cum-Survey"},{"paperId":"cc074c98f3fa83e5367f3e593fc07bde22d32a7a","title":"Intelligent Practices of Large Language Models in Digital Government Services"},{"paperId":"aa0ed1d8a3d128b6159b543b57ed7cfec035f23d","title":"Efficient Classification of Malicious URLs: M-BERT—A Modified BERT Variant for Enhanced Semantic Understanding"},{"paperId":"13b5b69355555e0c8b702261c5de3b4172ba653c","title":"The Art of SOCRATIC QUESTIONING: Zero-shot Multimodal Reasoning with Recursive Thinking and Self-Questioning"},{"paperId":"06d8562831c32844285a691c5250d04726df3c61","title":"A Systematic Survey of Prompt Engineering on Vision-Language Foundation Models"},{"paperId":"d1120d67b700e4dfe8b39eb1e48fbdea4e1a0c43","title":"HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face"},{"paperId":"c2408a3a8da4f12d3eb156fe359a96b428e5aff1","title":"Chain-of-Symbol Prompting Elicits Planning in Large Langauge Models"},{"paperId":"eb291a2e237774b162d9c51c21c4868795589e94","title":"Diving into the Inter-Consistency of Large Language Models: An Insightful Analysis through Debate"},{"paperId":"ed9943d73eb42116fe33564b5065c78b5ca0b16e","title":"RestGPT: Connecting Large Language Models with Real-World Applications via RESTful APIs"},{"paperId":"8cbc8c83b9f612f298e89b65500f4f362a6b0439","title":"Frontier Review of Multimodal AI"},{"paperId":"990eefe3b7f65736c8cf6a7dcaf4cdbc21cc71b3","title":"Towards AI-based Accessible Digital Media: Image Analysis Pipelines and Blind User Studies*"},{"paperId":"7e3bbd7be60bb50a8093152795f269a69a4a0fd9","title":"Chatting Makes Perfect - Chat-based Image Retrieval"},{"paperId":"6c5a1079d9705c0ee022cef77207daa20ce2cde5","title":"Summary of ChatGPT/GPT-4 Research and Perspective Towards the Future of Large Language Models"},{"paperId":"96a6df2b4aa50cfbd8984933e9c66b0763fc08a6","title":"Overleaf Example"},{"paperId":"f02241105c2a72943e24c37ae58a22c46db88720","title":"PCLmed at ImageCLEFmedical 2023: Customizing General-Purpose Foundation Models for Medical Report Generation"},{"paperId":"5ce94181ea702f69c3651dce721d6bd8026b8106","title":"TPTU: Task Planning and Tool Usage of Large Language Model-based AI Agents"},{"paperId":"e611f556e8c871bdad67f06f336ca05dda600035","title":"Diverse Hyperspectral Remote Sensing Image Synthesis With Diffusion Models"},{"paperId":"44ccf252018f71898d52d89539f17d77a4f8d548","title":"Chart Understanding with Large Language Model"}],"references":[{"paperId":"efbe97d20c4ffe356e8826c01dc550bacc405add","title":"Adding Conditional Control to Text-to-Image Diffusion Models"},{"paperId":"780a7f5e8ba9b4b451e3dfee1bcfb0f68aba5050","title":"Multimodal Chain-of-Thought Reasoning in Language Models"},{"paperId":"3f5b31c4f7350dc88002c121aecbdc82f86eb5bb","title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models"},{"paperId":"a2d2bbe4c542173662a444b33b76c66992697830","title":"InstructPix2Pix: Learning to Follow Image Editing Instructions"},{"paperId":"964bd39b546f0f6625ff3b9ef1083f797807ef2e","title":"BLOOM: A 176B-Parameter Open-Access Multilingual Language Model"},{"paperId":"90350aa626bed47b02d0c162462e5b0ca82be6b2","title":"Automatic Chain of Thought Prompting in Large Language Models"},{"paperId":"d3135733aa39dec20ce72aa138589dda27c8406d","title":"Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering"},{"paperId":"e7ad08848d5d7c5c47673ffe0da06af443643bda","title":"Large Language Models are Zero-Shot Reasoners"},{"paperId":"5437e8adab596d7294124c0e798708e050e25321","title":"Least-to-Most Prompting Enables Complex Reasoning in Large Language Models"},{"paperId":"26218bdcc3945c7edae7aa2adbfba4cd820a2df3","title":"Flamingo: a Visual Language Model for Few-Shot Learning"},{"paperId":"ada81a4de88a6ce474df2e2446ad11fea480616e","title":"Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language"},{"paperId":"23dd78e424d32f6a48660dcd67ce994b8a7db8be","title":"STaR: Bootstrapping Reasoning With Reasoning"},{"paperId":"5f19ae1135a9500940978104ec15a5b8751bc7d2","title":"Self-Consistency Improves Chain of Thought Reasoning in Language Models"},{"paperId":"d766bffc357127e0dc86dd69561d5aeb520d6f4c","title":"Training language models to follow instructions with human feedback"},{"paperId":"1b6e810ce0afd0dd093f789d2b2742d047e316d5","title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"a3b42a83669998f65df60d7c065a70d07ca95e99","title":"BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation"},{"paperId":"f4b11a696aa5a03fed1bfc47e65fdb7eb0e529c1","title":"UniFormer: Unifying Convolution and Self-Attention for Visual Recognition"},{"paperId":"400d619cbabeb669115bb7281a889ab869829ef5","title":"MERLOT RESERVE: Neural Script Knowledge through Vision and Language and Sound"},{"paperId":"c10075b3746a9f3dd5811970e93c8ca3ad39b39d","title":"High-Resolution Image Synthesis with Latent Diffusion Models"},{"paperId":"bdea16e93fc70f316002e5f6aac8ce17388c6ee9","title":"MAGMA - Multimodal Augmentation of Generative Models through Adapter-based Finetuning"},{"paperId":"ba9d736006b897d06f75586ad46e28e00a5e566e","title":"VIOLET : End-to-End Video-Language Transformers with Masked Visual-token Modeling"},{"paperId":"197d5867a45a2988f4dd159063cdfbfe90164962","title":"LiT: Zero-Shot Transfer with Locked-image text Tuning"},{"paperId":"cf7c2e0e4fb2af689aaf4b7a7cddf7b1f4d5e3f0","title":"VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts"},{"paperId":"767923635f2fd4467d848dba9655866e4f9b55c8","title":"Supervision Exists Everywhere: A Data Efficient Contrastive Language-Image Pre-training Paradigm"},{"paperId":"2672777d25562c9df6fc13b653181db62d39bece","title":"An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA"},{"paperId":"260ad39a1dac4b451019e2bf17925f4df8e3b69a","title":"Per-Pixel Classification is Not All You Need for Semantic Segmentation"},{"paperId":"01b5412f3d17e90e09226d7c40ad4d4468a1414d","title":"Multimodal Few-Shot Learning with Frozen Language Models"},{"paperId":"2a805d0e1b067444a554c5169d189fa1f649f411","title":"Scaling Vision Transformers"},{"paperId":"0cceb0393b87d3ff65a1f0beea696ce40e889597","title":"Towards Light-Weight and Real-Time Line Segment Detection"},{"paperId":"8e33914d6051dd031a5e096962b9398fc1d16067","title":"Vision Transformers for Dense Prediction"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"616e0ed02ca024a8c1d4b86167f7486ea92a13d9","title":"VisualGPT: Data-efficient Adaptation of Pretrained Language Models for Image Captioning"},{"paperId":"be0014c1fbc3e664686610d2c85f75038a4f6e4f","title":"VinVL: Making Visual Representations Matter in Vision-Language Models"},{"paperId":"053b1d7b97eb2c91fc3921d589c160b0923c70b1","title":"Learning to summarize from human feedback"},{"paperId":"f68ec3228f3c6a3b3de40fd4df2dbcc9c5857606","title":"Text as Neural Operator:Image Manipulation by Text Instruction"},{"paperId":"2f5f81bc516a6d085d39479378af1fc27104f91e","title":"Large-Scale Adversarial Training for Vision-and-Language Representation Learning"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"818e5cbc337e4e1b98e65a2d7c2d6d2a0318cd57","title":"Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks"},{"paperId":"953667588e089ae99f049e8574d013bb70aa8517","title":"ManiGAN: Text-Guided Image Manipulation"},{"paperId":"6c4b76232bb72897685d19b3d264c6ee3005bc2b","title":"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"},{"paperId":"dfc7b58b67c31932b48586b3e23a43cc94695290","title":"UNITER: UNiversal Image-TExt Representation Learning"},{"paperId":"7bd83b055702bc178aa26def5b6df463f8eab7b9","title":"Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-Shot Cross-Dataset Transfer"},{"paperId":"29ddc1f43f28af7c846515e32cc167bc66886d0c","title":"Parameter-Efficient Transfer Learning for NLP"},{"paperId":"6dfc2ff03534a4325d06c6f88c3144831996629b","title":"From Recognition to Cognition: Visual Commonsense Reasoning"},{"paperId":"19c12e12946eb3bb9aa7fdeb511eef79fc53b6b3","title":"Canny edge detection based on Open CV"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"dfad8f616bd2a05c8cae5f61060f743f966ece85","title":"Realtime Multi-person 2D Pose Estimation Using Part Affinity Fields"},{"paperId":"778ce81457383bd5e3fdb11b145ded202ebb4970","title":"Semantic Compositional Networks for Visual Captioning"},{"paperId":"8acbe90d5b852dadea7810345451a99608ee54c7","title":"Image-to-Image Translation with Conditional Adversarial Networks"},{"paperId":"2bf2c3598f1ee3a2fa22185cb2aaeb425de004ad","title":"Commonsense reasoning and commonsense knowledge in artificial intelligence"},{"paperId":"97ad70a9fa3f99adf18030e5e38ebe3d90daa2db","title":"VQA: Visual Question Answering"},{"paperId":"8da55e685a7bef9c897788ab519a8710c695c419","title":"Holistically-Nested Edge Detection"},{"paperId":"d4dc1012d780e8e2547237eb5a6dc7b1bf47d2f0","title":"Show and tell: A neural image caption generator"},{"paperId":"71b7178df5d2b112d07e45038cb5637208659ff7","title":"Microsoft COCO: Common Objects in Context"},{"paperId":"356770d13ad2e7b60961e5bc7368ffd3b9a2bcd9","title":"Learning to summarize with human feedback"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"9405cc0d6169988371b2755e573cc28650d14dfe","title":"Language Models are Unsupervised Multitask Learners"},{"paperId":null,"title":": ”timbrooks/instruct-pix2pix” from Hugging-Face, StableDiffusionInstructPix2PixPipeline model"},{"paperId":null,"title":"image path, textual how to modify → image path"},{"paperId":null,"title":"Harrison Chase"},{"paperId":null,"title":"Prompt: Generate Image Condition On Pose Image: useful for when you want to generate a new real image from both the user desciption and a human pose image"}],"id":"af997821231898a5f8d0fd78dad4eec526acabe5","summary":"A system to enable the user to interact with ChatGPT by sending and receiving not only languages but also images and providing complex visual questions or visual editing instructions that require the collaboration of multiple AI models with multi-steps, and opens the door to investigating the visual roles ofChatGPT with the help of Visual Foundation Models."},{"url":"https://www.semanticscholar.org/paper/6e754273d54a91371efbc928cd6b156364d517da","title":"ViperGPT: Visual Inference via Python Execution for Reasoning","venue":"arXiv.org","year":2023,"referenceCount":79,"citationCount":168,"influentialCitationCount":1,"publicationDate":"14/03/2023","authors":"D'idac Sur'is,Sachit Menon,Carl Vondrick","citations":[{"paperId":"ac8089bb7944090cf1de5df25aadf5e6356f3040","title":"TempCompass: Do Video LLMs Really Understand Videos?"},{"paperId":"bb7554df265c5b0fdb9aeef2a02e1b9054b59d93","title":"VCD: Knowledge Base Guided Visual Commonsense Discovery in Images"},{"paperId":"74c68aed85f2fe8019113bbdb533fcba7e3ce0bd","title":"ShapeLLM: Universal 3D Object Understanding for Embodied Interaction"},{"paperId":"2cea424c7dce71042c24d43317521abdc4c0ffb4","title":"Large Multimodal Agents: A Survey"},{"paperId":"f9a3f275c1d36f7a99e2617b3e4ca18315519077","title":"CI w/o TN: Context Injection without Task Name for Procedure Planning"},{"paperId":"1b5e69a5b0f179e90f356a9c8cc1a39f77471dab","title":"Selective\"Selective Prediction\": Reducing Unnecessary Abstention in Vision-Language Reasoning"},{"paperId":"5e7274bcda47b704b6797bb14be8b7a61c047a61","title":"Uncertainty-Aware Evaluation for Vision-Language Models"},{"paperId":"593908583968815003049398de77585a99038fb7","title":"DeiSAM: Segment Anything with Deictic Prompting"},{"paperId":"4315094bec44f310d662b349aff6305b3dde9a07","title":"EVEDIT: Event-based Knowledge Editing with Deductive Editing Boundaries"},{"paperId":"3871fe3b83090bebf3eba40a3afa4e0b66a3f165","title":"Question-Instructed Visual Descriptions for Zero-Shot Video Question Answering"},{"paperId":"83d201d503b863fec7d1225f00a141e722e03f17","title":"Using Left and Right Brains Together: Towards Vision and Language Planning"},{"paperId":"339f775915160c1293e02579f8226a227f270cbb","title":"AgentLens: Visual Analysis for Agent Behaviors in LLM-based Autonomous Systems"},{"paperId":"f86f71cfd2e9682a56d7334736a7b8a0b1c70b45","title":"Prismatic VLMs: Investigating the Design Space of Visually-Conditioned Language Models"},{"paperId":"637b28b5c8283b2118b7bc0aabae6f4795bbee67","title":"ContPhy: Continuum Physical Concept Learning and Reasoning from Videos"},{"paperId":"da68013fd293594ad21eb1c3d838d06e691a556b","title":"The Role of Foundation Models in Neuro-Symbolic Learning and Reasoning"},{"paperId":"78fbb6e7a1c568a04e8c935aa9909d0c942ea5f6","title":"Executable Code Actions Elicit Better LLM Agents"},{"paperId":"3a1910dc9212cc78e62d29a44e011d14e2516edc","title":"A Survey on Visual Anomaly Detection: Challenge, Approach, and Prospect"},{"paperId":"94701d9c6cfc1aecc174ff62ccda939f790c1710","title":"ReGAL: Refactoring Programs to Discover Generalizable Abstractions"},{"paperId":"1bdbb7b4d657a2fd3e9f64fc689a14a8c94ebfaf","title":"ChatterBox: Multi-round Multimodal Referring and Grounding"},{"paperId":"a050c9b0c321839e4427ab9defa3463be7825ac4","title":"MM-LLMs: Recent Advances in MultiModal Large Language Models"},{"paperId":"7e6c1bb54bb2e36cc1092b080e9928942f7f8a68","title":"TroVE: Inducing Verifiable and Efficient Toolboxes for Solving Programmatic Tasks"},{"paperId":"c00a86523bcacf4db91d9970e1957e78152cb103","title":"Zero Shot Open-ended Video Inference"},{"paperId":"140cfda71bfff852c3e205b7ad61854b78c76982","title":"Mastering Text-to-Image Diffusion: Recaptioning, Planning, and Generating with Multimodal LLMs"},{"paperId":"23957040943f883542f47850c709b9e7f9d6fa55","title":"Prompting Large Vision-Language Models for Compositional Reasoning"},{"paperId":"c2b0291ffedf70f5cc66425e49837cba49aaf8e9","title":"PhotoScout: Synthesis-Powered Multi-Modal Image Search"},{"paperId":"4f2a56102bcbf0fe79379c4c27daecbccfb35a26","title":"MLLM-Tool: A Multimodal Large Language Model For Tool Agent Learning"},{"paperId":"d4b1a1c62a03ccffcf24983eb4fe22335cbb89b6","title":"DiffusionGPT: LLM-Driven Text-to-Image Generation System"},{"paperId":"0a8a776054a087118f4f9523994ef084b2b2469a","title":"Advancing Large Multi-modal Models with Explicit Chain-of-Reasoning and Visual Question Generation"},{"paperId":"bf08ad906baad71283aa538532cf6e7ff12db352","title":"LangProp: A code optimization framework using Language Models applied to driving"},{"paperId":"4a48d628e53f554eb6ef09a457ca855188b96171","title":"DoraemonGPT: Toward Understanding Dynamic Scenes with Large Language Models"},{"paperId":"5502d769595981009e43344f8914e287acca2359","title":"ModaVerse: Efficiently Transforming Modalities with LLMs"},{"paperId":"fc7feeaddc5a38c0d6f0d793737584e5f0bb7519","title":"Towards Truly Zero-shot Compositional Visual Reasoning with LLMs as Programmers"},{"paperId":"a06d3e9e90008c64c45a0029d580541d5f646771","title":"If LLM Is the Wizard, Then Code Is the Wand: A Survey on How Code Empowers Large Language Models to Serve as Intelligent Agents"},{"paperId":"4641fe56cd44144b6cabea583233ed952f97f4c0","title":"A Simple LLM Framework for Long-Range Video Question-Answering"},{"paperId":"86e3e63e83011aecd4a59999c632d44f117efbc5","title":"WildCLIP: Scene and animal attribute retrieval from camera trap data with domain-adapted vision-language models"},{"paperId":"6a33e58ef961a3a0a5657518b2be86395eb7c8d0","title":"InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks"},{"paperId":"24fc9ad715372358bd0108eeb7c944b915963293","title":"ASSISTGUI: Task-Oriented Desktop Graphical User Interface Automation"},{"paperId":"35a17f896847614a71df772bbe2b66ae231cabc7","title":"CLOVA: A Closed-Loop Visual Assistant with Tool Usage and Update"},{"paperId":"dc1fd73926a3ea0751019ca6b027fa1042d4dcda","title":"Designing LLM Chains by Adapting Techniques from Crowdsourcing Workflows"},{"paperId":"7d24fd1248fac1ff022b436667b78e42919c3ba6","title":"InstructPipe: Building Visual Programming Pipelines with Human Instructions"},{"paperId":"6d2ab31aa75468f5458b9d96192c3f4a28f55d73","title":"DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving"},{"paperId":"3f622f71276ffdeca771f0f3758f3974d3a18f28","title":"Remote Sensing Vision-Language Foundation Models without Annotations via Ground Remote Alignment"},{"paperId":"33c3dbf86dd6e338e2a49bba9c2e2193d1eca08a","title":"Vista-LLaMA: Reliable Video Narrator via Equal Distance to Visual Tokens"},{"paperId":"3a56bc074b8f3f985599627404b70e16fc5bce1b","title":"Chain of Code: Reasoning with a Language Model-Augmented Code Emulator"},{"paperId":"1894b5729cb837cf1d9e17158c9dd2a5541512ff","title":"AVA: Towards Autonomous Visualization Agents through Visual Perception-Driven Decision-Making"},{"paperId":"10578bc0bdb3ebf9232931dd4961f55ba470caad","title":"LaMPilot: An Open Benchmark Dataset for Autonomous Driving with Language Model Programs"},{"paperId":"f32ea390686b1eee3ba5b53c7a85e9e9385d4b94","title":"Visual Program Distillation: Distilling Tools and Programmatic Reasoning into Vision-Language Models"},{"paperId":"0d8da8431bee9b2c2d40b605a754c5c840833323","title":"Recursive Visual Programming"},{"paperId":"cc9627c4475b9ad1c7e4105f3bceb1e7b59b7cab","title":"Zero-Shot Video Question Answering with Procedural Programs"},{"paperId":"246017780386eba39d6cda760a1c2c70356baa50","title":"VIoTGPT: Learning to Schedule Vision Tools towards Intelligent Video Internet of Things"},{"paperId":"70ea292c2312afdbe474e3c095dfa620b2cd46cc","title":"Video Summarization: Towards Entity-Aware Captions"},{"paperId":"d4ba81bda42b408cd6d48205a593826060efc1ed","title":"Evaluating VLMs for Score-Based, Multi-Probe Annotation of 3D Objects"},{"paperId":"7450c20d844ca05e643ece2461ff7aa2f381e22a","title":"Towards Top-Down Reasoning: An Explainable Multi-Agent Approach for Visual Question Answering"},{"paperId":"5eea245cc12c55905d4df827d0c9776c5ddfa743","title":"Compositional Chain-of-Thought Prompting for Large Multimodal Models"},{"paperId":"7b0a186b0140ee91fb13991c9c7187f3dc3b0670","title":"Visual Programming for Zero-shot Open-Vocabulary 3D Visual Grounding"},{"paperId":"033607afc2e08a58383ad78deb98c844017109c1","title":"ViStruct: Visual Structural Knowledge Extraction via Curriculum Guided Code-Vision Representation"},{"paperId":"1e838bd5fa2f5bca805493d0f672d03514b36869","title":"Vamos: Versatile Action Models for Video Understanding"},{"paperId":"7f807249c0ef0fe07d5e9c810684cd5daba0edc5","title":"De-fine: Decomposing and Refining Visual Programs with Auto-Feedback"},{"paperId":"ab8169d6e4dfabfe7c30ebec1bb871bf3e1551cd","title":"GAIA: a benchmark for General AI Assistants"},{"paperId":"6fa0677731184444df0e1fc8070938419cd6da47","title":"Igniting Language Intelligence: The Hitchhiker's Guide From Chain-of-Thought Reasoning to Language Agents"},{"paperId":"107fb6eec2febbae12db29bf3e311aaf5680027c","title":"Video-LLaVA: Learning United Visual Representation by Alignment Before Projection"},{"paperId":"aad3d2e690f6c73f04a14622ceff51464bbc560e","title":"Chat-UniVi: Unified Visual Representation Empowers Large Language Models with Image and Video Understanding"},{"paperId":"2fb605f67fee79cad94952ddfe0f686e926f49f5","title":"GPT-4V in Wonderland: Large Multimodal Models for Zero-Shot Smartphone GUI Navigation"},{"paperId":"6a28a2c2282c23e173cb3feefea27566eb2c6377","title":"Past as a Guide: Leveraging Retrospective Learning for Python Code Completion"},{"paperId":"ef321c6f174ac59916ac54ec40ad18bca5b58e5c","title":"PerceptionGPT: Effectively Fusing Visual Perception into LLM"},{"paperId":"cf7d69709bdeddd561c183178bbc1f0c2e156a08","title":"Analyzing Modular Approaches for Visual Question Decomposition"},{"paperId":"8ec7d50250203543a0098d99f04957b22bbe2c77","title":"How to Bridge the Gap between Modalities: A Comprehensive Survey on Multimodal Large Language Model"},{"paperId":"de27e0add04612f85b96180dc6fac9c713397d9f","title":"Follow-Up Differential Descriptions: Language Models Resolve Ambiguities for Image Classification"},{"paperId":"ecfff30e570498b6c87c5d1319a0cbcdf7a8b86d","title":"LLaVA-Plus: Learning to Use Tools for Creating Multimodal Agents"},{"paperId":"adff39f3972d349afec4bc7bcbc580b670a99097","title":"GENOME: GenerativE Neuro-symbOlic visual reasoning by growing and reusing ModulEs"},{"paperId":"c62711f6b5d8620ba36bc2c378ec6ab53f6e197c","title":"RoboGen: Towards Unleashing Infinite Data for Automated Robot Learning via Generative Simulation"},{"paperId":"c020f15be1dee20f9e2e0c5a6f05f272b5508325","title":"LLaVA-Interactive: An All-in-One Demo for Image Chat, Segmentation, Generation and Editing"},{"paperId":"5b0458d5e839117884739d4b9dead0285beb4a76","title":"Symbolic Planning and Code Generation for Grounded Dialogue"},{"paperId":"3dd8a397abc6fcd7e945cea2d02a1ea3266e0bc9","title":"What's Left? Concept Grounding with Logic-Enhanced Foundation Models"},{"paperId":"8e3e7deb95d2a984cba615ec847e64f354626cdf","title":"WebWISE: Web Interface Control and Sequential Exploration with Large Language Models"},{"paperId":"29b3ce4de9dd9d784ca1d876957950f4b2d3796a","title":"Towards Perceiving Small Visual Details in Zero-shot Visual Question Answering with Multimodal LLMs"},{"paperId":"133777180e326dfa53523bf53b0a969bbdccb0ee","title":"API-Assisted Code Generation for Question Answering on Varied Table Structures"},{"paperId":"729fc01274cc26798654a318d1a95e73c61f99a3","title":"Open-Ended Instructable Embodied Agents with Memory-Augmented Large Language Models"},{"paperId":"96d104dfe727f78a35faaafe81481f3672b485ee","title":"Large Language Models are Visual Reasoning Coordinators"},{"paperId":"02a00ce9e7bce14937f46af0423eea40b7b63303","title":"Steve-Eye: Equipping LLM-based Embodied Agents with Visual Perception in Open Worlds"},{"paperId":"f90c522b284a6c065fa5126216a26a7415a2b9fa","title":"MoqaGPT : Zero-Shot Multi-modal Open-domain Question Answering with Large Language Model"},{"paperId":"ad636bf86e81f976af6cb4b95a83bd65b6443ed7","title":"Neurosymbolic Grounding for Compositional World Models"},{"paperId":"beb3e8acd816bac1a5b7fccfd073f79048877e33","title":"Frozen Transformers in Language Models Are Effective Visual Encoder Layers"},{"paperId":"4f63c5a89c7299a864c6c48aa1844fb0fe8c9437","title":"Survey of Vulnerabilities in Large Language Models Revealed by Adversarial Attacks"},{"paperId":"a710efa9247207a72f06e0c9db302fd3ecab5fbb","title":"Towards Robust Multi-Modal Reasoning via Model Selection"},{"paperId":"b6c8c1745a18d6e59c7a8a99f0df7aa4c18a1e73","title":"Octopus: Embodied Vision-Language Programmer from Environmental Feedback"},{"paperId":"1d14a708622917da4b9820ada6d32af24fc1651a","title":"Idea2Img: Iterative Self-Refinement with GPT-4V(ision) for Automatic Image Design and Generation"},{"paperId":"7f1ba5630c3baa09b11cc665b3f71cdb117e5ffb","title":"OpenLEAF: Open-Domain Interleaved Image-Text Generation and Evaluation"},{"paperId":"8147cec9245d34d13732a08e915c920a1a499bb5","title":"Lemur: Harmonizing Natural Language and Code for Language Agents"},{"paperId":"33095b1334bed852e3652bd9d7da3f4df0cdf485","title":"ViCor: Bridging Visual Understanding and Commonsense Reasoning with Large Language Models"},{"paperId":"84f9bc5f89dac53662fb467b6af8ff26415ca3e7","title":"InstructDET: Diversifying Referring Object Detection with Generalized Instructions"},{"paperId":"700bd9681f1b9e9e2212e10415d27b11c7e6836b","title":"Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models"},{"paperId":"e7b67fe319f812db6ba6789eb2d9bf8b445c1d64","title":"GRID: A Platform for General Robot Intelligence Development"},{"paperId":"20ae101289965d36dbd93e9b8c47ec9deab03ed0","title":"What If the TV Was Off? Examining Counterfactual Reasoning Abilities of Multi-modal Language Models"},{"paperId":"092245d86b77181c36f972b1b7a17a59cd989c4a","title":"Guiding Instruction-based Image Editing via Multimodal Large Language Models"},{"paperId":"a1426b13b74dbad17b34606d25aabe1d61f6e11a","title":"CRAFT: Customizing LLMs by Creating and Retrieving from Specialized Toolsets"},{"paperId":"b7e0ea06f096d4963d96739ce154ba6bfe5af7ee","title":"Compositional Sculpting of Iterative Generative Processes"},{"paperId":"16753e0317730e8c1b297338300a8c6163dd06f2","title":"VideoDirectorGPT: Consistent Multi-scene Video Generation via LLM-Guided Planning"},{"paperId":"1ef9048e0465e1855b4b87c0630e85688e903b57","title":"Man vs the machine in the struggle for effective text anonymisation in the age of large language models"},{"paperId":"7b689adb8c156d6158660f90d1c86888ee281f63","title":"DreamLLM: Synergistic Multimodal Comprehension and Creation"},{"paperId":"f34b6b4f75fb4e6f2c5654ee13fb2479c6170b3a","title":"Kosmos-2.5: A Multimodal Literate Model"},{"paperId":"3ec464696db25acc2c39a6d967ec3df09e06f633","title":"Multimodal Multi-Hop Question Answering Through a Conversation Between Tools and Efficiently Finetuned Large Language Models"},{"paperId":"4eb87eaa193929dbef93fa2db9419245a8e8916f","title":"TextBind: Multi-turn Interleaved Multimodal Instruction-following in the Wild"},{"paperId":"4cf527e9e0d68e3fc16d39fbcdb3869cd3ccf60f","title":"Hypothesis Search: Inductive Reasoning with Language Models"},{"paperId":"67c09c9f93aa91f2419f2b348e1545bc2c115e71","title":"Gesture-Informed Robot Assistance via Foundation Models"},{"paperId":"6bcc6ab9c28805d4067e99b2cdc7524550fe80e1","title":"PointLLM: Empowering Large Language Models to Understand Point Clouds"},{"paperId":"894ed1aba8e42a4ec27ba53ecde383b14c5128ca","title":"Examining User-Friendly and Open-Sourced Large GPT Models: A Survey on Language, Multimodal, and Scientific GPT Models"},{"paperId":"28c6ac721f54544162865f41c5692e70d61bccab","title":"A Survey on Large Language Model based Autonomous Agents"},{"paperId":"451a657dabf80ebc43f6a3be518250b2cd5dfe1a","title":"Through the Lens of Core Competency: Survey on Evaluation of Large Language Models"},{"paperId":"d6c2523ab97416c2692cbbeab082ed1790e8e55e","title":"VisIT-Bench: A Benchmark for Vision-Language Instruction Following Inspired by Real-World Use"},{"paperId":"a6c57acfbcec39af366d33db18cff1cb8803b27a","title":"Dynamic Planning with a LLM"},{"paperId":"dd0612ce863f64b0f69d0d9f708c52e829f6f859","title":"TPTU: Large Language Model-based AI Agents for Task Planning and Tool Usage"},{"paperId":"446fb5dead075a1a08862662738f462e9a0e91c8","title":"Tool Documentation Enables Zero-Shot Tool-Usage with Large Language Models"},{"paperId":"6024f320e0a5b9b8fc29b86903aa9a96956b26dd","title":"AntGPT: Can Large Language Models Help Long-term Action Anticipation from Videos?"},{"paperId":"aa7bcd1f9453c9096ec78900a7b94e816ed0e1c5","title":"WavJourney: Compositional Audio Creation with Large Language Models"},{"paperId":"584ca135b61482fd89247113da87d784f738dbfa","title":"Foundational Models Defining a New Era in Vision: A Survey and Outlook"},{"paperId":"1cd8373490efc2d74c2796f4b2aa27c7d4415ec9","title":"VoxPoser: Composable 3D Value Maps for Robotic Manipulation with Language Models"},{"paperId":"ca31b8584b6c022ef15ddfe994fe361e002b7729","title":"A Comprehensive Overview of Large Language Models"},{"paperId":"ec592e12f45e20819afe203164bbbd0de8990510","title":"AmadeusGPT: a natural language interface for interactive animal behavioral analysis"},{"paperId":"094883e42bb9a41f602c0715c1059bc431e33fb2","title":"GPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest"},{"paperId":"03251361c1d67c6b5badffc7059fdd7fbfea1fed","title":"Statler: State-Maintaining Language Models for Embodied Reasoning"},{"paperId":"42aab468882c1efc4ea33198c2eaffd0daadf184","title":"Look, Remember and Reason: Grounded reasoning in videos with language models"},{"paperId":"efc694164312006c543ef745611348ef64e68dda","title":"Towards Language Models That Can See: Computer Vision Through the LENS of Natural Language"},{"paperId":"8efc20988021ce3b4b05dd44b13e27260ee9b99b","title":"Investigating Prompting Techniques for Zero- and Few-Shot Visual Question Answering"},{"paperId":"b937b5ad3c1ebe6007e744fa7864ec095e0070ab","title":"Tell Me Where to Go: A Composable Framework for Context-Aware Embodied Robot Navigation"},{"paperId":"966852963a88a28786b798c91b6662d6e501e590","title":"AssistGPT: A General Multi-modal Assistant that can Plan, Execute, Inspect, and Learn"},{"paperId":"051549d8ef56937b2f4d113afdcf8c7586d3770b","title":"Towards AGI in Computer Vision: Lessons Learned from GPT and Large Language Models"},{"paperId":"c03313fe8a59a2b8a9871b5226fc971fccbc2ba9","title":"Toward Grounded Commonsense Reasoning"},{"paperId":"d98536f24272e258b1d399074b64284d64786099","title":"AVIS: Autonomous Visual Information Seeking with Large Language Models"},{"paperId":"473eb062612a17c965eaa62136322f0dec6b1f8e","title":"Data-Copilot: Bridging Billions of Data and Humans with Autonomous Workflow"},{"paperId":"caae5e44957dea66fc309a55925e132de0fdb456","title":"Looking Around Corners: Generative Methods in Terrain Extension"},{"paperId":"fd755dc7b5b206c17fd953db04e1c888d45b6e4e","title":"LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark"},{"paperId":"ed30969f0e4811473144ffe83c1baa6d54f02202","title":"RestGPT: Connecting Large Language Models with Real-World RESTful APIs"},{"paperId":"da061a6e0016d6b625a8e86d64a797ca8ddb92a5","title":"Modular Visual Question Answering via Code Generation"},{"paperId":"50f44ef10335d59cec145b15effae20ff22c1fdb","title":"ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory"},{"paperId":"6847b9658f287f430098199cd81bf26308da13f9","title":"Domain Specialization as the Key to Make Large Language Models Disruptive: A Comprehensive Survey"},{"paperId":"af705d648b5b16daa3dcc593bc593f2574d76c07","title":"Grammar Prompting for Domain-Specific Language Generation with Large Language Models"},{"paperId":"e45036dddb5f27d3e87d2f14a2d9e6a402e7b5b7","title":"SheetCopilot: Bringing Software Productivity to the Next Level through Large Language Models"},{"paperId":"5ff2f5212713ec424662ac3c9e4aa5a8790d40cf","title":"ANPL: Towards Natural Programming with Interactive Decomposition"},{"paperId":"7cf64070fd3d7e53d80f260c10e6bd7018d580e1","title":"IdealGPT: Iteratively Decomposing Vision and Language Reasoning via Large Language Models"},{"paperId":"9837349417e36ef5be06da0fd6c74042148bdaa2","title":"Visual Programming for Text-to-Image Generation and Evaluation"},{"paperId":"66d755730f5d08a6f4fcc5e81f24982ba389dca9","title":"LayoutGPT: Compositional Visual Planning and Generation with Large Language Models"},{"paperId":"69335077fcacbff7a7cf25697da1949e6bdfa968","title":"The Art of SOCRATIC QUESTIONING: Recursive Thinking with Large Language Models"},{"paperId":"8da9b1436212b233fc49c7daf1ba15c22874ff5a","title":"CREATOR: Tool Creation for Disentangling Abstract and Concrete Reasoning of Large Language Models"},{"paperId":"3130643a5d02f0e849d83bb1f85577a924081f36","title":"Paxion: Patching Action Knowledge in Video-Language Foundation Models"},{"paperId":"2195676f111ad492c50f4d4c96abb2bd3d72f7fc","title":"Instruct2Act: Mapping Multi-modality Instructions to Robotic Actions with Large Language Model"},{"paperId":"692bc40edf4785d88c39e0c0fe9f270541fecf8a","title":"Towards Generalist Robots: A Promising Paradigm via Generative Simulation"},{"paperId":"8badb0587fef2ffc078b0cec549eb8ec96ed3ad4","title":"Self-Chained Image-Language Model for Video Localization and Question Answering"},{"paperId":"d6d3604f369bb0415cbe814e43ca3131323b03e2","title":"Otter: A Multi-Modal Model with In-Context Instruction Tuning"},{"paperId":"570079bbdd8758dfe865097e05719313c9c1301a","title":"LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model"},{"paperId":"ca6a2bc279be5a3349a22bfd6866ed633d18734b","title":"MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models"},{"paperId":"170c97c7215f42edfb20c2248f954879e91ef86e","title":"Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models"},{"paperId":"a5036f31f0e629dc661f120b8c3b1f374d479ab8","title":"Visual Instruction Tuning"},{"paperId":"0ebc861f5478561f12941e6b48aad30574e996d8","title":"Video ChatCaptioner: Towards Enriched Spatiotemporal Descriptions"},{"paperId":"2d3905c1a92c28c056dff1225d89e4ca72ac4d8e","title":"Man vs the machine: The Struggle for Effective Text Anonymisation in the Age of Large Language Models"},{"paperId":"c7a9c7302a72301ed79a7c0696d5af2e03ad3ac4","title":"MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action"},{"paperId":"f02d56e630986997e0aea3d92bf53e0f363ce401","title":"Prismer: A Vision-Language Model with Multi-Task Experts"},{"paperId":"6845bea94b2fb17d4377b3bb2bd10f73a959f9cc","title":"Reasoning with Language Model Prompting: A Survey"},{"paperId":"3d9703a974184ca8a1f3c97e1fb8acff1a50a18d","title":"Towards Data-and Knowledge-Driven Artificial Intelligence: A Survey on Neuro-Symbolic Computing"},{"paperId":"ed9943d73eb42116fe33564b5065c78b5ca0b16e","title":"RestGPT: Connecting Large Language Models with Real-World Applications via RESTful APIs"},{"paperId":"d7cdcedebd7013f102fc4f1ca5890ac352c7cfa0","title":"TOA: Task-oriented Active VQA"},{"paperId":"a3711dbf296b5ddd97ba93826660cd3995611625","title":"Towards A Foundation Model for Generalist Robots: Diverse Skill Learning at Scale via Automated Task and Scene Generation"},{"paperId":"13b5b69355555e0c8b702261c5de3b4172ba653c","title":"The Art of SOCRATIC QUESTIONING: Zero-shot Multimodal Reasoning with Recursive Thinking and Self-Questioning"},{"paperId":"d1120d67b700e4dfe8b39eb1e48fbdea4e1a0c43","title":"HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face"},{"paperId":"75c08892179fc478f87d7020b5daff9fca4f3389","title":"Beyond One-Model-Fits-All: A Survey of Domain Specialization for Large Language Models"},{"paperId":"72160b3c0f73c968fcb903db71817d1bed695f4d","title":"Look, Remember and Reason: Visual Reasoning with Grounded Rationales"},{"paperId":"5ce94181ea702f69c3651dce721d6bd8026b8106","title":"TPTU: Task Planning and Tool Usage of Large Language Model-based AI Agents"},{"paperId":"33cf37705ba20a78a32fbbb8a2cd080a08d58b25","title":"Experiencing InstructPipe: Building Multi-modal AI Pipelines via Prompting LLMs and Visual Programming"}],"references":[{"paperId":"da061a6e0016d6b625a8e86d64a797ca8ddb92a5","title":"Modular Visual Question Answering via Code Generation"},{"paperId":"af997821231898a5f8d0fd78dad4eec526acabe5","title":"Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models"},{"paperId":"fbfef4723d8c8467d7bd523e1d0b703cce0e0f9c","title":"Language Is Not All You Need: Aligning Perception with Language Models"},{"paperId":"53d128ea815bcc0526856eb5a9c42cc977cb36a7","title":"Toolformer: Language Models Can Teach Themselves to Use Tools"},{"paperId":"3f5b31c4f7350dc88002c121aecbdc82f86eb5bb","title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models"},{"paperId":"b909c1905063fe247a7c9359842e8437448f929d","title":"HiTeA: Hierarchical Temporal-Aware Video-Language Pre-training"},{"paperId":"4eb5198062f78ecf844ff48bcaefe4c1c0f395cc","title":"Doubly Right Object Recognition: A Why Prompt for Visual Rationales"},{"paperId":"3e8251f259dc529b3aa2366fc68c1516b202cfb9","title":"Reveal: Retrieval-Augmented Visual-Language Pre-Training with Multi-Source Multimodal Knowledge Memory"},{"paperId":"e1c2a926df37107358ac51e460361e2a249c8b26","title":"Open-vocabulary Attribute Detection"},{"paperId":"6c943670dca38bfc7c8b477ae7c2d1fba1ad3691","title":"Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks"},{"paperId":"6c1e1cc1e0e1f8fd026fe517607b2d4535565fa7","title":"PAL: Program-aided Language Models"},{"paperId":"af1c871282ec122869d03f5420ef5d9143358a91","title":"Visual Programming: Compositional visual reasoning without training"},{"paperId":"a5cb8f26acb71edd77ff9a143d3ddaab2367eb40","title":"PromptCap: Prompt-Guided Task-Aware Image Captioning"},{"paperId":"26fd105d0b5a458979c012cddb3ba2de943388c4","title":"Plug-and-Play VQA: Zero-shot VQA by Conjoining Large Pretrained Models with Zero Training"},{"paperId":"11f86abe3d6b1de0678390fed442fdcb62667768","title":"COFAR: Commonsense and Factual Reasoning in Image Search"},{"paperId":"a42b091adaf29b06a092b67192ac07cb93312f2a","title":"Visual Classification via Description from Large Language Models"},{"paperId":"39e40821b7207125e54e6ed7112e55cd38c6f0c3","title":"Language Models of Code are Few-Shot Commonsense Learners"},{"paperId":"6f85ec89d9c07a8db4545e64888ced820370a21b","title":"Retrieval Augmented Visual Question Answering with Outside Knowledge"},{"paperId":"f58ca7ba4a08b7082e86b7a5989b4b0fda2107ab","title":"Binding Language Models in Symbolic Languages"},{"paperId":"009e40cdc9d98b9e5f6279d38b46936ceffcc124","title":"Video Graph Transformer for Video Question Answering"},{"paperId":"57c64f233a0db4d17e0e750c12516364ca009fb2","title":"REVIVE: Regional Visual Representation Matters in Knowledge-Based Visual Question Answering"},{"paperId":"02720ba7a4c0c70506ef63e039387c10b227d8e3","title":"Transform-Retrieve-Generate: Natural Language-Centric Outside-Knowledge Visual Question Answering"},{"paperId":"809822d59203a462bc9f2e0f0e9a8314d6d469d4","title":"Revisiting the “Video” in Video-Language Understanding"},{"paperId":"354bf043179e3e9f05df73e3f04517e53c326d1f","title":"TALM: Tool Augmented Language Models"},{"paperId":"c1ace33daf974d3d16752c7a8565f32a63b09c49","title":"Language Models with Image Descriptors are Strong Few-Shot Video-Language Learners"},{"paperId":"9dae204dad41633188022002a04c8aa67c79a4e1","title":"Simple Open-Vocabulary Object Detection with Vision Transformers"},{"paperId":"26218bdcc3945c7edae7aa2adbfba4cd820a2df3","title":"Flamingo: a Visual Language Model for Few-Shot Learning"},{"paperId":"408efdd599b2b27ecb95a4d799869c9ff568fb31","title":"ReCLIP: A Strong Zero-Shot Baseline for Referring Expression Comprehension"},{"paperId":"ada81a4de88a6ce474df2e2446ad11fea480616e","title":"Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language"},{"paperId":"1bfa62ddfa3f6691e0e40c06f8ead594b6449cfa","title":"OFA: Unifying Architectures, Tasks, and Modalities Through a Simple Sequence-to-Sequence Learning Framework"},{"paperId":"1b6e810ce0afd0dd093f789d2b2742d047e316d5","title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"bba57c53ab9b600f71d888601ed0aa03812c8199","title":"MuMuQA: Multimedia Multi-Hop News Question Answering via Cross-Media Knowledge Extraction and Grounding"},{"paperId":"ab8ee8d06ed581c876da3a2e7a5fdb1cbec21a45","title":"KAT: A Knowledge Augmented Transformer for Vision-and-Language"},{"paperId":"5341b412383c43f4a693ad63ec4489e3ec7688c8","title":"Grounded Language-Image Pre-training"},{"paperId":"ec8afc75ec219f2a5f9ed9d7c9dde0720f69b5a2","title":"Multi-Grained Vision Language Pre-Training: Aligning Texts with Visual Concepts"},{"paperId":"09f2b1f1bd313cf9183c138fca8f17bb228b4435","title":"Coarse-to-Fine Reasoning for Visual Question Answering"},{"paperId":"2672777d25562c9df6fc13b653181db62d39bece","title":"An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA"},{"paperId":"acbdbf49f9bc3f151b93d9ca9a06009f4f6eb269","title":"Evaluating Large Language Models Trained on Code"},{"paperId":"f46f77630b35a43e8c247916da5d809d6e5b4210","title":"Interpretable visual reasoning: A survey"},{"paperId":"6be64445935dcdf4053a6e78b623b80a314d9bbc","title":"Separating Skills and Concepts for Novel Visual Question Answering"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"0416fda32c39fc9531e87bab6a8a1a552bf9ada0","title":"Obtaining Faithful Interpretations from Compositional Neural Networks"},{"paperId":"3c8a456509e6c0805354bd40a35e3f2dbf8069b1","title":"PyTorch: An Imperative Style, High-Performance Deep Learning Library"},{"paperId":"007ca8ca7a68451c32da034c72a06238434843c1","title":"Learning to Learn Words from Visual Scenes"},{"paperId":"79c93274429d6355959f1e4374c2147bb81ea649","title":"LXMERT: Learning Cross-Modality Encoder Representations from Transformers"},{"paperId":"136c05cb8dd359fb8e0dc7947172a9ecb74ccbec","title":"Learning by Abstraction: The Neural State Machine"},{"paperId":"7bd83b055702bc178aa26def5b6df463f8eab7b9","title":"Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-Shot Cross-Dataset Transfer"},{"paperId":"28ad018c39d1578bea84e7cedf94459e3dbe1e70","title":"OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge"},{"paperId":"2dc698077cb178286c737484dcf67c5ab19314d0","title":"Language-Conditioned Graph Networks for Relational Reasoning"},{"paperId":"a7ac99d7cf3f568ab1a741392144b646b856ae0c","title":"GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering"},{"paperId":"9d15ebe3f5aaf32a9f835f88703241461324c35b","title":"Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding"},{"paperId":"97b93509f6c3c33dd3665d05b1878e36d58a1efb","title":"Interpretable Visual Question Answering by Reasoning on Dependency Trees"},{"paperId":"b1b852d4bf934863397e7b965a5dd0124ad8670c","title":"Interpretable Visual Question Answering by Visual Grounding From Attention Supervision Mining"},{"paperId":"7af4a37e6e63b5f06e7bfb6e7c8910322774efb9","title":"Visual Reasoning by Progressive Module Networks"},{"paperId":"1fe32a88a2e4162f4b3fe73ffa1fcb120bd5b1bf","title":"Visual Grounding Via Accumulated Attention"},{"paperId":"289fb3709475f5c87df8d97f129af54029d27fee","title":"Compositional Attention Networks for Machine Reasoning"},{"paperId":"ef153ece43ee50f8208f6197f0eaf3d324e4475b","title":"Multimodal Explanations: Justifying Decisions and Pointing to the Evidence"},{"paperId":"0fff5c49c05c27c22ac7685130197146491f0b36","title":"The Consciousness Prior"},{"paperId":"2e17cf6a339fd071ad222062f868e882ef4120a4","title":"Inferring and Executing Programs for Visual Reasoning"},{"paperId":"a396a6febdacb84340d139096455e67049ac1e22","title":"Learning to Reason: End-to-End Module Networks for Visual Question Answering"},{"paperId":"5582bebed97947a41e3ddd9bd1f284b73f1648c2","title":"Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization"},{"paperId":"3c1bbd2672c11a796f1e6e6aa787257498ec8bec","title":"Revisiting Visual Question Answering Baselines"},{"paperId":"21c99706bb26e9012bfb4d8d48009a3d45af59b2","title":"Neural Module Networks"},{"paperId":"4d8f2d14af5991d4f0d050d22216825cac3157bd","title":"Show, Attend and Tell: Neural Image Caption Generation with Visual Attention"},{"paperId":"2f2961362355e45fa014ca0bb8ce4495aedf8824","title":"Thinking fast and slow."},{"paperId":"c3a24b0b38922c4f3a825edb97cc470a4ca7af75","title":"Vision"},{"paperId":"f9bdd27c48c57426179b1b09ffc517e94cbfca56","title":"Information Streams Sharing a Finite Buffer"},{"paperId":"450b8dff662a5d41388d04d994e5117020777ce5","title":"Code4Struct: Code Generation for Few-Shot Structured Prediction from Natural Language"},{"paperId":null,"title":"Plug-and-play VQA: Zeroshot VQA by conjoining large pretrained models with zero training. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 951–967"},{"paperId":null,"title":"the 2022 Conference on Empirical Methods in Natural Language Processing"},{"paperId":null,"title":"arXiv:2201.11903 [cs"},{"paperId":null,"title":"Systematic Generalization: What Is Required and Can It Be Learned"},{"paperId":"da0068c430b96347682bcc5b590b15ed8b1a41c4","title":"Visual Programming"},{"paperId":"c9ffae72c63dc28142d0828ceace60a8ace539eb","title":"2009 IEEE 12th International Conference on Computer Vision (ICCV)"},{"paperId":"48ffc7197f27ab7e96e1eba1f929513f1848e522","title":"Programs"},{"paperId":null,"title":"Christopher"},{"paperId":null,"title":"Felipe Petroski Such"},{"paperId":null,"title":"11862 applicable license agreement with IEEE. Restrictions apply"}],"id":"6e754273d54a91371efbc928cd6b156364d517da","summary":"ViperGPT is introduced, a framework that leverages code-generation models to compose vision-and-language models into subroutines to produce a result for any query and achieves state-of-the-art results across various complex visual tasks."},{"url":"https://www.semanticscholar.org/paper/28ff0816f19a5e3e37eac5569de41872fd262f0a","title":"Align, Reason and Learn: Enhancing Medical Vision-and-Language Pre-training with Knowledge","venue":"ACM Multimedia","year":2022,"referenceCount":61,"citationCount":24,"influentialCitationCount":1,"publicationDate":"15/09/2022","authors":"Zhihong Chen,Guanbin Li,Xiang Wan","citations":[{"paperId":"ecddb1287f241e4abf2874a7b95172a68c21a2e5","title":"MLIP: Enhancing Medical Visual Representation with Divergence Encoder and Knowledge-guided Contrastive Learning"},{"paperId":"420087f314633a381e61e6c5cd73ccc2070a749e","title":"PeFoMed: Parameter Efficient Fine-tuning on Multimodal Large Language Models for Medical Visual Question Answering"},{"paperId":"6bdfffbf92d01c8b543088d40d46233610e469a8","title":"CLIP in Medical Imaging: A Comprehensive Survey"},{"paperId":"2c7e346aa311fec4dda04bdf3a214ce2026d8807","title":"Medical Vision Language Pretraining: A survey"},{"paperId":"749104d1a207f5bc192c7d95a12856b5e7f84d1f","title":"Mapping medical image-text to a joint space via masked modeling"},{"paperId":"c7492913370b5726eaa6ced163a60de6c9d4bb7f","title":"A Survey of Large Language Models for Healthcare: from Data, Technology, and Applications to Accountability and Ethics"},{"paperId":"e9f0223f8dce8b04d37d1f56e6c976b5d0cb5956","title":"A Foundation LAnguage-Image model of the Retina (FLAIR): Encoding expert knowledge in text supervision"},{"paperId":"92a63df9582c25418b5de50705a0f68760f6aa49","title":"Re-mine, Learn and Reason: Exploring the Cross-modal Semantic Correlations for Language-guided HOI detection"},{"paperId":"2bc6d41caf81e62eb60b0829a521cfee085715c0","title":"Bridging Vision and Language Encoders: Parameter-Efficient Tuning for Referring Image Segmentation"},{"paperId":"9b348715d0311056eee850dd1cce1cdd3c64eec8","title":"Knowledge Boosting: Rethinking Medical Contrastive Vision-Language Pre-Training"},{"paperId":"baa1dc079d98ca76b0173c8d653fed759fd0a371","title":"A scoping review on multimodal deep learning in biomedical images and texts"},{"paperId":"64fa56962dd0f4bbe206be6142fbe0315c4e7c2f","title":"Multi-modal Pre-training for Medical Vision-language Understanding and Generation: An Empirical Study with A New Benchmark"},{"paperId":"8700c5af25450bf8e84b94783344b054d268738b","title":"Bi-VLGM : Bi-Level Class-Severity-Aware Vision-Language Graph Matching for Text Guided Medical Image Segmentation"},{"paperId":"0fe1b1bfd634ee42846afbd64cef1c682e02e5e7","title":"Retrieval-based Knowledge Augmented Vision Language Pre-training"},{"paperId":"ac4d13b6a4f9fb67337099f4602135a0351f5c99","title":"Towards Medical Artificial General Intelligence via Knowledge-Enhanced Multimodal Pretraining"},{"paperId":"8f3138f7ee5127faab265793be8ae278bc49d9b1","title":"PMC-CLIP: Contrastive Language-Image Pre-training using Biomedical Documents"},{"paperId":"4d4a96708fc67403176bb2b891b564af7a20c148","title":"RAMM: Retrieval-augmented Biomedical Visual Question Answering with Multi-modal Pre-training"},{"paperId":"5d937b7811d8fd4208b2810971cb2e33f64bcfa2","title":"Knowledge-enhanced visual-language pre-training on chest radiology images"},{"paperId":"880a0029ddeba3e3f8b6bd50cc6843ea760cc3d3","title":"MDF-Net for abnormality detection by fusing X-rays with clinical data"},{"paperId":"da9579539385daedd33a0de0f814e2977ad0d1f5","title":"Towards Unifying Medical Vision-and-Language Pre-training via Soft Prompts"},{"paperId":"e3c70b0b71b51872bbdaa0f4bf2b56908f97abec","title":"MedKLIP: Medical Knowledge Enhanced Language-Image Pre-Training for X-ray Diagnosis"},{"paperId":"508d9b43832790b4d35f4ae1fa76e9712859d6aa","title":"Vision and Structured-Language Pretraining for Cross-Modal Food Retrieval"},{"paperId":"b15469d0ab3dc3a9dec037d761817b3fe546bed6","title":"Pre-trained Language Models in Biomedical Domain: A Systematic Survey"},{"paperId":"10a8e7a7e07256178665f90074c5c41b071e73d3","title":"MDF-Net: Multimodal Dual-Fusion Network for Abnormality Detection using CXR Images and Clinical Data"}],"references":[{"paperId":"4ef3d9e492479e28fa57d107e52acc6a0c803de2","title":"Does CLIP Benefit Visual Question Answering in the Medical Domain as Much as it Does in the General Domain?"},{"paperId":"6351ebb4a3287f5f3e1273464b3b91e5df5a16d7","title":"Masked Autoencoders Are Scalable Vision Learners"},{"paperId":"94ff111c4d81bd03f159321728ceec8b4711c89d","title":"An Empirical Study of Training End-to-End Vision-and-Language Transformers"},{"paperId":"5e00596fa946670d894b1bdaeff5a98e3867ef13","title":"SimVLM: Simple Visual Language Model Pretraining with Weak Supervision"},{"paperId":"d0b59b3e34a79c8c79a31bf3944ded8ab7a803ae","title":"ROSITA: Enhancing Vision-and-Language Semantic Alignments via Cross- and Intra-modal Knowledge Integration"},{"paperId":"b82c5f9efdb2ae56baa084ca41aeddd8a665c1d1","title":"Align before Fuse: Vision and Language Representation Learning with Momentum Distillation"},{"paperId":"63c74d15940af1af9b386b5762e4445e54c73719","title":"VinVL: Revisiting Visual Representations in Vision-Language Models"},{"paperId":"d9317660e2a538d9c018028956fd114d55330f82","title":"Multi-Modal Understanding and Generation for Medical Images and Text via Vision-Language Pre-Training"},{"paperId":"3c83f80f06633ff4598d33c2959f8e4cdcad3e93","title":"Cross-Modal Self-Attention with Multi-Task Pre-Training for Medical Visual Question Answering"},{"paperId":"17c2bb358169541f2d0a769f80779f46d1cd3d37","title":"MMBERT: Multimodal BERT Pretraining for Improved Medical VQA"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"93b6b79b4ef6c345f31722ce7c829385c6dce0d6","title":"Slake: A Semantically-Labeled Knowledge-Enhanced Dataset For Medical Visual Question Answering"},{"paperId":"0839722fb5369c0abaff8515bfc08299efc790a1","title":"ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision"},{"paperId":"c9074d9719c5ce0dd3a7369dd0749cd08d7f67ed","title":"MELINDA: A Multimodal Dataset for Biomedical Experiment Method Classification"},{"paperId":"268d347e8a55b5eb82fb5e7d2f800e33c75ab18a","title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"},{"paperId":"5ba77a5bdeffb62aa0902ae68997bbc38db8a722","title":"MedICaT: A Dataset of Medical Images, Captions, and Textual References"},{"paperId":"b8d5b853f2212cbb48a43f1edec9b96d76d388ec","title":"Medical Visual Question Answering via Conditional Reasoning"},{"paperId":"7eda139d737eea10fc1d95364327a41ec0cee4a4","title":"CoLAKE: Contextualized Language and Knowledge Embedding"},{"paperId":"ed2a06388dd14b052f33bac5e3bfc0fa26243b55","title":"A Comparison of Pre-trained Vision-and-Language Models for Multimodal Representation Learning across Medical Images and Reports"},{"paperId":"bc996a4dbf9d4234eacdd0b930a94de1d158e256","title":"ERNIE-ViL: Knowledge Enhanced Vision-Language Representations Through Scene Graph"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"818e5cbc337e4e1b98e65a2d7c2d6d2a0318cd57","title":"Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks"},{"paperId":"598a2ee223e2949c3b28389e922c1892b4717d2a","title":"Pixel-BERT: Aligning Image Pixels with Text by Deep Multi-Modal Transformers"},{"paperId":"56cafbac34f2bb3f6a9828cd228ff281b810d6bb","title":"KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation"},{"paperId":"6007bd2a34385132a7885b934d90b519a1f65bba","title":"ZEN: Pre-training Chinese Text Encoder Enhanced by N-gram Representations"},{"paperId":"33301b25a297b701bdc287e985c006375cb7bb21","title":"Overcoming Data Limitation in Medical Visual Question Answering"},{"paperId":"dfc7b58b67c31932b48586b3e23a43cc94695290","title":"UNITER: UNiversal Image-TExt Representation Learning"},{"paperId":"06a73ad09664435f8b3cd90293f4e05a047cf375","title":"K-BERT: Enabling Language Representation with Knowledge Graph"},{"paperId":"bfeb827d06c1a3583b5cc6d25241203a81f6af09","title":"Knowledge Enhanced Contextual Word Representations"},{"paperId":"4aa6298b606941a282d735fa3143da293199d2ca","title":"VL-BERT: Pre-training of Generic Visual-Linguistic Representations"},{"paperId":"79c93274429d6355959f1e4374c2147bb81ea649","title":"LXMERT: Learning Cross-Modality Encoder Representations from Transformers"},{"paperId":"5aec474c31a2f4b74703c6f786c0a8ff85c450da","title":"VisualBERT: A Simple and Performant Baseline for Vision and Language"},{"paperId":"65a9c7b0800c86a196bc14e7621ff895cc6ab287","title":"ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks"},{"paperId":"077f8329a7b6fa3b7c877a57b81eb6c18b5f87de","title":"RoBERTa: A Robustly Optimized BERT Pretraining Approach"},{"paperId":"5f994dc8cae24ca9d1ed629e517fcc652660ddde","title":"ERNIE: Enhanced Language Representation with Informative Entities"},{"paperId":"031e4e43aaffd7a479738dcea69a2d5be7957aa3","title":"ERNIE: Enhanced Representation through Knowledge Integration"},{"paperId":"156d217b0a911af97fa1b5a71dc909ccef7a8028","title":"SciBERT: A Pretrained Language Model for Scientific Text"},{"paperId":"de28ec1d7bd38c8fc4e8ac59b6133800818b4e29","title":"ScispaCy: Fast and Robust Models for Biomedical Natural Language Processing"},{"paperId":"3d29ce781f297dc543e44dfb39990baff3a3acca","title":"MIMIC-CXR-JPG, a large publicly available database of labeled chest radiographs"},{"paperId":"a564fabf130ff6e2742cfba90c7a4018937d764d","title":"Radiology Objects in COntext (ROCO): A Multimodal Image Dataset"},{"paperId":"a5d10341717c0519cf63151b496a6d2ed67aa05f","title":"Bilinear Attention Networks"},{"paperId":"ff65e3bf34e892ef75d91c5e3d7294e0b64d867d","title":"Zero-Shot Recognition via Semantic Embeddings and Knowledge Graphs"},{"paperId":"d07284a6811f1b2745d91bdb06b040b57f226882","title":"Decoupled Weight Decay Regularization"},{"paperId":"33998aff64ce51df8dee45989cdca4b6b1329ec4","title":"Graph Attention Networks"},{"paperId":"8e9ad6f8b2bc97f0412fa0cc243ac6975864534a","title":"Multi-modal Factorized Bilinear Pooling with Co-attention Learning for Visual Question Answering"},{"paperId":"79baf8cf6be6510f69be8c515516136138678cf5","title":"The More You Know: Using Knowledge Graphs for Image Classification"},{"paperId":"c6850869aa5e78a107c378d2e8bfa39633158c0c","title":"Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"},{"paperId":"2c03df8b48bf3fa39054345bafabfeff15bfd11d","title":"Deep Residual Learning for Image Recognition"},{"paperId":"2c1890864c1c2b750f48316dc8b650ba4772adc5","title":"Stacked Attention Networks for Image Question Answering"},{"paperId":"2582ab7c70c9e7fcb84545944eba8f3a7f253248","title":"Translating Embeddings for Modeling Multi-relational Data"},{"paperId":"519799a9e2a72b808d81c6bcba5de263503de053","title":"Contrastive Pre-training and Representation Distillation for Medical Visual Question Answering Based on Radiology Images"},{"paperId":"e5d143ae82ede67726aa1a9aeac3de4bf53d8920","title":"KB-VLP: Knowledge Based Vision and Language Pretraining"},{"paperId":null,"title":"BERT-MK: Integrating Graph Contextualized Knowledge into Pre-trained Language Models"},{"paperId":null,"title":"Neuro-SymbolicVisualReasoning:Disentangling"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"9eeeb23546d3d2bbc73959bffc6819f2335f3c83","title":"VQA-Med: Overview of the Medical Visual Question Answering Task at ImageCLEF 2019"},{"paperId":"18f9a6045ba01cb079c4fa49a630d71bbd27cd92","title":"Descriptor : A dataset of clinically generated visual questions and answers about radiology images"},{"paperId":"441281c07b5e5949aeb56375e25623ddbdab94f4","title":"Intravascular Imaging and Computer Assisted Stenting, and Large-Scale Annotation of Biomedical Data and Expert Label Synthesis"},{"paperId":"1f1eaf19e38b541eec8a02f099e3090536a4c936","title":"The Unified Medical Language System (UMLS): integrating biomedical terminology"},{"paperId":null,"title":"Long short-termmemory"},{"paperId":null,"title":"MM ’22, October 10–14, 2022,"}],"id":"28ff0816f19a5e3e37eac5569de41872fd262f0a","summary":"A systematic and effective approach to enhance Med-VLP by structured medical knowledge from three perspectives is proposed, which align the representations of the vision encoder and the language encoder through knowledge."},{"url":"https://www.semanticscholar.org/paper/c7a9c7302a72301ed79a7c0696d5af2e03ad3ac4","title":"MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action","venue":"arXiv.org","year":2023,"referenceCount":45,"citationCount":178,"influentialCitationCount":3,"publicationDate":"20/03/2023","authors":"Zhengyuan Yang,Linjie Li,Jianfeng Wang,Kevin Lin,E. Azarnasab,Faisal Ahmed,Zicheng Liu,Ce Liu,Michael Zeng,Lijuan Wang","citations":[{"paperId":"b3dae5011b9b44b861f884b2b788f2fc9bf59a4e","title":"ToolNet: Connecting Large Language Models with Massive Tools via Tool Graph"},{"paperId":"c86a70ff639707e647da3a429fe8e1e5c04415f5","title":"A Multimodal Foundation Agent for Financial Trading: Tool-Augmented, Diversified, and Generalist"},{"paperId":"ded3266d047b36a963c1324aa9f98705d598bdcf","title":"From Summary to Action: Enhancing Large Language Models for Complex Tasks with Open World APIs"},{"paperId":"74c68aed85f2fe8019113bbdb533fcba7e3ce0bd","title":"ShapeLLM: Universal 3D Object Understanding for Embodied Interaction"},{"paperId":"2cea424c7dce71042c24d43317521abdc4c0ffb4","title":"Large Multimodal Agents: A Survey"},{"paperId":"1b5e69a5b0f179e90f356a9c8cc1a39f77471dab","title":"Selective\"Selective Prediction\": Reducing Unnecessary Abstention in Vision-Language Reasoning"},{"paperId":"45ea1b47195d3d381e4b08f8cc0be3568c780ea9","title":"API-BLEND: A Comprehensive Corpora for Training and Benchmarking API LLMs"},{"paperId":"5e7274bcda47b704b6797bb14be8b7a61c047a61","title":"Uncertainty-Aware Evaluation for Vision-Language Models"},{"paperId":"5a20aa49b81b4e14fdb36814e557b3da60259ce9","title":"Chain of Thought Empowers Transformers to Solve Inherently Serial Problems"},{"paperId":"fc3c717987218662f49243e2be6bacc093dd47d8","title":"KG-Agent: An Efficient Autonomous Agent Framework for Complex Reasoning over Knowledge Graph"},{"paperId":"710b1e23b09e0b826f9d47e7cc23b5f4c0808c7e","title":"Multi-modal preference alignment remedies regression of visual instruction tuning on language model"},{"paperId":"73d8b0794b863f16ba63c4e2291a7c914824c0ec","title":"Graph Descriptive Order Improves Reasoning with Large Language Model"},{"paperId":"eaad6e351ab7ddb5a31bce3c5fe8bf38cd08c7f2","title":"Multimodal Query Suggestion with Multi-Agent Reinforcement Learning from Human Feedback"},{"paperId":"798feda076ad710df65d509a7884bd15937c8056","title":"Leak, Cheat, Repeat: Data Contamination and Evaluation Malpractices in Closed-Source LLMs"},{"paperId":"d2d96dc3bbf9d63c85f445e3fa08ad695457a532","title":"LLM Agents in Interaction: Measuring Personality Consistency and Linguistic Alignment in Interacting Populations of Large Language Models"},{"paperId":"b6b6e59f3bfdda9d4a4dfe56c46b30706fd18cf3","title":"Enhance Reasoning for Large Language Models in the Game Werewolf"},{"paperId":"7d9d4595f4cb4fc1521b445a2c4be5735c10c186","title":"EEG-GPT: Exploring Capabilities of Large Language Models for EEG Classification and Interpretation"},{"paperId":"c5db6c2726911b72d534f97bd4d1ed63f6431340","title":"Mobile-Agent: Autonomous Multi-Modal Mobile Device Agent with Visual Perception"},{"paperId":"a050c9b0c321839e4427ab9defa3463be7825ac4","title":"MM-LLMs: Recent Advances in MultiModal Large Language Models"},{"paperId":"7e6c1bb54bb2e36cc1092b080e9928942f7f8a68","title":"TroVE: Inducing Verifiable and Efficient Toolboxes for Solving Programmatic Tasks"},{"paperId":"140cfda71bfff852c3e205b7ad61854b78c76982","title":"Mastering Text-to-Image Diffusion: Recaptioning, Planning, and Generating with Multimodal LLMs"},{"paperId":"23957040943f883542f47850c709b9e7f9d6fa55","title":"Prompting Large Vision-Language Models for Compositional Reasoning"},{"paperId":"4a48d628e53f554eb6ef09a457ca855188b96171","title":"DoraemonGPT: Toward Understanding Dynamic Scenes with Large Language Models"},{"paperId":"ff61aef2fef3a235bfaa123158a990c4f5f27d1a","title":"Small LLMs Are Weak Tool Learners: A Multi-LLM Agent"},{"paperId":"5502d769595981009e43344f8914e287acca2359","title":"ModaVerse: Efficiently Transforming Modalities with LLMs"},{"paperId":"af5f256e9771bf9cd02451195e3a7ac693fde3ed","title":"Exploring the Reasoning Abilities of Multimodal Large Language Models (MLLMs): A Comprehensive Survey on Emerging Trends in Multimodal Reasoning"},{"paperId":"f04e58782380383f4edaaa899fd85bfb3d20c2e1","title":"SpeechAgents: Human-Communication Simulation with Multi-Modal Multi-Agent Systems"},{"paperId":"9eab4104973f5de650544729a4a69d84c594da92","title":"A Vision Check-up for Language Models"},{"paperId":"516dafd778d6dcce0ef04ef0539257976b897d24","title":"Incorporating Geo-Diverse Knowledge into Prompting for Increased Geographical Robustness in Object Recognition"},{"paperId":"a06d3e9e90008c64c45a0029d580541d5f646771","title":"If LLM Is the Wizard, Then Code Is the Wand: A Survey on How Code Empowers Large Language Models to Serve as Intelligent Agents"},{"paperId":"46f64681d7a0c7f80303bebb0d62a3b7acbcdcd9","title":"An Improved Baseline for Reasoning Segmentation with Large Language Model"},{"paperId":"4599d5af850da482f591a02a3b17d56e0d358771","title":"Plan, Posture and Go: Towards Open-World Text-to-Motion Generation"},{"paperId":"6a33e58ef961a3a0a5657518b2be86395eb7c8d0","title":"InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks"},{"paperId":"24fc9ad715372358bd0108eeb7c944b915963293","title":"ASSISTGUI: Task-Oriented Desktop Graphical User Interface Automation"},{"paperId":"17a32c825bd746a2625eddc2728092171a9ef72a","title":"Jack of All Tasks, Master of Many: Designing General-purpose Coarse-to-Fine Vision-Language Model"},{"paperId":"35a17f896847614a71df772bbe2b66ae231cabc7","title":"CLOVA: A Closed-Loop Visual Assistant with Tool Usage and Update"},{"paperId":"6d2ab31aa75468f5458b9d96192c3f4a28f55d73","title":"DriveMLM: Aligning Multi-Modal Large Language Models with Behavioral Planning States for Autonomous Driving"},{"paperId":"55c6d16b550c606d62dd85084f0d373d8f087966","title":"VLAP: Efficient Video-Language Alignment via Frame Prompting and Distilling for Video Question Answering"},{"paperId":"b240a1d8ec2860bdd7370daa3144268ce46ac018","title":"Vary: Scaling up the Vision Vocabulary for Large Vision-Language Models"},{"paperId":"369b34826e23cb43bea9a91395e9603eacfa7420","title":"EgoPlan-Bench: Benchmarking Egocentric Embodied Planning with Multimodal Large Language Models"},{"paperId":"96a7b0fe722e6d2d5167ef25a6aff714a20233a0","title":"Exploring the Limits of ChatGPT in Software Security Applications"},{"paperId":"7e55d8701785818776323b4147cb13354c820469","title":"PaperQA: Retrieval-Augmented Generative Agent for Scientific Research"},{"paperId":"4ece984342174620705c0ba1186ea4a97ff76052","title":"LifelongMemory: Leveraging LLMs for Answering Queries in Egocentric Videos"},{"paperId":"f4e8e66557005af41e101caf9d16d63bf7fe6f9a","title":"LLM as OS, Agents as Apps: Envisioning AIOS, Agents and the AIOS-Agent Ecosystem"},{"paperId":"d77bc1a237b67c57b0c1b99b4802e703747a9688","title":"BenchLMM: Benchmarking Cross-style Visual Capability of Large Multimodal Models"},{"paperId":"f32ea390686b1eee3ba5b53c7a85e9e9385d4b94","title":"Visual Program Distillation: Distilling Tools and Programmatic Reasoning into Vision-Language Models"},{"paperId":"5220687c42520855db240b67415a20e09c137004","title":"Mismatch Quest: Visual and Textual Feedback for Image-Text Misalignment"},{"paperId":"06b2ac5153e3d8d05c13c82f93d7f4e13eee6d0f","title":"Mitigating Fine-Grained Hallucination by Fine-Tuning Large Vision-Language Models with Caption Rewrites"},{"paperId":"246017780386eba39d6cda760a1c2c70356baa50","title":"VIoTGPT: Learning to Schedule Vision Tools towards Intelligent Video Internet of Things"},{"paperId":"4673c2ac4abb4b055da87171231acb60801ffe74","title":"PoseGPT: Chatting about 3D Human Pose"},{"paperId":"8441c30ad4abdca9ee380aa6f22ffd731b10231b","title":"COLE: A Hierarchical Generation Framework for Graphic Design"},{"paperId":"7fb6302976205c75875978b20cb701c3e6d2132f","title":"RoboGPT: an intelligent agent of making embodied long-term decisions for daily instruction tasks"},{"paperId":"ee2c769943f9e46c3bbee117d1ecf14566b7bf1f","title":"Boosting the Power of Small Multimodal Reasoning Models to Match Larger Models with Self-Consistency Training"},{"paperId":"52941cadbd340344f3e0a6f50719fe55b3de5088","title":"Multimodal Large Language Models: A Survey"},{"paperId":"107fb6eec2febbae12db29bf3e311aaf5680027c","title":"Video-LLaVA: Learning United Visual Representation by Alignment Before Projection"},{"paperId":"0f993809c1fe00403ecea66d8f572832f075cfe4","title":"MMC: Advancing Multimodal Chart Understanding with Large-scale Instruction Tuning"},{"paperId":"aad3d2e690f6c73f04a14622ceff51464bbc560e","title":"Chat-UniVi: Unified Visual Representation Empowers Large Language Models with Image and Video Understanding"},{"paperId":"d6de7e7b86717c74a3e54839f6f8c2ee28f52b8e","title":"Towards Understanding the Geospatial Skills of ChatGPT: Taking a Geographic Information Systems (GIS) Exam"},{"paperId":"2fb605f67fee79cad94952ddfe0f686e926f49f5","title":"GPT-4V in Wonderland: Large Multimodal Models for Zero-Shot Smartphone GUI Navigation"},{"paperId":"76a3f4a79ae9a00db2f2b5f6877021d8deb96ada","title":"SPHINX: The Joint Mixing of Weights, Tasks, and Visual Embeddings for Multi-modal Large Language Models"},{"paperId":"ef321c6f174ac59916ac54ec40ad18bca5b58e5c","title":"PerceptionGPT: Effectively Fusing Visual Perception into LLM"},{"paperId":"cf7d69709bdeddd561c183178bbc1f0c2e156a08","title":"Analyzing Modular Approaches for Visual Question Decomposition"},{"paperId":"8ec7d50250203543a0098d99f04957b22bbe2c77","title":"How to Bridge the Gap between Modalities: A Comprehensive Survey on Multimodal Large Language Model"},{"paperId":"ecfff30e570498b6c87c5d1319a0cbcdf7a8b86d","title":"LLaVA-Plus: Learning to Use Tools for Creating Multimodal Agents"},{"paperId":"6ae4705139494fcb6b790b6dd6c4225b40ee40f8","title":"GLaMM: Pixel Grounding Large Multimodal Model"},{"paperId":"c62711f6b5d8620ba36bc2c378ec6ab53f6e197c","title":"RoboGen: Towards Unleashing Infinite Data for Automated Robot Learning via Generative Simulation"},{"paperId":"1672b010fe8abc50744b92a08bc9148c36aebe35","title":"Is GPT Powerful Enough to Analyze the Emotions of Memes?"},{"paperId":"c020f15be1dee20f9e2e0c5a6f05f272b5508325","title":"LLaVA-Interactive: An All-in-One Demo for Image Chat, Segmentation, Generation and Editing"},{"paperId":"e68ab63d505c3ee8d0518c734c3d2b13071cc18d","title":"MM-VID: Advancing Video Understanding with GPT-4V(ision)"},{"paperId":"0212dca18cd0765deed0b6ba80a796f0ad46e066","title":"mPLUG-Octopus: The Versatile Assistant Empowered by A Modularized End-to-End Multimodal LLM"},{"paperId":"807f336176070bd3f95b82a16f125ee99b7d2c80","title":"Woodpecker: Hallucination Correction for Multimodal Large Language Models"},{"paperId":"0b395ed1c8b284e551172b728e83cf257e33729a","title":"HallusionBench: An Advanced Diagnostic Suite for Entangled Language Hallucination and Visual Illusion in Large Vision-Language Models"},{"paperId":"79e7ead8f59b17431de2b86af10dc0c30a1f5a2b","title":"ToolChain*: Efficient Action Space Navigation in Large Language Models with A* Search"},{"paperId":"7451d756118628474dc022813eb952a21d34c5f6","title":"Set-of-Mark Prompting Unleashes Extraordinary Visual Grounding in GPT-4V"},{"paperId":"36b923d97d7cfaf73d11c55c15ea46605ba974a5","title":"BiLL-VTG: Bridging Large Language Models and Lightweight Visual Tools for Video-based Texts Generation"},{"paperId":"ac2e5bf716aed246ca8914a6816ef73e00286099","title":"Beyond Segmentation: Road Network Generation with Multi-Modal LLMs"},{"paperId":"03bf1da1caa5f63203d43ed78c12c35a78fc6ed9","title":"EasyGen: Easing Multimodal Generation with a Bidirectional Conditional Diffusion Model and LLMs"},{"paperId":"1d14a708622917da4b9820ada6d32af24fc1651a","title":"Idea2Img: Iterative Self-Refinement with GPT-4V(ision) for Automatic Image Design and Generation"},{"paperId":"a710efa9247207a72f06e0c9db302fd3ecab5fbb","title":"Towards Robust Multi-Modal Reasoning via Model Selection"},{"paperId":"90ff14a19419a0b6bb0965f0ab5e359462556172","title":"How Do Large Language Models Capture the Ever-changing World Knowledge? A Review of Recent Advances"},{"paperId":"7f1ba5630c3baa09b11cc665b3f71cdb117e5ffb","title":"OpenLEAF: Open-Domain Interleaved Image-Text Generation and Evaluation"},{"paperId":"8918e3cc21ecaf81532e452d3b9518360d14860e","title":"Reinforced UI Instruction Grounding: Towards a Generic UI Task Automation API"},{"paperId":"bee68767debbdc96d6f75947e544a8be98b869e3","title":"Towards End-to-End Embodied Decision Making via Multi-modal Large Language Model: Explorations with GPT4-Vision and Beyond"},{"paperId":"b783168c885ecbae0fccdb46ec8e9afd0ef99b7f","title":"Avalon's Game of Thoughts: Battle Against Deception through Recursive Contemplation"},{"paperId":"bfeda6c7aa7899a80adb01894555b09d24756a59","title":"Corex: Pushing the Boundaries of Complex Reasoning through Multi-Model Collaboration"},{"paperId":"e61a96cf602ebff6683929aaf916e25614a475bc","title":"UPAR: A Kantian-Inspired Prompting Framework for Enhancing Large Language Model Capabilities"},{"paperId":"092245d86b77181c36f972b1b7a17a59cd989c4a","title":"Guiding Instruction-based Image Editing via Multimodal Large Language Models"},{"paperId":"7fe071ea76e49bc3e573beb53f07721630954247","title":"Promptbreeder: Self-Referential Self-Improvement Via Prompt Evolution"},{"paperId":"11a4284e335ba39330b59d9f42ca3272a6166991","title":"A Survey of Chain of Thought Reasoning: Advances, Frontiers and Future"},{"paperId":"7b689adb8c156d6158660f90d1c86888ee281f63","title":"DreamLLM: Synergistic Multimodal Comprehension and Creation"},{"paperId":"f34b6b4f75fb4e6f2c5654ee13fb2479c6170b3a","title":"Kosmos-2.5: A Multimodal Literate Model"},{"paperId":"7a7128e9696dfedc24bf432c4b4c3aafa5e95a1a","title":"An Empirical Study of Scaling Instruct-Tuned Large Multimodal Models"},{"paperId":"3ec464696db25acc2c39a6d967ec3df09e06f633","title":"Multimodal Multi-Hop Question Answering Through a Conversation Between Tools and Efficiently Finetuned Large Language Models"},{"paperId":"4eb87eaa193929dbef93fa2db9419245a8e8916f","title":"TextBind: Multi-turn Interleaved Multimodal Instruction-following in the Wild"},{"paperId":"d39182113cd4176ead48027b4fc05fe06ec6aaca","title":"Language Models as Black-Box Optimizers for Vision-Language Models"},{"paperId":"09e55d5223104aa5726ff0fcf4043a1beeee410e","title":"Beyond Traditional Teaching: The Potential of Large Language Models and Chatbots in Graduate Engineering Education"},{"paperId":"c237a22698223e4060d83027f399f4fb2aa24291","title":"Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations"},{"paperId":"bb1083425517bdac8d9a6438fcf5032543acb20e","title":"Evaluation and Analysis of Hallucination in Large Vision-Language Models"},{"paperId":"3b36d16985286b03e06e8404a7be49a9713d37b9","title":"Confucius: Iterative Tool Learning from Introspection Feedback by Easy-to-Difficult Curriculum"},{"paperId":"894ed1aba8e42a4ec27ba53ecde383b14c5128ca","title":"Examining User-Friendly and Open-Sourced Large GPT Models: A Survey on Language, Multimodal, and Scientific GPT Models"},{"paperId":"ef1ebdb24ce45fb57e271783a0c9a877fe0e79c3","title":"Position-Enhanced Visual Instruction Tuning for Multimodal Large Language Models"},{"paperId":"28c6ac721f54544162865f41c5692e70d61bccab","title":"A Survey on Large Language Model based Autonomous Agents"},{"paperId":"5e4597eb21a393b23e473cf66cb5ae8b27cab03e","title":"ExpeL: LLM Agents Are Experiential Learners"},{"paperId":"eb5cf10406a8ad31e0ebe56b36571d5db4758a62","title":"PUMGPT: A Large Vision-Language Model for Product Understanding"},{"paperId":"d53945d4afb4528590d79e20de52883d29037e86","title":"FashionLOGO: Prompting Multimodal Large Language Models for Fashion Logo Embeddings"},{"paperId":"1fd31b74f5e1eeb67341982fd35a613c6fad10e0","title":"Link-Context Learning for Multimodal LLMs"},{"paperId":"d6c2523ab97416c2692cbbeab082ed1790e8e55e","title":"VisIT-Bench: A Benchmark for Vision-Language Instruction Following Inspired by Real-World Use"},{"paperId":"4f2be887e991efa85f7b874e7ab871080a745c39","title":"CAESURA: Language Models as Multi-Modal Query Planners"},{"paperId":"dd0612ce863f64b0f69d0d9f708c52e829f6f859","title":"TPTU: Large Language Model-based AI Agents for Task Planning and Tool Usage"},{"paperId":"94972e30504017156ef5b5debc419bf6edc67384","title":"MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities"},{"paperId":"659a12d71d8709c132ccd9ccd235f0024cae0239","title":"The All-Seeing Project: Towards Panoptic Visual Recognition and Understanding of the Open World"},{"paperId":"446fb5dead075a1a08862662738f462e9a0e91c8","title":"Tool Documentation Enables Zero-Shot Tool-Usage with Large Language Models"},{"paperId":"ad113d8b6f8f6dace7abd12dc88af520aaaf3fc7","title":"LISA: Reasoning Segmentation via Large Language Model"},{"paperId":"bbcd5cc4bf6c77282e88cae07f7f2adb1da818ca","title":"Testing the Depth of ChatGPT's Comprehension via Cross-Modal Tasks Based on ASCII-Art: GPT3.5's Abilities in Regard to Recognizing and Generating ASCII-Art Are Not Totally Lacking"},{"paperId":"584ca135b61482fd89247113da87d784f738dbfa","title":"Foundational Models Defining a New Era in Vision: A Survey and Outlook"},{"paperId":"2e52178416b93a0a7dc1cb7d21d04c7e72c2ce0f","title":"ChatSpot: Bootstrapping Multimodal LLMs via Precise Referring Instruction Tuning"},{"paperId":"ca31b8584b6c022ef15ddfe994fe361e002b7729","title":"A Comprehensive Overview of Large Language Models"},{"paperId":"094883e42bb9a41f602c0715c1059bc431e33fb2","title":"GPT4RoI: Instruction Tuning Large Language Model on Region-of-Interest"},{"paperId":"ef4b604fca0c62dcd0d5caf7ca24ad74e285632d","title":"MultiQG-TI: Towards Question Generation from Multi-modal Sources"},{"paperId":"ebddfdc5d845a788e8062eddbbf7a335737cb99b","title":"What Matters in Training a GPT4-Style Language Model with Multimodal Inputs?"},{"paperId":"ea566f87f6253bb2d32cf7b61cd3e2535a0c3f42","title":"mPLUG-DocOwl: Modularized Multimodal Large Language Model for Document Understanding"},{"paperId":"82a9b8984e26fdf234431459bdb445fbcfc3cb76","title":"Visual Instruction Tuning with Polite Flamingo"},{"paperId":"ebedc4d7a2356090904baba4104ef0832bc236df","title":"A Survey on Multimodal Large Language Models"},{"paperId":"966852963a88a28786b798c91b6662d6e501e590","title":"AssistGPT: A General Multi-modal Assistant that can Plan, Execute, Inspect, and Learn"},{"paperId":"051549d8ef56937b2f4d113afdcf8c7586d3770b","title":"Towards AGI in Computer Vision: Lessons Learned from GPT and Large Language Models"},{"paperId":"fe50667e1bea4c6f63909b90986231240818c1d6","title":"ChatGPT vs Human-authored Text: Insights into Controllable Text Summarization and Sentence Style Transfer"},{"paperId":"d98536f24272e258b1d399074b64284d64786099","title":"AVIS: Autonomous Visual Information Seeking with Large Language Models"},{"paperId":"4c4d176c6e28f48041f215d563f6ee8633534cff","title":"Valley: Video Assistant with Large Language model Enhanced abilitY"},{"paperId":"fd755dc7b5b206c17fd953db04e1c888d45b6e4e","title":"LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark"},{"paperId":"d47524cd5c3c4b57af2e5a29f6f91c420310f236","title":"MIMIC-IT: Multi-Modal In-Context Instruction Tuning"},{"paperId":"ccef05840870479632f9055479a1269d53033b7b","title":"Certified Reasoning with Language Models"},{"paperId":"f9e33614577f03ddfda62c3122186407aa6be7d3","title":"Certified Deductive Reasoning with Language Models"},{"paperId":"615962d8969c8e0ffe43319689dce6c50cbf1f29","title":"Responsible Task Automation: Empowering Large Language Models as Responsible Task Automators"},{"paperId":"811115d36e1eabe2cef03b38a0809514e40b658e","title":"Chain-Of-Thought Prompting Under Streaming Batch: A Case Study"},{"paperId":"b458fc5261595f44b36325e5eaea1f874d65138f","title":"GPT4Tools: Teaching Large Language Model to Use Tools via Self-instruction"},{"paperId":"af705d648b5b16daa3dcc593bc593f2574d76c07","title":"Grammar Prompting for Domain-Specific Language Generation with Large Language Models"},{"paperId":"e45036dddb5f27d3e87d2f14a2d9e6a402e7b5b7","title":"SheetCopilot: Bringing Software Productivity to the Next Level through Large Language Models"},{"paperId":"50c1414fe41d0cb9db6f0933c9319aa124beac5d","title":"Contextual Object Detection with Multimodal Large Language Models"},{"paperId":"5ff2f5212713ec424662ac3c9e4aa5a8790d40cf","title":"ANPL: Towards Natural Programming with Interactive Decomposition"},{"paperId":"8ecdbfe011b7189fa0ee49ffc4e42a93d728a371","title":"On Evaluating Adversarial Robustness of Large Vision-Language Models"},{"paperId":"32dcd0887537cece54e214f531d2c384470b023f","title":"Large Language Models as Tool Makers"},{"paperId":"c6ac708b65b24c20f80831d518c1795ce8133ad5","title":"ChatBridge: Bridging Modalities with Large Language Model as a Language Catalyst"},{"paperId":"9c3a9b4821daa03cb5369041d59d2714329a3811","title":"Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models"},{"paperId":"9837349417e36ef5be06da0fd6c74042148bdaa2","title":"Visual Programming for Text-to-Image Generation and Evaluation"},{"paperId":"66d755730f5d08a6f4fcc5e81f24982ba389dca9","title":"LayoutGPT: Compositional Visual Planning and Generation with Large Language Models"},{"paperId":"00cb69a9f280317d1c59ac5827551ee9b10642b8","title":"EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought"},{"paperId":"7cf64070fd3d7e53d80f260c10e6bd7018d580e1","title":"IdealGPT: Iteratively Decomposing Vision and Language Reasoning via Large Language Models"},{"paperId":"3c50ef336232da0885ef61da386c98eac964b7cd","title":"PathAsst: A Generative Foundation AI Assistant Towards Artificial General Intelligence of Pathology"},{"paperId":"90027ca7802645671a69b00b65e1fa94e6b63544","title":"ReWOO: Decoupling Reasoning from Observations for Efficient Augmented Language Models"},{"paperId":"f9bfc6d9ba1665b73af3323d46c7642b852759ef","title":"VideoLLM: Modeling Video Sequence with Large Language Models"},{"paperId":"6fe2a4f7594abadafc9feb05e96ab7e9e634ce8f","title":"Album Storytelling with Iterative Story-aware Captioning and Large Language Models"},{"paperId":"ca055cfb9d4d47124cc035c346f38577825fcacf","title":"Enhance Reasoning Ability of Visual-Language Models via Large Language Models"},{"paperId":"6a5525c316b9be7909c433a79e090ed731425083","title":"What Makes for Good Visual Tokenizers for Large Language Models?"},{"paperId":"42a30dc5470f54ec249f25d3c31e05d7c376c8e3","title":"VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks"},{"paperId":"692bc40edf4785d88c39e0c0fe9f270541fecf8a","title":"Towards Generalist Robots: A Promising Paradigm via Generative Simulation"},{"paperId":"d48cb91b9e555194f7494c4d4bb9815021d3ee45","title":"VideoChat: Chat-Centric Video Understanding"},{"paperId":"54a8b153ed04a872da878d695239bdc413dc782c","title":"InternGPT: Solving Vision-Centric Tasks by Interacting with Chatbots Beyond Language"},{"paperId":"d6d3604f369bb0415cbe814e43ca3131323b03e2","title":"Otter: A Multi-Modal Model with In-Context Instruction Tuning"},{"paperId":"570079bbdd8758dfe865097e05719313c9c1301a","title":"LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model"},{"paperId":"7e32aac43e9f1df49e116add03327ee6f365dbf3","title":"mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality"},{"paperId":"ca6a2bc279be5a3349a22bfd6866ed633d18734b","title":"MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models"},{"paperId":"170c97c7215f42edfb20c2248f954879e91ef86e","title":"Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models"},{"paperId":"352420ee61a8da783ca7750170793613b18b8d9c","title":"Tool Learning with Foundation Models"},{"paperId":"a5036f31f0e629dc661f120b8c3b1f374d479ab8","title":"Visual Instruction Tuning"},{"paperId":"354dcdebf3f8b5feeed5c62090e0bc1f0c28db06","title":"ChemCrow: Augmenting large-language models with chemistry tools"},{"paperId":"0ebc861f5478561f12941e6b48aad30574e996d8","title":"Video ChatCaptioner: Towards Enriched Spatiotemporal Descriptions"},{"paperId":"b61f4b2c0d20a6595cf1456d08e5e2770a5913ef","title":"Artificial intelligence—friend or foe in fake news campaigns"},{"paperId":"ac7771c332da42b29a913b116bd6ef622cbf89cf","title":"TaskMatrix.AI: Completing Tasks by Connecting Foundation Models with Millions of APIs"},{"paperId":"53df959bcf6499c45e316086a96a624389a39a52","title":"Accountable Textual-Visual Chat Learns to Reject Human Instructions in Image Re-creation"},{"paperId":"6845bea94b2fb17d4377b3bb2bd10f73a959f9cc","title":"Reasoning with Language Model Prompting: A Survey"},{"paperId":"a3711dbf296b5ddd97ba93826660cd3995611625","title":"Towards A Foundation Model for Generalist Robots: Diverse Skill Learning at Scale via Automated Task and Scene Generation"},{"paperId":"0167f681bac9b7f6cf396e8a5fc6e46c62fd1896","title":"Towards Understanding the Spatial Literacy of ChatGPT ∗ Taking a Geographic Information Systems (GIS) Exam"},{"paperId":"f01281b125128435ad134230c6a41cc55808eaac","title":"Can LLMs Effectively Leverage Graph Structural Information: When and Why"},{"paperId":"96a6df2b4aa50cfbd8984933e9c66b0763fc08a6","title":"Overleaf Example"},{"paperId":"65d5728ea17f016382870aa27aac1e78d590b50c","title":"HallusionBench: You See What You Think? Or You Think What You See? An Image-Context Reasoning Benchmark Challenging for GPT-4V(ision), LLaVA-1.5, and Other Multi-modality Models"},{"paperId":"bca0bbd01ea917b7a9fe369288ea3ba03d3b1ff3","title":"A Survey of Large Language Models in Medicine: Progress, Application, and Challenge"},{"paperId":"5ce94181ea702f69c3651dce721d6bd8026b8106","title":"TPTU: Task Planning and Tool Usage of Large Language Model-based AI Agents"},{"paperId":"44ccf252018f71898d52d89539f17d77a4f8d548","title":"Chart Understanding with Large Language Model"}],"references":[{"paperId":"d62c4d00b277e948956b6610ce2644e88fe1577b","title":"Large Language Models"},{"paperId":"a8e1f42412639275fd59e7ac9b702787ab59016a","title":"GPT-4 Technical Report"},{"paperId":"6e754273d54a91371efbc928cd6b156364d517da","title":"ViperGPT: Visual Inference via Python Execution for Reasoning"},{"paperId":"af997821231898a5f8d0fd78dad4eec526acabe5","title":"Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models"},{"paperId":"38fe8f324d2162e63a967a9ac6648974fc4c66f3","title":"PaLM-E: An Embodied Multimodal Language Model"},{"paperId":"fbfef4723d8c8467d7bd523e1d0b703cce0e0f9c","title":"Language Is Not All You Need: Aligning Perception with Language Models"},{"paperId":"61e721334296ebfbbf6443b5ed9eb8c83b708c95","title":"Scaling Vision Transformers to 22 Billion Parameters"},{"paperId":"53d128ea815bcc0526856eb5a9c42cc977cb36a7","title":"Toolformer: Language Models Can Teach Themselves to Use Tools"},{"paperId":"780a7f5e8ba9b4b451e3dfee1bcfb0f68aba5050","title":"Multimodal Chain-of-Thought Reasoning in Language Models"},{"paperId":"3f5b31c4f7350dc88002c121aecbdc82f86eb5bb","title":"BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models"},{"paperId":"967907503b24423b9b74621051811fcf684e3957","title":"Generalized Decoding for Pixel, Image, and Language"},{"paperId":"f208ea909fa7f54fea82def9a92fd81dfc758c39","title":"Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-Intensive Multi-Step Questions"},{"paperId":"6c1e1cc1e0e1f8fd026fe517607b2d4535565fa7","title":"PAL: Program-aided Language Models"},{"paperId":"a5cb8f26acb71edd77ff9a143d3ddaab2367eb40","title":"PromptCap: Prompt-Guided Task-Aware Image Captioning"},{"paperId":"d3135733aa39dec20ce72aa138589dda27c8406d","title":"Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering"},{"paperId":"60ee030773ba1b68eb222a265b052ca028353362","title":"GIT: A Generative Image-to-text Transformer for Vision and Language"},{"paperId":"e7ad08848d5d7c5c47673ffe0da06af443643bda","title":"Large Language Models are Zero-Shot Reasoners"},{"paperId":"c1ace33daf974d3d16752c7a8565f32a63b09c49","title":"Language Models with Image Descriptors are Strong Few-Shot Video-Language Learners"},{"paperId":"26218bdcc3945c7edae7aa2adbfba4cd820a2df3","title":"Flamingo: a Visual Language Model for Few-Shot Learning"},{"paperId":"094ff971d6a8b8ff870946c9b3ce5aa173617bfb","title":"PaLM: Scaling Language Modeling with Pathways"},{"paperId":"cb5e3f085caefd1f3d5e08637ab55d39e61234fc","title":"Do As I Can, Not As I Say: Grounding Language in Robotic Affordances"},{"paperId":"ada81a4de88a6ce474df2e2446ad11fea480616e","title":"Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language"},{"paperId":"1b6e810ce0afd0dd093f789d2b2742d047e316d5","title":"Chain of Thought Prompting Elicits Reasoning in Large Language Models"},{"paperId":"92a8f7f09f3705cb5a6009a42220a6f01ea084e8","title":"Language Models as Zero-Shot Planners: Extracting Actionable Knowledge for Embodied Agents"},{"paperId":"2f3efe44083af91cef562c1a3451eee2f8601d22","title":"WebGPT: Browser-assisted question-answering with human feedback"},{"paperId":"21ec90872abd986c12afe39bebe807732ffa70c9","title":"Florence: A New Foundation Model for Computer Vision"},{"paperId":"2672777d25562c9df6fc13b653181db62d39bece","title":"An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA"},{"paperId":"01b5412f3d17e90e09226d7c40ad4d4468a1414d","title":"Multimodal Few-Shot Learning with Frozen Language Models"},{"paperId":"268d347e8a55b5eb82fb5e7d2f800e33c75ab18a","title":"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"962dc29fdc3fbdc5930a10aba114050b82fe5a3e","title":"End-to-End Object Detection with Transformers"},{"paperId":"204e3073870fae3d05bcbc2f6a8e263d9b72e776","title":"Attention is All you Need"},{"paperId":"2c03df8b48bf3fa39054345bafabfeff15bfd11d","title":"Deep Residual Learning for Image Recognition"},{"paperId":"6424b69f3ff4d35249c0bb7ef912fbc2c86f4ff4","title":"Deep Learning Face Attributes in the Wild"},{"paperId":"71b7178df5d2b112d07e45038cb5637208659ff7","title":"Microsoft COCO: Common Objects in Context"},{"paperId":"b80a580a6f2eca77524302acd944fd6edf0a0611","title":"NUS-WIDE: a real-world web image database from National University of Singapore"},{"paperId":"f79131806747fce087d0fe73d0867cc621547b2a","title":"The MIR flickr retrieval evaluation"},{"paperId":"99832586d55f540f603637e458a292406a0ed75d","title":"LANGUAGE MODELS"},{"paperId":null,"title":"Langchain. https://langchain. readthedocs.io/, 2023"},{"paperId":null,"title":"Case studies of MM-REACT's extensibility. With the new release of GPT-4 (Language-only), we can upgrade ChatGPT in MM-REACT to GPT-4 (Langauge-only) and further improve the results of MM-REACT"},{"paperId":null,"title":"Here's a breakdown of the start and end times for each step in the video: 1"},{"paperId":null,"title":"GPT-4 (Multimodal)"},{"paperId":null,"title":"I am trying to create a short version of the video, can you suggest a shorter segment for each step? Here's a suggested shorter segment for each step: 1"},{"paperId":null,"title":"Ours w/ ChatGPT Ours w/ GPT-4 (Language-only)"},{"paperId":null,"title":"Can you write a template for this website in HTML? Replace the joke and punchline with a real one"}],"id":"c7a9c7302a72301ed79a7c0696d5af2e03ad3ac4","summary":"This paper defines and explores a comprehensive list of advanced vision tasks that are intriguing to solve, but may exceed the capabilities of existing vision and vision-language models and proposes MM-REACT, a system paradigm that integrates ChatGPT with a pool of vision experts to achieve multimodal reasoning and action."},{"url":"https://www.semanticscholar.org/paper/3d45e69557f0c6a54ec698304c2e27ec29bc1c2b","title":"PMC-CLIP: Contrastive Language-Image Pre-training using Biomedical Documents","venue":"arXiv.org","year":2023,"referenceCount":33,"citationCount":24,"influentialCitationCount":0,"publicationDate":"13/03/2023","authors":"Weixiong Lin,Ziheng Zhao,Xiaoman Zhang,Chaoyi Wu,Ya Zhang,Yanfeng Wang,Weidi Xie","citations":[{"paperId":"a3d418b4e35a02e4306505ab660a6bcd44c3c752","title":"Asclepius: A Spectrum Evaluation Benchmark for Medical Multi-Modal Large Language Models"},{"paperId":"ae7bf155b421e10d8772a5eb722c6e4cc69a3566","title":"Mind the Modality Gap: Towards a Remote Sensing Vision-Language Model via Cross-modal Alignment"},{"paperId":"7580327ffc9bd5daef83fe8285c0476ca074051d","title":"OmniMedVQA: A New Large-Scale Comprehensive Evaluation Benchmark for Medical LVLM"},{"paperId":"2cd8c14938aed3f1de4ccbe722dc5241bf62079a","title":"RAD-DINO: Exploring Scalable Medical Image Encoders Beyond Text Supervision"},{"paperId":"18d9b13e3383d98c181f4d7a2b3ca1503ed707a0","title":"No-boundary thinking for artificial intelligence in bioinformatics and education"},{"paperId":"93886752191db25efd096a65af7b09df5c0a64e0","title":"Data-Centric Foundation Models in Computational Healthcare: A Survey"},{"paperId":"b1721374889899950994f67029fe899de257c140","title":"A Foundational Multimodal Vision Language AI Assistant for Human Pathology"},{"paperId":"6bdfffbf92d01c8b543088d40d46233610e469a8","title":"CLIP in Medical Imaging: A Comprehensive Survey"},{"paperId":"43989495f64ca97a1ac0a5d9efad0c2d871c01ed","title":"TransMed: Large Language Models Enhance Vision Transformer for Biomedical Image Classification"},{"paperId":"2c7e346aa311fec4dda04bdf3a214ce2026d8807","title":"Medical Vision Language Pretraining: A survey"},{"paperId":"59ba440bdce4b9b963124a46ee87b63b67c4c58c","title":"Training CLIP models on Data from Scientific Papers"},{"paperId":"8d2709ed1788a67e64425fb410bb49f3ee49e088","title":"Large Language Models Illuminate a Progressive Pathway to Artificial Healthcare Assistant: A Review"},{"paperId":"c67a58bb5eb9cb6557a6032bb058a5cab978907f","title":"Qilin-Med-VL: Towards Chinese Large Vision-Language Model for General Healthcare"},{"paperId":"7bf2733f3e98c0e334a040186886bca8d1b238f7","title":"Uni-Dual: A Generic Unified Dual-Task Medical Self-Supervised Learning Framework"},{"paperId":"c7492913370b5726eaa6ced163a60de6c9d4bb7f","title":"A Survey of Large Language Models for Healthcare: from Data, Technology, and Applications to Accountability and Ethics"},{"paperId":"f0950a3f27c0fefffba60ae1c9a8ee360d5eb55f","title":"Instruction Tuning for Large Language Models: A Survey"},{"paperId":"df0ddb588a200d095743e9d26fc4a9318619766e","title":"Towards Generalist Foundation Model for Radiology"},{"paperId":"c9dbdae8146b9f97e254f5d26fd6efde96eaa703","title":"Med-Flamingo: a Multimodal Medical Few-shot Learner"},{"paperId":"813ba033b8f593c98f9af44c5b4901408ba6f70a","title":"Towards a Visual-Language Foundation Model for Computational Pathology"},{"paperId":"b0883ddf7fb07aabfeb2b2f593e7ac302aa42372","title":"PRIOR: Prototype Representation Joint Learning from Medical Images and Reports"},{"paperId":"3c50ef336232da0885ef61da386c98eac964b7cd","title":"PathAsst: A Generative Foundation AI Assistant Towards Artificial General Intelligence of Pathology"},{"paperId":"f4793adffd6f67ffcb93ccfc5672ab301b8a2b96","title":"PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering"},{"paperId":"bca0bbd01ea917b7a9fe369288ea3ba03d3b1ff3","title":"A Survey of Large Language Models in Medicine: Progress, Application, and Challenge"},{"paperId":"46c72e7fd996bf0c63a51abe5058dab49d87c4fe","title":"SciOL and MuLMS-Img: Introducing A Large-Scale Multimodal Scientific Dataset and Models for Image-Text Tasks in the Scientific Domain"}],"references":[{"paperId":"fa988afd9889451e5fcc46a316bd1e2a6abda367","title":"A Self-Adaptive Discriminative Autoencoder for Medical Applications"},{"paperId":"81adb80e390f25d4a2d764b1063eeaa2c334d441","title":"Multi-modal Masked Autoencoders for Medical Vision-and-Language Pre-training"},{"paperId":"28ff0816f19a5e3e37eac5569de41872fd262f0a","title":"Align, Reason and Learn: Enhancing Medical Vision-and-Language Pre-training with Knowledge"},{"paperId":"02251886950770e82b3d68564d60cdfe15e73199","title":"Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks"},{"paperId":"75bb9eda70751c63fc54dbe63377c673b7dbdb15","title":"CogView2: Faster and Better Text-to-Image Generation via Hierarchical Transformers"},{"paperId":"c57293882b2561e1ba03017902df9fc2f289dea2","title":"Hierarchical Text-Conditional Image Generation with CLIP Latents"},{"paperId":"38d089e36d630189eb6c5274066d57efd48a187d","title":"DWT-CV: Dense weight transfer-based cross validation strategy for model selection in biomedical data analysis"},{"paperId":"d766bffc357127e0dc86dd69561d5aeb520d6f4c","title":"Training language models to follow instructions with human feedback"},{"paperId":"94ff111c4d81bd03f159321728ceec8b4711c89d","title":"An Empirical Study of Training End-to-End Vision-and-Language Transformers"},{"paperId":"a66606a0bb11a1d5a7a14ec09df8a3481121ad6c","title":"MedMNIST v2 - A large-scale lightweight benchmark for 2D and 3D biomedical image classification"},{"paperId":"65a57c948755cffad38de4230937ffbff8a19783","title":"MedMNIST v2: A Large-Scale Lightweight Benchmark for 2D and 3D Biomedical Image Classification"},{"paperId":"b82c5f9efdb2ae56baa084ca41aeddd8a665c1d1","title":"Align before Fuse: Vision and Language Representation Learning with Momentum Distillation"},{"paperId":"ef39d344161d2af825b650168aa332f2217c406a","title":"EXSCLAIM! - An automated pipeline for the construction of labeled materials imaging datasets from literature"},{"paperId":"6f870f7f02a8c59c3e23f407f3ef00dd1dcf8fc4","title":"Learning Transferable Visual Models From Natural Language Supervision"},{"paperId":"93b6b79b4ef6c345f31722ce7c829385c6dce0d6","title":"Slake: A Semantically-Labeled Knowledge-Enhanced Dataset For Medical Visual Question Answering"},{"paperId":"0839722fb5369c0abaff8515bfc08299efc790a1","title":"ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision"},{"paperId":"5ba77a5bdeffb62aa0902ae68997bbc38db8a722","title":"MedICaT: A Dataset of Medical Images, Captions, and Textual References"},{"paperId":"a2f38d03fd363e920494ad65a5f0ad8bd18cd60b","title":"Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing"},{"paperId":"6b85b63579a916f705a8e10a49bd8d849d91b1fc","title":"Language Models are Few-Shot Learners"},{"paperId":"962dc29fdc3fbdc5930a10aba114050b82fe5a3e","title":"End-to-End Object Detection with Transformers"},{"paperId":"d1f407b16fb8d99487baee37ed0805676c58e7ac","title":"MIMIC-CXR, a de-identified publicly available database of chest radiographs with free-text reports"},{"paperId":"33301b25a297b701bdc287e985c006375cb7bb21","title":"Overcoming Data Limitation in Medical Visual Question Answering"},{"paperId":"da076089117ca1e8bce65dfa848d23da914b63c5","title":"DocFigure: A Dataset for Scientific Document Figure Classification"},{"paperId":"a564fabf130ff6e2742cfba90c7a4018937d764d","title":"Radiology Objects in COntext (ROCO): A Multimodal Image Dataset"},{"paperId":"b227f3e4c0dc96e5ac5426b85485a70f2175a205","title":"Representation Learning with Contrastive Predictive Coding"},{"paperId":"d07284a6811f1b2745d91bdb06b040b57f226882","title":"Decoupled Weight Decay Regularization"},{"paperId":"2c03df8b48bf3fa39054345bafabfeff15bfd11d","title":"Deep Residual Learning for Image Recognition"},{"paperId":"f7609414ae3a117cd2828c7dae19ad34ff7d72e6","title":"PubMed Central: The GenBank of the published literature."},{"paperId":"519799a9e2a72b808d81c6bcba5de263503de053","title":"Contrastive Pre-training and Representation Distillation for Medical Visual Question Answering Based on Radiology Images"},{"paperId":"df2b0e26d0599ce3e70df8a9da02e51594e0e992","title":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"},{"paperId":"18f9a6045ba01cb079c4fa49a630d71bbd27cd92","title":"Descriptor : A dataset of clinically generated visual questions and answers about radiology images"},{"paperId":"1f1eaf19e38b541eec8a02f099e3090536a4c936","title":"The Unified Medical Language System (UMLS): integrating biomedical terminology"},{"paperId":null,"title":"Authors Suppressed Due to Excessive Length"}],"id":"3d45e69557f0c6a54ec698304c2e27ec29bc1c2b","summary":"PMC-OA, a biomedical dataset with 1.6M image-caption pairs collected from PubMedCentral's OpenAccess subset, is built and released, which is 8 times larger than before and achieves state-of-the-art results on various downstream tasks."}]}